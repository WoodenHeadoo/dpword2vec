Provide an interface for creating families of related or dependent objects without specifying their concrete classes. Consider a user interface toolkit that supports multiple look-and-feel standards, such as Motif and Presentation Manager. Different look-and-feels define different appearances and behaviors for user interface "widgets" like scroll bars, windows, and buttons. To be portable across look-and-feel standards, an application should not hard-code its widgets for a particular look and feel. Instantiating look-and-feel-specific classes of widgets throughout the application makes it hard to change the look and feel later. We can solve this problem by defining an abstract WidgetFactory class that declares an interface for creating each basic kind of widget. There's also an abstract class for each kind of widget, and concrete subclasses implement widgets for specific look-and-feel standards. WidgetFactory's interface has an operation that returns a new widget object for each abstract widget class. Clients call these operations to obtain widget instances, but clients aren't aware of the concrete classes they're using. Thus clients stay independent of the prevailing look and feel. There is a concrete subclass of WidgetFactory for each look-and-feel standard. Each subclass implements the operations to create the appropriate widget for the look and feel. For example, the CreateScrollBar operation on the MotifWidgetFactory instantiates and returns a Motif scroll bar, while the corresponding operation on the PMWidgetFactory returns a scroll bar for Presentation Manager. Clients create widgets solely through the WidgetFactory interface and have no knowledge of the classes that implement widgets for a particular look and feel. In other words, clients only have to commit to an interface defined by an abstract class, not a particular concrete class. A WidgetFactory also enforces dependencies between the concrete widget classes. A Motif scroll bar should be used with a Motif button and a Motif text editor, and that constraint is enforced automatically as a consequence of using a MotifWidgetFactory. Use the Abstract Factory pattern when a system should be independent of how its products are created, composed, and represented. a system should be configured with one of multiple families of products. a family of related product objects is designed to be used together, and you need to enforce this constraint. you want to provide a class library of products, and you want to reveal just their interfaces, not their implementations. AbstractFactory (WidgetFactory) declares an interface for operations that create abstract product objects. ConcreteFactory (MotifWidgetFactory, PMWidgetFactory) implements the operations to create concrete product objects. AbstractProduct (Window, ScrollBar) declares an interface for a type of product object.?ConcreteProduct (MotifWindow, MotifScrollBar) defines a product object to be created by the corresponding concrete factory. implements the AbstractProduct interface.?Client uses only interfaces declared by AbstractFactory and AbstractProduct classes. Normally a single instance of a ConcreteFactory class is created at run-time. This concrete factory creates product objects having a particular implementation. To create different product objects, clients should use a different concrete factory.?AbstractFactory defers creation of product objects to its ConcreteFactory subclass. The Abstract Factory pattern has the following benefits and liabilities: It isolates concrete classes. The Abstract Factory pattern helps you control the classes of objects that an application creates. Because a factory encapsulates the responsibility and the process of creating product objects, it isolates clients from implementation classes. Clients manipulate instances through their abstract interfaces. Product class names are isolated in the implementation of the concrete factory; they do not appear in client code. It makes exchanging product families easy. The class of a concrete factory appears only once in an application¡ªthat is, where it's instantiated. This makes it easy to change the concrete factory an application uses. It can use different product configurations simply by changing the concrete factory. Because an abstract factory creates a complete family of products, the whole product family changes at once. In our user interface example, we can switch from Motif widgets to Presentation Manager widgets simply by switching the corresponding factory objects and recreating the interface. It promotes consistency among products. When product objects in a family are designed to work together, it's important that an application use objects from only one family at a time. AbstractFactory makes this easy to enforce. Supporting new kinds of products is difficult. Extending abstract factories to produce new kinds of Products isn't easy. That's because the AbstractFactory interface fixes the set of products that can be created. Supporting new kinds of products requires extending the factory interface, which involves changing the AbstractFactory class and all of its subclasses. We discuss one solution to this problem in the Implementation section. Here are some useful techniques for implementing the Abstract Factory pattern. Factories as singletons. An application typically needs only one instance of a ConcreteFactory per product family. So it's usually best implemented as a Singleton (144). Creating the products. AbstractFactory only declares an interface for creating products. It's up to ConcreteProduct subclasses to actually create them. The most common way to do this is to define a factory method (see Factory Method (121)) for each product. A concrete factory will specify its products by overriding the factory method for each. While this implementation is simple, it requires a new concrete factory subclass for each product family, even if the product families differ only slightly. If many product families are possible, the concrete factory can be implemented using the Prototype (133) pattern. The concrete factory is initialized with a prototypical instance of each product in the family, and it creates a new product by cloning its prototype. The Prototype-based approach eliminates the need for a new concrete factory class for each new product family. Here's a way to implement a Prototype-based factory in Smalltalk. The concrete factory stores the prototypes to be cloned in a dictionary called partCatalog. The method make: retrieves the prototype and clones it: The concrete factory has a method for adding parts to the catalog. Prototypes are added to the factory by identifying them with a symbol: A variation on the Prototype-based approach is possible in languages that treat classes as first-class objects (Smalltalk and Objective C, for example). You can think of a class in these languages as a degenerate factory that creates only one kind of product. You can store classes inside a concrete factory that create the various concrete products in variables, much like prototypes. These classes create new instances on behalf of the concrete factory. You define a new factory by initializing an instance of a concrete factory with classes of products rather than by subclassing. This approach takes advantage of language characteristics, whereas the pure Prototype-based approach is language-independent. Like the Prototype-based factory in Smalltalk just discussed, the class-based version will have a single instance variable partCatalog, which is a dictionary whose key is the name of the part. Instead of storing prototypes to be cloned, partCatalog stores the classes of the products. The method make: now looks like this: Defining extensible factories. AbstractFactory usually defines a different operation for each kind of product it can produce. The kinds of products are encoded in the operation signatures. Adding a new kind of product requires changing the AbstractFactory interface and all the classes that depend on it. A more flexible but less safe design is to add a parameter to operations that create objects. This parameter specifies the kind of object to be created. It could be a class identifier, an integer, a string, or anything else that identifies the kind of product. In fact with this approach, AbstractFactory only needs a single "Make" operation with a parameter indicating the kind of object to create. This is the technique used in the Prototype- and the class-based abstract factories discussed earlier. This variation is easier to use in a dynamically typed language like Smalltalk than in a statically typed language like C++. You can use it in C++ only when all objects have the same abstract base class or when the product objects can be safely coerced to the correct type by the client that requested them. The implementation section of Factory Method (121) shows how to implement such parameterized operations in C++. But even when no coercion is needed, an inherent problem remains: All products are returned to the client with the same abstract interface as given by the return type. The client will not be able to differentiate or make safe assumptions about the class of a product. If clients need to perform subclass-specific operations, they won't be accessible through the abstract interface. Although the client could perform a downcast (e.g., with dynamic_cast in C++), that's not always feasible or safe, because the downcast can fail. This is the classic trade-off for a highly flexible and extensible interface. Class MazeFactory can create components of mazes. It builds rooms, walls, and doors between rooms. It might be used by a program that reads plans for mazes from a file and builds the corresponding maze. Or it might be used by a program that builds mazes randomly. Programs that build mazes take a MazeFactory as an argument so that the programmer can specify the classes of rooms, walls, and doors to construct. Recall that the member function CreateMaze builds a small maze consisting of two rooms with a door between them. CreateMaze hard-codes the class names, making it difficult to create mazes with different components. Here's a version of CreateMaze that remedies that shortcoming by taking a MazeFactory as a parameter: We can create EnchantedMazeFactory, a factory for enchanted mazes, by subclassing MazeFactory. EnchantedMazeFactory will override different member functions and return different subclasses of Room, Wall, etc. Now suppose we want to make a maze game in which a room can have a bomb set in it. If the bomb goes off, it will damage the walls (at least). We can make a subclass of Room keep track of whether the room has a bomb in it and whether the bomb has gone off. We'll also need a subclass of Wall to keep track of the damage done to the wall. We'll call these classes RoomWithABomb and BombedWall. The last class we'll define is BombedMazeFactory, a subclass of MazeFactory that ensures walls are of class BombedWall and rooms are of class RoomWithABomb. BombedMazeFactory only needs to override two functions: To build a simple maze that can contain bombs, we simply call CreateMaze with a BombedMazeFactory. CreateMaze can take an instance of EnchantedMazeFactory just as well to build enchanted mazes. Notice that the MazeFactory is just a collection of factory methods. This is the most common way to implement the Abstract Factory pattern. Also note that MazeFactory is not an abstract class; thus it acts as both the AbstractFactory and the ConcreteFactory. This is another common implementation for simple applications of the Abstract Factory pattern. Because the MazeFactory is a concrete class consisting entirely of factory methods, it's easy to make a new MazeFactory by making a subclass and overriding the operations that need to change. CreateMaze used the SetSide operation on rooms to specify their sides. If it creates rooms with a BombedMazeFactory, then the maze will be made up of RoomWithABomb objects with BombedWall sides. If RoomWithABomb had to access a subclass-specific member of BombedWall, then it would have to cast a reference to its walls from Wall* to BombedWall*. This downcasting is safe as long as the argument is in fact a BombedWall, which is guaranteed to be true if walls are built solely with a BombedMazeFactory. Dynamically typed languages such as Smalltalk don't require downcasting, of course, but they might produce run-time errors if they encounter a Wall where they expect a subclass of Wall. Using Abstract Factory to build walls helps prevent these run-time errors by ensuring that only certain kinds of walls can be created. Let's consider a Smalltalk version of MazeFactory, one with a single make operation that takes the kind of object to make as a parameter. Moreover, the concrete factory stores the classes of the products it creates. First, we'll write an equivalent of CreateMaze in Smalltalk: As we discussed in the Implementation section, MazeFactory needs only a single instance variable partCatalog to provide a dictionary whose key is the class of the component. Also recall how we implemented the make: method: Now we can create a MazeFactory and use it to implement createMaze. We'll create the factory using a method createMazeFactory of class MazeGame. A BombedMazeFactory or EnchantedMazeFactory is created by associating different classes with the keys. For example, an EnchantedMazeFactory could be created like this: InterViews uses the "Kit" suffix [Lin92] to denote AbstractFactory classes. It defines WidgetKit and DialogKit abstract factories for generating look-and-feel-specific user interface objects. InterViews also includes a LayoutKit that generates different composition objects depending on the layout desired. For example, a layout that is conceptually horizontal may require different composition objects depending on the document's orientation (portrait or landscape). ET++ [WGM88] uses the Abstract Factory pattern to achieve portability across different window systems (X Windows and SunView, for example). The WindowSystem abstract base class defines the interface for creating objects that represent window system resources (MakeWindow, MakeFont, MakeColor, for example). Concrete subclasses implement the interfaces for a specific window system. At run-time, ET++ creates an instance of a concrete WindowSystem subclass that creates concrete system resource objects. AbstractFactory classes are often implemented with factory methods (Factory Method (121)), but they can also be implemented using Prototype (133). A concrete factory is often a singleton (Singleton (144)).
##%%&&
Separate the construction of a complex object from its representation so that the same construction process can create different representations. A reader for the RTF (Rich Text Format) document exchange format should be able to convert RTF to many text formats. The reader might convert RTF documents into plain ASCII text or into a text widget that can be edited interactively. The problem, however, is that the number of possible conversions is open-ended. So it should be easy to add a new conversion without modifying the reader. A solution is to configure the RTFReader class with a TextConverter object that converts RTF to another textual representation. As the RTFReader parses the RTF document, it uses the TextConverter to perform the conversion. Whenever the RTFReader recognizes an RTF token (either plain text or an RTF control word), it issues a request to the TextConverter to convert the token. TextConverter objects are responsible both for performing the data conversion and for representing the token in a particular format. Subclasses of TextConverter specialize in different conversions and formats. For example, an ASCIIConverter ignores requests to convert anything except plain text. A TeXConverter, on the other hand, will implement operations for all requests in order to produce a TeX representation that captures all the stylistic information in the text. A TextWidgetConverter will produce a complex user interface object that lets the user see and edit the text. Each kind of converter class takes the mechanism for creating and assembling a complex object and puts it behind an abstract interface. The converter is separate from the reader, which is responsible for parsing an RTF document. The Builder pattern captures all these relationships. Each converter class is called a builder in the pattern, and the reader is called the director. Applied to this example, the Builder pattern separates the algorithm for interpreting a textual format (that is, the parser for RTF documents) from how a converted format gets created and represented. This lets us reuse the RTFReader's parsing algorithm to create different text representations from RTF documents¡ªjust configure the RTFReader with different subclasses of TextConverter. Use the Builder pattern when?the algorithm for creating a complex object should be independent of the parts that make up the object and how they're assembled.?the construction process must allow different representations for the object that's constructed. Builder (TextConverter) specifies an abstract interface for creating parts of a Product object.?ConcreteBuilder (ASCIIConverter, TeXConverter, TextWidgetConverter) constructs and assembles parts of the product by implementing the Builder interface. defines and keeps track of the representation it creates. provides an interface for retrieving the product (e.g., GetASCIIText, GetTextWidget).?Director (RTFReader) constructs an object using the Builder interface.?Product (ASCIIText, TeXText, TextWidget) represents the complex object under construction. ConcreteBuilder builds the product's internal representation and defines the process by which it's assembled. includes classes that define the constituent parts, including interfaces for assembling the parts into the final result. The client creates the Director object and configures it with the desired Builder object.?Director notifies the builder whenever a part of the product should be built.?Builder handles requests from the director and adds parts to the product.?The client retrieves the product from the builder. The following interaction diagram illustrates how Builder and Director cooperate with a client. Here are key consequences of the Builder pattern: It lets you vary a product's internal representation. The Builder object provides the director with an abstract interface for constructing the product. The interface lets the builder hide the representation and internal structure of the product. It also hides how the product gets assembled. Because the product is constructed through an abstract interface, all you have to do to change the product's internal representation is define a new kind of builder. It isolates code for construction and representation. The Builder pattern improves modularity by encapsulating the way a complex object is constructed and represented. Clients needn't know anything about the classes that define the product's internal structure; such classes don't appear in Builder's interface. Each ConcreteBuilder contains all the code to create and assemble a particular kind of product. The code is written once; then different Directors can reuse it to build Product variants from the same set of parts. In the earlier RTF example, we could define a reader for a format other than RTF, say, an SGMLReader, and use the same TextConverters to generate ASCIIText, TeXText, and TextWidget renditions of SGML documents. It gives you finer control over the construction process. Unlike creational patterns that construct products in one shot, the Builder pattern constructs the product step by step under the director's control. Only when the product is finished does the director retrieve it from the builder. Hence the Builder interface reflects the process of constructing the product more than other creational patterns. This gives you finer control over the construction process and consequently the internal structure of the resulting product. Typically there's an abstract Builder class that defines an operation for each component that a director may ask it to create. The operations do nothing by default. A ConcreteBuilder class overrides operations for components it's interested in creating. Here are other implementation issues to consider: Assembly and construction interface. Builders construct their products in step-by-step fashion. Therefore the Builder class interface must be general enough to allow the construction of products for all kinds of concrete builders. A key design issue concerns the model for the construction and assembly process. A model where the results of construction requests are simply appended to the product is usually sufficient. In the RTF example, the builder converts and appends the next token to the text it has converted so far. But sometimes you might need access to parts of the product constructed earlier. In the Maze example we present in the Sample Code, the MazeBuilder interface lets you add a door between existing rooms. Tree structures such as parse trees that are built bottom-up are another example. In that case, the builder would return child nodes to the director, which then would pass them back to the builder to build the parent nodes. Why no abstract class for products? In the common case, the products produced by the concrete builders differ so greatly in their representation that there is little to gain from giving different products a common parent class. In the RTF example, the ASCIIText and the TextWidget objects are unlikely to have a common interface, nor do they need one. Because the client usually configures the director with the proper concrete builder, the client is in a position to know which concrete subclass of Builder is in use and can handle its products accordingly. Empty methods as default in Builder. In C++, the build methods are intentionally not declared pure virtual member functions. They're defined as empty methods instead, letting clients override only the operations they're interested in. We'll define a variant of the CreateMaze member function that takes a builder of class MazeBuilder as an argument. The MazeBuilder class defines the following interface for building mazes: This interface can create three things: (1) the maze, (2) rooms with a particular room number, and (3) doors between numbered rooms. The GetMaze operation returns the maze to the client. Subclasses of MazeBuilder will override this operation to return the maze that they build. All the maze-building operations of MazeBuilder do nothing by default. They're not declared pure virtual to let derived classes override only those methods in which they're interested. Given the MazeBuilder interface, we can change the CreateMaze member function to take this builder as a parameter. Compare this version of CreateMaze with the original. Notice how the builder hides the internal representation of the Maze¡ªthat is, the classes that define rooms, doors, and walls¡ªand how these parts are assembled to complete the final maze. Someone might guess that there are classes for representing rooms and doors, but there is no hint of one for walls. This makes it easier to change the way a maze is represented, since none of the clients of MazeBuilder has to be changed. Like the other creational patterns, the Builder pattern encapsulates how objects get created, in this case through the interface defined by MazeBuilder. That means we can reuse MazeBuilder to build different kinds of mazes. The CreateComplexMaze operation gives an example: Note that MazeBuilder does not create mazes itself; its main purpose is just to define an interface for creating mazes. It defines empty implementations primarily for convenience. Subclasses of MazeBuilder do the actual work. The subclass StandardMazeBuilder is an implementation that builds simple mazes. It keeps track of the maze it's building in the variable _currentMaze. CommonWall is a utility operation that determines the direction of the common wall between two rooms. The StandardMazeBuilder constructor simply initializes _currentMaze. BuildMaze instantiates a Maze that other operations will assemble and eventually return to the client (with GetMaze). The BuildRoom operation creates a room and builds the walls around it: To build a door between two rooms, StandardMazeBuilder looks up both rooms in the maze and finds their adjoining wall: Clients can now use CreateMaze in conjunction with StandardMazeBuilder to create a maze: We could have put all the StandardMazeBuilder operations in Maze and let each Maze build itself. But making Maze smaller makes it easier to understand and modify, and StandardMazeBuilder is easy to separate from Maze. Most importantly, separating the two lets you have a variety of MazeBuilders, each using different classes for rooms, walls, and doors. A more exotic MazeBuilder is CountingMazeBuilder. This builder doesn't create a maze at all; it just counts the different kinds of components that would have been created. The constructor initializes the counters, and the overridden MazeBuilder operations increment them accordingly. The RTF converter application is from ET++ [WGM88]. Its text building block uses a builder to process text stored in the RTF format. Builder is a common pattern in Smalltalk-80 [Par90]:?The Parser class in the compiler subsystem is a Director that takes a ProgramNodeBuilder object as an argument. A Parser object notifies its ProgramNodeBuilder object each time it recognizes a syntactic construct. When the parser is done, it asks the builder for the parse tree it built and returns it to the client. ClassBuilder is a builder that Classes use to create subclasses for themselves. In this case a Class is both the Director and the Product.?ByteCodeStream is a builder that creates a compiled method as a byte array. ByteCodeStream is a nonstandard use of the Builder pattern, because the complex object it builds is encoded as a byte array, not as a normal Smalltalk object. But the interface to ByteCodeStream is typical of a builder, and it would be easy to replace ByteCodeStream with a different class that represented programs as a composite object. The Service Configurator framework from the Adaptive Communications Environment uses a builder to construct network service components that are linked into a server at run-time [SS94]. The components are described with a configuration language that's parsed by an LALR(1) parser. The semantic actions of the parser perform operations on the builder that add information to the service component. In this case, the parser is the Director. Abstract Factory (99) is similar to Builder in that it too may construct complex objects. The primary difference is that the Builder pattern focuses on constructing a complex object step by step. Abstract Factory's emphasis is on families of product objects (either simple or complex). Builder returns the product as a final step, but as far as the Abstract Factory pattern is concerned, the product gets returned immediately. A Composite (183) is what the builder often builds.
##%%&&
Define an interface for creating an object, but let subclasses decide which class to instantiate. Factory Method lets a class defer instantiation to subclasses. Frameworks use abstract classes to define and maintain relationships between objects. A framework is often responsible for creating these objects as well. Consider a framework for applications that can present multiple documents to the user. Two key abstractions in this framework are the classes Application and Document. Both classes are abstract, and clients have to subclass them to realize their application-specific implementations. To create a drawing application, for example, we define the classes DrawingApplication and DrawingDocument. The Application class is responsible for managing Documents and will create them as required when the user selects Open or New from a menu, for example. Because the particular Document subclass to instantiate is application-specific, the Application class can't predict the subclass of Document to instantiate the Application class only knows when a new document should be created, not what kind of Document to create. This creates a dilemma: The framework must instantiate classes, but it only knows about abstract classes, which it cannot instantiate. The Factory Method pattern offers a solution. It encapsulates the knowledge of which Document subclass to create and moves this knowledge out of the framework. Application subclasses redefine an abstract CreateDocument operation on Application to return the appropriate Document subclass. Once an Application subclass is instantiated, it can then instantiate application-specific Documents without knowing their class. We call CreateDocument a factory method because it's responsible for "manufacturing" an object. Use the Factory Method pattern when?a class can't anticipate the class of objects it must create.?a class wants its subclasses to specify the objects it creates.?classes delegate responsibility to one of several helper subclasses, and you want to localize the knowledge of which helper subclass is the delegate. Product (Document) defines the interface of objects the factory method creates.?ConcreteProduct (MyDocument) implements the Product interface.?Creator (Application) declares the factory method, which returns an object of type Product. Creator may also define a default implementation of the factory method that returns a default ConcreteProduct object. may call the factory method to create a Product object.?ConcreteCreator (MyApplication) overrides the factory method to return an instance of a ConcreteProduct. Creator relies on its subclasses to define the factory method so that it returns an instance of the appropriate ConcreteProduct. Factory methods eliminate the need to bind application-specific classes into your code. The code only deals with the Product interface; therefore it can work with any user-defined ConcreteProduct classes. A potential disadvantage of factory methods is that clients might have to subclass the Creator class just to create a particular ConcreteProduct object. Subclassing is fine when the client has to subclass the Creator class anyway, but otherwise the client now must deal with another point of evolution. Here are two additional consequences of the Factory Method pattern: Provides hooks for subclasses. Creating objects inside a class with a factory method is always more flexible than creating an object directly. Factory Method gives subclasses a hook for providing an extended version of an object. In the Document example, the Document class could define a factory method called CreateFileDialog that creates a default file dialog object for opening an existing document. A Document subclass can define an application-specific file dialog by overriding this factory method. In this case the factory method is not abstract but provides a reasonable default implementation. Connects parallel class hierarchies. In the examples we've considered so far, the factory method is only called by Creators. But this doesn't have to be the case; clients can find factory methods useful, especially in the case of parallel class hierarchies. Parallel class hierarchies result when a class delegates some of its responsibilities to a separate class. Consider graphical figures that can be manipulated interactively; that is, they can be stretched, moved, or rotated using the mouse. Implementing such interactions isn't always easy. It often requires storing and updating information that records the state of the manipulation at a given time. This state is needed only during manipulation; therefore it needn't be kept in the figure object. Moreover, different figures behave differently when the user manipulates them. For example, stretching a line figure might have the effect of moving an endpoint, whereas stretching a text figure may change its line spacing. With these constraints, it's better to use a separate Manipulator object that implements the interaction and keeps track of any manipulation-specific state that's needed. Different figures will use different Manipulator subclasses to handle particular interactions. The resulting Manipulator class hierarchy parallels (at least partially) the Figure class hierarchy: The Figure class provides a CreateManipulator factory method that lets clients create a Figure's corresponding Manipulator. Figure subclasses override this method to return an instance of the Manipulator subclass that's right for them. Alternatively, the Figure class may implement CreateManipulator to return a default Manipulator instance, and Figure subclasses may simply inherit that default. The Figure classes that do so need no corresponding Manipulator subclass¡ªhence the hierarchies are only partially parallel. Notice how the factory method defines the connection between the two class hierarchies. It localizes knowledge of which classes belong together. Consider the following issues when applying the Factory Method pattern: Two major varieties. The two main variations of the Factory Method pattern are (1) the case when the Creator class is an abstract class and does not provide an implementation for the factory method it declares, and (2) the case when the Creator is a concrete class and provides a default implementation for the factory method. It's also possible to have an abstract class that defines a default implementation, but this is less common. The first case requires subclasses to define an implementation, because there's no reasonable default. It gets around the dilemma of having to instantiate unforeseeable classes. In the second case, the concrete Creator uses the factory method primarily for flexibility. It's following a rule that says, "Create objects in a separate operation so that subclasses can override the way they're created." This rule ensures that designers of subclasses can change the class of objects their parent class instantiates if necessary. Parameterized factory methods. Another variation on the pattern lets the factory method create multiple kinds of products. The factory method takes a parameter that identifies the kind of object to create. All objects the factory method creates will share the Product interface. In the Document example, Application might support different kinds of Documents. You pass CreateDocument an extra parameter to specify the kind of document to create. The Unidraw graphical editing framework [VL90] uses this approach for reconstructing objects saved on disk. Unidraw defines a Creator class with a factory method Create that takes a class identifier as an argument. The class identifier specifies the class to instantiate. When Unidraw saves an object to disk, it writes out the class identifier first and then its instance variables. When it reconstructs the object from disk, it reads the class identifier first. Once the class identifier is read, the framework calls Create, passing the identifier as the parameter. Create looks up the constructor for the corresponding class and uses it to instantiate the object. Last, Create calls the object's Read operation, which reads the remaining information on the disk and initializes the object's instance variables. A parameterized factory method has the following general form, where MyProduct and YourProduct are subclasses of Product: Overriding a parameterized factory method lets you easily and selectively extend or change the products that a Creator produces. You can introduce new identifiers for new kinds of products, or you can associate existing identifiers with different products. For example, a subclass MyCreator could swap MyProduct and YourProduct and support a new TheirProduct subclass: Notice that the last thing this operation does is call Create on the parent class. That's because MyCreator::Create handles only YOURS, MINE, and THEIRS differently than the parent class. It isn't interested in other classes. Hence MyCreator extends the kinds of products created, and it defers responsibility for creating all but a few products to its parent. Language-specific variants and issues. Different languages lend themselves to other interesting variations and caveats. Smalltalk programs often use a method that returns the class of the object to be instantiated. A Creator factory method can use this value to create a product, and a ConcreteCreator may store or even compute this value. The result is an even later binding for the type of ConcreteProduct to be instantiated. A Smalltalk version of the Document example can define a documentClass method on Application. The documentClass method returns the proper Document class for instantiating documents. The implementation of documentClass in MyApplication returns the MyDocument class. Thus in class Application we have which returns the class MyDocument to be instantiated to Application. An even more flexible approach akin to parameterized factory methods is to store the class to be created as a class variable of Application. That way you don't have to subclass Application to vary the product. Factory methods in C++ are always virtual functions and are often pure virtual. Just be careful not to call factory methods in the Creator's constructor¡ªthe factory method in the ConcreteCreator won't be available yet. You can avoid this by being careful to access products solely through accessor operations that create the product on demand. Instead of creating the concrete product in the constructor, the constructor merely initializes it to 0. The accessor returns the product. But first it checks to make sure the product exists, and if it doesn't, the accessor creates it. This technique is sometimes called lazy initialization. The following code shows a typical implementation: Using templates to avoid subclassing. As we've mentioned, another potential problem with factory methods is that they might force you to subclass just to create the appropriate Product objects. Another way to get around this in C++ is to provide a template subclass of Creator that's parameterized by the Product class: With this template, the client supplies just the product class¡ªno subclassing of Creator is required. Naming conventions. It's good practice to use naming conventions that make it clear you're using factory methods. For example, the MacApp Macintosh application framework [App89] always declares the abstract operation that defines the factory method as Class* DoMakeClass(), where Class is the Product class. The function CreateMaze builds and returns a maze. One problem with this function is that it hard-codes the classes of maze, rooms, doors, and walls. We'll introduce factory methods to let subclasses choose these components. First we'll define factory methods in MazeGame for creating the maze, room, wall, and door objects: Each factory method returns a maze component of a given type. MazeGame provides default implementations that return the simplest kinds of maze, rooms, walls, and doors. Now we can rewrite CreateMaze to use these factory methods: Different games can subclass MazeGame to specialize parts of the maze. MazeGame subclasses can redefine some or all of the factory methods to specify variations in products. For example, a BombedMazeGame can redefine the Room and Wall products to return the bombed varieties: Factory methods pervade toolkits and frameworks. The preceding document example is a typical use in MacApp and ET++ [WGM88]. The manipulator example is from Unidraw. Class View in the Smalltalk-80 Model/View/Controller framework has a method defaultController that creates a controller, and this might appear to be a factory method [Par90]. But subclasses of View specify the class of their default controller by defining defaultControllerClass, which returns the class from which defaultController creates instances. So defaultControllerClass is the real factory method, that is, the method that subclasses should override. A more esoteric example in Smalltalk-80 is the factory method parserClass defined by Behavior (a superclass of all objects representing classes). This enables a class to use a customized parser for its source code. For example, a client can define a class SQLParser to analyze the source code of a class with embedded SQL statements. The Behavior class implements parserClass to return the standard Smalltalk Parser class. A class that includes embedded SQL statements overrides this method (as a class method) and returns the SQLParser class. The Orbix ORB system from IONA Technologies [ION94] uses Factory Method to generate an appropriate type of proxy (see Proxy (233)) when an object requests a reference to a remote object. Factory Method makes it easy to replace the default proxy with one that uses client-side caching, for example. Abstract Factory (99) is often implemented with factory methods. The Motivation example in the Abstract Factory pattern illustrates Factory Method as well. Factory methods are usually called within Template Methods (360). In the document example above, NewDocument is a template method. Prototypes (133) don't require subclassing Creator. However, they often require an Initialize operation on the Product class. Creator uses Initialize to initialize the object. Factory Method doesn't require such an operation.
##%%&&
Specify the kinds of objects to create using a prototypical instance, and create new objects by copying this prototype. You could build an editor for music scores by customizing a general framework for graphical editors and adding new objects that represent notes, rests, and staves. The editor framework may have a palette of tools for adding these music objects to the score. The palette would also include tools for selecting, moving, and otherwise manipulating music objects. Users will click on the quarter-note tool and use it to add quarter notes to the score. Or they can use the move tool to move a note up or down on the staff, thereby changing its pitch. Let's assume the framework provides an abstract Graphic class for graphical components, like notes and staves. Moreover, it'll provide an abstract Tool class for defining tools like those in the palette. The framework also predefines a GraphicTool subclass for tools that create instances of graphical objects and add them to the document. But GraphicTool presents a problem to the framework designer. The classes for notes and staves are specific to our application, but the GraphicTool class belongs to the framework. GraphicTool doesn't know how to create instances of our music classes to add to the score. We could subclass GraphicTool for each kind of music object, but that would produce lots of subclasses that differ only in the kind of music object they instantiate. We know object composition is a flexible alternative to subclassing. The question is, how can the framework use it to parameterize instances of GraphicTool by the class of Graphic they're supposed to create? The solution lies in making GraphicTool create a new Graphic by copying or "cloning" an instance of a Graphic subclass. We call this instance a prototype. GraphicTool is parameterized by the prototype it should clone and add to the document. If all Graphic subclasses support a Clone operation, then the GraphicTool can clone any kind of Graphic. So in our music editor, each tool for creating a music object is an instance of GraphicTool that's initialized with a different prototype. Each GraphicTool instance will produce a music object by cloning its prototype and adding the clone to the score. We can use the Prototype pattern to reduce the number of classes even further. We have separate classes for whole notes and half notes, but that's probably unnecessary. Instead they could be instances of the same class initialized with different bitmaps and durations. A tool for creating whole notes becomes just a GraphicTool whose prototype is a MusicalNote initialized to be a whole note. This can reduce the number of classes in the system dramatically. It also makes it easier to add a new kind of note to the music editor. Use the Prototype pattern when a system should be independent of how its products are created, composed, and represented; and?when the classes to instantiate are specified at run-time, for example, by dynamic loading; or?to avoid building a class hierarchy of factories that parallels the class hierarchy of products; or?when instances of a class can have one of only a few different combinations of state. It may be more convenient to install a corresponding number of prototypes and clone them rather than instantiating the class manually, each time with the appropriate state. Prototype (Graphic) declares an interface for cloning itself.?ConcretePrototype (Staff, WholeNote, HalfNote) implements an operation for cloning itself.?Client (GraphicTool) creates a new object by asking a prototype to clone itself. A client asks a prototype to clone itself. Prototype has many of the same consequences that Abstract Factory (99) and Builder (110) have: It hides the concrete product classes from the client, thereby reducing the number of names clients know about. Moreover, these patterns let a client work with application-specific classes without modification. Additional benefits of the Prototype pattern are listed below. Adding and removing products at run-time. Prototypes let you incorporate a new concrete product class into a system simply by registering a prototypical instance with the client. That's a bit more flexible than other creational patterns, because a client can install and remove prototypes at run-time. Specifying new objects by varying values. Highly dynamic systems let you define new behavior through object composition¡ªby specifying values for an object's variables, for example¡ªand not by defining new classes. You effectively define new kinds of objects by instantiating existing classes and registering the instances as prototypes of client objects. A client can exhibit new behavior by delegating responsibility to the prototype. This kind of design lets users define new "classes" without programming. In fact, cloning a prototype is similar to instantiating a class. The Prototype pattern can greatly reduce the number of classes a system needs. In our music editor, one GraphicTool class can create a limitless variety of music objects. Specifying new objects by varying structure. Many applications build objects from parts and subparts. Editors for circuit design, for example, build circuits out of subcircuits.1 For convenience, such applications often let you instantiate complex, user-defined structures, say, to use a specific subcircuit again and again. The Prototype pattern supports this as well. We simply add this subcircuit as a prototype to the palette of available circuit elements. As long as the composite circuit object implements Clone as a deep copy, circuits with different structures can be prototypes. Reduced subclassing. Factory Method (121) often produces a hierarchy of Creator classes that parallels the product class hierarchy. The Prototype pattern lets you clone a prototype instead of asking a factory method to make a new object. Hence you don't need a Creator class hierarchy at all. This benefit applies primarily to languages like C++ that don't treat classes as first-class objects. Languages that do, like Smalltalk and Objective C, derive less benefit, since you can always use a class object as a creator. Class objects already act like prototypes in these languages. Configuring an application with classes dynamically. Some run-time environments let you load classes into an application dynamically. The Prototype pattern is the key to exploiting such facilities in a language like C++. An application that wants to create instances of a dynamically loaded class won't be able to reference its constructor statically. Instead, the run-time environment creates an instance of each class automatically when it's loaded, and it registers the instance with a prototype manager (see the Implementation section). Then the application can ask the prototype manager for instances of newly loaded classes, classes that weren't linked with the program originally. The ET++ application framework [WGM88] has a run-time system that uses this scheme. The main liability of the Prototype pattern is that each subclass of Prototype must implement the Clone operation, which may be difficult. For example, adding Clone is difficult when the classes under consideration already exist. Implementing Clone can be difficult when their internals include objects that don't support copying or have circular references. Prototype is particularly useful with static languages like C++, where classes are not objects, and little or no type information is available at run-time. It's less important in languages like Smalltalk or Objective C that provide what amounts to a prototype (i.e., a class object) for creating instances of each class. This pattern is built into prototype-based languages like Self [US87], in which all object creation happens by cloning a prototype. Consider the following issues when implementing prototypes: Using a prototype manager. When the number of prototypes in a system isn't fixed (that is, they can be created and destroyed dynamically), keep a registry of available prototypes. Clients won't manage prototypes themselves but will store and retrieve them from the registry. A client will ask the registry for a prototype before cloning it. We call this registry a prototype manager. A prototype manager is an associative store that returns the prototype matching a given key. It has operations for registering a prototype under a key and for unregistering it. Clients can change or even browse through the registry at run-time. This lets clients extend and take inventory on the system without writing code. Implementing the Clone operation. The hardest part of the Prototype pattern is implementing the Clone operation correctly. It's particularly tricky when object structures contain circular references. Most languages provide some support for cloning objects. For example, Smalltalk provides an implementation of copy that's inherited by all subclasses of Object. C++ provides a copy constructor. But these facilities don't solve the "shallow copy versus deep copy" problem [GR83]. That is, does cloning an object in turn clone its instance variables, or do the clone and original just share the variables? A shallow copy is simple and often sufficient, and that's what Smalltalk provides by default. The default copy constructor in C++ does a member-wise copy, which means pointers will be shared between the copy and the original. But cloning prototypes with complex structures usually requires a deep copy, because the clone and the original must be independent. Therefore you must ensure that the clone's components are clones of the prototype's components. Cloning forces you to decide what if anything will be shared. If objects in the system provide Save and Load operations, then you can use them to provide a default implementation of Clone simply by saving the object and loading it back immediately. The Save operation saves the object into a memory buffer, and Load creates a duplicate by reconstructing the object from the buffer. Initializing clones. While some clients are perfectly happy with the clone as is, others will want to initialize some or all of its internal state to values of their choosing. You generally can't pass these values in the Clone operation, because their number will vary between classes of prototypes. Some prototypes might need multiple initialization parameters; others won't need any. Passing parameters in the Clone operation precludes a uniform cloning interface. It might be the case that your prototype classes already define operations for (re)setting key pieces of state. If so, clients may use these operations immediately after cloning. If not, then you may have to introduce an Initialize operation (see the Sample Code section) that takes initialization parameters as arguments and sets the clone's internal state accordingly. Beware of deep-copying Clone operations¡ªthe copies may have to be deleted (either explicitly or within Initialize) before you reinitialize them. We'll define a MazePrototypeFactory subclass of the MazeFactory class. MazePrototypeFactory will be initialized with prototypes of the objects it will create so that we don't have to subclass it just to change the classes of walls or rooms it creates. MazePrototypeFactory augments the MazeFactory interface with a constructor that takes the prototypes as arguments: The member functions for creating walls, rooms, and doors are similar: Each clones a prototype and then initializes it. Here are the definitions of MakeWall and MakeDoor: We can use MazePrototypeFactory to create a prototypical or default maze just by initializing it with prototypes of basic maze components: To change the type of maze, we initialize MazePrototypeFactory with a different set of prototypes. The following call creates a maze with a BombedDoor and a RoomWithABomb: An object that can be used as a prototype, such as an instance of Wall, must support the Clone operation. It must also have a copy constructor for cloning. It may also need a separate operation for reinitializing internal state. We'll add the Initialize operation to Door to let clients initialize the clone's rooms. Compare the following definition of Door to the one on page 96: Although BombedWall::Clone returns a Wall*, its implementation returns a pointer to a new instance of a subclass, that is, a BombedWall*. We define Clone like this in the base class to ensure that clients that clone the prototype don't have to know about their concrete subclasses. Clients should never need to downcast the return value of Clone to the desired type. In Smalltalk, you can reuse the standard copy method inherited from Object to clone any MapSite. You can use MazeFactory to produce the prototypes you'll need; for example, you can create a room by supplying the name #room. The MazeFactory has a dictionary that maps names to prototypes. Its make: method looks like this: Given appropriate methods for initializing the MazeFactory with prototypes, you could create a simple maze with the following code: Perhaps the first example of the Prototype pattern was in Ivan Sutherland's Sketchpad system [Sut63]. The first widely known application of the pattern in an object-oriented language was in ThingLab, where users could form a composite object and then promote it to a prototype by installing it in a library of reusable objects [Bor81]. Goldberg and Robson mention prototypes as a pattern [GR83], but Coplien [Cop92] gives a much more complete description. He describes idioms related to the Prototype pattern for C++ and gives many examples and variations. Etgdb is a debugger front-end based on ET++ that provides a point-and-click interface to different line-oriented debuggers. Each debugger has a corresponding DebuggerAdaptor subclass. For example, GdbAdaptor adapts etgdb to the command syntax of GNU gdb, while SunDbxAdaptor adapts etgdb to Sun's dbx debugger. Etgdb does not have a set of DebuggerAdaptor classes hard-coded into it. Instead, it reads the name of the adaptor to use from an environment variable, looks for a prototype with the specified name in a global table, and then clones the prototype. New debuggers can be added to etgdb by linking it with the DebuggerAdaptor that works for that debugger. The "interaction technique library" in Mode Composer stores prototypes of objects that support various interaction techniques [Sha90]. Any interaction technique created by the Mode Composer can be used as a prototype by placing it in this library. The Prototype pattern lets Mode Composer support an unlimited set of interaction techniques. The music editor example discussed earlier is based on the Unidraw drawing framework [VL90]. Prototype and Abstract Factory (99) are competing patterns in some ways, as we discuss at the end of this chapter. They can also be used together, however. An Abstract Factory might store a set of prototypes from which to clone and return product objects. Designs that make heavy use of the Composite (183) and Decorator (196) patterns often can benefit from Prototype as well.
##%%&&
Ensure a class only has one instance, and provide a global point of access to it. It's important for some classes to have exactly one instance. Although there can be many printers in a system, there should be only one printer spooler. There should be only one file system and one window manager. A digital filter will have one A/D converter. An accounting system will be dedicated to serving one company. How do we ensure that a class has only one instance and that the instance is easily accessible? A global variable makes an object accessible, but it doesn't keep you from instantiating multiple objects. A better solution is to make the class itself responsible for keeping track of its sole instance. The class can ensure that no other instance can be created (by intercepting requests to create new objects), and it can provide a way to access the instance. This is the Singleton pattern. Use the Singleton pattern when?there must be exactly one instance of a class, and it must be accessible to clients from a well-known access point.?when the sole instance should be extensible by subclassing, and clients should be able to use an extended instance without modifying their code. Singleton defines an Instance operation that lets clients access its unique instance. Instance is a class operation (that is, a class method in Smalltalk and a static member function in C++). may be responsible for creating its own unique instance. Clients access a Singleton instance solely through Singleton's Instance operation. The Singleton pattern has several benefits: Controlled access to sole instance. Because the Singleton class encapsulates its sole instance, it can have strict control over how and when clients access it. Reduced name space. The Singleton pattern is an improvement over global variables. It avoids polluting the name space with global variables that store sole instances. Permits refinement of operations and representation. The Singleton class may be subclassed, and it's easy to configure an application with an instance of this extended class. You can configure the application with an instance of the class you need at run-time. Permits a variable number of instances. The pattern makes it easy to change your mind and allow more than one instance of the Singleton class. Moreover, you can use the same approach to control the number of instances that the application uses. Only the operation that grants access to the Singleton instance needs to change. More flexible than class operations. Another way to package a singleton's functionality is to use class operations (that is, static member functions in C++ or class methods in Smalltalk). But both of these language techniques make it hard to change a design to allow more than one instance of a class. Moreover, static member functions in C++ are never virtual, so subclasses can't override them polymorphically. Here are implementation issues to consider when using the Singleton pattern: Ensuring a unique instance. The Singleton pattern makes the sole instance a normal instance of a class, but that class is written so that only one instance can ever be created. A common way to do this is to hide the operation that creates the instance behind a class operation (that is, either a static member function or a class method) that guarantees only one instance is created. This operation has access to the variable that holds the unique instance, and it ensures the variable is initialized with the unique instance before returning its value. This approach ensures that a singleton is created and initialized before its first use. You can define the class operation in C++ with a static member function Instance of the Singleton class. Singleton also defines a static member variable _instance that contains a pointer to its unique instance. Clients access the singleton exclusively through the Instance member function. The variable _instance is initialized to 0, and the static member function Instance returns its value, initializing it with the unique instance if it is 0. Instance uses lazy initialization; the value it returns isn't created and stored until it's first accessed. Notice that the constructor is protected. A client that tries to instantiate Singleton directly will get an error at compile-time. This ensures that only one instance can ever get created. Moreover, since the _instance is a pointer to a Singleton object, the Instance member function can assign a pointer to a subclass of Singleton to this variable. We'll give an example of this in the Sample Code. There's another thing to note about the C++ implementation. It isn't enough to define the singleton as a global or static object and then rely on automatic initialization. There are three reasons for this: We can't guarantee that only one instance of a static object will ever be declared. We might not have enough information to instantiate every singleton at static initialization time. A singleton might require values that are computed later in the program's execution. C++ doesn't define the order in which constructors for global objects are called across translation units [ES90]. This means that no dependencies can exist between singletons; if any do, then errors are inevitable. An added (albeit small) liability of the global/static object approach is that it forces all singletons to be created whether they are used or not. Using a static member function avoids all of these problems. In Smalltalk, the function that returns the unique instance is implemented as a class method on the Singleton class. To ensure that only one instance is created, override the new operation. The resulting Singleton class might have the following two class methods, where SoleInstance is a class variable that is not used anywhere else: Subclassing the Singleton class. The main issue is not so much defining the subclass but installing its unique instance so that clients will be able to use it. In essence, the variable that refers to the singleton instance must get initialized with an instance of the subclass. The simplest technique is to determine which singleton you want to use in the Singleton's Instance operation. An example in the Sample Code shows how to implement this technique with environment variables. Another way to choose the subclass of Singleton is to take the implementation of Instance out of the parent class (e.g., MazeFactory) and put it in the subclass. That lets a C++ programmer decide the class of singleton at link-time (e.g., by linking in an object file containing a different implementation) but keeps it hidden from the clients of the singleton. The link approach fixes the choice of singleton class at link-time, which makes it hard to choose the singleton class at run-time. Using conditional statements to determine the subclass is more flexible, but it hard-wires the set of possible Singleton classes. Neither approach is flexible enough in all cases. A more flexible approach uses a registry of singletons. Instead of having Instance define the set of possible Singleton classes, the Singleton classes can register their singleton instance by name in a well-known registry. The registry maps between string names and singletons. When Instance needs a singleton, it consults the registry, asking for the singleton by name. The registry looks up the corresponding singleton (if it exists) and returns it. This approach frees Instance from knowing all possible Singleton classes or instances. All it requires is a common interface for all Singleton classes that includes operations for the registry: Register registers the Singleton instance under the given name. To keep the registry simple, we'll have it store a list of NameSingletonPair objects. Each NameSingletonPair maps a name to a singleton. The Lookup operation finds a singleton given its name. We'll assume that an environment variable specifies the name of the singleton desired. Where do Singleton classes register themselves? One possibility is in their constructor. For example, a MySingleton subclass could do the following: Of course, the constructor won't get called unless someone instantiates the class, which echoes the problem the Singleton pattern is trying to solve! We can get around this problem in C++ by defining a static instance of MySingleton. For example, we can define in the file that contains MySingleton's implementation. No longer is the Singleton class responsible for creating the singleton. Instead, its primary responsibility is to make the singleton object of choice accessible in the system. The static object approach still has a potential drawback namely that instances of all possible Singleton subclasses must be created, or else they won't get registered. Suppose we define a MazeFactory class for building mazes as described on page 92. MazeFactory defines an interface for building different parts of a maze. Subclasses can redefine the operations to return instances of specialized product classes, like BombedWall objects instead of plain Wall objects. What's relevant here is that the Maze application needs only one instance of a maze factory, and that instance should be available to code that builds any part of the maze. This is where the Singleton pattern comes in. By making the MazeFactory a singleton, we make the maze object globally accessible without resorting to global variables. For simplicity, let's assume we'll never subclass MazeFactory. (We'll consider the alternative in a moment.) We make it a Singleton class in C++ by adding a static Instance operation and a static _instance member to hold the one and only instance. We must also protect the constructor to prevent accidental instantiation, which might lead to more than one instance. Now let's consider what happens when there are subclasses of MazeFactory, and the application must decide which one to use. We'll select the kind of maze through an environment variable and add code that instantiates the proper MazeFactory subclass based on the environment variable's value. The Instance operation is a good place to put this code, because it already instantiates MazeFactory: Note that Instance must be modified whenever you define a new subclass of MazeFactory. That might not be a problem in this application, but it might be for abstract factories defined in a framework. A possible solution would be to use the registry approach described in the Implementation section. Dynamic linking could be useful here as well¡ªit would keep the application from having to load all the subclasses that are not used. An example of the Singleton pattern in Smalltalk-80 [Par90] is the set of changes to the code, which is ChangeSet current. A more subtle example is the relationship between classes and their metaclasses. A metaclass is the class of a class, and each metaclass has one instance. Metaclasses do not have names (except indirectly through their sole instance), but they keep track of their sole instance and will not normally create another. The InterViews user interface toolkit [LCI+92] uses the Singleton pattern to access the unique instance of its Session and WidgetKit classes, among others. Session defines the application's main event dispatch loop, stores the user's database of stylistic preferences, and manages connections to one or more physical displays. WidgetKit is an Abstract Factory (99) for defining the look and feel of user interface widgets. The WidgetKit::instance() operation determines the particular WidgetKit subclass that's instantiated based on an environment variable that Session defines. A similar operation on Session determines whether monochrome or color displays are supported and configures the singleton Session instance accordingly. Many patterns can be implemented using the Singleton pattern. See Abstract Factory (99), Builder (110), and Prototype (133).
##%%&&
Convert the interface of a class into another interface clients expect. Adapter lets classes work together that couldn't otherwise because of incompatible interfaces. Sometimes a toolkit class that's designed for reuse isn't reusable only because its interface doesn't match the domain-specific interface an application requires. Consider for example a drawing editor that lets users draw and arrange graphical elements (lines, polygons, text, etc.) into pictures and diagrams. The drawing editor's key abstraction is the graphical object, which has an editable shape and can draw itself. The interface for graphical objects is defined by an abstract class called Shape. The editor defines a subclass of Shape for each kind of graphical object: a LineShape class for lines, a PolygonShape class for polygons, and so forth. Classes for elementary geometric shapes like LineShape and PolygonShape are rather easy to implement, because their drawing and editing capabilities are inherently limited. But a TextShape subclass that can display and edit text is considerably more difficult to implement, since even basic text editing involves complicated screen update and buffer management. Meanwhile, an off-the-shelf user interface toolkit might already provide a sophisticated TextView class for displaying and editing text. Ideally we'd like to reuse TextView to implement TextShape, but the toolkit wasn't designed with Shape classes in mind. So we can't use TextView and Shape objects interchangeably. How can existing and unrelated classes like TextView work in an application that expects classes with a different and incompatible interface? We could change the TextView class so that it conforms to the Shape interface, but that isn't an option unless we have the toolkit's source code. Even if we did, it wouldn't make sense to change TextView; the toolkit shouldn't have to adopt domain-specific interfaces just to make one application work. Instead, we could define TextShape so that it adapts the TextView interface to Shape's. We can do this in one of two ways: (1) by inheriting Shape's interface and TextView's implementation or (2) by composing a TextView instance within a TextShape and implementing TextShape in terms of TextView's interface. These two approaches correspond to the class and object versions of the Adapter pattern. We call TextShape an adapter. This diagram illustrates the object adapter case. It shows how BoundingBox requests, declared in class Shape, are converted to GetExtent requests defined in TextView. Since TextShape adapts TextView to the Shape interface, the drawing editor can reuse the otherwise incompatible TextView class. Often the adapter is responsible for functionality the adapted class doesn't provide. The diagram shows how an adapter can fulfill such responsibilities. The user should be able to "drag" every Shape object to a new location interactively, but TextView isn't designed to do that. TextShape can add this missing functionality by implementing Shape's CreateManipulator operation, which returns an instance of the appropriate Manipulator subclass. Manipulator is an abstract class for objects that know how to animate a Shape in response to user input, like dragging the shape to a new location. There are subclasses of Manipulator for different shapes; TextManipulator, for example, is the corresponding subclass for TextShape. By returning a TextManipulator instance, TextShape adds the functionality that TextView lacks but Shape requires. Use the Adapter pattern when?you want to use an existing class, and its interface does not match the one you need. you want to create a reusable class that cooperates with unrelated or unforeseen classes, that is, classes that don't necessarily have compatible interfaces.?(object adapter only) you need to use several existing subclasses, but it's impractical to adapt their interface by subclassing every one. An object adapter can adapt the interface of its parent class. Target (Shape) defines the domain-specific interface that Client uses.?Client (DrawingEditor) collaborates with objects conforming to the Target interface.?Adaptee (TextView) defines an existing interface that needs adapting. Adapter (TextShape) adapts the interface of Adaptee to the Target interface. Clients call operations on an Adapter instance. In turn, the adapter calls Adaptee operations that carry out the request. Class and object adapters have different trade-offs. A class adapter?adapts Adaptee to Target by committing to a concrete Adapter class. As a consequence, a class adapter won't work when we want to adapt a class and all its subclasses.?lets Adapter override some of Adaptee's behavior, since Adapter is a subclass of Adaptee.?introduces only one object, and no additional pointer indirection is needed to get to the adaptee. An object adapter?lets a single Adapter work with many Adaptees¡ªthat is, the Adaptee itself and all of its subclasses (if any). The Adapter can also add functionality to all Adaptees at once.?makes it harder to override Adaptee behavior. It will require subclassing Adaptee and making Adapter refer to the subclass rather than the Adaptee itself. Here are other issues to consider when using the Adapter pattern: How much adapting does Adapter do? Adapters vary in the amount of work they do to adapt Adaptee to the Target interface. There is a spectrum of possible work, from simple interface conversion¡ªfor example, changing the names of operations to supporting an entirely different set of operations. The amount of work Adapter does depends on how similar the Target interface is to Adaptee's. Pluggable adapters. A class is more reusable when you minimize the assumptions other classes must make to use it. By building interface adaptation into a class, you eliminate the assumption that other classes see the same interface. Put another way, interface adaptation lets us incorporate our class into existing systems that might expect different interfaces to the class. ObjectWorks\Smalltalk [Par90] uses the term pluggable adapter to describe classes with built-in interface adaptation. Consider a TreeDisplay widget that can display tree structures graphically. If this were a special-purpose widget for use in just one application, then we might require the objects that it displays to have a specific interface; that is, all must descend from a Tree abstract class. But if we wanted to make TreeDisplay more reusable (say we wanted to make it part of a toolkit of useful widgets), then that requirement would be unreasonable. Applications will define their own classes for tree structures. They shouldn't be forced to use our Tree abstract class. Different tree structures will have different interfaces. In a directory hierarchy, for example, children might be accessed with a GetSubdirectories operation, whereas in an inheritance hierarchy, the corresponding operation might be called GetSubclasses. A reusable TreeDisplay widget must be able to display both kinds of hierarchies even if they use different interfaces. In other words, the TreeDisplay should have interface adaptation built into it. We'll look at different ways to build interface adaptation into classes in the Implementation section. Using two-way adapters to provide transparency. A potential problem with adapters is that they aren't transparent to all clients. An adapted object no longer conforms to the Adaptee interface, so it can't be used as is wherever an Adaptee object can. Two-way adapters can provide such transparency. Specifically, they're useful when two different clients need to view an object differently. Consider the two-way adapter that integrates Unidraw, a graphical editor framework [VL90], and QOCA, a constraint-solving toolkit [HHMV92]. Both systems have classes that represent variables explicitly: Unidraw has StateVariable, and QOCA has ConstraintVariable. To make Unidraw work with QOCA, ConstraintVariable must be adapted to StateVariable; to let QOCA propagate solutions to Unidraw, StateVariable must be adapted to ConstraintVariable. The solution involves a two-way class adapter ConstraintStateVariable, a subclass of both StateVariable and ConstraintVariable, that adapts the two interfaces to each other. Multiple inheritance is a viable solution in this case because the interfaces of the adapted classes are substantially different. The two-way class adapter conforms to both of the adapted classes and can work in either system. Although the implementation of Adapter is usually straightforward, here are some issues to keep in mind: Implementing class adapters in C++. In a C++ implementation of a class adapter, Adapter would inherit publicly from Target and privately from Adaptee. Thus Adapter would be a subtype of Target but not of Adaptee. Pluggable adapters. Let's look at three ways to implement pluggable adapters for the TreeDisplay widget described earlier, which can lay out and display a hierarchical structure automatically. The first step, which is common to all three of the implementations discussed here, is to find a "narrow" interface for Adaptee, that is, the smallest subset of operations that lets us do the adaptation. A narrow interface consisting of only a couple of operations is easier to adapt than an interface with dozens of operations. For TreeDisplay, the adaptee is any hierarchical structure. A minimalist interface might include two operations, one that defines how to present a node in the hierarchical structure graphically, and another that retrieves the node's children. The narrow interface leads to three implementation approaches: a. Using abstract operations. Define corresponding abstract operations for the narrow Adaptee interface in the TreeDisplay class. Subclasses must implement the abstract operations and adapt the hierarchically structured object. For example, a DirectoryTreeDisplay subclass will implement these operations by accessing the directory structure. DirectoryTreeDisplay specializes the narrow interface so that it can display directory structures made up of FileSystemEntity objects. Using delegate objects. In this approach, TreeDisplay forwards requests for accessing the hierarchical structure to a delegate object. TreeDisplay can use a different adaptation strategy by substituting a different delegate. For example, suppose there exists a DirectoryBrowser that uses a TreeDisplay. DirectoryBrowser might make a good delegate for adapting TreeDisplay to the hierarchical directory structure. In dynamically typed languages like Smalltalk or Objective C, this approach only requires an interface for registering the delegate with the adapter. Then TreeDisplay simply forwards the requests to the delegate. NEXTSTEP [Add94] uses this approach heavily to reduce subclassing. Statically typed languages like C++ require an explicit interface definition for the delegate. We can specify such an interface by putting the narrow interface that TreeDisplay requires into an abstract TreeAccessorDelegate class. Then we can mix this interface into the delegate of our choice DirectoryBrowser in this case using inheritance. We use single inheritance if the DirectoryBrowser has no existing parent class, multiple inheritance if it does. Mixing classes together like this is easier than introducing a new TreeDisplay subclass and implementing its operations individually. Parameterized adapters. The usual way to support pluggable adapters in Smalltalk is to parameterize an adapter with one or more blocks. The block construct supports adaptation without subclassing. A block can adapt a request, and the adapter can store a block for each individual request. In our example, this means TreeDisplay stores one block for converting a node into a GraphicNode and another block for accessing a node's children. We'll give a brief sketch of the implementation of class and object adapters for the Motivation example beginning with the classes Shape and TextView. Shape assumes a bounding box defined by its opposing corners. In contrast, TextView is defined by an origin, height, and width. Shape also defines a CreateManipulator operation for creating a Manipulator object, which knows how to animate a shape when the user manipulates it.1 TextView has no equivalent operation. The class TextShape is an adapter between these different interfaces. A class adapter uses multiple inheritance to adapt interfaces. The key to class adapters is to use one inheritance branch to inherit the interface and another branch to inherit the implementation. The usual way to make this distinction in C++ is to inherit the interface publicly and inherit the implementation privately. We'll use this convention to define the TextShape adapter. The BoundingBox operation converts TextView's interface to conform to Shape's. The IsEmpty operation demonstrates the direct forwarding of requests common in adapter implementations: Finally, we define CreateManipulator (which isn't supported by TextView) from scratch. Assume we've already implemented a TextManipulator class that supports manipulation of a TextShape. The object adapter uses object composition to combine classes with different interfaces. In this approach, the adapter TextShape maintains a pointer to TextView. TextShape must initialize the pointer to the TextView instance, and it does so in the constructor. It must also call operations on its TextView object whenever its own operations are called. In this example, assume that the client creates the TextView object and passes it to the TextShape constructor: CreateManipulator's implementation doesn't change from the class adapter version, since it's implemented from scratch and doesn't reuse any existing TextView functionality. Compare this code to the class adapter case. The object adapter requires a little more effort to write, but it's more flexible. For example, the object adapter version of TextShape will work equally well with subclasses of TextView the client simply passes an instance of a TextView subclass to the TextShape constructor. The Motivation example comes from ET++Draw, a drawing application based on ET++ [WGM88]. ET++Draw reuses the ET++ classes for text editing by using a TextShape adapter class. InterViews 2.6 defines an Interactor abstract class for user interface elements such as scroll bars, buttons, and menus [VL88]. It also defines a Graphic abstract class for structured graphic objects such as lines, circles, polygons, and splines. Both Interactors and Graphics have graphical appearances, but they have different interfaces and implementations (they share no common parent class) and are therefore incompatible¡ªyou can't embed a structured graphic object in, say, a dialog box directly. Instead, InterViews 2.6 defines an object adapter called GraphicBlock, a subclass of Interactor that contains a Graphic instance. The GraphicBlock adapts the interface of the Graphic class to that of Interactor. The GraphicBlock lets a Graphic instance be displayed, scrolled, and zoomed within an Interactor structure. Pluggable adapters are common in ObjectWorks\Smalltalk [Par90]. Standard Smalltalk defines a ValueModel class for views that display a single value. ValueModel defines a value, value: interface for accessing the value. These are abstract methods. Application writers access the value with more domain-specific names like width and width:, but they shouldn't have to subclass ValueModel to adapt such application-specific names to the ValueModel interface. Instead, ObjectWorks\Smalltalk includes a subclass of ValueModel called PluggableAdaptor. A PluggableAdaptor object adapts other objects to the ValueModel interface (value, value:). It can be parameterized with blocks for getting and setting the desired value. PluggableAdaptor uses these blocks internally to implement the value, value: interface. PluggableAdaptor also lets you pass in the selector names (e.g., width, width:) directly for syntactic convenience. It converts these selectors into the corresponding blocks automatically. Another example from ObjectWorks\Smalltalk is the TableAdaptor class. A TableAdaptor can adapt a sequence of objects to a tabular presentation. The table displays one object per row. The client parameterizes TableAdaptor with the set of messages that a table can use to get the column values from an object. Some classes in NeXT's AppKit [Add94] use delegate objects to perform interface adaptation. An example is the NXBrowser class that can display hierarchical lists of data. NXBrowser uses a delegate object for accessing and adapting the data. Meyer's "Marriage of Convenience" [Mey88] is a form of class adapter. Meyer describes how a FixedStack class adapts the implementation of an Array class to the interface of a Stack class. The result is a stack containing a fixed number of entries. Bridge (171) has a structure similar to an object adapter, but Bridge has a different intent: It is meant to separate an interface from its implementation so that they can be varied easily and independently. An adapter is meant to change the interface of an existing object. Decorator (196) enhances another object without changing its interface. A decorator is thus more transparent to the application than an adapter is. As a consequence, Decorator supports recursive composition, which isn't possible with pure adapters. Proxy (233) defines a representative or surrogate for another object and does not change its interface.
##%%&&
Decouple an abstraction from its implementation so that the two can vary independently. When an abstraction can have one of several possible implementations, the usual way to accommodate them is to use inheritance. An abstract class defines the interface to the abstraction, and concrete subclasses implement it in different ways. But this approach isn't always flexible enough. Inheritance binds an implementation to the abstraction permanently, which makes it difficult to modify, extend, and reuse abstractions and implementations independently. Consider the implementation of a portable Window abstraction in a user interface toolkit. This abstraction should enable us to write applications that work on both the X Window System and IBM's Presentation Manager (PM), for example. Using inheritance, we could define an abstract class Window and subclasses XWindow and PMWindow that implement the Window interface for the different platforms. But this approach has two drawbacks: It's inconvenient to extend the Window abstraction to cover different kinds of windows or new platforms. Imagine an IconWindow subclass of Window that specializes the Window abstraction for icons. To support IconWindows for both platforms, we have to implement two new classes, XIconWindow and PMIconWindow. Worse, we'll have to define two classes for every kind of window. Supporting a third platform requires yet another new Window subclass for every kind of window. It makes client code platform-dependent. Whenever a client creates a window, it instantiates a concrete class that has a specific implementation. For example, creating an XWindow object binds the Window abstraction to the X Window implementation, which makes the client code dependent on the X Window implementation. This, in turn, makes it harder to port the client code to other platforms. Clients should be able to create a window without committing to a concrete implementation. Only the window implementation should depend on the platform on which the application runs. Therefore client code should instantiate windows without mentioning specific platforms. The Bridge pattern addresses these problems by putting the Window abstraction and its implementation in separate class hierarchies. There is one class hierarchy for window interfaces (Window, IconWindow, TransientWindow) and a separate hierarchy for platform-specific window implementations, with WindowImp as its root. The XWindowImp subclass, for example, provides an implementation based on the X Window System. All operations on Window subclasses are implemented in terms of abstract operations from the WindowImp interface. This decouples the window abstractions from the various platform-specific implementations. We refer to the relationship between Window and WindowImp as a bridge, because it bridges the abstraction and its implementation, letting them vary independently. Use the Bridge pattern when?you want to avoid a permanent binding between an abstraction and its implementation. This might be the case, for example, when the implementation must be selected or switched at run-time.?both the abstractions and their implementations should be extensible by subclassing. In this case, the Bridge pattern lets you combine the different abstractions and implementations and extend them independently.?changes in the implementation of an abstraction should have no impact on clients; that is, their code should not have to be recompiled.?(C++) you want to hide the implementation of an abstraction completely from clients. In C++ the representation of a class is visible in the class interface.?you have a proliferation of classes as shown earlier in the first Motivation diagram. Such a class hierarchy indicates the need for splitting an object into two parts. Rumbaugh uses the term "nested generalizations" [RBP+91] to refer to such class hierarchies.?you want to share an implementation among multiple objects (perhaps using reference counting), and this fact should be hidden from the client. A simple example is Coplien's String class [Cop92], in which multiple objects can share the same string representation (StringRep). Abstraction (Window) defines the abstraction's interface. maintains a reference to an object of type Implementor.?RefinedAbstraction (IconWindow) Extends the interface defined by Abstraction.?Implementor (WindowImp) defines the interface for implementation classes. This interface doesn't have to correspond exactly to Abstraction's interface; in fact the two interfaces can be quite different. Typically the Implementor interface provides only primitive operations, and Abstraction defines higher-level operations based on these primitives.?ConcreteImplementor (XWindowImp, PMWindowImp) implements the Implementor interface and defines its concrete implementation. Abstraction forwards client requests to its Implementor object. The Bridge pattern has the following consequences: Decoupling interface and implementation. An implementation is not bound permanently to an interface. The implementation of an abstraction can be configured at run-time. It's even possible for an object to change its implementation at run-time. Decoupling Abstraction and Implementor also eliminates compile-time dependencies on the implementation. Changing an implementation class doesn't require recompiling the Abstraction class and its clients. This property is essential when you must ensure binary compatibility between different versions of a class library. Furthermore, this decoupling encourages layering that can lead to a better-structured system. The high-level part of a system only has to know about Abstraction and Implementor. Improved extensibility. You can extend the Abstraction and Implementor hierarchies independently. Hiding implementation details from clients. You can shield clients from implementation details, like the sharing of implementor objects and the accompanying reference count mechanism (if any). Consider the following implementation issues when applying the Bridge pattern: Only one Implementor. In situations where there's only one implementation, creating an abstract Implementor class isn't necessary. This is a degenerate case of the Bridge pattern; there's a one-to-one relationship between Abstraction and Implementor. Nevertheless, this separation is still useful when a change in the implementation of a class must not affect its existing clients¡ªthat is, they shouldn't have to be recompiled, just relinked. Carolan [Car89] uses the term "Cheshire Cat" to describe this separation. In C++, the class interface of the Implementor class can be defined in a private header file that isn't provided to clients. This lets you hide an implementation of a class completely from its clients. Creating the right Implementor object. How, when, and where do you decide which Implementor class to instantiate when there's more than one? If Abstraction knows about all ConcreteImplementor classes, then it can instantiate one of them in its constructor; it can decide between them based on parameters passed to its constructor. If, for example, a collection class supports multiple implementations, the decision can be based on the size of the collection. A linked list implementation can be used for small collections and a hash table for larger ones. Another approach is to choose a default implementation initially and change it later according to usage. For example, if the collection grows bigger than a certain threshold, then it switches its implementation to one that's more appropriate for a large number of items. It's also possible to delegate the decision to another object altogether. In the Window/WindowImp example, we can introduce a factory object (see Abstract Factory (99)) whose sole duty is to encapsulate platform-specifics. The factory knows what kind of WindowImp object to create for the platform in use; a Window simply asks it for a WindowImp, and it returns the right kind. A benefit of this approach is that Abstraction is not coupled directly to any of the Implementor classes. Sharing implementors. Coplien illustrates how the Handle/Body idiom in C++ can be used to share implementations among several objects [Cop92]. The Body stores a reference count that the Handle class increments and decrements. The code for assigning handles with shared bodies has the following general form: Using multiple inheritance. You can use multiple inheritance in C++ to combine an interface with its implementation [Mar91]. For example, a class can inherit publicly from Abstraction and privately from a ConcreteImplementor. But because this approach relies on static inheritance, it binds an implementation permanently to its interface. Therefore you can't implement a true Bridge with multiple inheritance¡ªat least not in C++. The following C++ code implements the Window/WindowImp example from the Motivation section. The Window class defines the window abstraction for client applications: Window maintains a reference to a WindowImp, the abstract class that declares an interface to the underlying windowing system. Subclasses of Window define the different kinds of windows the application might use, such as application windows, icons, transient windows for dialogs, floating palettes of tools, and so on. For example, ApplicationWindow will implement DrawContents to draw the View instance it stores: Many other variations of Window are possible. A TransientWindow may need to communicate with the window that created it during the dialog; hence it keeps a reference to that window. A PaletteWindow always floats above other windows. An IconDockWindow holds IconWindows and arranges them neatly. Window operations are defined in terms of the WindowImp interface. For example, DrawRect extracts four coordinates from its two Point parameters before calling the WindowImp operation that draws the rectangle in the window: Concrete subclasses of WindowImp support different window systems. The XWindowImp subclass supports the X Window System: For Presentation Manager (PM), we define a PMWindowImp class: These subclasses implement WindowImp operations in terms of window system primitives. For example, DeviceRect is implemented for X as follows: How does a window obtain an instance of the right WindowImp subclass? We'll assume Window has that responsibility in this example. Its GetWindowImp operation gets the right instance from an abstract factory (see Abstract Factory (99)) that effectively encapsulates all window system specifics. WindowSystemFactory::Instance() returns an abstract factory that manufactures all window system-specific objects. For simplicity, we've made it a Singleton (144) and have let the Window class access the factory directly. The Window example above comes from ET++ [WGM88]. In ET++, WindowImp is called "WindowPort" and has subclasses such as XWindowPort and SunWindowPort. The Window object creates its corresponding Implementor object by requesting it from an abstract factory called "WindowSystem." WindowSystem provides an interface for creating platform-specific objects such as fonts, cursors, bitmaps, and so forth. The ET++ Window/WindowPort design extends the Bridge pattern in that the WindowPort also keeps a reference back to the Window. The WindowPort implementor class uses this reference to notify Window about WindowPort-specific events: the arrival of input events, window resizes, etc. Both Coplien [Cop92] and Stroustrup [Str91] mention Handle classes and give some examples. Their examples emphasize memory management issues like sharing string representations and support for variable-sized objects. Our focus is more on supporting independent extension of both an abstraction and its implementation. libg++ [Lea88] defines classes that implement common data structures, such as Set, LinkedSet, HashSet, LinkedList, and HashTable. Set is an abstract class that defines a set abstraction, while LinkedList and HashTable are concrete implementors for a linked list and a hash table, respectively. LinkedSet and HashSet are Set implementors that bridge between Set and their concrete counterparts LinkedList and HashTable. This is an example of a degenerate bridge, because there's no abstract Implementor class. NeXT's AppKit [Add94] uses the Bridge pattern in the implementation and display of graphical images. An image can be represented in several different ways. The optimal display of an image depends on the properties of a display device, specifically its color capabilities and its resolution. Without help from AppKit, developers would have to determine which implementation to use under various circumstances in every application. To relieve developers of this responsibility, AppKit provides an NXImage/NXImageRep bridge. NXImage defines the interface for handling images. The implementation of images is defined in a separate NXImageRep class hierarchy having subclasses such as NXEPSImageRep, NXCachedImageRep, and NXBitMapImageRep. NXImage maintains a reference to one or more NXImageRep objects. If there is more than one image implementation, then NXImage selects the most appropriate one for the current display device. NXImage is even capable of converting one implementation to another if necessary. The interesting aspect of this Bridge variant is that NXImage can store more than one NXImageRep implementation at a time. An Abstract Factory (99) can create and configure a particular Bridge. The Adapter (157) pattern is geared toward making unrelated classes work together. It is usually applied to systems after they're designed. Bridge, on the other hand, is used up-front in a design to let abstractions and implementations vary independently.
##%%&&
Compose objects into tree structures to represent part-whole hierarchies. Composite lets clients treat individual objects and compositions of objects uniformly. Graphics applications like drawing editors and schematic capture systems let users build complex diagrams out of simple components. The user can group components to form larger components, which in turn can be grouped to form still larger components. A simple implementation could define classes for graphical primitives such as Text and Lines plus other classes that act as containers for these primitives. But there's a problem with this approach: Code that uses these classes must treat primitive and container objects differently, even if most of the time the user treats them identically. Having to distinguish these objects makes the application more complex. The Composite pattern describes how to use recursive composition so that clients don't have to make this distinction. The key to the Composite pattern is an abstract class that represents both primitives and their containers. For the graphics system, this class is Graphic. Graphic declares operations like Draw that are specific to graphical objects. It also declares operations that all composite objects share, such as operations for accessing and managing its children. The subclasses Line, Rectangle, and Text (see preceding class diagram) define primitive graphical objects. These classes implement Draw to draw lines, rectangles, and text, respectively. Since primitive graphics have no child graphics, none of these subclasses implements child-related operations. The Picture class defines an aggregate of Graphic objects. Picture implements Draw to call Draw on its children, and it implements child-related operations accordingly. Because the Picture interface conforms to the Graphic interface, Picture objects can compose other Pictures recursively. The following diagram shows a typical composite object structure of recursively composed Graphic objects: Use the Composite pattern when?you want to represent part-whole hierarchies of objects.?you want clients to be able to ignore the difference between compositions of objects and individual objects. Clients will treat all objects in the composite structure uniformly. Component (Graphic) declares the interface for objects in the composition. implements default behavior for the interface common to all classes, as appropriate. declares an interface for accessing and managing its child components. (optional) defines an interface for accessing a component's parent in the recursive structure, and implements it if that's appropriate.?Leaf (Rectangle, Line, Text, etc.) represents leaf objects in the composition. A leaf has no children. defines behavior for primitive objects in the composition.?Composite (Picture) defines behavior for components having children.stores child components. implements child-related operations in the Component interface.?Client manipulates objects in the composition through the Component interface. Clients use the Component class interface to interact with objects in the composite structure. If the recipient is a Leaf, then the request is handled directly. If the recipient is a Composite, then it usually forwards requests to its child components, possibly performing additional operations before and/or after forwarding. The Composite pattern?defines class hierarchies consisting of primitive objects and composite objects. Primitive objects can be composed into more complex objects, which in turn can be composed, and so on recursively. Wherever client code expects a primitive object, it can also take a composite object.?makes the client simple. Clients can treat composite structures and individual objects uniformly. Clients normally don't know (and shouldn't care) whether they're dealing with a leaf or a composite component. This simplifies client code, because it avoids having to write tag-and-case-statement-style functions over the classes that define the composition.?makes it easier to add new kinds of components. Newly defined Composite or Leaf subclasses work automatically with existing structures and client code. Clients don't have to be changed for new Component classes.?can make your design overly general. The disadvantage of making it easy to add new components is that it makes it harder to restrict the components of a composite. Sometimes you want a composite to have only certain components. With Composite, you can't rely on the type system to enforce those constraints for you. You'll have to use run-time checks instead. There are many issues to consider when implementing the Composite pattern: Explicit parent references. Maintaining references from child components to their parent can simplify the traversal and management of a composite structure. The parent reference simplifies moving up the structure and deleting a component. Parent references also help support the Chain of Responsibility (251) pattern. The usual place to define the parent reference is in the Component class. Leaf and Composite classes can inherit the reference and the operations that manage it. With parent references, it's essential to maintain the invariant that all children of a composite have as their parent the composite that in turn has them as children. The easiest way to ensure this is to change a component's parent only when it's being added or removed from a composite. If this can be implemented once in the Add and Remove operations of the Composite class, then it can be inherited by all the subclasses, and the invariant will be maintained automatically.  Sharing components. It's often useful to share components, for example, to reduce storage requirements. But when a component can have no more than one parent, sharing components becomes difficult. A possible solution is for children to store multiple parents. But that can lead to ambiguities as a request propagates up the structure. The Flyweight (218) pattern shows how to rework a design to avoid storing parents altogether. It works in cases where children can avoid sending parent requests by externalizing some or all of their state. Maximizing the Component interface. One of the goals of the Composite pattern is to make clients unaware of the specific Leaf or Composite classes they're using. To attain this goal, the Component class should define as many common operations for Composite and Leaf classes as possible. The Component class usually provides default implementations for these operations, and Leaf and Composite subclasses will override them. However, this goal will sometimes conflict with the principle of class hierarchy design that says a class should only define operations that are meaningful to its subclasses. There are many operations that Component supports that don't seem to make sense for Leaf classes. How can Component provide a default implementation for them? Sometimes a little creativity shows how an operation that would appear to make sense only for Composites can be implemented for all Components by moving it to the Component class. For example, the interface for accessing children is a fundamental part of a Composite class but not necessarily Leaf classes. But if we view a Leaf as a Component that never has children, then we can define a default operation for child access in the Component class that never returns any children. Leaf classes can use the default implementation, but Composite classes will reimplement it to return their children. The child management operations are more troublesome and are discussed in the next item. Declaring the child management operations. Although the Composite class implements the Add and Remove operations for managing children, an important issue in the Composite pattern is which classes declare these operations in the Composite class hierarchy. Should we declare these operations in the Component and make them meaningful for Leaf classes, or should we declare and define them only in Composite and its subclasses? The decision involves a trade-off between safety and transparency: Defining the child management interface at the root of the class hierarchy gives you transparency, because you can treat all components uniformly. It costs you safety, however, because clients may try to do meaningless things like add and remove objects from leaves. Defining child management in the Composite class gives you safety, because any attempt to add or remove objects from leaves will be caught at compile-time in a statically typed language like C++. But you lose transparency, because leaves and composites have different interfaces. We have emphasized transparency over safety in this pattern. If you opt for safety, then at times you may lose type information and have to convert a component into a composite. How can you do this without resorting to a type-unsafe cast? One approach is to declare an operation Composite* GetComposite() in the Component class. Component provides a default operation that returns a null pointer. The Composite class redefines this operation to return itself through the this pointer: Similar tests for a Composite can be done using the C++ dynamic_cast construct. Of course, the problem here is that we don't treat all components uniformly. We have to revert to testing for different types before taking the appropriate action. The only way to provide transparency is to define default Add and Remove operations in Component. That creates a new problem: There's no way to implement Component::Add without introducing the possibility of it failing. You could make it do nothing, but that ignores an important consideration; that is, an attempt to add something to a leaf probably indicates a bug. In that case, the Add operation produces garbage. You could make it delete its argument, but that might not be what clients expect. Usually it's better to make Add and Remove fail by default (perhaps by raising an exception) if the component isn't allowed to have children or if the argument of Remove isn't a child of the component, respectively. Another alternative is to change the meaning of "remove" slightly. If the component maintains a parent reference, then we could redefine Component::Remove to remove itself from its parent. However, there still isn't a meaningful interpretation for a corresponding Add. Should Component implement a list of Components? You might be tempted to define the set of children as an instance variable in the Component class where the child access and management operations are declared. But putting the child pointer in the base class incurs a space penalty for every leaf, even though a leaf never has children. This is worthwhile only if there are relatively few children in the structure. Child ordering. Many designs specify an ordering on the children of Composite. In the earlier Graphics example, ordering may reflect front-to-back ordering. If Composites represent parse trees, then compound statements can be instances of a Composite whose children must be ordered to reflect the program. When child ordering is an issue, you must design child access and management interfaces carefully to manage the sequence of children. The Iterator (289) pattern can guide you in this. Caching to improve performance. If you need to traverse or search compositions frequently, the Composite class can cache traversal or search information about its children. The Composite can cache actual results or just information that lets it short-circuit the traversal or search. For example, the Picture class from the Motivation example could cache the bounding box of its children. During drawing or selection, this cached bounding box lets the Picture avoid drawing or searching when its children aren't visible in the current window. Changes to a component will require invalidating the caches of its parents. This works best when components know their parents. So if you're using caching, you need to define an interface for telling composites that their caches are invalid. Who should delete components? In languages without garbage collection, it's usually best to make a Composite responsible for deleting its children when it's destroyed. An exception to this rule is when Leaf objects are immutable and thus can be shared. What's the best data structure for storing components? Composites may use a variety of data structures to store their children, including linked lists, trees, arrays, and hash tables. The choice of data structure depends (as always) on efficiency. In fact, it isn't even necessary to use a general-purpose data structure at all. Sometimes composites have a variable for each child, although this requires each subclass of Composite to implement its own management interface. See Interpreter (274) for an example. Equipment such as computers and stereo components are often organized into part-whole or containment hierarchies. For example, a chassis can contain drives and planar boards, a bus can contain cards, and a cabinet can contain chassis, buses, and so forth. Such structures can be modeled naturally with the Composite pattern. Equipment class defines an interface for all equipment in the part-whole hierarchy. Equipment declares operations that return the attributes of a piece of equipment, like its power consumption and cost. Subclasses implement these operations for specific kinds of equipment. Equipment also declares a CreateIterator operation that returns an Iterator (see Appendix C) for accessing its parts. The default implementation for this operation returns a NullIterator, which iterates over the empty set. Subclasses of Equipment might include Leaf classes that represent disk drives, integrated circuits, and switches: CompositeEquipment is the base class for equipment that contains other equipment. It's also a subclass of Equipment. CompositeEquipment defines the operations for accessing and managing subequipment. The operations Add and Remove insert and delete equipment from the list of equipment stored in the _equipment member. The operation CreateIterator returns an iterator (specifically, an instance of ListIterator) that will traverse this list. A default implementation of NetPrice might use CreateIterator to sum the net prices of the subequipment: Now we can represent a computer chassis as a subclass of CompositeEquipment called Chassis. Chassis inherits the child-related operations from CompositeEquipment. We can define other equipment containers such as Cabinet and Bus in a similar way. That gives us everything we need to assemble equipment into a (pretty simple) personal computer: Examples of the Composite pattern can be found in almost all object-oriented systems. The original View class of Smalltalk Model/View/Controller [KP88] was a Composite, and nearly every user interface toolkit or framework has followed in its steps, including ET++ (with its VObjects [WGM88]) and InterViews (Styles [LCI+92], Graphics [VL88], and Glyphs [CL90]). It's interesting to note that the original View of Model/View/Controller had a set of subviews; in other words, View was both the Component class and the Composite class. Release 4.0 of Smalltalk-80 revised Model/View/Controller with a VisualComponent class that has subclasses View and CompositeView. The RTL Smalltalk compiler framework [JML92] uses the Composite pattern extensively. RTLExpression is a Component class for parse trees. It has subclasses, such as BinaryExpression, that contain child RTLExpression objects. These classes define a composite structure for parse trees. RegisterTransfer is the Component class for a program's intermediate Single Static Assignment (SSA) form. Leaf subclasses of RegisterTransfer define different static assignments such as?primitive assignments that perform an operation on two registers and assign the result to a third;?an assignment with a source register but no destination register, which indicates that the register is used after a routine returns; and?an assignment with a destination register but no source, which indicates that the register is assigned before the routine starts. Another subclass, RegisterTransferSet, is a Composite class for representing assignments that change several registers at once. Another example of this pattern occurs in the financial domain, where a portfolio aggregates individual assets. You can support complex aggregations of assets by implementing a portfolio as a Composite that conforms to the interface of an individual asset [BE93]. The Command (263) pattern describes how Command objects can be composed and sequenced with a MacroCommand Composite class. Often the component-parent link is used for a Chain of Responsibility (251). Decorator (196) is often used with Composite. When decorators and composites are used together, they will usually have a common parent class. So decorators will have to support the Component interface with operations like Add, Remove, and GetChild. Flyweight (218) lets you share components, but they can no longer refer to their parents. Iterator (289) can be used to traverse composites. Visitor (366) localizes operations and behavior that would otherwise be distributed across Composite and Leaf classes.
##%%&&
Attach additional responsibilities to an object dynamically. Decorators provide a flexible alternative to subclassing for extending functionality. Sometimes we want to add responsibilities to individual objects, not to an entire class. A graphical user interface toolkit, for example, should let you add properties like borders or behaviors like scrolling to any user interface component. One way to add responsibilities is with inheritance. Inheriting a border from another class puts a border around every subclass instance. This is inflexible, however, because the choice of border is made statically. A client can't control how and when to decorate the component with a border. A more flexible approach is to enclose the component in another object that adds the border. The enclosing object is called a decorator. The decorator conforms to the interface of the component it decorates so that its presence is transparent to the component's clients. The decorator forwards requests to the component and may perform additional actions (such as drawing a border) before or after forwarding. Transparency lets you nest decorators recursively, thereby allowing an unlimited number of added responsibilities. For example, suppose we have a TextView object that displays text in a window. TextView has no scroll bars by default, because we might not always need them. When we do, we can use a ScrollDecorator to add them. Suppose we also want to add a thick black border around the TextView. We can use a BorderDecorator to add this as well. We simply compose the decorators with the TextView to produce the desired result. The following object diagram shows how to compose a TextView object with BorderDecorator and ScrollDecorator objects to produce a bordered, scrollable text view: The ScrollDecorator and BorderDecorator classes are subclasses of Decorator, an abstract class for visual components that decorate other visual components. VisualComponent is the abstract class for visual objects. It defines their drawing and event handling interface. Note how the Decorator class simply forwards draw requests to its component, and how Decorator subclasses can extend this operation. Decorator subclasses are free to add operations for specific functionality. For example, ScrollDecorator's ScrollTo operation lets other objects scroll the interface if they know there happens to be a ScrollDecorator object in the interface. The important aspect of this pattern is that it lets decorators appear anywhere a VisualComponent can. That way clients generally can't tell the difference between a decorated component and an undecorated one, and so they don't depend at all on the decoration. Use Decorator?to add responsibilities to individual objects dynamically and transparently, that is, without affecting other objects.?for responsibilities that can be withdrawn.?when extension by subclassing is impractical. Sometimes a large number of independent extensions are possible and would produce an explosion of subclasses to support every combination. Or a class definition may be hidden or otherwise unavailable for subclassing. Component (VisualComponent) defines the interface for objects that can have responsibilities added to them dynamically.?ConcreteComponent (TextView) defines an object to which additional responsibilities can be attached.?Decorator maintains a reference to a Component object and defines an interface that conforms to Component's interface.?ConcreteDecorator (BorderDecorator, ScrollDecorator) adds responsibilities to the component. Decorator forwards requests to its Component object. It may optionally perform additional operations before and after forwarding the request. The Decorator pattern has at least two key benefits and two liabilities: More flexibility than static inheritance. The Decorator pattern provides a more flexible way to add responsibilities to objects than can be had with static (multiple) inheritance. With decorators, responsibilities can be added and removed at run-time simply by attaching and detaching them. In contrast, inheritance requires creating a new class for each additional responsibility (e.g., BorderedScrollableTextView, BorderedTextView). This gives rise to many classes and increases the complexity of a system. Furthermore, providing different Decorator classes for a specific Component class lets you mix and match responsibilities. Decorators also make it easy to add a property twice. For example, to give a TextView a double border, simply attach two BorderDecorators. Inheriting from a Border class twice is error-prone at best. Avoids feature-laden classes high up in the hierarchy. Decorator offers a pay-as-you-go approach to adding responsibilities. Instead of trying to support all foreseeable features in a complex, customizable class, you can define a simple class and add functionality incrementally with Decorator objects. Functionality can be composed from simple pieces. As a result, an application needn't pay for features it doesn't use. It's also easy to define new kinds of Decorators independently from the classes of objects they extend, even for unforeseen extensions. Extending a complex class tends to expose details unrelated to the responsibilities you're adding. A decorator and its component aren't identical. A decorator acts as a transparent enclosure. But from an object identity point of view, a decorated component is not identical to the component itself. Hence you shouldn't rely on object identity when you use decorators. Lots of little objects. A design that uses Decorator often results in systems composed of lots of little objects that all look alike. The objects differ only in the way they are interconnected, not in their class or in the value of their variables. Although these systems are easy to customize by those who understand them, they can be hard to learn and debug. Several issues should be considered when applying the Decorator pattern: Interface conformance. A decorator object's interface must conform to the interface of the component it decorates. ConcreteDecorator classes must therefore inherit from a common class (at least in C++). Omitting the abstract Decorator class. There's no need to define an abstract Decorator class when you only need to add one responsibility. That's often the case when you're dealing with an existing class hierarchy rather than designing a new one. In that case, you can merge Decorator's responsibility for forwarding requests to the component into the ConcreteDecorator. Keeping Component classes lightweight. To ensure a conforming interface, components and decorators must descend from a common Component class. It's important to keep this common class lightweight; that is, it should focus on defining an interface, not on storing data. The definition of the data representation should be deferred to subclasses; otherwise the complexity of the Component class might make the decorators too heavyweight to use in quantity. Putting a lot of functionality into Component also increases the probability that concrete subclasses will pay for features they don't need. Changing the skin of an object versus changing its guts. We can think of a decorator as a skin over an object that changes its behavior. An alternative is to change the object's guts. The Strategy (349) pattern is a good example of a pattern for changing the guts. Strategies are a better choice in situations where the Component class is intrinsically heavyweight, thereby making the Decorator pattern too costly to apply. In the Strategy pattern, the component forwards some of its behavior to a separate strategy object. The Strategy pattern lets us alter or extend the component's functionality by replacing the strategy object. For example, we can support different border styles by having the component defer border-drawing to a separate Border object. The Border object is a Strategy object that encapsulates a border-drawing strategy. By extending the number of strategies from just one to an open-ended list, we achieve the same effect as nesting decorators recursively. In MacApp 3.0 [App89] and Bedrock [Sym93a], for example, graphical components (called "views") maintain a list of "adorner" objects that can attach additional adornments like borders to a view component. If a view has any adorners attached, then it gives them a chance to draw additional embellishments. MacApp and Bedrock must use this approach because the View class is heavyweight. It would be too expensive to use a full-fledged View just to add a border. Since the Decorator pattern only changes a component from the outside, the component doesn't have to know anything about its decorators; that is, the decorators are transparent to the component: With strategies, the component itself knows about possible extensions. So it has to reference and maintain the corresponding strategies: The Strategy-based approach might require modifying the component to accommodate new extensions. On the other hand, a strategy can have its own specialized interface, whereas a decorator's interface must conform to the component's. A strategy for rendering a border, for example, need only define the interface for rendering a border (DrawBorder, GetWidth, etc.), which means that the strategy can be lightweight even if the Component class is heavyweight. MacApp and Bedrock use this approach for more than just adorning views. They also use it to augment the event-handling behavior of objects. In both systems, a view maintains a list of "behavior" objects that can modify and intercept events. The view gives each of the registered behavior objects a chance to handle the event before nonregistered behaviors, effectively overriding them. You can decorate a view with special keyboard-handling support, for example, by registering a behavior object that intercepts and handles key events. The following code shows how to implement user interface decorators in C++. We'll assume there's a Component class called VisualComponent. We define a subclass of VisualComponent called Decorator, which we'll subclass to obtain different decorations. Decorator decorates the VisualComponent referenced by the _component instance variable, which is initialized in the constructor. For each operation in VisualComponent's interface, Decorator defines a default implementation that passes the request on to _component: Subclasses of Decorator define specific decorations. For example, the class BorderDecorator adds a border to its enclosing component. BorderDecorator is a subclass of Decorator that overrides the Draw operation to draw the border. BorderDecorator also defines a private DrawBorder helper operation that does the drawing. The subclass inherits all other operation implementations from Decorator. A similar implementation would follow for ScrollDecorator and DropShadowDecorator, which would add scrolling and drop shadow capabilities to a visual component. Now we can compose instances of these classes to provide different decorations. The following code illustrates how we can use decorators to create a bordered scrollable TextView. First, we need a way to put a visual component into a window object. We'll assume our Window class provides a SetContents operation for this purpose: Now we can create the text view and a window to put it in: TextView is a VisualComponent, which lets us put it into the window: But we want a bordered and scrollable TextView. So we decorate it accordingly before putting it in the window. Because Window accesses its contents through the VisualComponent interface, it's unaware of the decorator's presence. You, as the client, can still keep track of the text view if you have to interact with it directly, for example, when you need to invoke operations that aren't part of the VisualComponent interface. Clients that rely on the component's identity should refer to it directly as well. Many object-oriented user interface toolkits use decorators to add graphical embellishments to widgets. Examples include InterViews [LVC98, LCI+92], ET++ [WGM88], and the ObjectWorks\Smalltalk class library [Par90]. More exotic applications of Decorator are the DebuggingGlyph from InterViews and the PassivityWrapper from ParcPlace Smalltalk. A DebuggingGlyph prints out debugging information before and after it forwards a layout request to its component. This trace information can be used to analyze and debug the layout behavior of objects in a complex composition. The PassivityWrapper can enable or disable user interactions with the component. But the Decorator pattern is by no means limited to graphical user interfaces, as the following example (based on the ET++ streaming classes [WGM88]) illustrates. Streams are a fundamental abstraction in most I/O facilities. A stream can provide an interface for converting objects into a sequence of bytes or characters. That lets us transcribe an object to a file or to a string in memory for retrieval later. A straightforward way to do this is to define an abstract Stream class with subclasses MemoryStream and FileStream. But suppose we also want to be able to do the following:?Compress the stream data using different compression algorithms (run-length encoding, Lempel-Ziv, etc.).?Reduce the stream data to 7-bit ASCII characters so that it can be transmitted over an ASCII communication channel. The Decorator pattern gives us an elegant way to add these responsibilities to streams. The diagram below shows one solution to the problem: The Stream abstract class maintains an internal buffer and provides operations for storing data onto the stream (PutInt, PutString). Whenever the buffer is full, Stream calls the abstract operation HandleBufferFull, which does the actual data transfer. The FileStream version of this operation overrides this operation to transfer the buffer to a file. The key class here is StreamDecorator, which maintains a reference to a component stream and forwards requests to it. StreamDecorator subclasses override HandleBufferFull and perform additional actions before calling StreamDecorator's HandleBufferFull operation. For example, the CompressingStream subclass compresses the data, and the ASCII7Stream converts the data into 7-bit ASCII. Now, to create a FileStream that compresses its data and converts the compressed binary data to 7-bit ASCII, we decorate a FileStream with a CompressingStream and an ASCII7Stream: Adapter (157): A decorator is different from an adapter in that a decorator only changes an object's responsibilities, not its interface; an adapter will give an object a completely new interface. Composite (183): A decorator can be viewed as a degenerate composite with only one component. However, a decorator adds additional responsibilities¡ªit isn't intended for object aggregation. Strategy (349): A decorator lets you change the skin of an object; a strategy lets you change the guts. These are two alternative ways of changing an object.
##%%&&
Provide a unified interface to a set of interfaces in a subsystem. Facade defines a higher-level interface that makes the subsystem easier to use. Structuring a system into subsystems helps reduce complexity. A common design goal is to minimize the communication and dependencies between subsystems. One way to achieve this goal is to introduce a facade object that provides a single, simplified interface to the more general facilities of a subsystem. Consider for example a programming environment that gives applications access to its compiler subsystem. This subsystem contains classes such as Scanner, Parser, ProgramNode, BytecodeStream, and ProgramNodeBuilder that implement the compiler. Some specialized applications might need to access these classes directly. But most clients of a compiler generally don't care about details like parsing and code generation; they merely want to compile some code. For them, the powerful but low-level interfaces in the compiler subsystem only complicate their task. To provide a higher-level interface that can shield clients from these classes, the compiler subsystem also includes a Compiler class. This class defines a unified interface to the compiler's functionality. The Compiler class acts as a facade: It offers clients a single, simple interface to the compiler subsystem. It glues together the classes that implement compiler functionality without hiding them completely. The compiler facade makes life easier for most programmers without hiding the lower-level functionality from the few that need it. Use the Facade pattern when?you want to provide a simple interface to a complex subsystem. Subsystems often get more complex as they evolve. Most patterns, when applied, result in more and smaller classes. This makes the subsystem more reusable and easier to customize, but it also becomes harder to use for clients that don't need to customize it. A facade can provide a simple default view of the subsystem that is good enough for most clients. Only clients needing more customizability will need to look beyond the facade.?there are many dependencies between clients and the implementation classes of an abstraction. Introduce a facade to decouple the subsystem from clients and other subsystems, thereby promoting subsystem independence and portability.?you want to layer your subsystems. Use a facade to define an entry point to each subsystem level. If subsystems are dependent, then you can simplify the dependencies between them by making them communicate with each other solely through their facades. Facade (Compiler) knows which subsystem classes are responsible for a request. delegates client requests to appropriate subsystem objects.?subsystem classes (Scanner, Parser, ProgramNode, etc.) implement subsystem functionality. handle work assigned by the Facade object. have no knowledge of the facade; that is, they keep no references to it. Clients communicate with the subsystem by sending requests to Facade, which forwards them to the appropriate subsystem object(s). Although the subsystem objects perform the actual work, the facade may have to do work of its own to translate its interface to subsystem interfaces.?Clients that use the facade don't have to access its subsystem objects directly. The Facade pattern offers the following benefits: It shields clients from subsystem components, thereby reducing the number of objects that clients deal with and making the subsystem easier to use. It promotes weak coupling between the subsystem and its clients. Often the components in a subsystem are strongly coupled. Weak coupling lets you vary the components of the subsystem without affecting its clients. Facades help layer a system and the dependencies between objects. They can eliminate complex or circular dependencies. This can be an important consequence when the client and the subsystem are implemented independently. Reducing compilation dependencies is vital in large software systems. You want to save time by minimizing recompilation when subsystem classes change. Reducing compilation dependencies with facades can limit the recompilation needed for a small change in an important subsystem. A facade can also simplify porting systems to other platforms, because it's less likely that building one subsystem requires building all others. It doesn't prevent applications from using subsystem classes if they need to. Thus you can choose between ease of use and generality. Consider the following issues when implementing a facade: Reducing client-subsystem coupling. The coupling between clients and the subsystem can be reduced even further by making Facade an abstract class with concrete subclasses for different implementations of a subsystem. Then clients can communicate with the subsystem through the interface of the abstract Facade class. This abstract coupling keeps clients from knowing which implementation of a subsystem is used. An alternative to subclassing is to configure a Facade object with different subsystem objects. To customize the facade, simply replace one or more of its subsystem objects. Public versus private subsystem classes. A subsystem is analogous to a class in that both have interfaces, and both encapsulate something a class encapsulates state and operations, while a subsystem encapsulates classes. And just as it's useful to think of the public and private interface of a class, we can think of the public and private interface of a subsystem. The public interface to a subsystem consists of classes that all clients can access; the private interface is just for subsystem extenders. The Facade class is part of the public interface, of course, but it's not the only part. Other subsystem classes are usually public as well. For example, the classes Parser and Scanner in the compiler subsystem are part of the public interface. Making subsystem classes private would be useful, but few object-oriented languages support it. Both C++ and Smalltalk traditionally have had a global name space for classes. Recently, however, the C++ standardization committee added name spaces to the language [Str94], which will let you expose just the public subsystem classes. Let's take a closer look at how to put a facade on a compiler subsystem. The compiler subsystem defines a {BytecodeStream} class that implements a stream of Bytecode objects. A Bytecode object encapsulates a bytecode, which can specify machine instructions. The subsystem also defines a Token class for objects that encapsulate tokens in the programming language. The Scanner class takes a stream of characters and produces a stream of tokens, one token at a time. The class Parser uses a ProgramNodeBuilder to construct a parse tree from a Scanner's tokens. Parser calls back on ProgramNodeBuilder to build the parse tree incrementally. These classes interact according to the Builder (110) pattern. The parse tree is made up of instances of ProgramNode subclasses such as StatementNode, ExpressionNode, and so forth. The ProgramNode hierarchy is an example of the Composite (183) pattern. ProgramNode defines an interface for manipulating the program node and its children, if any. The Traverse operation takes a CodeGenerator object. ProgramNode subclasses use this object to generate machine code in the form of Bytecode objects on a BytecodeStream. The class CodeGenerator is a visitor (see Visitor (366)). CodeGenerator has subclasses, for example, StackMachineCodeGenerator and RISCCodeGenerator, that generate machine code for different hardware architectures. Each subclass of ProgramNode implements Traverse to call Traverse on its child ProgramNode objects. In turn, each child does the same for its children, and so on recursively. For example, ExpressionNode defines Traverse as follows: The classes we've discussed so far make up the compiler subsystem. Now we'll introduce a Compiler class, a facade that puts all these pieces together. Compiler provides a simple interface for compiling source and generating code for a particular machine. This implementation hard-codes the type of code generator to use so that programmers aren't required to specify the target architecture. That might be reasonable if there's only ever one target architecture. If that's not the case, then we might want to change the Compiler constructor to take a CodeGenerator parameter. Then programmers can specify the generator to use when they instantiate Compiler. The compiler facade can parameterize other participants such as Scanner and ProgramNodeBuilder as well, which adds flexibility, but it also detracts from the Facade pattern's mission, which is to simplify the interface for the common case. The compiler example in the Sample Code section was inspired by the ObjectWorks\Smalltalk compiler system [Par90]. In the ET++ application framework [WGM88], an application can have built-in browsing tools for inspecting its objects at run-time. These browsing tools are implemented in a separate subsystem that includes a Facade class called "ProgrammingEnvironment." This facade defines operations such as InspectObject and InspectClass for accessing the browsers. An ET++ application can also forgo built-in browsing support. In that case, ProgrammingEnvironment implements these requests as null operations; that is, they do nothing. Only the ETProgrammingEnvironment subclass implements these requests with operations that display the corresponding browsers. The application has no knowledge of whether a browsing environment is available or not; there's abstract coupling between the application and the browsing subsystem. The Choices operating system [CIRM93] uses facades to compose many frameworks into one. The key abstractions in Choices are processes, storage, and address spaces. For each of these abstractions there is a corresponding subsystem, implemented as a framework, that supports porting Choices to a variety of different hardware platforms. Two of these subsystems have a "representative" (i.e., facade). These representatives are FileSystemInterface (storage) and Domain (address spaces). For example, the virtual memory framework has Domain as its facade. A Domain represents an address space. It provides a mapping between virtual addresses and offsets into memory objects, files, or backing store. The main operations on Domain support adding a memory object at a particular address, removing a memory object, and handling a page fault. As the preceding diagram shows, the virtual memory subsystem uses the following components internally: MemoryObject represents a data store. MemoryObjectCache caches the data of MemoryObjects in physical memory. MemoryObjectCache is actually a Strategy (349) that localizes the caching policy.?AddressTranslation encapsulates the address translation hardware. The RepairFault operation is called whenever a page fault interrupt occurs. The Domain finds the memory object at the address causing the fault and delegates the RepairFault operation to the cache associated with that memory object. Domains can be customized by changing their components. Abstract Factory (99) can be used with Facade to provide an interface for creating subsystem objects in a subsystem-independent way. Abstract Factory can also be used as an alternative to Facade to hide platform-specific classes. Mediator (305) is similar to Facade in that it abstracts functionality of existing classes. However, Mediator's purpose is to abstract arbitrary communication between colleague objects, often centralizing functionality that doesn't belong in any one of them. A mediator's colleagues are aware of and communicate with the mediator instead of communicating with each other directly. In contrast, a facade merely abstracts the interface to subsystem objects to make them easier to use; it doesn't define new functionality, and subsystem classes don't know about it. Usually only one Facade object is required. Thus Facade objects are often Singletons (144).
##%%&&
Use sharing to support large numbers of fine-grained objects efficiently. Some applications could benefit from using objects throughout their design, but a naive implementation would be prohibitively expensive. For example, most document editor implementations have text formatting and editing facilities that are modularized to some extent. Object-oriented document editors typically use objects to represent embedded elements like tables and figures. However, they usually stop short of using an object for each character in the document, even though doing so would promote flexibility at the finest levels in the application. Characters and embedded elements could then be treated uniformly with respect to how they are drawn and formatted. The application could be extended to support new character sets without disturbing other functionality. The application's object structure could mimic the document's physical structure. The following diagram shows how a document editor can use objects to represent characters. The drawback of such a design is its cost. Even moderate-sized documents may require hundreds of thousands of character objects, which will consume lots of memory and may incur unacceptable run-time overhead. The Flyweight pattern describes how to share objects to allow their use at fine granularities without prohibitive cost. A flyweight is a shared object that can be used in multiple contexts simultaneously. The flyweight acts as an independent object in each context¡ªit's indistinguishable from an instance of the object that's not shared. Flyweights cannot make assumptions about the context in which they operate. The key concept here is the distinction between intrinsic and extrinsic state. Intrinsic state is stored in the flyweight; it consists of information that's independent of the flyweight's context, thereby making it sharable. Extrinsic state depends on and varies with the flyweight's context and therefore can't be shared. Client objects are responsible for passing extrinsic state to the flyweight when it needs it. Flyweights model concepts or entities that are normally too plentiful to represent with objects. For example, a document editor can create a flyweight for each letter of the alphabet. Each flyweight stores a character code, but its coordinate position in the document and its typographic style can be determined from the text layout algorithms and formatting commands in effect wherever the character appears. The character code is intrinsic state, while the other information is extrinsic. Logically there is an object for every occurrence of a given character in the document: A flyweight is a shared object that can be used in multiple contexts simultaneously. The flyweight acts as an independent object in each context it's indistinguishable from an instance of the object that's not shared. Flyweights cannot make assumptions about the context in which they operate. The key concept here is the distinction between intrinsic and extrinsic state. Intrinsic state is stored in the flyweight; it consists of information that's independent of the flyweight's context, thereby making it sharable. Extrinsic state depends on and varies with the flyweight's context and therefore can't be shared. Client objects are responsible for passing extrinsic state to the flyweight when it needs it. Flyweights model concepts or entities that are normally too plentiful to represent with objects. For example, a document editor can create a flyweight for each letter of the alphabet. Each flyweight stores a character code, but its coordinate position in the document and its typographic style can be determined from the text layout algorithms and formatting commands in effect wherever the character appears. The character code is intrinsic state, while the other information is extrinsic. Logically there is an object for every occurrence of a given character in the document: Physically, however, there is one shared flyweight object per character, and it appears in different contexts in the document structure. Each occurrence of a particular character object refers to the same instance in the shared pool of flyweight objects: The class structure for these objects is shown next. Glyph is the abstract class for graphical objects, some of which may be flyweights. Operations that may depend on extrinsic state have it passed to them as a parameter. For example, Draw and Intersects must know which context the glyph is in before they can do their job. A flyweight representing the letter "a" only stores the corresponding character code; it doesn't need to store its location or font. Clients supply the context-dependent information that the flyweight needs to draw itself. For example, a Row glyph knows where its children should draw themselves so that they are tiled horizontally. Thus it can pass each child its location in the draw request. Because the number of different character objects is far less than the number of characters in the document, the total number of objects is substantially less than what a naive implementation would use. A document in which all characters appear in the same font and color will allocate on the order of 100 character objects (roughly the size of the ASCII character set) regardless of the document's length. And since most documents use no more than 10 different font-color combinations, this number won't grow appreciably in practice. An object abstraction thus becomes practical for individual characters. The Flyweight pattern's effectiveness depends heavily on how and where it's used. Apply the Flyweight pattern when all of the following are true:?An application uses a large number of objects.?Storage costs are high because of the sheer quantity of objects.?Most object state can be made extrinsic.?Many groups of objects may be replaced by relatively few shared objects once extrinsic state is removed.?The application doesn't depend on object identity. Since flyweight objects may be shared, identity tests will return true for conceptually distinct objects. Flyweight declares an interface through which flyweights can receive and act on extrinsic state.?ConcreteFlyweight (Character) implements the Flyweight interface and adds storage for intrinsic state, if any. A ConcreteFlyweight object must be sharable. Any state it stores must be intrinsic; that is, it must be independent of the ConcreteFlyweight object's context.?UnsharedConcreteFlyweight (Row, Column) not all Flyweight subclasses need to be shared. The Flyweight interface enables sharing; it doesn't enforce it. It's common for UnsharedConcreteFlyweight objects to have ConcreteFlyweight objects as children at some level in the flyweight object structure (as the Row and Column classes have).?FlyweightFactory creates and manages flyweight objects. ensures that flyweights are shared properly. When a client requests a flyweight, the FlyweightFactory object supplies an existing instance or creates one, if none exists.?Client maintains a reference to flyweight(s). computes or stores the extrinsic state of flyweight(s). State that a flyweight needs to function must be characterized as either intrinsic or extrinsic. Intrinsic state is stored in the ConcreteFlyweight object; extrinsic state is stored or computed by Client objects. Clients pass this state to the flyweight when they invoke its operations.?Clients should not instantiate ConcreteFlyweights directly. Clients must obtain ConcreteFlyweight objects exclusively from the FlyweightFactory object to ensure they are shared properly. Flyweights may introduce run-time costs associated with transferring, finding, and/or computing extrinsic state, especially if it was formerly stored as intrinsic state. However, such costs are offset by space savings, which increase as more flyweights are shared. Storage savings are a function of several factors:?the reduction in the total number of instances that comes from sharing?the amount of intrinsic state per object?whether extrinsic state is computed or stored. The more flyweights are shared, the greater the storage savings. The savings increase with the amount of shared state. The greatest savings occur when the objects use substantial quantities of both intrinsic and extrinsic state, and the extrinsic state can be computed rather than stored. Then you save on storage in two ways: Sharing reduces the cost of intrinsic state, and you trade extrinsic state for computation time. The Flyweight pattern is often combined with the Composite (183) pattern to represent a hierarchical structure as a graph with shared leaf nodes. A consequence of sharing is that flyweight leaf nodes cannot store a pointer to their parent. Rather, the parent pointer is passed to the flyweight as part of its extrinsic state. This has a major impact on how the objects in the hierarchy communicate with each other. Consider the following issues when implementing the Flyweight pattern: Removing extrinsic state. The pattern's applicability is determined largely by how easy it is to identify extrinsic state and remove it from shared objects. Removing extrinsic state won't help reduce storage costs if there are as many different kinds of extrinsic state as there are objects before sharing. Ideally, extrinsic state can be computed from a separate object structure, one with far smaller storage requirements. In our document editor, for example, we can store a map of typographic information in a separate structure rather than store the font and type style with each character object. The map keeps track of runs of characters with the same typographic attributes. When a character draws itself, it receives its typographic attributes as a side-effect of the draw traversal. Because documents normally use just a few different fonts and styles, storing this information externally to each character object is far more efficient than storing it internally. Managing shared objects. Because objects are shared, clients shouldn't instantiate them directly. FlyweightFactory lets clients locate a particular flyweight. FlyweightFactory objects often use an associative store to let clients look up flyweights of interest. For example, the flyweight factory in the document editor example can keep a table of flyweights indexed by character codes. The manager returns the proper flyweight given its code, creating the flyweight if it does not already exist. Sharability also implies some form of reference counting or garbage collection to reclaim a flyweight's storage when it's no longer needed. However, neither is necessary if the number of flyweights is fixed and small (e.g., flyweights for the ASCII character set). In that case, the flyweights are worth keeping around permanently. Returning to our document formatter example, we can define a Glyph base class for flyweight graphical objects. Logically, glyphs are Composites (see Composite (183)) that have graphical attributes and can draw themselves. Here we focus on just the font attribute, but the same approach can be used for any other graphical attributes a glyph might have. To keep from allocating space for a font attribute in every glyph, we'll store the attribute extrinsically in a GlyphContext object. GlyphContext acts as a repository of extrinsic state. It maintains a compact mapping between a glyph and its font (and any other graphical attributes it might have) in different contexts. Any operation that needs to know the glyph's font in a given context will have a GlyphContext instance passed to it as a parameter. The operation can then query the GlyphContext for the font in that context. The context depends on the glyph's location in the glyph structure. Therefore Glyph's child iteration and manipulation operations must update the GlyphContext whenever they're used. GlyphContext must be kept informed of the current position in the glyph structure during traversal. GlyphContext::Next increments _index as the traversal proceeds. Glyph subclasses that have children (e.g., Row and Column) must implement Next so that it calls GlyphContext::Next at each point in the traversal. GlyphContext::GetFont uses the index as a key into a BTree structure that stores the glyph-to-font mapping. Each node in the tree is labeled with the length of the string for which it gives font information. Leaves in the tree point to a font, while interior nodes break the string into substrings, one for each child. Consider the following excerpt from a glyph composition: Interior nodes define ranges of glyph indices. BTree is updated in response to font changes and whenever glyphs are added to or removed from the glyph structure. For example, assuming we're at index 102 in the traversal, the following code sets the font of each character in the word "expect" to that of the surrounding text (that is, times12, an instance of Font for 12-point Times Roman): When the GlyphContext is queried for the font of the current glyph, it descends the BTree, adding up indices as it goes until it finds the font for the current index. Because the frequency of font changes is relatively low, the tree stays small relative to the size of the glyph structure. This keeps storage costs down without an inordinate increase in look-up time. The last object we need is a FlyweightFactory that creates glyphs and ensures they're shared properly. Class GlyphFactory instantiates Character and other kinds of glyphs. We only share Character objects; composite glyphs are far less plentiful, and their important state (i.e., their children) is intrinsic anyway. The _character array contains pointers to Character glyphs indexed by character code. The array is initialized to zero in the constructor. CreateCharacter looks up a character in the character glyph in the array, and it returns the corresponding glyph if it exists. If it doesn't, then CreateCharacter creates the glyph, puts it in the array, and returns it: The other operations simply instantiate a new object each time they're called, since noncharacter glyphs won't be shared: We could omit these operations and let clients instantiate unshared glyphs directly. However, if we decide to make these glyphs sharable later, we'll have to change client code that creates them. The concept of flyweight objects was first described and explored as a design technique in InterViews 3.0 [CL90]. Its developers built a powerful document editor called Doc as a proof of concept [CL92]. Doc uses glyph objects to represent each character in the document. The editor builds one Glyph instance for each character in a particular style (which defines its graphical attributes); hence a character's intrinsic state consists of the character code and its style information (an index into a style table).4 That means only position is extrinsic, making Doc fast. Documents are represented by a class Document, which also acts as the FlyweightFactory. Measurements on Doc have shown that sharing flyweight characters is quite effective. In a typical case, a document containing 180,000 characters required allocation of only 480 character objects. ET++ [WGM88] uses flyweights to support look-and-feel independence. The look-and-feel standard affects the layout of user interface elements (e.g., scroll bars, buttons, menus¡ªknown collectively as "widgets") and their decorations (e.g., shadows, beveling). A widget delegates all its layout and drawing behavior to a separate Layout object. Changing the Layout object changes the look and feel, even at run-time. For each widget class there is a corresponding Layout class (e.g., ScrollbarLayout, MenubarLayout, etc.). An obvious problem with this approach is that using separate layout objects doubles the number of user interface objects: For each user interface object there is an additional Layout object. To avoid this overhead, Layout objects are implemented as flyweights. They make good flyweights because they deal mostly with defining behavior, and it's easy to pass them what little extrinsic state they need to lay out or draw an object. The Layout objects are created and managed by Look objects. The Look class is an Abstract Factory (99) that retrieves a specific Layout object with operations like GetButtonLayout, GetMenuBarLayout, and so forth. For each look-and-feel standard there is a corresponding Look subclass (e.g., MotifLook, OpenLook) that supplies the appropriate Layout objects. By the way, Layout objects are essentially strategies (see Strategy (349)). They are an example of a strategy object implemented as a flyweight. The Flyweight pattern is often combined with the Composite (183) pattern to implement a logically hierarchical structure in terms of a directed-acyclic graph with shared leaf nodes. It's often best to implement State (338) and Strategy (349) objects as flyweights.
##%%&&
Provide a surrogate or placeholder for another object to control access to it. One reason for controlling access to an object is to defer the full cost of its creation and initialization until we actually need to use it. Consider a document editor that can embed graphical objects in a document. Some graphical objects, like large raster images, can be expensive to create. But opening a document should be fast, so we should avoid creating all the expensive objects at once when the document is opened. This isn't necessary anyway, because not all of these objects will be visible in the document at the same time. These constraints would suggest creating each expensive object on demand, which in this case occurs when an image becomes visible. But what do we put in the document in place of the image? And how can we hide the fact that the image is created on demand so that we don't complicate the editor's implementation? This optimization shouldn't impact the rendering and formatting code, for example. The solution is to use another object, an image proxy, that acts as a stand-in for the real image. The proxy acts just like the image and takes care of instantiating it when it's required. The image proxy creates the real image only when the document editor asks it to display itself by invoking its Draw operation. The proxy forwards subsequent requests directly to the image. It must therefore keep a reference to the image after creating it. Let's assume that images are stored in separate files. In this case we can use the file name as the reference to the real object. The proxy also stores its extent, that is, its width and height. The extent lets the proxy respond to requests for its size from the formatter without actually instantiating the image. The following class diagram illustrates this example in more detail. The document editor accesses embedded images through the interface defined by the abstract Graphic class. ImageProxy is a class for images that are created on demand. ImageProxy maintains the file name as a reference to the image on disk. The file name is passed as an argument to the ImageProxy constructor. ImageProxy also stores the bounding box of the image and a reference to the real Image instance. This reference won't be valid until the proxy instantiates the real image. The Draw operation makes sure the image is instantiated before forwarding it the request. GetExtent forwards the request to the image only if it's instantiated; otherwise ImageProxy returns the extent it stores. Proxy is applicable whenever there is a need for a more versatile or sophisticated reference to an object than a simple pointer. Here are several common situations in which the Proxy pattern is applicable: A remote proxy provides a local representative for an object in a different address space. NEXTSTEP [Add94] uses the class NXProxy for this purpose. Coplien [Cop92] calls this kind of proxy an "Ambassador." A virtual proxy creates expensive objects on demand. The ImageProxy described in the Motivation is an example of such a proxy. A protection proxy controls access to the original object. Protection proxies are useful when objects should have different access rights. For example, KernelProxies in the Choices operating system [CIRM93] provide protected access to operating system objects. A smart reference is a replacement for a bare pointer that performs additional actions when an object is accessed. Typical uses include counting the number of references to the real object so that it can be freed automatically when there are no more references (also called smart pointers [Ede92]). loading a persistent object into memory when it's first referenced. checking that the real object is locked before it's accessed to ensure that no other object can change it. Proxy (ImageProxy) maintains a reference that lets the proxy access the real subject. Proxy may refer to a Subject if the RealSubject and Subject interfaces are the same. provides an interface identical to Subject's so that a proxy can by substituted for the real subject. controls access to the real subject and may be responsible for creating and deleting it. other responsibilities depend on the kind of proxy:?remote proxies are responsible for encoding a request and its arguments and for sending the encoded request to the real subject in a different address space.?virtual proxies may cache additional information about the real subject so that they can postpone accessing it. For example, the ImageProxy from the Motivation caches the real image's extent.?protection proxies check that the caller has the access permissions required to perform a request.?Subject (Graphic) defines the common interface for RealSubject and Proxy so that a Proxy can be used anywhere a RealSubject is expected.?RealSubject (Image) defines the real object that the proxy represents. Proxy forwards requests to RealSubject when appropriate, depending on the kind of proxy. The Proxy pattern introduces a level of indirection when accessing an object. The additional indirection has many uses, depending on the kind of proxy: A remote proxy can hide the fact that an object resides in a different address space. A virtual proxy can perform optimizations such as creating an object on demand. Both protection proxies and smart references allow additional housekeeping tasks when an object is accessed. There's another optimization that the Proxy pattern can hide from the client. It's called copy-on-write, and it's related to creation on demand. Copying a large and complicated object can be an expensive operation. If the copy is never modified, then there's no need to incur this cost. By using a proxy to postpone the copying process, we ensure that we pay the price of copying the object only if it's modified. To make copy-on-write work, the subject must be reference counted. Copying the proxy will do nothing more than increment this reference count. Only when the client requests an operation that modifies the subject does the proxy actually copy it. In that case the proxy must also decrement the subject's reference count. When the reference count goes to zero, the subject gets deleted. Copy-on-write can reduce the cost of copying heavyweight subjects significantly. The Proxy pattern can exploit the following language features: Overloading the member access operator in C++. C++ supports overloading operator->, the member access operator. Overloading this operator lets you perform additional work whenever an object is dereferenced. This can be helpful for implementing some kinds of proxy; the proxy behaves just like a pointer. The following example illustrates how to use this technique to implement a virtual proxy called ImagePtr. This approach lets you call Image operations through ImagePtr objects without going to the trouble of making the operations part of the ImagePtr interface: Notice how the image proxy acts like a pointer, but it's not declared to be a pointer to an Image. That means you can't use it exactly like a real pointer to an Image. Hence clients must treat Image and ImagePtr objects differently in this approach. Overloading the member access operator isn't a good solution for every kind of proxy. Some proxies need to know precisely which operation is called, and overloading the member access operator doesn't work in those cases. Consider the virtual proxy example in the Motivation. The image should be loaded at a specific time¡ªnamely when the Draw operation is called¡ªand not whenever the image is referenced. Overloading the access operator doesn't allow this distinction. In that case we must manually implement each proxy operation that forwards the request to the subject. These operations are usually very similar to each other, as the Sample Code demonstrates. Typically all operations verify that the request is legal, that the original object exists, etc., before forwarding the request to the subject. It's tedious to write this code again and again. So it's common to use a preprocessor to generate it automatically. Using doesNotUnderstand in Smalltalk. Smalltalk provides a hook that you can use to support automatic forwarding of requests. Smalltalk calls doesNotUnderstand: aMessage when a client sends a message to a receiver that has no corresponding method. The Proxy class can redefine doesNotUnderstand so that the message is forwarded to its subject. To ensure that a request is forwarded to the subject and not just absorbed by the proxy silently, you can define a Proxy class that doesn't understand any messages. Smalltalk lets you do this by defining Proxy as a class with no superclass. The main disadvantage of doesNotUnderstand: is that most Smalltalk systems have a few special messages that are handled directly by the virtual machine, and these do not cause the usual method look-up. The only one that's usually implemented in Object (and so can affect proxies) is the identity operation ==. If you're going to use doesNotUnderstand: to implement Proxy, then you must design around this problem. You can't expect identity on proxies to mean identity on their real subjects. An added disadvantage is that doesNotUnderstand: was developed for error handling, not for building proxies, and so it's generally not very fast. Proxy doesn't always have to know the type of real subject. If a Proxy class can deal with its subject solely through an abstract interface, then there's no need to make a Proxy class for each RealSubject class; the proxy can deal with all RealSubject classes uniformly. But if Proxies are going to instantiate RealSubjects (such as in a virtual proxy), then they have to know the concrete class. Another implementation issue involves how to refer to the subject before it's instantiated. Some proxies have to refer to their subject whether it's on disk or in memory. That means they must use some form of address space-independent object identifiers. We used a file name for this purpose in the Motivation. The following code implements two kinds of proxy: the virtual proxy described in the Motivation section, and a proxy implemented with doesNotUnderstand: A virtual proxy. The Graphic class defines the interface for graphical objects: The Image class implements the Graphic interface to display image files. Image overrides HandleMouse to let users resize the image interactively. The constructor saves a local copy of the name of the file that stores the image, and it initializes _extent and _image: The implementation of GetExtent returns the cached extent if possible; otherwise the image is loaded from the file. Draw loads the image, and HandleMouse forwards the event to the real image. The Save operation saves the cached image extent and the image file name to a stream. Load retrieves this information and initializes the corresponding members. Finally, suppose we have a class TextDocument that can contain Graphic objects: Proxies that use doesNotUnderstand. You can make generic proxies in Smalltalk by defining classes whose superclass is nil8 and defining the doesNotUnderstand: method to handle messages. The following method assumes the proxy has a realSubject method that returns its real subject. In the case of ImageProxy, this method would check to see if the the Image had been created, create it if necessary, and finally return it. It uses perform:withArguments: to perform the message being trapped on the real subject. The argument to doesNotUnderstand: is an instance of Message that represents the message not understood by the proxy. So the proxy responds to all messages by making sure that the real subject exists before forwarding the message to it. One of the advantages of doesNotUnderstand: is it can perform arbitrary processing. For example, we could produce a protection proxy by specifying a set legalMessages of messages to accept and then giving the proxy the following method: This method checks to see that a message is legal before forwarding it to the real subject. If it isn't legal, then it will send error: to the proxy, which will result in an infinite loop of errors unless the proxy define error:. Consequently, the definition of error: should be copied from class Object along with any methods it uses. The virtual proxy example in the Motivation section is from the ET++ text building block classes. NEXTSTEP [Add94] uses proxies (instances of class NXProxy) as local representatives for objects that may be distributed. A server creates proxies for remote objects when clients request them. On receiving a message, the proxy encodes it along with its arguments and then forwards the encoded message to the remote subject. Similarly, the subject encodes any return results and sends them back to the NXProxy object. McCullough [McC87] discusses using proxies in Smalltalk to access remote objects. Pascoe [Pas86] describes how to provide side-effects on method calls and access control with "Encapsulators." Adapter (157): An adapter provides a different interface to the object it adapts. In contrast, a proxy provides the same interface as its subject. However, a proxy used for access protection might refuse to perform an operation that the subject will perform, so its interface may be effectively a subset of the subject's. Decorator (196): Although decorators can have similar implementations as proxies, decorators have a different purpose. A decorator adds one or more responsibilities to an object, whereas a proxy controls access to an object. Proxies vary in the degree to which they are implemented like a decorator. A protection proxy might be implemented exactly like a decorator. On the other hand, a remote proxy will not contain a direct reference to its real subject but only an indirect reference, such as "host ID and local address on host." A virtual proxy will start off with an indirect reference such as a file name but will eventually obtain and use a direct reference.
##%%&&
Avoid coupling the sender of a request to its receiver by giving more than one object a chance to handle the request. Chain the receiving objects and pass the request along the chain until an object handles it. Consider a context-sensitive help facility for a graphical user interface. The user can obtain help information on any part of the interface just by clicking on it. The help that's provided depends on the part of the interface that's selected and its context; for example, a button widget in a dialog box might have different help information than a similar button in the main window. If no specific help information exists for that part of the interface, then the help system should display a more general help message about the immediate context¡ªthe dialog box as a whole, for example. Hence it's natural to organize help information according to its generality¡ªfrom the most specific to the most general. Furthermore, it's clear that a help request is handled by one of several user interface objects; which one depends on the context and how specific the available help is. The problem here is that the object that ultimately provides the help isn't known explicitly to the object (e.g., the button) that initiates the help request. What we need is a way to decouple the button that initiates the help request from the objects that might provide help information. The Chain of Responsibility pattern define show that happens. The idea of this pattern is to decouple senders and receivers by giving multiple objects a chance to handle a request. The request gets passed along a chain of objects until one of them handles it. The first object in the chain receives the request and either handle sit or forwards it to the next candidate on the chain, which does likewise. The object that made the request has no explicit knowledge of who will handle it¡ªwe say the request has an implicit receiver. Let's assume the user clicks for help on a button widget marked" Print." The button is contained in an instance of PrintDialog, which knows the application object it belongs to (see preceding object diagram).The following interaction diagram illustrates how the help request gets forwarded along the chain: In this case, neither a PrintButton nor a PrintDialog handles the request; it stops at an Application, which can handle it or ignore it. The client that issued the request has no direct reference to the object that ultimately fulfills it. To forward the request along the chain, and to ensure receivers remain implicit, each object on the chain shares a common interface for handling requests and for accessing its successor on the chain. For example, the help system might define a HelpHandler class with a corresponding HandleHelp operation. HelpHandler can be the parent class for candidate object classes, or it can be defined as amixin class. Then classes that want to handle help requests can make HelpHandler a parent: The Button, Dialog, and Application classes use HelpHandler operations to handle help requests. HelpHandler's HandleHelp operation forwards the request to the successor by default. Subclasses can override this operation to provide help under the right circumstances; otherwise they can use the default implementation to forward the request. Use Chain of Responsibility when?more than one object may handle a request, and the handler isn't known a priori. The handler should be ascertained automatically.?you want to issue a request to one of several objects without specifying the receiver explicitly.?the set of objects that can handle a request should be specified dynamically. Handler (HelpHandler) defines an interface for handling requests. (optional) implements the successor link.?ConcreteHandler (PrintButton, PrintDialog) handles requests it is responsible for. can access its successor. if the ConcreteHandler can handle the request, it does so; otherwise it forwards the request to its successor.?Client initiates the request to a ConcreteHandler object on the chain. When a client issues a request, the request propagates along the chain until a ConcreteHandler object takes responsibility for handling it. Chain of Responsibility has the following benefits and liabilities: Reduced coupling.The pattern frees an object from knowing which other object handles are quest. An object only has to know that a request will be handled"appropriately." Both the receiver and the sender have no explicit knowledge of each other, and an object in the chain doesn't have to know about the chain's structure. As a result, Chain of Responsibility can simplify object interconnections. Instead of objects maintaining references to all candidate receivers, they keep a single reference to their successor. Added flexibility in assigning responsibilities to objects.Chain of Responsibility gives you added flexibility in distributing responsibilities among objects. You can add or change responsibilities for handling a request by adding to or otherwise changing the chain at run-time. You can combine this with subclassing to specialize handlers statically. Receipt isn't guaranteed. Since a request has no explicit receiver, there's no guarantee it'll be handled¡ªthe request can fall off the end of the chain without ever being handled. A request can also go unhandled when the chain is not configured properly. Here are implementation issues to consider in Chain of Responsibility: Implementing the successor chain.There are two possible ways to implement the successor chain: Define new links (usually in the Handler, but ConcreteHandlerscould define them instead). Use existing links. Our examples so far define new links, but often you can use existing object references to form the successor chain. For example, parent references in a part-whole hierarchy can define a part's successor. A widget structure might already have such links. Composite (183) discusses parent references in more detail. Using existing links works well when the links support the chain you need. It saves you from defining links explicitly, and it saves space. But if the structure doesn't reflect the chain of responsibility your application requires, then you'll have to define redundant links. Connecting successors. If there are no preexisting references for defining a chain, then you'll have to introduce them yourself. In that case, the Handler not only defines the interface for the requests but usually maintains the successor as well. That lets the handler provide a default implementation of HandleRequest that forwards the request to the successor (if any). If a ConcreteHandler subclass isn't interested in the request, it doesn't have to override the forwarding operation, since its default implementation forwards unconditionally. Representing requests. Different options are available for representing requests. In the simplest form, the request is a hard-coded operation invocation, as in the case of HandleHelp. This is convenient and safe, but you can forward only the fixed set of requests that the Handler class defines. An alternative is to use a single handler function that takes are quest code (e.g., an integer constant or a string) as parameter. This supports an open-ended set of requests. The only requirement is that the sender and receiver agree on how the request should been coded. This approach is more flexible, but it requires conditional statements for dispatching the request based on its code. Moreover, there's no type-safe way to pass parameters, so they must be packed and unpacked manually. Obviously this is less safe than invoking an operation directly. To address the parameter-passing problem, we can use separate request objects that bundle request parameters. A Request class can represent requests explicitly, and new kinds of requests can be defined by subclassing. Subclasses can define different parameters. Handlers must know the kind of request (that is, which Request subclass they're using) to access these parameters. To identify the request, Request can define an accessor function that returns an identifier for the class. Alternatively, the receiver can use run-time type information if the implementation languages supports it. Here is a sketch of a dispatch function that uses request objects to identify requests. A GetKind operation defined in the base Request class identifies the kind of request: Automatic forwarding in Smalltalk.You can use the does NotUnderstand mechanism in Smalltalk to forward requests. Messages that have no corresponding methods are trapped in the implementation of doesNotUnderstand, which can be overridden to forward the message to an object's successor. Thus it isn't necessary to implement forwarding manually; the class handles only the request in which it's interested, and it relies on doesNotUnderstand to forward all others. The following example illustrates how a chain of responsibility can handle requests for an on-line help system like the one described earlier. The help request is an explicit operation. We'll use existing parent references in the widget hierarchy to propagate requests between widgets in the chain, and we'll define a reference in the Handler class to propagate help requests between nonwidgets in the chain. The HelpHandler class defines the interface for handling help requests. It maintains a help topic (which is empty by default)and keeps a reference to its successor on the chain of help handlers. The key operation is HandleHelp, which subclasses override. HasHelp is a convenience operation for checking whether there is an associated help topic. All widgets are subclasses of the Widget abstract class.Widget is a subclass of HelpHandler, since all user interface elements can have help associated with them. (We could have used a mixin-based implementation just as well.) Button's version of HandleHelp first tests to see ifthere is a help topic for buttons. If the developer hasn't defined one, then the request gets forwarded to the successor using the HandleHelp operation in HelpHandler. If there is a help topic, then the button displays it, and the search ends. Dialog implements a similar scheme, except that its successor is not a widget but any help handler. In our application this successor will be an instance of Application. At the end of the chain is an instance of Application. The application is not a widget, so Application is subclassed directly from HelpHandler.When a help request propagates to this level, the application can supply information on the application in general, or it can offer a list of different help topics: In this case, the button will handle the request immediately. Note that any HelpHandler class could be made the successor of Dialog. Moreover, its successor could be changed dynamically. So no matter where a dialog is used, you'll get the proper context-dependent help information for it. Several class libraries use the Chain of Responsibility pattern to handle user events. They use different names for the Handler class,but the idea is the same: When the user clicks the mouse or presses a key, an event gets generated and passed along the chain.MacApp [App89] and ET++ [WGM88] call it "EventHandler,"Symantec's TCL library [Sym93b] calls it "Bureaucrat," andNeXT's AppKit [Add94] uses the name "Responder." The Unidraw framework for graphical editors defines Command objects that encapsulate requests to Component and ComponentViewobjects [VL90]. Commands are requests in the sense that a component or component view may interpret a command to performan operation. This corresponds to the "requests as objects"approach described in Implementation. Components and component views may be structured hierarchically. A component or a component view may forward command interpretation to its parent, which may in turn forward it to its parent, and so on, thereby forming a chain of responsibility. ET++ uses Chain of Responsibility to handle graphical update. A graphical object calls the InvalidateRect operation whenever it must update a part of its appearance. A graphical object can't handleInvalidateRect by itself, because it doesn't know enough about itscontext. For example, a graphical object can be enclosed in objects like Scrollers or Zoomers that transform its coordinate system. That means the object might be scrolled or zoomed so that it's partiallyout of view. Therefore the default implementation of InvalidateRect forwards the request to the enclosing container object. The last object in the forwarding chain is a Window instance. By the time Window receives the request, the invalidation rectangle is guaranteed to be transformed properly. The Window handles InvalidateRect by notifying the window system interface and requesting an update. Chain of Responsibility is often applied in conjunction with Composite (183). There, a component's parent can act as its successor.
##%%&&
Encapsulate a request as an object, thereby letting you parameterize clients with different requests, queue or log requests, and support undoable operations. Sometimes it's necessary to issue requests to objects without knowing anything about the operation being requested or the receiver of the request. For example, user interface toolkits include objects like buttons and menus that carry out a request in response to user input.But the toolkit can't implement the request explicitly in the button or menu, because only applications that use the toolkit know what should be done on which object. As toolkit designers we have no way of knowing the receiver of the request or the operations that will carry it out. The Command pattern lets toolkit objects make requests of unspecified application objects by turning the request itself into an object. This object can be stored and passed around like other objects. The key to this pattern is an abstract Command class, which declares an interface for executing operations. In the simplest form this interface includes an abstract Execute operation. Concrete Command subclasses specify a receiver-action pair by storing the receiver as an instance variable and by implementing Execute to invoke the request. The receiver has the knowledge required to carry out the request. Menus can be implemented easily with Command objects. Each choice in a Menu is an instance of a Menu Item class. An Application class creates these menus and their menu items along with the rest of the user interface.The Application class also keeps track of Document objects that a user has opened. The application configures each MenuItem with an instance of a concrete Command subclass. When the user selects a MenuItem, the MenuItem calls Execute on its command, and Execute carries out theoperation. MenuItems don't know which subclass of Command they use.Command subclasses store the receiver of the request and invoke one or more operations on the receiver. For example, PasteCommand supports pasting text from the clipboard into a Document. PasteCommand's receiver is the Document object it is supplied upon instantiation. The Execute operation invokes Paste on the receiving Document. Open Command's Execute operation is different: it prompts the user for a document name, creates a corresponding Document object, adds the document to the receiving application, and opens the document. Sometimes a Menu Item needs to execute a sequence of commands. For example, a Menu Item for centering a page at normal size could be constructed from a CenterDocumentCommand object and aNormalSizeCommand object. Because it's common to string commands together in this way, we can define a Macro Command class to allow amenities to execute an open-ended number of commands. Macro Command Is a concrete Command subclass that simply executes a sequence of Commands. Macro Command has no explicit receiver, because the commands it sequences define their own receiver. In each of these examples, notice how the Command pattern decouples the object that invokes the operation from the one having the knowledge to perform it. This gives us a lot of flexibility in designing our user interface. An application can provide both a menu and a push button interface to a feature just by making the menu and the push button share an instance of the same concrete Command subclass. We can replace commands dynamically, which would be useful for implementing context-sensitive menus. We can also support command scripting by composing commands into larger ones. All of this is possible because the object that issues a request only needs to know how to issue it; it doesn't need to know how the request will be carried out. Use the Command pattern when you want to?parameterize objects by an action to perform, as MenuItem objects did above. You can express such parameterization in a procedural language with a callback function, that is, a function that's registered somewhere to be called at a later point. Commands are an object-oriented replacement for callbacks. specify, queue, and execute requests at different times. A Command object can have a lifetime independent of the original request. If the receiver of a request can be represented in an address space-independent way, then you can transfer a command object for the request to a different process and fulfill the request there.?support undo. The Command's Execute operation can store state for reversing its effects in the command itself. The Command interface must have an added Unexecute operation that reverses the effects of a previous call to Execute. Executed commands are stored in a history list. Unlimited-level undo and redo is achieved by traversing this list backwards and forwards calling Unexecute and Execute, respectively.?support logging changes so that they can be reapplied in case of a system crash. By augmenting the Command interface with load and store operations, you can keep a persistent log of changes. Recovering from a crash involves reloading logged commands from disk and reexecuting them with the Execute operation.?structure a system around high-level operations built on primitives operations. Such a structure is common in information systems that support transactions. A transaction encapsulates a set of changes to data. The Command pattern offers a way to model transactions. Commands have a common interface, letting you invoke all transactions the same way. The pattern also makes it easy to extend the system with new transactions. Command declares an interface for executing an operation.?ConcreteCommand (PasteCommand, OpenCommand) defines a binding between a Receiver object and an action. implements Execute by invoking the corresponding operation(s) on Receiver.?Client (Application) creates a ConcreteCommand object and sets its receiver.?Invoker (MenuItem) asks the command to carry out the request.?Receiver (Document, Application) knows how to perform the operations associated with carrying out a request. Any class may serve as a Receiver. The client creates a ConcreteCommand object and specifies its receiver.?An Invoker object stores the ConcreteCommand object.?The invoker issues a request by calling Execute on the command. When commands are undoable, ConcreteCommand stores state for undoing the command prior to invoking Execute.?The ConcreteCommand object invokes operations on its receiver to carry out the request. The following diagram shows the interactions between these objects.It illustrates how Command decouples the invoker from the receiver(and the request it carries out). The Command pattern has the following consequences: Command decouples the object that invokes the operation from the one that knows how to perform it. Commands are first-class objects. They can be manipulated and extended like any other object. You can assemble commands into a composite command. An example is the MacroCommand class described earlier. In general, composite commands are an instance of the Composite (183) pattern. It's easy to add new Commands, because you don't have to change existing classes. Consider the following issues when implementing the Command pattern: How intelligent should a command beta command can have a wide range of abilities. At one extreme it merely defines a binding between a receiver and the actions that carryout the request. At the other extreme it implements everything itself without delegating to a receiver at all. The latter extreme is useful when you want to define commands that are independent of existing classes, when no suitable receiver exists, or when a command knows its receiver implicitly. For example, a command that creates another application window may be just as capable of creating the window as any other object. Somewhere in between these extremes are commands that have enough knowledge to find their receiver dynamically. Supporting undo and redo. Commands can support undo and redo capabilities if they provide a way to reverse their execution (e.g., an Unexecuted or Undo operation). AConcreteCommand class might need to store additional state to do so. This state can include the Receiver object, which actually carries out operations in response to the request, the arguments to the operation performed on the receiver, and any original values in the receiver that can changes a result of handling the request. The receiver must provide operations that let the command return the receiver to its prior state. To support one level of undo, an application needs to store only the command that was executed last. For multiple-level undo and redo, the application needs a history list of commands that have been executed, where the maximum length of the list determines the number of undo/redo levels. The history list stores sequences of commands that have been executed. Traversing backward through theist and reverse-executing commands cancels their effect; traversing forward and executing commands executes them. An undoable command might have to be copied before it can be placed on the history list. That's because the command object that carried out the original request, say, from a Menu Item, will perform other requests at later times. Copying is required to distinguish different invocations of the same command if its state can vary across invocations. For example, a Delete Command that deletes selected objects must store different sets of objects each time it's executed. Therefore theDeleteCommand object must be copied following execution, and the copyist placed on the history list. If the command's state never change son execution, then copying is not required¡ªonly a reference to the command need be placed on the history list. Commands that must be copied before being placed on the history list act as prototypes (see Prototype (133)). Avoiding error accumulation in the undo process. Hysteresis can be a problem in ensuring a reliable, semantics-preserving undo/redo mechanism. Errors can accumulate as commands are executed, unexecuted, and reelected repeatedly so that an application's state eventually diverges from original values. It may be necessary therefore to store more information in the command tonsure that objects are restored to their original state. The Memento (316) pattern can be applied to give the command access to this information without exposing the internals of other objects. Using C++ templates. For commands that (1) aren't undoable and (2) don't require arguments, we can use C++ templates to avoid creating a Command subclass forever kind of action and receiver. We show how to do this in the Sample Code section. The C++ code shown here sketches the implementation of the Command classes in the Motivation section. We'll define OpenCommand,PasteCommand, and MacroCommand. OpenCommand opens a document whose name is supplied by the user. An OpenCommand must be passed an Application object in its constructor. AskUser is an implementation routine that prompts the user for the name of the document to open. A PasteCommand must be passed a Document object as its receiver. The receiver is given as a parameter to PasteCommand'sconstructor. For simple commands that aren't undoable and don't require arguments,we can use a class template to parameterize the command's receiver.We'll define a template subclass SimpleCommand for such commands. SimpleCommand is parameterized by the Receiver type and maintains a binding between a receiver object and an action stored as a pointer to a member function. The constructor stores the receiver and the action in the corresponding instance variables. Execute simply applies the action to the receiver. Keep in mind that this solution only works for simple commands. More complex commands that keep track of not only their receivers but also arguments and/or undo state require a Command subclass. A MacroCommand manages a sequence of subcommands and provides operations for adding and removing subcommands. No explicit receivers required, because the subcommands already define their receiver. The key to the MacroCommand is its Execute member function. This traverses all the subcommands and performs Execute on each of them. Note that should the MacroCommand implement anUnexecute operation, then its subcommands must be unexecuted in reverse order relative to Execute's implementation. Finally, MacroCommand must provide operations to manage its subcommands. The MacroCommand is also responsible for deleting its subcommands. Perhaps the first example of the Command pattern appears in a paper by Lieberman [Lie85]. Madcap [App89] popularized the notion of commands for implementing undoable operations.ET++ [WGM88], Interviews [LCI+92], and Unidraw [VL90] also define classes that follow the Command pattern. Interviews defines an Action abstract class that provides command functionality. It also defines an ActionCallback template, parameterized by action method, that can instantiate command subclasses automatically. The THINK class library [Sym93b] also uses commands to support undoable actions. Commands in THINK are called "Tasks." Task objects are passed along a Chain of Responsibility (251) for consumption. Unigram's command objects are unique in that they can behave like messages. A Unidraw command may be sent to another object for interpretation, and the result of the interpretation varies with the receiving object. Moreover, the receiver may delegate the interpretation to another object, typically the receiver's parent in larger structure as in a Chain of Responsibility. The receiver of a Unidraw command is thus computed rather than stored. Unidraw's interpretation mechanism depends on run-time type information. Coplien describes how to implement functors, objects that are functions, in C++ [Cop92]. He achieves a degree of transparency in their use by overloading the function call operator(operator()). The Command pattern is different; its focus is on maintaining a binding between a receiver and a function(i.e., action), not just maintaining a function. A Composite (183)can be used to implement MacroCommands. A Memento (316)can keep state the command requires to undo its effect. A command that must be copied before being placed on the history list acts as a Prototype (133).
##%%&&
Given a language, define a representation for its grammar along with an interpreter that uses the representation to interpret sentences in the language. If a particular kind of problem occurs often enough, then it might be worthwhile to express instances of the problem as sentences in a simple language. Then you can build an interpreter that solves the problem by interpreting these sentences. For example, searching for strings that match a pattern is a common problem. Regular expressions are a standard language for specifying patterns of strings. Rather than building custom algorithms to match each pattern against strings, search algorithms could interpret a regular expression that specifies a set of strings to match. The Interpreter pattern describes how to define a grammar for simple languages, represent sentences in the language, and interpret these sentences. In this example, the pattern describes how to define a grammar for regular expressions, represent a particular regular expression, and how to interpret that regular expression. Suppose the following grammar defines the regular expressions: The symbol expression is the start symbol, and literal is a terminal symbol defining simple words. The Interpreter pattern uses a class to represent each grammar rule.Symbols on the right-hand side of the rule are instance variables of these classes. The grammar above is represented by five classes: an abstract class RegularExpression and its four subclasses LiteralExpression, AlternationExpression, SequenceExpression, and RepetitionExpression. The last three classes define variables that hold subexpressions. Every regular expression defined by this grammar is represented by an abstract syntax tree made up of instances of these classes. For example, the abstract syntax tree represents the regular expression We can create an interpreter for these regular expressions by defining the Interpret operation on each subclass of RegularExpression.Interpret takes as an argument the context in which to interpret the expression. The context contains the input string and information on how much of it has been matched so far. Each subclass of RegularExpression implements Interpret to match the next part of the input string based on the current context. For example,?LiteralExpression will check if the input matches the literal it defines,?AlternationExpression will check if the input matches any of its alternatives,?RepetitionExpression will check if the input has multiple copies of expression it repeats, and so on. Use the Interpreter pattern when there is a language to interpret, and you can represent statements in the language as abstract syntax trees.The Interpreter pattern works best when?the grammar is simple. For complex grammars, the class hierarchy for the grammar becomes large and unmanageable. Tools such as parser generators are a better alternative in such cases. They can interpret expressions without building abstract syntax trees, which can save space and possibly time.?efficiency is not a critical concern. The most efficient interpreters are usually not implemented by interpreting parse trees directly but by first translating them into another form. For example, regular expressions are often transformed into state machines. But even then,the translator can be implemented by the Interpreter pattern, so the pattern is still applicable. AbstractExpression (RegularExpression) declares an abstract Interpret operation that is common to all nodes in the abstract syntax tree.?TerminalExpression (LiteralExpression) implements an Interpret operation associated with terminal symbols in the grammar. an instance is required for every terminal symbol in a sentence.?NonterminalExpression (AlternationExpression,RepetitionExpression, SequenceExpressions) one such class is required for every rule R ::= R1 R2 ... Rn in the grammar. maintains instance variables of type AbstractExpression for each of the symbols R1 through Rn. implements an Interpret operation for nonterminal symbols in the grammar. Interpret typically calls itself recursively on the variables representing R1 through Rn.?Context contains information that's global to the interpreter.?Client builds (or is given) an abstract syntax tree representing a particular sentence in the language that the grammar defines. The abstract syntax tree is assembled from instances of the NonterminalExpression and TerminalExpression classes. invokes the Interpret operation. The client builds (or is given) the sentence as an abstract syntax tree of NonterminalExpression and TerminalExpression instances. Then the client initializes the context and invokes the Interpret operation.?Each NonterminalExpression node defines Interpret in terms of Interpret on each sub-expression. The Interpret operation of each TerminalExpression defines the base case in the recursion.?The Interpret operations at each node use the context to store and access the state of the interpreter. The Interpreter pattern has the following benefits and liabilities: It's easy to change and extend the grammar.Because the pattern uses classes to represent grammar rules, you can use inheritance to change or extend the grammar. Existing expressions can be modified incrementally, and new expressions can be defined as variations on old ones. Implementing the grammar is easy, too.Classes defining nodes in the abstract syntax tree have similar implementations. These classes are easy to write, and often their generation can be automated with a compiler or parser generator. Complex grammars are hard to maintain.The Interpreter pattern defines at least one class for every rule in the grammar (grammar rules defined using BNF may require multiple classes). Hence grammars containing many rules can be hard to manage and maintain. Other design patterns can be applied to mitigate the problem (see Implementation).But when the grammar is very complex, other techniques such as parser or compiler generators are more appropriate. Adding new ways to interpret expressions.The Interpreter pattern makes it easier to evaluate an expression in a new way. For example, you can support pretty printing or type-checking an expression by defining a new operation on the expression classes. If you keep creating new ways of interpreting an expression, then consider using the Visitor (366) pattern to avoid changing the grammar classes. The Interpreter and Composite (183) patterns share many implementation issues. The following issues are specific to Interpreter: Creating the abstract syntax tree.The Interpreter pattern doesn't explain how to create an abstract syntax tree. In other words, it doesn't address parsing.The abstract syntax tree can be created by a table-driven parser, by a hand-crafted (usually recursive descent) parser, or directly by the client. Defining the Interpret operation.You don't have to define the Interpret operation in the expression classes. If it's common to create a new interpreter, then it's better to use the Visitor (366) pattern to put Interpret in a separate "visitor" object. For example, a grammar for a programming language will have many operations on abstract syntax trees, such as as type-checking, optimization, code generation, and so on. It will be more likely to use a visitor to avoid defining these operations on every grammar class. Sharing terminal symbols with the Flyweight pattern.Grammars whose sentences contain many occurrences of a terminal symbol might benefit from sharing a single copy of that symbol. Grammars for computer programs are good examples each program variable will appear in many places throughout the code. In the Motivation example,a sentence can have the terminal symbol dog (modeled by the LiteralExpression class) appearing many times. Terminal nodes generally don't store information about their position in the abstract syntax tree. Parent nodes pass them whatever context they need during interpretation. Hence there is a distinction between shared (intrinsic) state and passed-in (extrinsic) state, and the Flyweight (218) pattern applies. For example, each instance of LiteralExpression for dog receives a context containing the substring matched so far. And every such LiteralExpression does the same thing in its Interpret operation it checks whether the next part of the input contains a dog no matter where the instance appears in the tree. Here are two examples. The first is a complete example in Smalltalk for checking whether a sequence matches a regular expression. The second is a C++ program for evaluating Boolean expressions. The regular expression matcher tests whether a string is in the language defined by the regular expression. The regular expression is defined by the following grammar: This grammar is a slight modification of the Motivation example. We changed the concrete syntax of regular expressions a little, because symbol "*" can't be a postfix operation in Smalltalk. So we use repeat instead. For example, the regular expression matches the input string "dog dog cat weather". To implement the matcher, we define the five classes described on page 274. The class SequenceExpression has instance variablesexpression1 and expression2 for its children in the abstract syntax tree. AlternationExpression stores its alternatives in the instance variablesalternative1 and alternative2, while RepetitionExpression holds the expression it repeats in its repetition instance variable.LiteralExpression has a components instance variable that holds a list of objects (probably characters). These represent the literal string that must match the input sequence. The match: operation implements an interpreter for the regular expression. Each of the classes defining the abstract syntax tree implements this operation. It takes input State as an argument representing the current state of the matching process, having read part of the input string. This current state is characterized by a set of input streams representing the set of inputs that the regular expression could have accepted so far. (This is roughly equivalent to recording all states that the equivalent finite state automata would be in, having recognized the input stream to this point). Now we consider the definitions of match: for each class defining the regular expression. The definition for SequenceExpression matches each of its subexpressions in sequence. Usually it will eliminate input streams from its input State. Its output state usually contains more states than its input state,because a RepetitionExpression can match one, two, or many occurrences of repetition on the input state. The output states represent all these possibilities, allowing subsequent elements of the regular expression to decide which state is the correct one. The next Available: message advances the input stream. This is the only match: operation that advances the stream.Notice how the state that's returned contains a copy of the input stream, thereby ensuring that matching a literal never changes the input stream. This is important because each alternative of an AlternationExpression should see identical copies of the input stream. Now that we've defined the classes that make up an abstract syntax tree, we can describe how to build it.Rather than write a parser for regular expressions, we'll define some operations on the RegularExpression classes so that evaluating a Smalltalk expression will produce an abstract syntax tree for the corresponding regular expression. That lets us use the built-in Smalltalk compiler as if it were a parser for regular expressions. If we defined these operations higher up in the class hierarchy(SequenceableCollection in Smalltalk-80,IndexedCollection in Smalltalk/V), then they would also be defined for classes such as Array and OrderedCollection. This would let regular expressions match sequences of any kind of object. The second example is a system for manipulating and evaluating Boolean expressions implemented in C++. The terminal symbols in this language are Boolean variables, that is, the constants true and false. Nonterminal symbols represent expressions containing the operators and, or, and not. The grammar is defined as follows We define two operations on Boolean expressions. The first,Evaluate, evaluates a Boolean expression in a context that assigns a true or false value to each variable. The second operation, Replace, produces a new Boolean expression by replacing a variable with an expression. Replace show the Interpreter pattern can be used for more than just evaluating expressions. In this case, it manipulates the expression itself. We give details of just the BooleanExp,VariableExp, and AndExp classes here. ClassesOrExp and NotExp are similar to AndExp.The Constant class represents the Boolean constants. The expression evaluates to true for this assignment to x and y. We can evaluate the expression with a different assignment to the variables simply by changing the context. This example illustrates an important point about the Interpreter pattern: many kinds of operations can "interpret" a sentence. Of the three operations defined for BooleanExp,Evaluate fits our idea of what an interpreter should do most closely that is, it interprets a program or expression and returns a simple result. However, Replace can be viewed as an interpreter as well.It's an interpreter whose context is the name of the variable being replaced along with the expression that replaces it, and whose result is a new expression. Even Copy can be thought of as an interpreter with an empty context. It may seem a little strange to consider Replace and Copy to be interpreters, because these are just basic operations on trees. The examples in Visitor (366) illustrate how all three operations can be refactored into a separate "interpreter" visitor, thus showing that the similarity is deep. The Interpreter pattern is more than just an operation distributed over a class hierarchy that uses the Composite (183) pattern. We consider Evaluate an interpreter because we think of the BooleanExp class hierarchy as representing a language. Given a similar class hierarchy for representing automotive part assemblies, it's unlikely we'd consider operations like Weight and Copy as interpreters even though they are distributed over a class hierarchy that uses the Composite pattern we just don't think of automotive parts as a language. It's a matter of perspective; if we started publishing grammars of automotive parts, then we could consider operations on those parts to be ways of interpreting the language. The Interpreter pattern is widely used in compilers implemented with object-oriented languages, as the Smalltalk compilers are. SPECTalk uses the pattern to interpret descriptions of input file formats [Sza92]. The QOCA constraint-solving toolkit uses it to evaluate constraints [HHMV92]. Considered in its most general form (i.e., an operation distributed over a class hierarchy based on the Composite pattern), nearly every use of the Composite pattern will also contain the Interpreter pattern. But the Interpreter pattern should be reserved for those cases in which you want to think of the class hierarchy as defining a language. Composite (183):The abstract syntax tree is an instance of the Composite pattern. Flyweight (218) shows how to share terminal symbols within the abstract syntax tree. Iterator (289):The interpreter can use an Iterator to traverse the structure. Visitor (366) can be used to maintain the behavior in each node in the abstract syntax tree in one class.
##%%&&
Provide a way to access the elements of an aggregate object sequentially without exposing its underlying representation. An aggregate object such as a list should give you a way to access its elements without exposing its internal structure. Moreover, you might want to traverse the list in different ways, depending on what you want to accomplish. But you probably don't want to bloat the List interface with operations for different traversals, even if you could anticipate the ones you will need. You might also need to have more than one traversal pending on the same list. The Iterator pattern lets you do all this. The key idea in this pattern is to take the responsibility for access and traversal out of the list object and put it into an iterator object. The Iterator class defines an interface for accessing the list's elements.An iterator object is responsible for keeping track of the current element; that is, it knows which elements have been traversed already. For example, a List class would call for a List Iterator with the following relationship between them: Before you can instantiate ListIterator, you must supply the List to traverse. Once you have the ListIterator instance, you can access the list's elements sequentially. The CurrentItem operation returns the current element in the list, First initializes the current element to the first element, Next advances the current element to the nextelement, and IsDone tests whether we've advanced beyond the last element that is, we're finished with the traversal. Separating the traversal mechanism from the List object lets us define iterators for different traversal policies without enumerating them in the List interface. For example, Filtering ListIterator might provide access only to those elements that satisfy specific filtering constraints. Notice that the iterator and the list are coupled, and the client must know that it is a list that's traversed as opposed to some other aggregate structure. Hence the client commits to a particular aggregate structure. It would be better if we could change the aggregate class without changing client code. We can do this by generalizing the iterator concept to support polymorphic iteration. As an example, let's assume that we also have a SkipList implementation of a list. A skip list [Pug90] is a probabilistic data structure with characteristics similar to balanced trees. We want to be able to write code that works for both List and SkipList objects. We define an AbstractList class that provides a common interface for manipulating lists. Similarly, we need an abstract Iterator class that defines a common iteration interface. Then we can define concrete Iterator subclasses for the different list implementations.As a result, the iteration mechanism becomes independent of concrete aggregate classes. The remaining problem is how to create the iterator. Since we want to write code that's independent of the concrete List subclasses, we cannot simply instantiate a specific class. Instead, we make the list objects responsible for creating their corresponding iterator. This requires an operation like CreateIterator through which clients request an iterator object. CreateIterator is an example of a factory method (see Factory Method (121)). We use it here to let a client ask a list object for the appropriate iterator. The Factory Method approach give rise to two class hierarchies, one for lists and another for iterators. The CreateIterator factory method "connects" the two hierarchies. Use the Iterator pattern?to access an aggregate object's contents without exposing its internal representation.?to support multiple traversals of aggregate objects.?to provide a uniform interface for traversing different aggregate structures (that is, to support polymorphic iteration). Iterator defines an interface for accessing and traversing elements.?ConcreteIterator implements the Iterator interface. keeps track of the current position in the traversal of the aggregate.?Aggregate defines an interface for creating an Iterator object.?ConcreteAggregate implements the Iterator creation interface to return an instance of the proper ConcreteIterator. A ConcreteIterator keeps track of the current object in the aggregate and can compute the succeeding object in the traversal. The Iterator pattern has three important consequences: It supports variations in the traversal of an aggregate.Complex aggregates may be traversed in many ways. For example, code generation and semantic checking involve traversing parse trees. Code generation may traverse the parse tree in order or preorder.Iterators make it easy to change the traversal algorithm: Just replace the iterator instance with a different one. You can also define Iterator subclasses to support new traversals. Iterators simplify the Aggregate interface.Iterator's traversal interface obviates the need for a similar interface in Aggregate, thereby simplifying the aggregate's interface. More than one traversal can be pending on an aggregate.An iterator keeps track of its own traversal state. Therefore you can have more than one traversal in progress at once. Iterator has many implementation variants and alternatives. Some important ones follow. The trade-offs often depend on the control structures your language provides. Some languages (CLU [LG86], for example) even support this pattern directly. Who controls the iteration?A fundamental issue is deciding which party controls the iteration,the iterator or the client that uses the iterator. When the client controls the iteration, the iterator is called an external iterator, and when the iterator controls it, the iterator is an internal iterator. Clients that use an external iterator must advance the traversal and request the next element explicitly from the iterator. In contrast, the client hands an internal iterator an operation to perform, and the iterator applies that operation to every element in the aggregate. External iterators are more flexible than internal iterators. It'seasy to compare two collections for equality with an external iterator, for example, but it's practically impossible with internal iterators. Internal iterators are especially weak in a language likeC++ that does not provide anonymous functions, closures, or continuations like Smalltalk and CLOS. But on the other hand,internal iterators are easier to use, because they define the iteration logic for you. Who defines the traversal algorithm?The iterator is not the only place where the traversal algorithm can be defined. The aggregate might define the traversal algorithm and use the iterator to store just the state of the iteration. We call this kind of iterator a cursor, since it merely points to the current position in the aggregate. A client will invoke the Next operation on the aggregate with the cursor as an argument, and the Next operation will change the state of the cursor. If the iterator is responsible for the traversal algorithm, then it'seasy to use different iteration algorithms on the same aggregate, and it can also be easier to reuse the same algorithm on different aggregates. On the other hand, the traversal algorithm might need to access the private variables of the aggregate. If so, putting the traversal algorithm in the iterator violates the encapsulation of the aggregate. How robust is the iterator?It can be dangerous to modify an aggregate while you're traversing it.If elements are added or deleted from the aggregate, you might end upaccessing an element twice or missing it completely. A simple solution is to copy the aggregate and traverse the copy, but that's too expensive to do in general. A robust iterator ensures that insertions and removals won't interfere with traversal, and it does it without copying the aggregate. There are many ways to implement robust iterators. Most rely on registering the iterator with the aggregate. On insertion or removal, the aggregate either adjusts the internal state of iterators it has produced, or it maintains information internally to ensure proper traversal. Kofler provides a good discussion of how robust iterators are implemented in ET++ [Kof93]. Murray discusses the implementation of robust iterators for the USL StandardComponents'List class [Mur93]. Additional Iterator operations.The minimal interface to Iterator consists of the operations First,Next, IsDone, and CurrentItem. Some additional operations might prove useful. For example, ordered aggregates can have a Previous operation that positions the iterator to the previous element. A SkipTo operation is useful for sorted or indexed collections. SkipTo positions the iterator to an object matching specific criteria. Using polymorphic iterators in C++.Polymorphic iterators have their cost. They require the iterator object to be allocated dynamically by a factory method. Hence theyshould be used only when there's a need for polymorphism. Otherwise use concrete iterators, which can be allocated on the stack. Polymorphic iterators have another drawback: the client is responsible for deleting them. This is error-prone, because it's easy to forget to free a heap-allocated iterator object when you're finished with it.That's especially likely when there are multiple exit points in an operation. And if an exception is triggered, the iterator object will never be freed. The Proxy (233) pattern provides a remedy. We can use a stack-allocated proxy as a stand-in for the real iterator. The proxy deletes the iterator in its destructor. Thus when the proxy goes out of scope, the real iterator will get deal located along with it. Theproxy ensures proper cleanup, even in the face of exceptions. This is an application of the well-known C++ technique "resource allocation is initialization" [ES90]. The Sample Code gives an example. Iterators may have privileged access.An iterator can be viewed as an extension of the aggregate that created it. The iterator and the aggregate are tightly coupled. We can express this close relationship in C++ by making the iterator a friend of its aggregate. Then you don't need to define aggregate operations whose sole purpose is to let iterators implement traversal efficiently. However, such privileged access can make defining new traversals difficult, since it'll require changing the aggregate interface to add another friend. To avoid this problem, the Iterator class can include protected operations for accessing important but publicly unavailable members of the aggregate. Iterator subclasses (and only Iterator subclasses) may use these protected operations to gain privileged access to the aggregate. Iterators for composites.External iterators can be difficult to implement over recursive aggregate structures like those in the Composite (183) pattern, because a position in the structure may span many levels of nested aggregates. Therefore an external iterator has to store a path through the Composite to keep track of the current object. Sometimes it's easier just to use an internal iterator. It can record the current position simply by calling itself recursively, thereby storing the path implicitly in the call stack. If the nodes in a Composite have an interface for moving from a node to its siblings, parents, and children, then a cursor-based iterator may offer a better alternative. The cursor only needs to keep track of the current node; it can rely on the node interface to traverse the Composite. Composites often need to be traversed in more than one way. Preorder,postorder, in order, and breadth-first traversals are common. You can support each kind of traversal with a different class of iterator. Null iterators.A NullIterator is a degenerate iterator that's helpful for handling boundary conditions. By definition, a NullIterator is always done with traversal; that is, its IsDone operation always evaluates to true. NullIterator can make traversing tree-structured aggregates (like Composites) easier. At each point in the traversal, we ask the current element for an iterator for its children. Aggregate elements return a concrete iterator as usual. But leaf elements return an instance of NullIterator. That lets us implement traversal over the entire structure in a uniform way. We'll look at the implementation of a simple List class, which is part of our foundation library (Appendix C) .We'll show two Iterator implementations, one for traversing the List in front-to-back order, and another for traversing back-to-front (the foundation library supports only the first one). Then we show how to use these iterators and how to avoid committing to a particular implementation. After that, we change the design to make sure iterators get deleted properly. The last example illustrates an internal iterator and compares it to its external counterpart. List and Iterator interfaces.First let's look at the part of the List interface that's relevant to implementing iterators. Refer to (Appendix C). for the full interface. The List class provides a reasonably efficient way to support iteration through its public interface. It's sufficient to implement both traversals. So there's no need to give iterators privileged access to the underlying data structure; that is, the iterator classes are not friends of List. To enable transparent use of the different traversals we define an abstract Iterator class, which defines the iterator interface. The implementation of ReverseListIterator is identical, except itsFirst operation positions _current to the end of the list, and Next decrements_current toward the first item. Using the iterators.Let's assume we have a List of Employee objects,and we would like to print all the contained employees. The Employee class supports this with a Print operation. To print the list, we define a PrintEmployees operation that takes an iterator as an argument. It uses the iterator to traverse and print the list. Since we have iterators for both back-to-front and front-to-back traversals, we can reuse this operation to print the employees in both orders. Avoiding commitment to a specific list implementation.Let's consider how a skiplist variation of List would affect our iteration code. A SkipList subclass of List must provide a SkipListIterator that implements the Iterator interface. Internally, theSkipListIterator has to keep more than just an index to do the iteration efficiently. But sinceSkipListIterator conforms to the Iterator interface, the PrintEmployees operation can also be used when the employees are stored in a SkipList object. Although this approach works, it would be better if we didn't have to commit to a specific List implementation, namely SkipList. We can introduce an AbstractList class to standardize the list interface for different list implementations. List and SkipList become subclasses of AbstractList. An alternative would be to define a general mixin class Traversable that defines the interface for creating an iterator. Aggregate classes can mix in Traversable to support polymorphic iteration. Making sure iterators get deleted.Notice that CreateIterator returns a newly allocated iterator object. We're responsible for deleting it. If we forget,then we've created a storage leak. To make life easier for clients,we'll provide an IteratorPtr that acts as a proxy for an iterator. It takes care of cleaning up the Iterator object when it goes out of scope. IteratorPtr is always allocated on the stack. C++ automatically takes care of calling its destructor, which deletes the real iterator.IteratorPtr overloads both operator-> and operator* in such a way that an IteratorPtr can be treated just like a pointer to an iterator. The members of IteratorPtr are all implemented inline; thus they can incur no overhead. An internal ListIterator.As a final example, let's look at a possible implementation of an internal or passive ListIterator class. Here the iterator controls the iteration, and it applies an operation to each element. The issue in this case is how to parameterize the iterator with the operation we want to perform on each element. C++ does not support anonymous functions or closures that other languages provide for this task. There are at least two options: (1) Pass in a pointer to a function (global or static), or (2) rely on subclassing. In the first case, the iterator calls the operation passed to it at each point in the iteration. In the second case, the iterator calls an operation that a subclass overrides to enact specific behavior. Neither option is perfect. Often you want to accumulate state during the iteration, and functions aren't well-suited to that; we would have to use static variables to remember the state. An Iterator subclass provides us with a convenient place to store the accumulated state, like in an instance variable. But creating a subclass for every different traversal is more work. Here's a sketch of the second option, which uses subclassing. We call the internal iterator a ListTraverser. ListTraverser takes a List instance as a parameter.Internally it uses an external ListIterator to do the traversal. Traverse starts the traversal and calls ProcessItem for each item. The internal iterator can choose to terminate a traversal by returning false from ProcessItem. Traverse returns whether the traversal terminated prematurely. Let's use a ListTraverser to print the first 10employees from our employee list. To do it we have to subclass ListTraverser and override ProcessItem. We count the number of printed employees in a _count instance variable. Note how the client doesn't specify the iteration loop. The entire iteration logic can be reused. This is the primary benefit of an internal iterator. It's a bit more work than an external iterator,though, because we have to define a new class. Contrast this with using an external iterator: Internal iterators can encapsulate different kinds of iteration. For example, FilteringListTraverser encapsulates an iteration that processes only items that satisfy a test: This interface is the same as ListTraverser's except for an added TestItem member function that defines the test.Subclasses override TestItem to specify the test. A variant of this class could define Traverse to return if at least one item satisfies the test. Iterators are common in object-oriented systems. Most collection class libraries offer iterators in one form or another. Here's an example from the Booch components [Boo94], a popular collection class library. It provides both a fixed size(bounded) and dynamically growing (unbounded) implementation of a queue. The queue interface is defined by an abstract Queue class. To support polymorphic iteration over the different queue implementations, the queue iterator is implemented in the terms of the abstract Queue class interface. This variation has the advantage that you don't need a factory method to ask the queue implementations for their appropriate iterator. However, it requires the interface of the abstract Queue class to be powerful enough to implement the iterator efficiently. Iterators don't have to be defined as explicitly in Smalltalk. The standard collection classes (Bag, Set, Dictionary, OrderedCollection,String, etc.) define an internal iterator method do:, which takes a block (i.e., closure) as an argument. Each element in the collection is bound to the local variable in the block; then the block is executed. Smalltalk also includes a set of Stream classes that support an iterator-like interface. ReadStream is essentially an Iterator, and it can act as an external iterator for all the sequential collections. There are no standard external iterators for nonsequential collections such as Set and Dictionary. Polymorphic iterators and the cleanup Proxy described earlier are provided by the ET++ container classes [WGM88]. The Unidraw graphical editing framework classes use cursor-based iterators [VL90]. ObjectWindows 2.0 [Bor94] provides a class hierarchy of iterators for containers. You can iterate over different container types in the same way. The ObjectWindow iteration syntax relies on overloading the post increment operator ++ to advance the iteration. Composite (183):Iterators are often applied to recursive structures such as Composites. Factory Method (121):Polymorphic iterators rely on factory methods to instantiate the appropriate Iterator subclass. Memento (316) is often used in conjunction with the Iterator pattern. An iterator can use a memento to capture the state of an iteration. The iterator stores the memento internally.
##%%&&
Define an object that encapsulates how a set of objects interact.Mediator promotes loose coupling by keeping objects from referring to each other explicitly, and it lets you vary their interaction independently. Object-oriented design encourages the distribution of behavior among objects. Such distribution can result in an object structure with many connections between objects; in the worst case, every object ends up knowing about every other. Though partitioning a system into many objects generally enhances reusability, proliferating interconnections tend to reduce it again.Lots of interconnections make it less likely that an object can work without the support of others the system acts as though it were monolithic. Moreover, it can be difficult to change the system's behavior in any significant way, since behavior is distributed among many objects. As a result, you may be forced to define many subclasses to customize the system's behavior. As an example, consider the implementation of dialog boxes in a graphical user interface. A dialog box uses a window to present a collection of widgets such as buttons, menus, and entry fields, as shown here: Often there are dependencies between the widgets in the dialog. For example, a button gets disabled when a certain entry field is empty.Selecting an entry in a list of choices called a list box might change the contents of an entry field. Conversely, typing text into the entry field might automatically select one or more corresponding entries in the list box. Once text appears in the entry field, other buttons may become enabled that let the user do something with the text, such as changing or deleting the thing to which it refers. Different dialog boxes will have different dependencies between widgets. So even though dialogs display the same kinds of widgets,they can't simply reuse stock widget classes; they have to be customized to reflect dialog-specific dependencies. Customizing them individually by subclassing will be tedious, since many classes are involved. You can avoid these problems by encapsulating collective behavior in a separate mediator object. A mediator is responsible for controlling and coordinating the interactions of a group of objects.The mediator serves as an intermediary that keeps objects in the group from referring to each other explicitly. The objects only know the mediator, thereby reducing the number of interconnections. For example, FontDialogDirector can be the mediator between the widgets in a dialog box. A FontDialogDirector object knows the widgets in a dialog and coordinates their interaction. It acts as a hub of communication for widgets: The following interaction diagram illustrates how the objects cooperate to handle a change in a list box's selection: Here's the succession of events by which a list box's selection passes to an entry field: The list box tells its director that it's changed. The director gets the selection from the list box. The director passes the selection to the entry field. Now that the entry field contains some text, the director enables button(s) for initiating an action (e.g., "demibold," "oblique") Note how the director mediates between the list box and the entry field.Widgets communicate with each other only indirectly, through the director. They don't have to know about each other; all they know is the director. Furthermore, because the behavior is localized in one class,it can be changed or replaced by extending or replacing that class. Here's how the FontDialogDirector abstraction can be integrated into a class library: DialogDirector is an abstract class that defines the overall behavior of a dialog. Clients call the ShowDialog operation to display the dialog on the screen. CreateWidgets is an abstract operation for creating the widgets of a dialog. WidgetChanged is another abstract operation;widgets call it to inform their director that they have changed.DialogDirector subclasses override CreateWidgets to create the proper widgets, and they override WidgetChanged to handle the changes. Use the Mediator pattern when?a set of objects communicate in well-defined but complex ways. The resulting interdependencies are unstructured and difficult to understand.?reusing an object is difficult because it refers to and communicates with many other objects.?a behavior that's distributed between several classes should be customizable without a lot of subclassing. Mediator (DialogDirector) defines an interface for communicating with Colleague objects.?ConcreteMediator (FontDialogDirector) implements cooperative behavior by coordinating Colleague objects. knows and maintains its colleagues.?Colleague classes (ListBox, EntryField) each Colleague class knows its Mediator object. each colleague communicates with its mediator whenever it would have otherwise communicated with another colleague. Colleagues send and receive requests from a Mediator object. The mediator implements the cooperative behavior by routing requests between the appropriate colleague(s). The Mediator pattern has the following benefits and drawbacks: It limits subclassing.A mediator localizes behavior that otherwise would be distributed among several objects. Changing this behavior requires subclassing Mediator only; Colleague classes can be reused as is. It decouples colleagues.A mediator promotes loose coupling between colleagues. You can vary and reuse Colleague and Mediator classes independently. It simplifies object protocols.A mediator replaces many-to-many interactions with one-to-many interactions between the mediator and its colleagues. One-to-many relationships are easier to understand, maintain, and extend. It abstracts how objects cooperate.Making mediation an independent concept and encapsulating it in an object lets you focus on how objects interact apart from their individual behavior. That can help clarify how objects interact in a system. It centralizes control.The Mediator pattern trades complexity of interaction for complexity in the mediator. Because a mediator encapsulates protocols, it can become more complex than any individual colleague. This can make the mediator itself a monolith that's hard to maintain. The following implementation issues are relevant to the Mediatorpattern: Omitting the abstract Mediator class.There's no need to define an abstract Mediator class when colleagues work with only one mediator. The abstract coupling that the Mediator class provides lets colleagues work with different Mediatorsubclasses, and vice versa. Colleague-Mediator communication.Colleagues have to communicate with their mediator when an event of interest occurs. One approach is to implement the Mediator as an Observer using the Observer (326) pattern. Colleague classes act as Subjects, sending notifications to the mediator whenever they change state. The mediator responds by propagating the effects of the change to other colleagues. Another approach defines a specialized notification interface in Mediator that lets colleagues be more direct in their communication.Smalltalk/V for Windows uses a form of delegation: When communicating with the mediator, a colleague passes itself as an argument, allowing the mediator to identify the sender. The Sample Code uses this approach, and the Smalltalk/V implementation is discussed further in the Known Uses. We'll use a DialogDirector to implement the font dialog box shown in the Motivation. The abstract class DialogDirector defines the interface for directors. Changed calls the director's WidgetChanged operation. Widgets call WidgetChanged on their director to inform it of a significant event. Subclasses of DialogDirector override WidgetChanged to affect the appropriate widgets. The widget passes a reference to itself as an argument to WidgetChanged to let the director identify the widget that changed.DialogDirector subclasses redefine theCreateWidgets pure virtual to construct the widgets in the dialog. The ListBox, EntryField, and Button are subclasses of Widget for specialized user interface elements. ListBox provides a GetSelection operation to get the current selection, and EntryField'sSetText operation puts new text into the field. The complexity of WidgetChanged increases proportionally with the complexity of the dialog. Large dialogs are undesirable for other reasons, of course, but mediator complexity might mitigate the pattern's benefits in other applications. Both ET++ [WGM88] and the THINK C class library [Sym93b] use director-like objects in dialogs as mediators between widgets. The application architecture of Smalltalk/V for Windows is based on a mediator structure [LaL94]. In that environment, an application consists of a Window containing a set of panes. The library contains several predefined Pane objects; examples includeTextPane, ListBox, Button, and so on.These panes can be used without subclassing. An application developer only subclasses from ViewManager, a class that's responsible for doing inter-pane coordination. ViewManager is the Mediator, and each pane only knows its view manager, which is considered the "owner" of the pane. Panes don't refer to each other directly. The following object diagram shows a snapshot of an application at run-time: Smalltalk/V uses an event mechanism for Pane-ViewManager communication. A pane generates an event when it wants to get information from the mediator or when it wants to inform the mediator that something significant happened. An event defines a symbol (e.g.,#select) that identifies the event. To handle the event, the view manager registers a method selector with the pane. This selector is the event's handler; it will be invoked whenever the event occurs. The following code excerpt shows how a ListPane object gets created inside a ViewManager subclass and how ViewManager registers an event handler for the #select event: Another application of the Mediator pattern is in coordinating complex updates. An example is the ChangeManager class mentioned in Observer (326). ChangeManager mediates between subjects and observers to avoid redundant updates. When an object changes, it notifies the ChangeManager, which in turn coordinates the update by notifying the object's dependents. A similar application appears in the Unidraw drawing framework [VL90] and uses a class called CSolver to enforce connectivity constraints between "connectors." Objects in graphical editors can appear to stick to one another in different ways. Connectors are useful in applications that maintain connectivity automatically, like diagram editors and circuit design systems. CSolver is a mediator between connectors. It solves the connectivity constraints and updates the connectors' positions to reflect them. Facade (208) differs from Mediator in that it abstracts a subsystem of objects to provide a more convenient interface. Its protocol is unidirectional; that is, Facade objects make requests of the subsystem classes but not vice versa. In contrast, Mediator enables cooperative behavior that colleague objects don't or can't provide, and the protocol is multidirectional. Colleagues can communicate with the mediator using the Observer (326) pattern.
##%%&&
Without violating encapsulation, capture and externalize an object's internal state so that the object can be restored to this state later. Sometimes it's necessary to record the internal state of an object.This is required when implementing checkpoints and undo mechanisms that let users back out of tentative operations or recover from errors. You must save state information somewhere so that you can restore objects to their previous states. But objects normally encapsulate some or all of their state, making it inaccessible to other objects and impossible to save externally. Exposing this state would violate encapsulation, which can compromise the application's reliability and extensibility. Consider for example a graphical editor that supports connectivity between objects. A user can connect two rectangles with a line, and the rectangles stay connected when the user moves either of them. The editor ensures that the line stretches to maintain the connection. A well-known way to maintain connectivity relationships between objects is with a constraint-solving system. We can encapsulate this functionality in a ConstraintSolver object.ConstraintSolver records connections as they are made and generates mathematical equations that describe them. It solves these equations whenever the user makes a connection or otherwise modifies the diagram. ConstraintSolver uses the results of its calculations to rearrange the graphics so that they maintain the proper connections. Supporting undo in this application isn't as easy as it may seem. An obvious way to undo a move operation is to store the original distance moved and move the object back an equivalent distance. However, this does not guarantee all objects will appear where they did before.Suppose there is some slack in the connection. In that case, simply moving the rectangle back to its original location won't necessarily achieve the desired effect. In general, the ConstraintSolver's public interface might be insufficient to allow precise reversal of its effects on other objects. The undo mechanism must work more closely with ConstraintSolver to reestablish previous state, but we should also avoid exposing the ConstraintSolver's internals to the undo mechanism. We can solve this problem with the Memento pattern. A memento is an object that stores a snapshot of the internal state of another object the memento's originator. The undo mechanism will request a memento from the originator when it needs to checkpoint the originator's state. The originator initializes the memento with information that characterizes its current state. Only the originator can store and retrieve information from the memento the memento is "opaque" to other objects. In the graphical editor example just discussed, the ConstraintSolver can act as an originator. The following sequence of events characterizes the undo process: The editor requests a memento from the ConstraintSolver as a side-effect of the move operation. The ConstraintSolver creates and returns a memento, an instance of a class SolverState in this case. A SolverState memento contains data structures that describe the current state of the ConstraintSolver'sinternal equations and variables. Later when the user undoes the move operation, the editor gives the SolverState back to the ConstraintSolver. Based on the information in the SolverState, the ConstraintSolver changes its internal structures to return its equations and variables to their exact previous state. This arrangement lets the ConstraintSolver entrust other objects with the information it needs to revert to a previous state without exposing its internal structure and representations. Use the Memento pattern when?a snapshot of (some portion of) an object's state must be saved so that it can be restored to that state later, and?a direct interface to obtaining the state would expose implementation details and break the object's encapsulation. Memento (SolverState) stores internal state of the Originator object. The memento may store as much or as little of the originator's internal state as necessary at its originator's discretion. protects against access by objects other than the originator. Mementos have effectively two interfaces. Caretaker sees a narrow interface to the Memento it can only pass the memento to other objects. Originator, in contrast, sees a wide interface, one that lets it access all the data necessary to restore itself to its previous state. Ideally, only the originator that produced the memento would be permitted to access the memento's internal state.?Originator (ConstraintSolver) creates a memento containing a snapshot of its current internal state. uses the memento to restore its internal state.?Caretaker (undo mechanism) is responsible for the memento's safekeeping. never operates on or examines the contents of a memento. A caretaker requests a memento from an originator, holds it for a time, and passes it back to the originator, as the following interaction diagram illustrates: Sometimes the caretaker won't pass the memento back to the originator,because the originator might never need to revert to an earlier state.?Mementos are passive. Only the originator that created a memento will assign or retrieve its state. The Memento pattern has several consequences: Preserving encapsulation boundaries.Memento avoids exposing information that only an originator should manage but that must be stored nevertheless outside the originator.The pattern shields other objects from potentially complex Originator internals, thereby preserving encapsulation boundaries. It simplifies Originator.In other encapsulation-preserving designs, Originator keeps the versions of internal state that clients have requested. That puts all the storage management burden on Originator. Having clients manage the state they ask for simplifies Originator and keeps clients from having to notify originators when they're done. Using mementos might be expensive.Mementos might incur considerable overhead if Originator must copy large amounts of information to store in the memento or if clients create and return mementos to the originator often enough. Unless encapsulating and restoring Originator state is cheap, the pattern might not be appropriate. See the discussion of incrementality in the Implementation section. Defining narrow and wide interfaces.It may be difficult in some languages to ensure that only the originator can access the memento's state. Hidden costs in caring for mementos.A caretaker is responsible for deleting the mementos it cares for.However, the caretaker has no idea how much state is in the memento.Hence an otherwise lightweight caretaker might incur large storage costs when it stores mementos. Here are two issues to consider when implementing the Memento pattern: Language support.Mementos have two interfaces: a wide one for originators and a narrow one for other objects. Ideally the implementation language will support two levels of static protection. C++ lets you do this by making the Originator a friend of Memento and making Memento's wide interface private. Only the narrow interface should be declared public. Storing incremental changes.When mementos get created and passed back to their originator in a predictable sequence, then Memento can save just the incremental change to the originator's internal state. For example, undoable commands in a history list can use mementos to ensure that commands are restored to their exact state when they're undone (see Command (263)). The history list defines a specific order in which commands can be undone and redone. That means mementos can store just the incremental change that a command makes rather than the full state of every object they affect. In the Motivation example given earlier, the constraint solver can store only those internal structures that change to keep the line connecting the rectangles, as opposed to storing the absolute positions of these objects. The C++ code given here illustrates the ConstraintSolver example discussed earlier. Weuse MoveCommand objects (see Command (263)) to (un)do the translation of a graphical object from one position to another.The graphical editor calls the command's Execute operation to move a graphical object and Unexecute to undo the move.The command stores its target, the distance moved, and an instance ofConstraintSolverMemento, a memento containing state from the constraint solver. The connection constraints are established by the classConstraintSolver. Its key member function is Solve, which solves the constraints registered with the AddConstraint operation. To support undo,ConstraintSolver's state can be externalized withCreateMemento into a ConstraintSolverMementoinstance. The constraint solver can be returned to a previous state by calling SetMemento. ConstraintSolveris a Singleton (144). Execute acquires a ConstraintSolverMemento memento before it moves the graphic. Unexecute moves the graphic back, sets the constraint solver's state to the previous state, and finally tells the constraint solver to solve the constraints. The preceding sample code is based on Unidraw's support for connectivity through its CSolver class [VL90]. Collections in Dylan [App92] provide an iteration interface that reflects the Memento pattern. Dylan's collections have the notion of a"state" object, which is a memento that represents the state of the iteration. Each collection can represent the current state of the iteration in any way it chooses; the representation is completely hidden from clients. The Dylan iteration approach might be translated to C++ as follows: CreateInitialState returns an initializedIterationState object for the collection. Next advances the state object to the next position in the iteration; it effectively increments the iteration index. IsDone returns true if Next has advanced beyond the last element in the collection. CurrentItem dereferences the state object and returns the element in the collection to which it refers.Copy returns a copy of the given state object. This is useful for marking a point in an iteration. The memento-based iteration interface has two interesting benefits: More than one state can work on the same collection. (The same is true of the Iterator (289) pattern.) It doesn't require breaking a collection's encapsulation to support iteration. The memento is only interpreted by the collection itself; no one else has access to it. Other approaches to iteration require breaking encapsulation by making iterator classes friends of their collection classes (see Iterator (289)). The situation is reversed in the memento-based implementation: Collection is a friend of theIteratorState. The QOCA constraint-solving toolkit stores incremental information in mementos [HHMV92]. Clients can obtain a memento that characterizes the current solution to a system of constraints. The memento contains only those constraint variables that have changed since the last solution. Usually only a small subset of the solver's variables changes for each new solution. This subset is enough to return the solver to the preceding solution; reverting to earlier solutions requires restoring mementos from the intervening solutions. Hence you can't set mementos in any order; QOCA relies on a history mechanism to revert to earlier solutions. Command (263): Commands can use mementos to maintain state for undoable operations. Iterator (289): Mementos can be used for iteration as described earlier.
##%%&&
Define a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically. A common side-effect of partitioning a system into a collection of cooperating classes is the need to maintain consistency between related objects. You don't want to achieve consistency by making the classes tightly coupled, because that reduces their reusability. For example, many graphical user interface toolkits separate the presentational aspects of the user interface from the underlying application data [KP88, LVC89, P+88, WGM88].Classes defining application data and presentations can be reused independently. They can work together, too. Both a spreadsheet object and bar chart object can depict information in the same application data object using different presentations. The spreadsheet and the bar chart don't know about each other, thereby letting you reuse only the one you need. But they behave as though they do. When the user changes the information in the spreadsheet, the bar chart reflects the changes immediately, and vice versa. This behavior implies that the spreadsheet and bar chart are dependent on the data object and therefore should be notified of any change in its state. And there's no reason to limit the number of dependent objects to two; there may be any number of different user interfaces to the same data. The Observer pattern describes how to establish these relationships.The key objects in this pattern are subject and observer. A subject may have any number of dependent observers. All observers are notified whenever the subject undergoes a change in state. In response, each observer will query the subject to synchronize its state with the subject's state. This kind of interaction is also known as publish-subscribe. The subject is the publisher of notifications. It sends out these notifications without having to know who its observers are. Any number of observers can subscribe to receive notifications. Use the Observer pattern in any of the following situations:?When an abstraction has two aspects, one dependent on the other.Encapsulating these aspects in separate objects lets you vary and reuse them independently.?When a change to one object requires changing others, and you don't know how many objects need to be changed.?When an object should be able to notify other objects without making assumptions about who these objects are. In other words, you don't want these objects tightly coupled. Subject knows its observers. Any number of Observer objects may observe a subject. provides an interface for attaching and detaching Observer objects.?Observer defines an updating interface for objects that should be notified of changes in a subject.?ConcreteSubject stores state of interest to ConcreteObserver objects. sends a notification to its observers when its state changes.?ConcreteObserver maintains a reference to a ConcreteSubject object. stores state that should stay consistent with the subject's. implements the Observer updating interface to keep its state consistent with the subject's. ConcreteSubject notifies its observers whenever a change occurs that could make its observers' state inconsistent with its own.?After being informed of a change in the concrete subject, aConcreteObserver object may query the subject for information.ConcreteObserver uses this information to reconcile its state with that of the subject. The following interaction diagram illustrates the collaborations between a subject and two observers: Note how the Observer object that initiates the change request postpones its update until it gets a notification from the subject.Notify is not always called by the subject. It can be called by an observer or by another kind of object entirely. The Implementation section discusses some common variations. The Observer pattern lets you vary subjects and observers independently. You can reuse subjects without reusing their observers, and vice versa. It lets you add observers without modifying the subject or other observers. Further benefits and liabilities of the Observer pattern include the following: Abstract coupling between Subject and Observer.All a subject knows is that it has a list of observers, each conforming to the simple interface of the abstract Observer class.The subject doesn't know the concrete class of any observer. Thus the coupling between subjects and observers is abstract and minimal. Because Subject and Observer aren't tightly coupled, they can belong to different layers of abstraction in a system. A lower-level subject can communicate and inform a higher-level observer, thereby keeping the system's layering intact. If Subject and Observer are lumped together, then the resulting object must either span two layers (and violate the layering), or it must be forced to live in one layer or the other (which might compromise the layering abstraction). Support for broadcast communication.Unlike an ordinary request, the notification that a subject sends needn't specify its receiver. The notification is broadcast automatically to all interested objects that subscribed to it. The subject doesn't care how many interested objects exist; its only responsibility is to notify its observers. This gives you the freedom to add and remove observers at any time. It's up to the observer to handle or ignore a notification. Unexpected updates.Because observers have no knowledge of each other's presence, they can be blind to the ultimate cost of changing the subject. A seemingly innocuous operation on the subject may cause a cascade of updates to observers and their dependent objects. Moreover, dependency criteria that aren't well-defined or maintained usually lead to spurious updates, which can be hard to track down. This problem is aggravated by the fact that the simple update protocol provides no details on what changed in the subject. Without additional protocol to help observers discover what changed, they maybe forced to work hard to deduce the changes. Several issues related to the implementation of the dependency mechanism are discussed in this section. Mapping subjects to their observers.The simplest way for a subject to keep track of the observers it should notify is to store references to them explicitly in the subject. However, such storage may be too expensive when there are many subjects and few observers. One solution is to trade space for time by using an associative look-up (e.g., a hash table) to maintain the subject-to-observer mapping. Thus a subject with no observers does not incur storage overhead. On the other hand, this approach increases the cost of accessing the observers. Observing more than one subject.It might make sense in some situations for an observer to depend on more than one subject. For example, a spreadsheet may depend on more than one data source. It's necessary to extend the Update interface in such cases to let the observer know which subject is sending the notification. The subject can simply pass itself as a parameter in the Update operation, thereby letting the observer know which subject to examine. Who triggers the update?The subject and its observers rely on the notification mechanism to stay consistent. But what object actually calls Notify to trigger the update? Here are two options: Have state-setting operations on Subject call Notify after they change the subject's state. The advantage of this approach is that clients don't have to remember to call Notify on the subject. The disadvantage is that several consecutive operations will cause several consecutive updates, which may be inefficient. Make clients responsible for calling Notify at the right time.The advantage here is that the client can wait to trigger the update until after a series of state changes has been made, thereby avoiding needless intermediate updates. The disadvantage is that clients have an added responsibility to trigger the update. That makes errors more likely, since clients might forget to call Notify. Dangling references to deleted subjects.Deleting a subject should not produce dangling references in its observers. One way to avoid dangling references is to make the subject notify its observers as it is deleted so that they can reset their reference to it. In general, simply deleting the observers is not an option, because other objects may reference them, or they may be observing other subjects as well. Making sure Subject state is self-consistent beforenotification.It's important to make sure Subject state is self-consistent before calling Notify, because observers query the subject for its current state in the course of updating their own state. This self-consistency rule is easy to violate unintentionally when Subject subclass operations call inherited operations. For example,the notification in the following code sequence is trigged when the subject is in an inconsistent state: You can avoid this pitfall by sending notifications from template methods (Template Method (360)) in abstract Subject classes. Define a primitive operation for subclasses to override, and make Notify the last operation in the template method, which will ensure that the object is self-consistent when subclasses override Subject operations. By the way, it's always a good idea to document which Subject operations trigger notifications. Avoiding observer-specific update protocols: the push and pull models. Implementations of the Observer pattern often have the subject broadcast additional information about the change. The subject passes this information as an argument to Update. The amount of information may vary widely. At one extreme, which we call the push model, the subjects ends observers detailed information about the change, whether they want it or not. At the other extreme is the pull model; the subject sends nothing but the most minimal notification, and observers ask for details explicitly thereafter. The pull model emphasizes the subject's ignorance of its observers,whereas the push model assumes subjects know something about their observers' needs. The push model might make observers less reusable,because Subject classes make assumptions about Observer classes that might not always be true. On the other hand, the pull model may be inefficient, because Observer classes must ascertain what changed without help from the Subject. Specifying modifications of interest explicitly.You can improve update efficiency by extending the subject's registration interface to allow registering observers only for specific events of interest. When such an event occurs, the subject informs only those observers that have registered interest in that event. One way to support this uses the notion of aspects for Subject objects. To register interest in particular events, observers are attached to their subjects using where interest specifies the event of interest. At notification time, the subject supplies the changed aspect to its observers as a parameter to the Update operation. For example: Encapsulating complex update semantics.When the dependency relationship between subjects and observers is particularly complex, an object that maintains these relationships might be required. We call such an object a ChangeManager. Its purpose is to minimize the work required to make observers reflect a change in their subject. For example, if an operation involves changes to several interdependent subjects, you might have to ensure that their observers are notified only after all the subjects have been modified to avoid notifying observers more than once. ChangeManager has three responsibilities: It maps a subject to its observers and provides an interface to maintain this mapping. This eliminates the need for subjects to maintain references to their observers and vice versa. It defines a particular update strategy. It updates all dependent observers at the request of a subject. The following diagram depicts a simple ChangeManager-based implementation of the Observer pattern. There are two specialized ChangeManagers.SimpleChangeManager is naive in that it always updates all observers of each subject. In contrast, DAGChangeManager handles directed-acyclic graphs of dependencies between subjects and their observers. A DAGChangeManager is preferable to a SimpleChangeManager when an observer observes more than one subject. In that case, a change in two or more subjects might cause redundant updates. The DAGChangeManager ensures the observer receives just one update. SimpleChangeManager is fine when multiple updates aren't an issue. ChangeManager is an instance of the Mediator (305) pattern. In general there is only one ChangeManager, and it is known globally. The Singleton (144) pattern would be useful here. Combining the Subject and Observer classes.Class libraries written in languages that lack multiple inheritance (like Smalltalk) generally don't define separate Subject and Observer classes but combine their interfaces in one class. That lets you define an object that acts as both a subject and an observer without multiple inheritance. In Smalltalk, for example, the Subject and Observer interfaces are defined in the root class Object, making them available to all classes. This implementation supports multiple subjects for each observer. The subject passed to the Update operation lets the observer determine which subject changed when it observes more than one. ClockTimer is a concrete subject for storing and maintaining the time of day. It notifies its observers every second.ClockTimer provides the interface for retrieving individual time units such as the hour, minute, and second. The Tick operation gets called by an internal timer at regular intervals to provide an accurate time base. Tick updates the ClockTimer's internal state and calls Notify to inform observers of the change: Now we can define a class DigitalClock that displays the time. It inherits its graphical functionality from a Widget class provided by a user interface toolkit. The Observer interface is mixed into the DigitalClock interface by inheriting from Observer. Whenever the timer ticks, the two clocks will be updated and will redisplay themselves appropriately. The first and perhaps best-known example of the Observer pattern appears in Smalltalk Model/View/Controller (MVC), the user interface framework in the Smalltalk environment [KP88]. MVC's Model class plays the role of Subject, while View is the base class for observers. Smalltalk,ET++ [WGM88], and the THINK class library [Sym93b] provide a general dependency mechanism by putting Subject and Observer interfaces in the parent class for all other classes in the system. Other user interface toolkits that employ this pattern are Interviews [LVC89], the AndrewToolkit [P+88], and Unidraw [VL90]. InterViewsdefines Observer and Observable (for subjects) classes explicitly.Andrew calls them "view" and "data object," respectively. Unidraw splits graphical editor objects into View (for observers) and Subject parts. Mediator (305): By encapsulating complex update semantics, the ChangeManager acts as mediator between subjects and observers. Singleton (144):The ChangeManager may use the Singleton pattern to make it unique and globally accessible.
##%%&&
Allow an object to alter its behavior when its internal state changes.The object will appear to change its class. Consider a class TCPConnection that represents a network connection.A TCPConnection object can be in one of several different states:Established, Listening, Closed. When a TCPConnection object receives requests from other objects, it responds differently depending on its current state. For example, the effect of an Open request depends on whether the connection is in its Closed state or its Established state. The State pattern describes how TCPConnection can exhibit different behavior in each state. The key idea in this pattern is to introduce an abstract class called TCPState to represent the states of the network connection. The TCPState class declares an interface common to all classes that represent different operational states. Subclasses of TCPStateimplement state-specific behavior. For example, the classes TCPEstablished and TCPClosed implement behavior particular to the Established and Closed states of TCPConnection. The class TCPConnection maintains a state object (an instance of a subclass of TCPState) that represents the current state of the TCPconnection. The class TCPConnection delegates all state-specific requests to this state object. TCPConnection uses its TCP State subclass instance to perform operations particular to the state of the connection. Whenever the connection changes state, the TCPConnection object changes the state object it uses. When the connection goes from established to closed, for example, TCPConnection will replace its TCPEstablished instance with a TCP Closed instance. Use the State pattern in either of the following cases:?An object's behavior depends on its state, and it must change its behavior at run-time depending on that state.?Operations have large, multipart conditional statements that depend on the object's state. This state is usually represented by one or more enumerated constants. Often, several operations will contain this same conditional structure. The State pattern puts each branch of the conditional in a separate class. This lets you treat the object's state as an object in its own right that can vary independently from other objects. Context (TCPConnection) defines the interface of interest to clients. maintains an instance of a ConcreteState subclass that defines the current state. State (TCPState) defines an interface for encapsulating the behavior associated with a particular state of the Context.?ConcreteState subclasses (TCPEstablished, TCPListen, TCPClosed) each subclass implements a behavior associated with a state of the Context. Context delegates state-specific requests to the current ConcreteStateobject.?A context may pass itself as an argument to the State object handling the request. This lets the State object access the context if necessary.?Context is the primary interface for clients. Clients can configure a context with State objects. Once a context is configured, its clients don't have to deal with the State objects directly.?Either Context or the ConcreteState subclasses can decide which state succeeds another and under what circumstances. The State pattern has the following consequences: It localizes state-specific behavior and partitions behavior for different states.The State pattern puts all behavior associated with a particular state into one object. Because all state-specific code lives in a State subclass, new states and transitions can be added easily by defining new subclasses. An alternative is to use data values to define internal states and have Context operations check the data explicitly. But then we'd have look-alike conditional or case statements scattered throughout Context's implementation. Adding a new state could require changing several operations, which complicates maintenance. The State pattern avoids this problem but might introduce another,because the pattern distributes behavior for different states across several State subclasses. This increases the number of classes and is less compact than a single class. But such distribution is actually good if there are many states, which would otherwise necessitate large conditional statements. Like long procedures, large conditional statements are undesirable.They're monolithic and tend to make the code less explicit, which in turn makes them difficult to modify and extend. The State pattern offers a better way to structure state-specific code. The logic that determines the state transitions doesn't reside in monolithic if or switch statements but instead is partitioned between the State subclasses. Encapsulating each state transition and action in a class elevates the idea of an execution state to full object status. That imposes structure on the code and makes its intent clearer. It makes state transitions explicit.When an object defines its current state solely in terms of internal data values, its state transitions have no explicit representation;they only show up as assignments to some variables. Introducing separate objects for different states makes the transitions more explicit. Also, State objects can protect the Context from inconsistent internal states, because state transitions are atomic from the Context's perspective they happen by rebinding one variable (the Context's State object variable), not several [dCLF93]. State objects can be shared.If State objects have no instance variables that is, the state they represent is encoded entirely in their type then contexts can share a State object. When states are shared in this way, they are essentially flyweights (see Flyweight (218)) with no intrinsic state, only behavior. The State pattern raises a variety of implementation issues: Who defines the state transitions?The State pattern does not specify which participant defines the criteria for state transitions. If the criteria are fixed, then they can be implemented entirely in the Context. It is generally more flexible and appropriate, however, to let the State subclasses themselves specify their successor state and when to make the transition. This requires adding an interface to the Context that lets State objects set the Context's current state explicitly. Decentralizing the transition logic in this way makes it easy to modify or extend the logic by defining new State subclasses. A disadvantage of decentralization is that one State subclass will have knowledge of at least one other, which introduces implementation dependencies between subclasses. A table-based alternative.In C++ Programming Style [Car92], Cargill describes another way to impose structure on state-driven code: He uses tables to map inputs to state transitions. For each state, a table maps every possible input to a succeeding state. In effect,this approach converts conditional code (and virtual functions, in the case of the State pattern) into a table look-up. The main advantage of tables is their regularity: You can change the transition criteria by modifying data instead of changing program code. There are some disadvantages, however: A table look-up is often less efficient than a (virtual)function call. Putting transition logic into a uniform, tabular format makes the transition criteria less explicit and therefore harder to understand. It's usually difficult to add actions to accompany the state transitions. The table-driven approach captures the states and their transitions, but it must be augmented to perform arbitrary computation on each transition. The key difference between table-driven state machines and the State pattern can be summed up like this: The State pattern models state-specific behavior, whereas the table-driven approach focuses on defining state transitions. Creating and destroying State objects.A common implementation trade-off worth considering is whether(1) to create State objects only when they are needed and destroy them thereafter versus (2) creating them ahead of time and never destroying them. The first choice is preferable when the states that will be entered aren't known at run-time, and contexts change state infrequently. This approach avoids creating objects that won't be used, which is important if the State objects store a lot of information. The second approach is better when state changes occur rapidly, in which case you want to avoid destroying states, because they may be needed again shortly. Instantiation costs are paid once up-front, and there are no destruction costs at all. This approach might be inconvenient, though, because the Context must keep references to all states that might be entered. Using dynamic inheritance.Changing the behavior for a particular request could be accomplished by changing the object's class at run-time, but this is not possible in most object-oriented programming languages. Exceptions include Self [US87] and other delegation-based languages that provide such a mechanism and hence support the State pattern directly.Objects in Self can delegate operations to other objects to achieve a form of dynamic inheritance. Changing the delegation target at run-time effectively changes the inheritance structure. This mechanism lets objects change their behavior and amounts to changing their class. The following example gives the C++ code for the TCP connection example described in the Motivation section. This example is a simplified version of the TCP protocol; it doesn't describe the complete protocol or all the states of TCPconnections. First, we define the class TCPConnection, which provides an interface for transmitting data and handles requests to change state. TCPConnection keeps an instance of the TCPStateclass in the _state member variable. The classTCPState duplicates the state-changing interface of TCPConnection. Each TCPState operation takes a TCPConnection instance as a parameter, letting TCPState access data from TCPConnection and change the connection's state. TCPConnection delegates all state-specific requests to its TCPState instance _state.TCPConnection also provides an operation for changing this variable to a new TCPState. The constructor for TCPConnection initializes the object to the TCPClosed state (defined later). TCPState implements default behavior for all requests delegated to it. It can also change the state of a TCPConnection with the ChangeState operation.TCPState is declared a friend of TCPConnection to give it privileged access to this operation. Subclasses of TCPState implement state-specific behavior. ATCP connection can be in many states: Established, Listening, Closed,etc., and there's a subclass of TCPState for each state.We'll discuss three subclasses in detail: TCPEstablished,TCPListen, and TCPClosed. TCPState subclasses maintain no local state, so they can be shared, and only one instance of each is required. The unique instance of each TCPState subclass is obtained by the static Instance operation. After performing state-specific work, these operations call theChangeState operation to change the state of the TCPConnection. TCPConnection itself doesn't know a thing about the TCP connection protocol; it's theTCPState subclasses that define each state transition and action in TCP. Johnson and Zweig [JZ91] characterize theState pattern and its application to TCP connection protocols. Most popular interactive drawing programs provide "tools" for performing operations by direct manipulation. For example, alien-drawing tool lets a user click and drag to create a new line. As election tool lets the user select shapes. There's usually a palette of such tools to choose from. The user thinks of this activity as picking up a tool and wielding it, but in reality the editor's behavior changes with the current tool: When a drawing tool is active we create shapes; when the selection tool is active we select shapes; and so forth. We can use the State pattern to change the editor's behavior depending on the current tool. We can define an abstract Tool class from which to define subclasses that implement tool-specific behavior. The drawing editor maintains a current Tool object and delegates requests to it. It replaces this object when the user chooses a new tool, causing the behavior of the drawing editor to change accordingly. This technique is used in both the HotDraw [Joh92] and Unidraw [VL90] drawing editor frameworks. It allows clients to define new kinds of tools easily. In HotDraw, the DrawingController class forwards the requests to the current Tool object. In Unidraw, the corresponding classes are Viewer and Tool. The following class diagram sketches the Tool and DrawingController interfaces: Coplien's Envelope-Letter idiom [Cop92] is related toState. Envelope-Letter is a technique for changing an object's class at run-time. The State pattern is more specific, focusing on how to deal with an object whose behavior depends on its state. The Flyweight (218) pattern explains when and how State objects can be shared. State objects are often Singletons (144).
##%%&&
Define a family of algorithms, encapsulate each one, and make them interchangeable. Strategy lets the algorithm vary independently from clients that use it. Many algorithms exist for breaking a stream of text into lines.Hard-wiring all such algorithms into the classes that require them isn't desirable for several reasons:?Clients that need line breaking get more complex if they include the line breaking code. That makes clients bigger and harder to maintain, especially if they support multiple line breaking algorithms.?Different algorithms will be appropriate at different times. We don't want to support multiple line breaking algorithms if we don't use them all.?It's difficult to add new algorithms and vary existing ones when line breaking is an integral part of a client. We can avoid these problems by defining classes that encapsulate different line breaking algorithms. An algorithm that's encapsulated in this way is called a strategy. Suppose a Composition class is responsible for maintaining and updating the line breaks of text displayed in a text viewer.Line breaking strategies aren't implemented by the class Composition.Instead, they are implemented separately by subclasses of the abstract Compositor class. Compositor subclasses implement different strategies:?SimpleCompositor implements a simple strategy that determines line breaks one at a time.?TeXCompositor implements the TeX algorithm for finding line breaks. This strategy tries to optimize line breaks globally, that is, one paragraph at a time.?ArrayCompositor implements a strategy that selects breaks so that each row has a fixed number of items. It's useful for breaking a collection of icons into rows, for example. A Composition maintains a reference to a Compositor object. Whenever a Composition reformats its text, it forwards this responsibility to its Compositor object. The client of Composition specifies which Compositor should be used by installing the Compositor it desires into the Composition. Use the Strategy pattern when?many related classes differ only in their behavior. Strategies provide a way to configure a class with one of many behaviors.?you need different variants of an algorithm. For example, you might define algorithms reflecting different space/time trade-offs.Strategies can be used when these variants are implemented as a class hierarchy of algorithms [HO87].?an algorithm uses data that clients shouldn't know about. Use theStrategy pattern to avoid exposing complex, algorithm-specific data structures.?a class defines many behaviors, and these appear as multiple conditional statements in its operations. Instead of many conditionals, move related conditional branches into their ownStrategy class. Strategy (Compositor) declares an interface common to all supported algorithms. Context uses this interface to call the algorithm defined by a ConcreteStrategy.?ConcreteStrategy (SimpleCompositor, TeXCompositor,ArrayCompositor) implements the algorithm using the Strategy interface.?Context (Composition) is configured with a ConcreteStrategy object. maintains a reference to a Strategy object. may define an interface that lets Strategy access its data. Strategy and Context interact to implement the chosen algorithm. A context may pass all data required by the algorithm to the strategy when the algorithm is called. Alternatively, the context can pass itself as an argument to Strategy operations. That lets the strategy call back on the context as required.?A context forwards requests from its clients to its strategy. Clients usually create and pass a ConcreteStrategy object to the context;thereafter, clients interact with the context exclusively. There is often a family of ConcreteStrategy classes for a client to choose from. Families of related algorithms.Hierarchies of Strategy classes define a family of algorithms or behaviors for contexts to reuse. Inheritance can help factor out common functionality of the algorithms. An alternative to subclassing.Inheritance offers another way to support a variety of algorithms or behaviors. You can subclass a Context class directly to give it different behaviors. But this hard-wires the behavior into Context.It mixes the algorithm implementation with Context's, making Context harder to understand, maintain, and extend. And you can't vary the algorithm dynamically. You wind up with many related classes whose only difference is the algorithm or behavior they employ.Encapsulating the algorithm in separate Strategy classes lets you vary the algorithm independently of its context, making it easier to switch, understand, and extend. Strategies eliminate conditional statements.The Strategy pattern offers an alternative to conditional statements for selecting desired behavior. When different behaviors are lumped into one class, it's hard to avoid using conditional statements to select the right behavior. Encapsulating the behavior in separate Strategy classes eliminates these conditional statements. Code containing many conditional statements often indicates the need to apply the Strategy pattern. A choice of implementations.Strategies can provide different implementations of the same behavior. The client can choose among strategies with different time and space trade-offs. Clients must be aware of different Strategies.The pattern has a potential drawback in that a client must understand how Strategies differ before it can select the appropriate one.Clients might be exposed to implementation issues. Therefore you should use the Strategy pattern only when the variation in behavior is relevant to clients. Communication overhead between Strategy and Context.The Strategy interface is shared by all ConcreteStrategy classes whether the algorithms they implement are trivial or complex. Hence it's likely that some ConcreteStrategies won't use all the information passed to them through this interface; simple ConcreteStrategies may use none of it! That means there will be times when the context creates and initializes parameters that never get used. If this is an issue, then you'll need tighter coupling between Strategy and Context. Increased number of objects.Strategies increase the number of objects in an application. Sometimes you can reduce this overhead by implementing strategies as stateless objects that contexts can share. Any residual state is maintained by the context, which passes it in each request to the Strategy object. Shared strategies should not maintain state across invocations. The Flyweight (218) pattern describes this approach in more detail. Consider the following implementation issues: Defining the Strategy and Context interfaces.The Strategy and Context interfaces must give a ConcreteStrategy efficient access to any data it needs from a context, and vice versa. One approach is to have Context pass data in parameters to Strategy operations¡ªin other words, take the data to the strategy. This keeps Strategy and Context decoupled. On the other hand, Context might pass data the Strategy doesn't need. Another technique has a context pass itself as an argument, and the strategy requests data from the context explicitly.Alternatively, the strategy can store a reference to its context,eliminating the need to pass anything at all. Either way, the strategy can request exactly what it needs. But now Context must define a more elaborate interface to its data, which couples Strategy and Context more closely. The needs of the particular algorithm and its data requirements will determine the best technique. Strategies as template parameters.In C++ templates can be used to configure a class with a strategy.This technique is only applicable if (1) the Strategy can be selected at compile-time, and (2) it does not have to be changed at run-time.In this case, the class to be configured (e.g., Context) is defined as a template class that has a Strategy class as a parameter: With templates, there's no need to define an abstract class that defines the interface to the Strategy. Using Strategy as a template parameter also lets you bind a Strategy to its Context statically, which can increase efficiency. Making Strategy objects optional.The Context class may be simplified if it's meaningful not to have a Strategy object. Context checks to see if it has a Strategy object before accessing it. If there is one, then Context uses it normally. If there isn't a strategy, then Context carries out default behavior. The benefit of this approach is that clients don't have to deal with Strategy objects at all unless they don't like the default behavior. We'll give the high-level code for the Motivation example, which is based on the implementation of Composition and Compositor classes in Interviews [LCI+92]. The Composition class maintains a collection of Component instances, which represent text and graphical elements in a document. A composition arranges component objects into lines using an instance of a Compositor subclass, which encapsulates a line breaking strategy. Each component has an associated natural size, stretchability, and shrinkability. The stretchability defines how much the component can grow beyond its natural size; shrinkability is how much it can shrink. The composition passes these values to a compositor, which uses them to determine the best location for line breaks. When a new layout is required, the composition asks its compositor to determine where to place line breaks. The composition passes the compositor three arrays that define natural sizes, stretchabilities,and shrinkabilities of the components. It also passes the number of components, how wide the line is, and an array that the compositor fills with the position of each line break. The compositor returns the number of calculated breaks. The Compositor interface lets the composition pass the compositor all the information it needs. This is an example of "taking the data to the strategy": Note that Compositor is an abstract class. Concrete subclasses define specific line breaking strategies. The composition calls its compositor in its Repair operation. Repair first initializes arrays with the natural size, stretchability, and shrinkability of each component (the details of which we omit for brevity). Then it calls on the compositor to obtain the line breaks and finally lays out the components according to the breaks (also omitted): TeXCompositor uses a more global strategy. It examines a paragraph at a time, taking into account the components' size and stretchability. It also tries to give an even "color" to the paragraph by minimizing the whitespace between components. These classes don't use all the information passed inCompose. SimpleCompositor ignores the stretchability of the components, taking only their natural widths into account.TeXCompositor uses all the information passed to it, whereas ArrayCompositor ignores everything. Compositor's interface is carefully designed to support all layout algorithms that subclasses might implement. You don't want to have to change this interface with every new subclass, because that will require changing existing subclasses. In general, the Strategy and Context interfaces determine how well the pattern achieves its intent. Both ET++ [WGM88] and InterViews use strategies to encapsulate different linebreaking algorithms as we've described. In the RTL System for compiler code optimization [JML92],strategies define different register allocation schemes(RegisterAllocator) and instruction set scheduling policies(RISCscheduler, CISCscheduler). This provides flexibility in targeting the optimizer for different machine architectures. The ET++SwapsManager calculation engine framework computes prices for different financial instruments [EG92]. Its key abstractions are Instrument and YieldCurve. Different instruments are implemented as subclasses of Instrument. YieldCurve calculates discount factors, which determine the present value of future cash flows. Both of these classes delegate some behavior to Strategy objects. The framework provides a family of ConcreteStrategy classes for generating cash flows, valuing swaps, and calculating discount factors. You can create new calculation engines by configuring Instrument and YieldCurve with the different ConcreteStrategy objects.This approach supports mixing and matching existing Strategy implementations as well as defining new ones. The Booch components [BV90] use strategies as template arguments. The Booch collection classes support three different kinds of memory allocation strategies: managed (allocation out of a pool),controlled (allocations/deal locations are protected by locks), and unmanaged (the normal memory allocator). These strategies are passed as template arguments to a collection class when it's instantiated. For example, an UnboundedCollection that uses the unmanaged strategy is instantiated as UnboundedCollection. RApp is a system for integrated circuit layout [GA89, AG90].RApp must lay out and route wires that connect subsystems on the circuit. Routing algorithms in RApp are defined as subclasses of an abstract Router class. Router is a Strategy class. Borland's ObjectWindows [Bor94] uses strategies in dialogs boxes to ensure that the user enters valid data. For example, numbers might have to be in a certain range, and a numeric entry field should accept only digits. Validating that a string is correct can require a table look-up. ObjectWindows uses Validator objects to encapsulate validation strategies. Validators are examples of Strategy objects. Data entry fields delegate the validation strategy to an optional Validatorobject. The client attaches a validator to a field if validation is required (an example of an optional strategy). When the dialog is closed, the entry fields ask their validators to validate the data.The class library provides validators for common cases, such as aRangeValidator for numbers. New client-specific validation strategies can be defined easily by subclassing the Validator class. Flyweight (218): Strategy objects often make good flyweights.
##%%&&
Define the skeleton of an algorithm in an operation, deferring some steps to subclasses. Template Method lets subclasses redefine certain steps of an algorithm without changing the algorithm's structure. Consider an application framework that provides Application and Document classes. The Application class is responsible for opening existing documents stored in an external format, such as a file. A Document object represents the information in a document once it's read from the file. Applications built with the framework can subclass Application and Document to suit specific needs. For example, a drawing application defines DrawApplication and DrawDocument subclasses; a spreadsheet application defines SpreadsheetApplication and SpreadsheetDocument subclasses. OpenDocument defines each step for opening a document. It checks if the document can be opened, creates the application-specific Document object, adds it to its set of documents, and reads the Document from a file. We call OpenDocument a template method. A template method defines an algorithm in terms of abstract operations that subclasses override to provide concrete behavior. Application subclasses define the steps of the algorithm that check if the document can be opened(CanOpenDocument) and that create the Document (DoCreateDocument).Document classes define the step that reads the document (DoRead).The template method also defines an operation that lets Application subclasses know when the document is about to be opened(AboutToOpenDocument), in case they care. By defining some of the steps of an algorithm using abstract operations, the template method fixes their ordering, but it lets Application and Document subclasses vary those steps to suit their needs. The Template Method pattern should be used?to implement the invariant parts of an algorithm once and leave it up to subclasses to implement the behavior that can vary.?when common behavior among subclasses should be factored and localized in a common class to avoid code duplication. This is a good example of "refactoring to generalize" as described by Opdyke and Johnson. You first identify the differences in the existing code and then separate the differences into new operations. Finally, you replace the differing code with a template method that calls one of these new operations.?to control subclasses extensions. You can define a template method that calls "hook" operations (see Consequences) at specific points,thereby permitting extensions only at those points. AbstractClass (Application) defines abstract primitive operations that concrete subclasses define to implement steps of an algorithm. implements a template method defining the skeleton of an algorithm.The template method calls primitive operations as well as operations defined in AbstractClass or those of other objects.?ConcreteClass (MyApplication) implements the primitive operations to carry out subclass-specific steps of the algorithm. ConcreteClass relies on AbstractClass to implement the invariant steps of the algorithm. Template methods are a fundamental technique for code reuse. They are particularly important in class libraries, because they are the means for factoring out common behavior in library classes. Template methods lead to an inverted control structure that's sometimes referred to as "the Hollywood principle," that is, "Don't call us, we'll call you" [Swe85]. This refers to how a parent class calls the operations of a subclass and not the other way around. Template methods call the following kinds of operations:?concrete operations (either on the ConcreteClass or on client classes);?concrete AbstractClass operations (i.e., operations that are generally useful to subclasses);?primitive operations (i.e., abstract operations);?factory methods (see Factory Method (121)); and?hook operations, which provide default behavior that subclasses can extend if necessary. A hook operation often does nothing by default. It's important for template methods to specify which operations are hooks (may be overridden) and which are abstract operations(must be overridden). To reuse an abstract class effectively,subclass writers must understand which operations are designed for overriding. Unfortunately, it's easy to forget to call the inherited operation.We can transform such an operation into a template method to give the parent control over how subclasses extend it. The idea is to call a hook operation from a template method in the parent class.Then subclasses can then override this hook operation: Three implementation issues are worth noting: Using C++ access control.In C++, the primitive operations that a template method calls can be declared protected members. This ensures that they are only called by the template method. Primitive operations that must be overridden are declared pure virtual. The template method itself should not be overridden; therefore you can make the template method a non-virtual member function. Minimizing primitive operations.An important goal in designing template methods is to minimize the number of primitive operations that a subclass must override to flesh out the algorithm. The more operations that need overriding, the more tedious things get for clients. Naming conventions.You can identify the operations that should be overridden by adding a prefix to their names. For example, the MacApp framework for Macintosh applications [App89] prefixes template method names with "Do-":"DoCreateDocument", "DoRead", and so forth. The following C++ example shows how a parent class can enforce an invariant for its subclasses. The example comes from NeXT'sAppKit [Add94]. Consider a class View that supports drawing on the screen. View enforces the invariant that its subclasses can draw into a view only after it becomes the "focus,"which requires certain drawing state (for example, colors and fonts) tobe set up properly. We can use a Display template method to set up this state.View defines two concrete operations,SetFocus and ResetFocus, that set up and clean up the drawing state, respectively. View's DoDisplayhook operation performs the actual drawing. Display callsSetFocus before DoDisplay to set up the drawing state; Display calls ResetFocus afterwards to release the drawing state. To maintain the invariant, the View's clients always call Display, and View subclasses always override DoDisplay. Template methods are so fundamental that they can be found in almost every abstract class. Wirfs-Brock et al. [WBWW90, WBJ90] provide a good overview and discussion of template methods. Factory Methods (121) are often called by template methods. In the Motivation example,the factory method DoCreateDocument is called by the template method OpenDocument. Strategy (349): Template methods use inheritance to vary part of an algorithm.Strategies use delegation to vary the entire algorithm.
##%%&&
Represent an operation to be performed on the elements of an object structure. Visitor lets you define a new operation without changing the classes of the elements on which it operates. Consider a compiler that represents programs as abstract syntax trees.It will need to perform operations on abstract syntax trees for "static semantic" analyses like checking that all variables are defined. It will also need to generate code. So it might define operations for type-checking, code optimization, flow analysis, checking for variables being assigned values before they're used, and so on. Moreover, we could use the abstract syntax trees for pretty-printing, program restructuring, code instrumentation, and computing various metrics of a program. Most of these operations will need to treat nodes that represent assignment statements differently from nodes that represent variables or arithmetic expressions. Hence there will be one class for assignment statements, another for variable accesses, another for arithmetic expressions, and so on. The set of node classes depends on the language being compiled, of course, but it doesn't change much for a given language. This diagram shows part of the Node class hierarchy. The problem here is that distributing all these operations across the various node classes leads to a system that's hard to understand, maintain, and change. It will be confusing to have type-checking code mixed with pretty-printing code or flow analysis code. Moreover, adding a new operation usually requires recompiling all of these classes. It would be better if each new operation could be added separately, and the node classes were independent of the operations that apply to them. We can have both by packaging related operations from each class in a separate object, called a visitor, and passing it to elements of the abstract syntax tree as it's traversed. When an element "accepts" the visitor, it sends a request to the visitor that encodes the element's class. It also includes the element as an argument. The visitor will then execute the operation for that element¡ªthe operation that used to be in the class of the element. For example, a compiler that didn't use visitors might type-check a procedure by calling the TypeCheck operation on its abstract syntax tree. Each of the nodes would implement TypeCheck by calling TypeCheckon its components (see the preceding class diagram). If the compiler type-checked a procedure using visitors, then it would create a TypeCheckingVisitor object and call the Accept operation on the abstract syntax tree with that object as an argument. Each of the nodes would implement Accept by calling back on the visitor: an assignment node calls VisitAssignment operation on the visitor, while a variable reference calls VisitVariableReference. What used to be theTypeCheck operation in class AssignmentNode is now the VisitAssignment operation on TypeCheckingVisitor. To make visitors work for more than just type-checking, we need an abstract parent class NodeVisitor for all visitors of an abstract syntax tree. NodeVisitor must declare an operation for each node class. An application that needs to compute program metrics will define new subclasses of NodeVisitor and will no longer need to add application-specific code to the node classes. The Visitor pattern encapsulates the operations for each compilation phase in a Visitor associated with that phase. With the Visitor pattern, you define two class hierarchies: one for the elements being operated on (the Node hierarchy) and one for the visitors that define operations on the elements (the NodeVisitor hierarchy). You create a new operation by adding a new subclass to the visitor class hierarchy. As long as the grammar that the compiler accepts doesn't change (that is, we don't have to add new Node subclasses), we can add new functionality simply by defining new NodeVisitor subclasses. Use the Visitor pattern when?an object structure contains many classes of objects with differing interfaces, and you want to perform operations on these objects that depend on their concrete classes.?many distinct and unrelated operations need to be performed on objects in an object structure, and you want to avoid "polluting" their classes with these operations. Visitor lets you keep related operations together by defining them in one class. When the object structure is shared by many applications, use Visitor to put operations in just those applications that need them.?the classes defining the object structure rarely change, but you often want to define new operations over the structure. Changing the object structure classes requires redefining the interface to all visitors,which is potentially costly. If the object structure classes change often, then it's probably better to define the operations in those classes. Visitor (NodeVisitor) declares a Visit operation for each class of ConcreteElement in the object structure. The operation's name and signature identifies the class that sends the Visit request to the visitor. That lets the visitor determine the concrete class of the element being visited. Then the visitor can access the element directly through its particular interface.?ConcreteVisitor (TypeCheckingVisitor) implements each operation declared by Visitor. Each operation implements a fragment of the algorithm defined for the corresponding class of object in the structure. ConcreteVisitor provides the context for the algorithm and stores its local state. This state often accumulates results during the traversal of the structure.?Element (Node) defines an Accept operation that takes a visitor as an argument.?ConcreteElement (AssignmentNode,VariableRefNode) implements an Accept operation that takes a visitor as an argument.?ObjectStructure (Program) can enumerate its elements. may provide a high-level interface to allow the visitor to visit its elements. may either be a composite (see Composite (183)) or a collection such as a list or a set. A client that uses the Visitor pattern must create a ConcreteVisitor object and then traverse the object structure, visiting each element with the visitor.?When an element is visited, it calls the Visitor operation that corresponds to its class. The element supplies itself as an argument to this operation to let the visitor access its state, if necessary. The following interaction diagram illustrates the collaborations between an object structure, a visitor, and two elements: Some of the benefits and liabilities of the Visitor pattern are as follows: Visitor makes adding new operations easy.Visitors make it easy to add operations that depend on the components of complex objects. You can define a new operation over an object structure simply by adding a new visitor. In contrast, if you spread functionality over many classes, then you must change each class to define a new operation. A visitor gathers related operations and separates unrelated ones.Related behavior isn't spread over the classes defining the object structure; it's localized in a visitor. Unrelated sets of behavior are partitioned in their own visitor subclasses. That simplifies both the classes defining the elements and the algorithms defined in the visitors. Any algorithm-specific data structures can be hidden in the visitor. Adding new ConcreteElement classes is hard.The Visitor pattern makes it hard to add new subclasses of Element. Each new ConcreteElement gives rise to a new abstract operation on Visitor and a corresponding implementation in every ConcreteVisitor class. Sometimes a default implementation can be provided in Visitor that can be inherited by most of the ConcreteVisitors, but this is the exception rather than the rule. So the key consideration in applying the Visitor pattern is whether you are mostly likely to change the algorithm applied over an object structure or the classes of objects that make up the structure. TheVisitor class hierarchy can be difficult to maintain when newConcreteElement classes are added frequently. In such cases, it's probably easier just to define operations on the classes that make up the structure. If the Element class hierarchy is stable, but you are continually adding operations or changing algorithms, then the Visitor pattern will help you manage the changes. Visiting across class hierarchies.An iterator (see Iterator (289)) can visit the objects in a structure as it traverses them by calling their operations. But an iterator can't work across object structures with different types of elements. For example, the Iterator interface defined on page 295 can access only objects of type Item: Visitor does not have this restriction. It can visit objects that don't have a common parent class. You can add any type of object to a Visitor interface. MyType and YourType do not have to be related through inheritance at all. Accumulating state.Visitors can accumulate state as they visit each element in the object structure. Without a visitor, this state would be passed as extra arguments to the operations that perform the traversal, or they might appear as global variables. Breaking encapsulation.Visitor's approach assumes that the ConcreteElement interface is powerful enough to let visitors do their job. As a result, the pattern often forces you to provide public operations that access an element's internal state, which may compromise its encapsulation. Each object structure will have an associated Visitor class. This abstract visitor class declares a VisitConcreteElement operation for each class of ConcreteElement defining the object structure. EachVisit operation on the Visitor declares its argument to be a particular ConcreteElement, allowing the Visitor to access the interface of the ConcreteElement directly. ConcreteVisitor classes override each Visit operation to implement visitor-specific behavior for the corresponding ConcreteElement class. Each class of ConcreteElement implements an Accept operation that calls the matching Visit... operation on the visitor for that ConcreteElement. Thus the operation that ends up getting called depends on both the class of the element and the class of the visitor. Here are two other implementation issues that arise when you apply the Visitor pattern: Double dispatch.Effectively, the Visitor pattern lets you add operations to classes without changing them. Visitor achieves this by using a technique called double-dispatch. It's a well-known technique. In fact, some programming languages support it directly (CLOS, for example). Languages like C++ and Smalltalk support single-dispatch. In single-dispatch languages, two criteria determine which operation will fulfill a request: the name of the request and the type of receiver. For example, the operation that a GenerateCode request will call depends on the type of node object you ask. In C++, calling GenerateCode on an instance of VariableRefNode will call VariableRefNode::GenerateCode (which generates code for avariable reference). Calling GenerateCode on an AssignmentNode will call AssignmentNode::GenerateCode (which will generate code for an assignment). The operation that gets executed depends both on the kind of request and the type of the receiver. "Double-dispatch" simply means the operation that gets executed depends on the kind of request and the types of two receivers.Accept is a double-dispatch operation. Its meaning depends on two types: the Visitor's and the Element's. Double-dispatching lets visitors request different operations on each class of element. This is the key to the Visitor pattern: The operation that gets executed depends on both the type of Visitor and the type of Element it visits. Instead of binding operations statically into the Element interface, you can consolidate the operations in a Visitor and use Accept to do the binding at run-time. Extending the Element interface amounts to defining one new Visitor subclass rather than many newElement subclasses. Who is responsible for traversing the object structure?A visitor must visit each element of the object structure. The question is, how does it get there? We can put responsibility for traversal in any of three places: in the object structure, in the visitor, or in a separate iterator object (see Iterator (289)). Often the object structure is responsible for iteration. A collection will simply iterate over its elements, calling the Accept operation on each. A composite will commonly traverse itself by having each Accept operation traverse the element's children and call Accept on each of them recursively. Another solution is to use an iterator to visit the elements. In C++,you could use either an internal or external iterator, depending on what is available and what is most efficient. In Smalltalk, you usually use an internal iterator using do: and a block. Since internal iterators are implemented by the object structure, using an internal iterator is a lot like making the object structure responsible for iteration. The main difference is that an internal iterator will not cause double-dispatching¡ªit will call an operation on the visitor with an element as an argument as opposed to calling an operation on the element with the visitor as an argument.But it's easy to use the Visitor pattern with an internal iterator if the operation on the visitor simply calls the operation on the element without recursing. You could even put the traversal algorithm in the visitor, although you'll end up duplicating the traversal code in each ConcreteVisitor for each aggregate ConcreteElement. The main reason to put the traversal strategy in the visitor is to implement a particularly complex traversal, one that depends on the results of the operations on the object structure.We'll give an example of such a case in the Sample Code. Because visitors are usually associated with composites, we'll use the Equipment classes defined in the Sample Code of Composite (183) to illustrate the Visitor pattern. We will use Visitor to define operations for computing the inventory of materials and the total cost for a piece of equipment.The Equipment classes are so simple that using Visitor isn't really necessary, but they make it easy to see what's involved in implementing the pattern. Here again is the Equipment class from Composite (183). We've augmented it with an Accept operation to let it work with a visitor. The Equipment operations return the attributes of a piece of equipment, such as its power consumption and cost. Subclasses redefine these operations appropriately for specific types of equipment (e.g.,a chassis, drives, and planar boards). The abstract class for all visitors of equipment has a virtual function for each subclass of equipment, as shown next. All of the virtual functions do nothing by default. Equipment that contains other equipment (in particular, subclasses of CompositeEquipment in the Composite pattern) implements Accept by iterating over its children and calling Accept on each of them. Then it calls the Visit operation as usual.For example, Chassis::Accept could traverse all the parts in the chassis as follows: Subclasses of EquipmentVisitor define particular algorithms over the equipment structure. The PricingVisitor computes the cost of the equipment structure. It computes the net price of all simple equipment (e.g., floppies) and the discount price of all composite equipment (e.g., chassis and buses). PricingVisitor will compute the total cost of all nodes in the equipment structure. Note that PricingVisitor chooses the appropriate pricing policy for a class of equipment by dispatching to the corresponding member function. What's more, we can change the pricing policy of an equipment structure just by changing thePricingVisitor class. The InventoryVisitor accumulates the totals for each type of equipment in the object structure. InventoryVisitor uses an Inventory class that defines an interface for adding equipment(which we won't bother defining here). Now we'll show how to implement the Smalltalk example from the Interpreter pattern (see page 279) with theVisitor pattern. Like the previous example, this one is so small thatVisitor probably won't buy us much, but it provides a good illustration of how to use the pattern. Further, it illustrates a situation in which iteration is the visitor's responsibility. The object structure (regular expressions) is made of four classes,and all of them have an accept: method that takes the visitor as an argument. In class RepeatExpression, the accept: method sends the visitRepeat: message.In class AlternationExpression, it sends the visitAlternation: message.In class LiteralExpression, it sends the visitLiteral: message. The four classes also must have accessing functions that the visitor can use. For SequenceExpression these areexpression1 and expression2; for AlternationExpression these are alternative1and alternative2; for RepeatExpression it is repetition; and for LiteralExpression these are components. The ConcreteVisitor class is REMatchingVisitor. It is responsible for the traversal because its traversal algorithm is irregular. The biggest irregularity is that a RepeatExpression will repeatedly traverse its component.The class REMatchingVisitor has an instance variable inputState. Its methods are essentially the same as the match: methods of the expression classes in the Interpreter pattern except they replace the argument named inputState with the expression node being matched. However, they still return the set of streams that the expression would match to identify the current state. The Smalltalk-80 compiler has a Visitor class called ProgramNodeEnumerator.It's used primarily for algorithms that analyze source code.It isn't used for code generation or pretty-printing, although it could be. IRIS Inventor [Str93]is a toolkit for developing 3-D graphics applications. Inventor represents a three-dimensional scene as a hierarchy of nodes, each representing either a geometric object or an attribute of one.Operations like rendering a scene or mapping an input event require traversing this hierarchy in different ways. Inventor does this using visitors called "actions." There are different visitors for rendering, event handling, searching, filing, and determining bounding boxes. To make adding new nodes easier, Inventor implements a double-dispatch scheme for C++. The scheme relies on run-time type information and a two-dimensional table in which rows represent visitors and columns represent node classes. The cells store a pointer to the function bound to the visitor and node class. Mark Linton coined the term "Visitor" in the X Consortium's Fresco Application Toolkit specification [LP93]. Composite (183):Visitors can be used to apply an operation over an object structure defined by the Composite pattern. Interpreter (274):Visitor may be applied to do the interpretation.
##%%&&
Organize Business Logic by procedures that carry out what needs to be done in a transaction
Most business applications can be thought of as a series of transactions. A transaction may view some
information organized in a particular way. Another will make changes the database. Each interaction
between a client system and a server system contains a certain amount of logic. In some cases this can
be as simple as just displaying some information that's in the database. In others this may involve many
steps of validations, and calculations.
A Transaction Script organizes all this logic primarily as a single procedure, making calls directly to the
database or through a thin database wrapper. Each transaction will have its own Transaction Script,
although common subtasks can be broken out into sub-procedures. Each Transaction Script is a public
method on a class.
With Transaction Script this logic is primarily organized by the transactions that you do with the system.
If your need is to book a hotel room, the logic to check the room is available, calculate the rates, and
update the database is found inside the book hotel room procedure.
For simple cases, there isn't really much to say about how you organize this. Of course, like any other
program, you should structure the code into modules in a way that makes sense. Unless the transaction
is particularly complicated that won't be too much of a challenge. One of the benefits of this approach is
that you don't need to worry about what other transactions are doing. Your task is to get the input,
interrogate the database, munge, and save results to the database.
Where you put the Transaction Script will depend on how you organize your layers. It may be in a
server page, CGI script, distributed session object. Whatever your architecture there is some transaction
controller that initiate everything that goes on. For a simple Transaction Script the business logic can be
placed in that transaction controller module, either as inline code within the handling of the request, or
(better) broken into some separate routines.
The challenges for implementing Transaction Script come in two forms which often come together:
complicated transaction logic and duplicated logic.
ABC Amber CHM Converter Trial version, http://www.processtext.com/abcchm.html
You know you have complicated transaction logic when the customer says something like "for this use
case step, here's the ten pages that explains what goes on". At this point the most important thing to do is
get any complicated code isolated from where it fits into your architecture. Create a module that is has
no dependencies on the upper layers of the system. The Transaction Script will know about the
database, or some Gateways. Your transaction controller will then call this module at appropriate
points. The module might be a single routine, or a bunch of routines.
Separating the business logic from the layers will make it easier to modify and test the business logic. It's
easier to modify because when you're modifying the logic, you don't have to worry about the issues that
come from the module's place in your layers. Also you can address layering issues without worrying
about the details of the business logic. It's more testable because the module can be tested outside the
confines of your layered architecture. This may improve the cycle time of development process quite
significantly.
You can organize your Transaction Scripts into classes in two ways. One is to have one Transaction
Script per class. In this case your classes are commands and should have a common Layer Supertype
that defines the execute method. The advantage of this is that it allows you to manipulate instances of
scripts as objects at runtime, although I've rarely seen people need to do this with the kinds of systems
that use Transaction Scripts to organize domain logic. The alternative is to have several Transaction
Scripts in a single class, where each class defines a subject area of related Transaction Scripts. This is
the simpler, and more common technique. Of course you can ignore classes completely in many
languages and just use global functions. However you'll often find that instantiating a new object helps
contain threading issues as it makes it easier to isolate data.
The glory of Transaction Script is its simplicity. Organizing logic this way is natural for applications with
only a small amount of logic and implies very little overhead: both in performance and in understanding.
But as the business logic gets more complicated it gets progressively harder to keep the business logic in
a well designed state. One particular problem to watch carefully for is duplication between transactions.
Since the whole point is handle one transaction, if there's common code then it tends to be duplicated.
Careful factoring can alleviate many of these problems, but more complex business domains really need
to build a Domain Model. A Domain Model will give you much more options in structuring the code,
increasing readability and decreasing duplication.
It's hard to quantify the cut over level, especially when you're more familiar with one than the other. You
can refactor a Transaction Script oriented design to a Domain Model style of design. But it's a harder
change than it would otherwise need to be, so an early shot is often the best way to move forwards.
But however much of an object bigot you become, don't rule out Transaction Script. There are a lot of
simple problems out there, and a simple solution will get you up and running much faster.
For this pattern, and the others that talk about domain logic, I'm going to use the same problem. To
avoid typing the problem statement several times, I'm just putting it in here.
Revenue recognition is a common problem in business systems, it's all about when you can actually count
the money you receive on your books. If I sell you a cup of coffee, it's a simple matter. I give you the
coffee, I take the money, and I can count the money to the books that nanosecond. But for many things
it gets complicated. Say you pay me a retainer to be available that year. Even if you pay me some
ridiculous fee today, I may not be able to put it on my books right now because the service is to be
performed over the course of a year. One approach might be that I should only count one twelfth of that
fee for each month over the course of the year, since you might pull out of the contract after a month and
you realize that writing has atrophied my programming skills.
The rules for revenue recognition are many, various, and volatile. Some are set by regulation, some by
professional standards, some by company policy. Tracking revenue ends up being quite complex
problem.
Not that I fancy delving into the complexity right now. Instead we'll imagine a company that sells three
kinds of products: word processors, databases, and spreadsheets. The rules are that when you sign a
contract for a word processor, you can book all the revenue right away. If it's a spreadsheet you can
book one third today, on third in sixty days and one third in ninety days. If it's a database you can book
one third today, one third in thirty days, and one third in sixty days. There is no basis for these rules
other than my own fevered imagination. I'm told that the real rules are equally rational.
Figure 1: A conceptual model for simplified revenue recognition. Each contract has multiple
revenue recognitions that indicate when the various parts of the revenue should be recognized.
This example uses two transaction scripts: one to calculate the revenue recognitions for a contract, and
the other to tell us how much revenue on a contract has been recognized by a certain date. We'll make
the unrealistic simplifying assumption that all contracts are only about one product.
The database structure has three tables: one for the products, one for the contracts, and one for the
revenue recognitions.
The first script calculates the amount of recognition due by a particular day. I can do this in two stages,
the first part to select the appropriate rows in the revenue recognitions table, the second part to sum up
the amounts.
Many Transaction Script designs have Transaction Scripts that operate directly on the database,
putting SQL code into the procedure. Here I'm using a simple Table Data Gateway to wrap the SQL
queries. Since this example is so simple, I'm using a single gateway rather than one for each table. I can
define an appropriate find method on the gateway.
I then use the script to sum up based on the result set passed back from the gateway.
For calculating the revenue recognitions on an existing contract, I use a similar split. The script on the
service carries out the business logic.
Notice I'm using Money to carry out the allocation. When splitting an amount three ways it's very easy to
lose a penny.
The Table Data Gateway provides support on the SQL. Firstly there's a finder for a contract.
And secondly there's a wrapper for the insert.
In a Java system, the recognition service might be a regular class, or it could be a session bean.
As you compare this to the example in Domain Model you probably think, unless your mind is as
twisted as mine, that this is much simpler. The harder thing to imagine is what happens as the rules get
more complicated. Typical revenue recognition rules get very involved, varying not just by product but
also by date (if the contract was signed before April 15 then this rule applies....) It's difficult to keep a
coherent design with Transaction Script once things get that complicated, which is why object bigots
like me prefer using a Domain Model in those circumstances.
##%%&&
Build an object model of the domain that incorporate both behavior and data.
At its worst, business logic can be a very complex matter. Rules and logic describe many different cases
and slants of behavior. It is this complexity that objects were designed to work with. A Domain Model
creates a web of interconnected objects, where each object represents some meaningful individual,
which may be as large as a corporation, or as small as a single line on an order.
There's whole books written on this, so I hardly know where to start.
Putting a Domain Model into an application involves inserting a whole layer of objects. These objects
model the business area that you're working in. You'll find objects that mimic the data in the business,
and objects that capture the rules that the business uses. Mostly the data and process is combined
together to cluster the processes close to the data the processes work with.
An OO domain model will often look similar to a database model, yet it will still have a lot of differences.
A Domain Model mingles data and process, has multi-valued attributes, and there's inheritance.
Since the behavior of the business is subject to a lot of change, it's important to be able to modify, build,
and test this layer easily. As a result you'll want the minimum of coupling from the Domain Model to
other layers in the system. You'll notice that a guiding force of many layering patterns is to keep the as
few dependencies as possible from the domain model to other parts of the system.
When you use a Domain Model there are a number of different scopes you might use. The simplest
case is a single user application where the whole object graph is read from a file and put into memory. A
desktop application may work this way, but it's less common for a multi-tiered IS application simply
because there are too many objects. Putting every object into memory takes too long and consumes too
much memory. The beauty of object-oriented databases is that they give the impression of doing this,
while moving objects between memory and disk.
Without an OODB you have to do this yourself. Usually each session will involve pulling in an object
graph of all the objects involved in that session. This will certainly not be all the objects, and usually not
all the classes. So if you are looking at a set of contracts you might only pull in the products referenced
by contracts within your working set. If you are just performing calculations on contracts and revenue
recognition objects you may not pull in any product objects at all. Exactly what you pull into memory is
governed by your database mapping objects.
If you need the same object graph between calls to the server, you'll need to save the server state
somewhere, which is the subject the section on saving server state.
A common concern with domain logic is bloated domain objects. As you're building a screen to
manipulate orders you notice that some of the behavior of the order is only needed for this one screen. If
you put these responsibilities on the order the risk is that the order class will become too big, because it's
full of responsibilities that are only used in a single use case. This concern leads people to consider
whether some responsibility is general, in which case it should sit in the order class; or specific, in which
case it should sit in some usage specific class - which might be a Transaction Script or perhaps the
presentation itself.
The problem with separating usage specific behavior is that it tends to lead to duplication. Behavior that
separated from the order is harder to find, so people tend to not see it and duplicate it instead. The
duplication can quickly lead to more complexity and inconsistency. On the other hand I've found that a
bloated domain object is much less of a problem than I used to think. Bloating seems to occur much less
frequently than predicted. If bloating does occur, it's relatively easy to see and not difficult to fix. So my
advice is not to try to separate usage specific behavior. Put all behavior in the object that is the natural
fit. Fix the bloating when, and if, it becomes a problem.
There's always a lot of heat generated when people talk about developing a Domain Model in J2EE.
Many of the teaching materials and introductory J2EE books suggest that you use entity beans to
develop a domain model. But there are some serious problems with this approach. Entity Beans are
remotable, and thus suggest a coarse-grained interface, but a Domain Model is at its best when you use
fine grained objects. One solution to this is to use session beans to wrap the entity beans so that entity
beans are never accessed remotely. While this is good practice, there is still some overhead in calling
entity bean methods.
Another strike against entity beans is that, at least in J2EE version 1, the container managed persistence
mapping is very limited. This really means that you are forced to make the entity beans map one to one
with the database tables, which only works for an Active Record style mapping. You can get more
flexibility by using bean managed persistence, but if you are using BMP and not remoting your entity
beans their value drops considerably.
True, entity beans handle in-memory caching, which reduces the need for a Unit of Work, but they are
also very difficult to debug, require a lot of classes to use, awkward in handling association and
inheritance relationships, increase the build times, and often cause as many performance problems as
they can solve.
The alternative is to use normal Java objects. This always causes surprise because it's amazing how
many people think you can't run regular Java objects in an EJB container. I've come to the conclusion
that people forget about regular Java objects because they haven't got a fancy name - so while preparing
for a talk Rebecca Parsons, Josh Mackenzie and I gave them one: POJO (Plain Old Java Object). A
POJO domain model is easier to put together, quick to build, can run and test outside of an EJB
container, and isn't dependent on EJB (maybe that's why EJB vendors don't encourage you to use
them.)
Having said that I've seen projects do well with entity beans if they have modest domain logic and you
have a simple relationship with the database. Most of the time, I'd prefer to take the POJO route.
Of course all this is moot with the appearance of EJB 2.0. It's hard for me to comment on this at the
moment, however, since the EJB 2.0 spec has had more final drafts than The Who had farewell
concerts. What I really want is a Domain Model that is as independent as possible from the vagaries of
development platforms.
If the how for a Domain Model is difficult because it's such a big subject, the when is hard due to both
the vagueness and simplicity of the advice. In a nutshell it all comes down to the complexity of the
behavior in your system. If you have complicated and ever changing business rules involving validation,
calculations and derivations... chances are you'll want an object model to handle them. On the other
hand if you just have a simple not null checks and a couple of sums to calculate, then a Transaction
Script is a better bet.
A factor that comes into this is how used the development team is with using domain objects. Learning
how to design and use a Domain Model is a significant exercise - one that led to many articles on the
"paradigm shift" of using objects. It certainly takes practice and coaching to get used to using a Domain
Model, but once you're used to it I've found that few people want to go back to a Transaction Script
for any but the simplest problems.
If you're using Domain Model then my first choice for database interaction is Data Mapper. This will
help keep your Domain Model independent from the database and is the best approach to handle cases
where the Domain Model and database schema diverge.
One of the biggest frustrations of describing a Domain Model is the fact that any example I show is
necessarily simple, so you can understand it; yet that simplicity hides the strength of a Domain Model.
You only really appreciate the strengths of a Domain Model when you have a really complicated
domain.
But even if the example can't really do justice to why you'd want a Domain Model, at least the example
will give you a sense of what it can look like. So I'm using the same example that I used for Transaction
Script, a little matter of revenue recognition. You can compare it with the example there.
Figure 1: Class diagram of the example classes for a domain model
An immediate thing to notice is that every class, even in this small example, contains both behavior and
data. Even the humble Revenue Recognition class contains a simple method to find out if that object's
value is recognizable on a certain date.
Calculating how much revenue is recognized on a particular date involves both the contract and revenue
recognition classes.
A common thing you find in domain models is how multiple classes interact in order to do even the
simplest tasks. This is what often leads to the complaint that with OO programs you spend a lot of time
hunting around from class to class trying to find the program. There's a lot of sense to this complaint. The
value comes as the decision whether something is recognizable by a certain date gets more complex and
as other objects need to know. Containing the behavior on the object that needs to know both avoids
duplication and reduces coupling between the different objects.
Looking at calculating and creating these revenue recognition objects further demonstrates this notion of
lots of little objects. In this case the calculation and creation begins with the customer and is handed off
via the product to a strategy hierarchy. The strategy pattern is a well known OO pattern that allows you
combine a group of operations in a small class hierarchy. Each instance of product is connected to a
single instance of Recognition Strategy which determines which algorithm is used to calculate revenue
recognition. In this case we have two subclasses of Recognition Strategy for the two different cases. The
structure of the code looks like this.
The great value of the strategies is that they provide well-contained plug points to extend the application.
Adding a new revenue recognition algorithm involves creating a new subclass and overriding the
calculateRevenueRecognitions method. This makes it easy to extend the algorithmic behavior of the
application.
When you create products, you hook them up with the appropriate strategy objects. In this case I'm
doing this in my test code.
Once everything is setup, then calculating the recognitions doesn't involve any knowledge of the strategy
subclasses.
The OO habit of successive forwarding from object to object both moves the behavior to the object
most qualified to handle it, but also resolves much of the conditional behavior. You'll notice there's no
conditionals in this calculation. The decision path was set up when the products were created with the
appropriate strategy, once setup like this the algorithms just follow the path. Domain models work very
well when you have examples of similar conditionals because the similar conditionals can be factored out
into the object structure itself. This moves complexity out of the algorithms and into the relationships
between objects. The more similar the logic, the more you find the same network of relationships used
by different parts of the system. Any algorithm that's dependent on the type of recognition calculation
can follow this particular network of objects.
You'll notice that in this example, I've not shown anything about how the objects get retrieved from, and
written to, the database. This is for a couple of reasons. Firstly mapping a Domain Model to a database
is always somewhat hard, so I'm chickening out and not providing an example. Secondly in many ways
the whole point of a Domain Model is to hide the database, both from upper layers but also from
people working the Domain Model itself. So hiding it here reflects what it's like to actually program in
this environment.
##%%&&
Provide a single object for all the behavior on a table
One of the key messages of object-orientation is to bundle together data with the behavior that uses that
data. The traditional object-oriented approach is based on objects with identity, along the lines of
Domain Model. So if we have an employee class, any instance of that class corresponds to a particular
employee. This scheme works well because once we have a reference to an employee, we can execute
operations, follow relationships, and gather data on that individual.
One of the problems with Domain Model is the interface with relational databases. In many ways the
Domain Model approach treats the relational database like some old granny that's shut up in an attic
and nobody wants to talk about. As a result you often need considerable programmatic gymnastics to
pull data in and out of the database, transforming between two different representations of the data.
A Table Module organizes domain logic with one class per table in the database, and a single instance
of a class contains the various procedures that will act on the data. The primary distinction with Domain
Model is that if you have many orders, a Domain Model will have one order object per order, while a
Table Module will have one object to handle all the orders.
The strength of Table Module is that it allows you to package the data and behavior together, while at
the same time playing to the strengths of a relational database. On the surface Table Module looks much
the same as regular objects. The key difference is that Table Module has no notion of an identity for the
objects that it's working with. So if you wanted to obtain the address of an employee you would have a
method like anEmployeeModule.getAddress(long employeeID). Every time you want to do something
to a particular employee you have to pass in some kind of identity reference. Often this will be the
primary key that's used in the database.
Usually you use Table Module with a backing data structure that's table-oriented. The tabular data is
usually the result of a SQL call, and held in some record set object that mimics a SQL table. The Table
Module gives you an explicit method based interface that acts on the tabular data.
The Table Module may be an instance or it may be a collection of static methods. The advantage of an
instance is that it allows you to initialize the Table Module with an existing record set, perhaps the result
of a query. We can then use the instance to manipulate the rows in the record set. Using instances also
makes it possible to use inheritance, so we can write a ManagerModule that contains additional
behavior.
The Table Module may include queries as factory methods. The alternative route is to have a Table
Data Gateway. The disadvantage of using an extra Table Data Gateway class and mechanism in the
design. The advantage of the Table Data Gateway is that it allows you to use a single Table Module on
data from different data sources, since the you use a different Table Data Gateway for each data
source.
Table Module is very much based on table-oriented data, so obviously it makes sense when you are
accessing tabular data. It also puts that data structure very much into the center of the code, so you also
want the way you access the data structure to be fairly straightforward.
The most well-known situation where I've come across this pattern is in Microsoft COM designs. In
COM the record set is the primary repository of data in an application. Record sets can be passed to
the UI where data aware widgets display information. Microsoft's ADO libraries give you a good
mechanism to access the relational data as record sets. In this situation Table Module allows you to fit
business logic into the application in a well organized way, without losing the way the various elements
work on the tabular data.
So you can access data from the database and pass it to a Table Module. You can then use the Table
Module to perform calculations of derived data which adds to the underlying data set. Then the you can
pass the data set to the UI for display and modification using the table aware widgets. The table aware
widgets can't tell if the record sets came directly from the relational database, or if a Table Module
manipulated the data on the way out. After modification in the GUI, the data set goes back to the Table
Module for validation before it's saved to the database.
However Table Module does not give you the full power of objects in organizing complex logic. You
can't have direct instance to instance relationships, and polymorphism does not work well. So for
handling complicated domain logic, a Domain Model is a better choice. Essentially you have to trade off
ABC Amber CHM Converter Trial version, http://www.processtext.com/abcchm.html
Domain Model's ability to handle complex logic versus Table Module's easier integration with the
underlying table oriented data structures.
If the objects in a Domain Model and the database tables are relatively similar, then it may be better to
use a Domain Model that uses Active Record. Table Module works better than using a combination of
Domain Model and Active Record when other parts of the application are based around a common
table-oriented data structure. That's why you don't usually see Table Module very much in the Java
environment, although that may change as row sets become more widely used.
Time to revisit the revenue recognition example I used in the other domain modelling patterns, but this
time with a Table Module. To recap, our mission is to recognize revenue on orders when the rules vary
depending on the type of product. In this fantasy example we have different rules for word processors,
spreadsheets, and databases.
Table Module is based on a data schema of some kind, usually a relational data model (although in the
future we may well see XML an model used in a similar way). In this case I'll use the relational schema
from Figure 1.
Figure 1: Database schema for revenue recognition
The classes that manipulate this data are of pretty much the same form, there is one Table Module class
for each table. In the .NET architecture there is a data set object which provides an in-memory
representation of a database structure. So it makes sense to create classes that operate on this data set.
Each Table Module class has a data member of a Data Table, which is the .NET system class
corresponding to a table within the data set. This ability to read a table is common to all the Table
Modules and so can appear in the superclass
This allows me to create a new Table Module by just passing in a data set to the Table Module's
constructor
This keeps the code that creates the data set away from the Table Modules, following the guidelines of
using ADO.NET.
A useful capability is to use the C# indexer to get to a particular row in the data table given the primary
key.
The first piece of functionality is to calculate the revenue recognition for a contract, updating the revenue
recognition tables accordingly. The amount recognized depends on the kind of product we have. Since
this behavior mainly uses data from the contract table, I decided to add the method to the contract class.
Usually I would use Money here, but for variety's sake, I'll show this using a decimal. I use a similar
allocation method to the one I would use for a Money.
To carry this out, we need some behavior that's defined on the other classes. The product needs to be
able to tell us which product type it is. We can do this with an enum for the product type and a lookup
method.
GetProductType encapsulates the data in the data table. There is an argument for doing this for all the
columns of data, as opposed to accessing them directly as I did with the amount on the contract. While
encapsulation is generally a Good Thing, I don't do it here because it doesn't fit in with the assumption of
the environment that different parts of the system access the data set directly. There's no encapsulation
when the data set moves over to the UI. So column access functions really only make sense when
there's some additional functionality to be done, such as converting a string to a product type.
The other additional behavior is inserting a new revenue recognition record.
Again the point of this method is less to encapsulate the data row, and more to have a method instead of
several lines of code that would get repeated.
The second behavior is to sum up all the revenue recognized on a contract by a given date. Since this
uses the revenue recognition table it makes sense to define the method there.
This fragment takes advantage of the really nice feature of ADO.NET that allows you to define a where
clause and then select a subset of the data table to manipulate. Indeed you can go further and use an
aggregate function.
##%%&&
Model View Controller (MVC) has been one of the most quoted (and most misquoted) patterns
around. It started life as a framework developed by Trygve Reenskaug for the Smalltalk platform in the
late 70's. Since then it's played an influential role for most UI frameworks and thinking about UI design.
MVC considers three roles. The model is an object that represents some information about the domain.
It is a non-visual object and contains all the data and behavior other than that used for the UI. In its most
pure OO notion the model is an object within a Domain Model. You might also think of a Transaction
Script as the model providing the Transaction Script contained no UI machinery. Such a definition
stretches the notion of model, but fits the role breakdown of MVC.
The view represents the display of the model in the UI. So if our model is a customer object our view
might be a frame full of UI widgets or an HTML page rendered with information from the model. The
view is only about display of information, any changes to the information are handled by the third
member of the MVC trinity: the controller. The controller takes user input and manipulates the model
and causes the view to update appropriately. The UI is therefore a combination of the view and the
controller.
As I think about MVC I see two principal separations: separating the presentation from the model and
separating the controller from the view. Of these the separation of presentation from model is one of the
most fundamental heuristics of good software design. This separation is important for several reasons
Fundamentally they are about different concerns. When you are developing a view you are
thinking about the mechanisms of UI and how to lay out a good user interface. When you are
working with a model you are thinking about business policies, perhaps database interactions.
Certainly you will use different very different libraries when working with one or the other. Often
you find people prefer one area to another and you'll see people specialize in one side of the line.
Depending on context users like to see the same basic model information in different ways.
Separating the two allows you develop multiple presentations, indeed entire different kinds of
interfaces and yet use the same model code. In its most noticeable form this could be providing a
rich client, web browser, remote API, and command line interface to the same model. Even
within a sole web interface you might have different customer pages at different points in an
application.
Non visual objects are usually easier to test than visual ones. Separating presentation and model
allows you to test all the domain logic easily without resorting to awkward GUI scripting tools
and the like.
A key point in this separation is the direction of the dependencies: the presentation depends on the
model but not the other way around. People programming in the model should be entirely unaware of
what presentation is being used. This both simplifies their task and makes it easier to add new
presentations later on. It also means that presentation changes can be made freely without altering the
model.
This principle introduces a common issue. With a rich client interface of multiple windows it's quite likely
that there will be several presentations of a model on a screen at once. If a user makes a change to the
model from one presentation, then the others need to change. To do this without creating a dependency
you'll usually find the observer pattern, such as event propagation or a listener. The presentation acts as
the observer of the model: whenever the model changes it sends out an event and the presentations
refresh the information.
The second separation, that of view and controller, is less important. Indeed the irony is that almost
every version of Smalltalk didn't actually make a view/controller separation. The classic example of why
you'd want to separate them is to support editable and non-editable behavior. You can do this with one
view and two controllers for the two cases, where the controllers are strategies for the view. But in
practice most systems have only one controller per view, so this separation is usually not done. However
this separation has come back into vogue with web interfaces where it does become useful to separate
the controller and view again.
The fact that most GUI frameworks combine view and controller has led to the many misquotations of
MVC that I've run into. The model and the view is obvious - but where is the controller. The common
idea is that it sits between the model and the view as in the Application Controller, a fact that isn't
helped that the word "controller" is used in both contexts. Whatever the merits of a Application
Controller it's a very different beast to an MVC controller
For the purposes of this set of patterns these principles are really all you need to know. If you want to
dig deeper into MVC the best available reference is[POSA].
The value of MVC really lies in its two principles. Of these the separation of presentation and model is
one of the most important design principles in software and the only time you shouldn't follow it is in very
simple systems where the model wouldn't have any real behavior in it anyway. As soon as you get some
non visual logic you should look to apply the separation. Unfortunately a lot of UI frameworks make it
difficult, and those that don't often are taught without a separation.
The separation of view and controller is less important, so I'd only recommend doing it when it is really
helpful. For rich client systems that ends up being hardly ever. It is however common in web front ends
where the controller is separated out - most of the patterns on web design here are based on that
principle.
##%%&&
An object that handles a request for a specific page or action on a web site
Most people's basic web experience is with static HTML pages. When you request static HTML you
pass to the web server the name and path for a HTML document stored on the web server. The key
notion is that each page on the web site is a separate document on the server. With dynamic pages
things can get much more interesting since there is a much more complex relationship between path
names and the file that responds. However the approach of one path leading to one file that handles the
request is a simple model to understand.
As a result Page Controller has one input controller for each logical page of the web site. That
controller may be the page itself, as it often is in server page environments, or it may be a separate
object that corresponds to that page.
The basic idea behind a Page Controller is to have one module on the web server act as the controller
for each page on the web site. In practice, it doesn't work out to exactly one per page, since sometimes
you may hit a link and get a different page depending on some dynamic information. More strictly the
controllers tie to each action on the web site, where an action may be clicking a link or a button.
The Page Controller can be structured either as a script (CGI script, servlet, ...), or a server page
(ASP, PHP, JSP, ...). Using a server page usually combines the Page Controller and a Template View
into the same file. While this works well for the Template View it works less well for the Page
Controller since it is more awkward to properly structure the module. If the page is a simple display
then this is not a problem. If there is logic involved in either pulling data out of the request, or deciding
which actual view to display, then you can end up with awkward scriptlet code in the server page.
One way of dealing with this scriptlet code is to use a helper object. In this case the first thing the server
page does is to call the helper object to handle all the logic. The helper may return control to the original
server page, or it may forward to a different server page to act as the view. In this case the server page
is the handler of the request, but most of the controller logic lies in the helper object
Another approach is to let a script be the handler and controller. The web server passes control to the
script, the script carries out the controller's responsibilities, and finally forward to an appropriate view to
display any results.
The basic responsibilities of a Page Controller are:
??Decode the URL and extract any form data to figure out all the data for the action
??Create and invoke any model objects to process the data. All relevant data from the HTML
request should be passed to the model so that the model objects do not need any connection to
the HTML request
??Determine which view should display the result page and forward the model information to that
view.
The Page Controller need not be a single class, it can invoke helper objects. This is particularly useful if
several handlers need to do similar tasks. A helper class can then be a good spot to put any code that
would otherwise be duplicated.
There's no reason why you can't have some URLs handled by server pages and some URLs handled by
scripts. Any URLs that have little or no controller logic are best handled with a server page, since that
provides a simple mechanism that's easy to understand and modify. Any URLs with more complicated
logic go to a script. I've often come across teams who want to handle everything the same way, either
everything is a server page or everything is a script. Any advantages of consistency in such an application
are usually offset by the problems of either scriptlet laden server pages or lots of simple pass-through
scripts.
The main decision point is whether to use Page Controller or Front Controller. Of the two Page
Controller is the most familiar one to work with and leads to a natural structuring mechanism where
particular actions are handled by particular server pages or script classes. You thus have to trade off the
greater complexity of the Front Controller against the various advantages that it offers, most of which
make a difference in web sites that have more navigational complexity.
Page Controller works particularly well in a site where most of the controller logic is pretty simple. In
this case most URLs can be handled with a server page with the more complicated cases in helpers.
When you're controller logic is simple, Front Controller adds a lot of overhead.
It's not uncommon to have a mix in a site where some requests are dealt with by Page Controllers and
others are dealt with by Front Controllers, particularly when a team is refactoring from one to another.
The two patterns actually mix together without too much trouble.
A simple example of an action controller is to display some information about something. Here we'll use
an example of displaying some information about a recording artist. The URL would run something along
the lines of http://www.thingy.com/recordingApp/artist?name=danielaMercury
The web server needs to be configured to recognize "/artist" as a call to ArtistController. In Tomcat you
do this with the following code in the web.xml file.
The artist controller needs to implement a method to handle the request.
Although this is a very simple case, it covers the salient points. First the controller needs to get create the
necessary model objects to do their thing, in this case it just need to find the correct model object to
display. The second part is to put the right information in the http request so that the JSP can display it
properly. In this case it creates a helper and puts it into the request. Finally it forwards to the Template
View to handle the display of the data. Forwarding is a common behavior, so it sits naturally on a
superclass for all action controllers
The main point of coupling between the Template View and the Page Controller is the parameter
names in the request to pass on any objects that the JSP needs to use.
In this case the controller logic is really very simple, but as we get more complex controller logic we can
continue to use the servlet as a controller. We can have a similar behavior for albums, with the twist that
classical albums both have a different model object and are rendered with a different JSP. To do this
behavior we can again use a controller class.
Notice that in this case I'm using the model objects as the helper, rather than creating a separate helper
class. This is worth doing if the helper class would otherwise be just a dumb forwarder to the model
class. When doing this make sure that the model class doesn't have any servlet dependent code in it - if
there is any servlet dependent code it should sit in a separate helper class.
Using a servlet as a controller is one route to take, but the most common route is to have the server page
itself be the controller. The problem with this approach is that it results in scriptlet code at the beginning
of the server page, and as you may have gathered I think that scriptlet code has the same relationship to
well-designed software that professional wrestling does to sport.
Despite this you can have a server page as the handler of the request, while delegating control to the
helper to actually carry out the controller function. This preserves the simple property of having your
URLs be denoted by server pages. In this case I'll do this for the album display, where you can display
an album with a URL of the form http://localhost:8080/isa/album.jsp?id=zero. Most albums are
displayed directly with the album JSP, but classical albums require a different display and are displayed
with a classicalAlbum JSP.
This controller behavior appears in a helper class to the JSP. The helper is set up in the album JSP itself.
The call to init sets the helper up to carry out the controller behavior.
Common helper behavior naturally sits on a helper superclass.
The key difference between the controller behavior here and that when using a servlet, is that the handler
JSP is also the default view. Unless the controller forwards to a different JSP, control reverts to the
original handler. This is an advantage when you have pages where most of the time the JSP directly acts
as the view. In these cases there's no forwarding to be done. The initialization of the helper acts to kick
off any model behavior and set things up for the view later on. It's a simple model to follow, since people
generally associate a web page with the server page that is the view for the page. This also often fits
naturally with web server configuration.
The call to initialize the handler is rather clumsy, in a JSP environment this awkwardness can be handled
much better with a custom tag. Such a tag can automatically create an appropriate object, put it in the
request, and initialize it. With that all you need is to put a simple tag into the JSP page.
The custom tag's implementation then does the work.
If I'm going to use a custom tag like this, I might as well make custom tags for property access too.
(You may be thinking that it's better to use the Java beans mechanism than to just invoke a getter using
reflection. If so, you're probably right... and also probably intelligent enough to figure out how to change
the method to do that)
With the getting tag defined, I can use it to pull information out of the helper. The tag is both shorter and
removes the chances of me mizpelling 'helper'.
The web system in .NET is designed to work with the Page Controller and Template View patterns,
although you can certainly decide to handle web events with a different approach. With this example, I'll
take the preferred style of .NET, with the presentation layer built on top of a domain using Table Module
, using data sets as the main carrier of information between the layers.
For our example this time we'll have a page that displays runs scored and the run rate for one innings of
a cricket match. As I know I'll have many readers who are afflicted with no material experience of this
art form, I can summarize by saying that the runs scored is the score of batsman and the run rate is how
many runs they score divided by the number of balls they face. The runs scored and balls faced are in
the database, the run rate needs to be calculated by the application - a tiny but pedagogically useful
piece of domain logic.
The handler in this design is an ASP.NET web page, captured in a .aspx file. As with other server page
constructs, the aspx file allows you to embed programming logic directly into the page as scriptlets.
Since you know I'd rather drink bad beer than write scriptlets, you know there's little chance that I'd
want to do that. My savior in this case is the code behind mechanism that allows you to associate a
regular file and class with the aspx page. You signal this in the header of the aspx page.
The page is setup as a subclass of the code behind class, and as such can use all the protected
properties and methods of the code behind. The page object is the handler of the request, and the code
behind can define how to handle the request by defining a Page_Load method. If most pages follow a
common flow, I can define a Layer Supertype that has a template method for this.
The template method breaks the handling of the request down into a number of common steps. This way
we can define a single common flow for handling web requests, while allowing each Page Controller to
supply implementations for the specific steps. If you do this, you will find that once you've written a few
Page Controllers, you will find what common flow to use for the template method. If any page needs to
do something completely different, it can always override the page load method.
The first task is to do validation on the parameters coming into the page. In a more realistic example this
might do initial sanity checking of various form values, but in this case we are just decoding a URL of the
form http://localhost/batsmen/bat.aspx?team=England&innings=2&match=905. The only validation in
this example is that the various parameters required for the database query are present. As usual I've
been overly simplistic in the error handling until somebody writes a good set of patterns on validation. So
here the particular page defines a set of mandatory parameters and the Layer Supertype has the logic
for checking them.
The next stage is to pull the data out of the database into a ADO.NET's disconnected data set object. In
this case this is a single query to the batting table.
Now the domain logic gets its turn to play. The domain logic is organized as a Table Module. The
controller passes the retrieved data set to the Table Module to process.
At this point the controller part of the page hander is done. By this I mean that in classic Model View
Controller terms the controller should now hand over to the view to do display. In this design the
BattingPage acts as both the controller and the view and the last call to prepareUI is part of the view
behavior, so I can say farewell to this example in this pattern. However I suspect you'll find this to lack a
certain dramatic closure, so you can find the example continued in Template View.
##%%&&
A controller that handles all requests for a web site
In a complex web site, there are many similar things you need to do when handling a web request. These
things include handling security, internationalization, providing particular views for certain kinds of users,
etc. If the input controller behavior is scattered across multiple objects, then much of this behavior can
end up duplicated, and it's difficult to change behavior at run time.
The Front Controller consolidates all of this by channeling all the requests through a single object. This
handler can carry out common behavior which can be modified at runtime with decorators. The handler
then dispatches to command objects for behavior particular to a request.
A Front Controller handles all calls for a web site. It's usually structured in two parts: a web handler
and a hierarchy of commands. The web server software directs the http request to the handler. The
handler pulls just enough information from the URL and request to decide what kind of action to initiate.
The handler then delegates the request to a command, which usually isn't a web handler, to carry out the
action.
The web handler is almost always implemented as a class, rather than a server page, as it doesn't
produce any response. The commands are also classes rather than server pages, and indeed don't need
to be web handlers at all, although they are usually passed the http information. The web handler itself is
usually a fairly simple program which does nothing other than decide which command should be run.
The web handler can make the decision of which command to run either statically or dynamically. The
static version involves parsing the URL and using conditional logic to decide which command to run. The
dynamic version usually involves taking a standard piece of the URL and using dynamic instantiation to
create a command class.
The static case has the advantage of explicit logic, compile time error checking on the dispatch, and lots
of flexibility in what your URLs look like. The dynamic case allows you to add new commands without
changing the web handler.
A particularly useful pattern to use in conjunction with Front Controller is Intercepting Filter,
described in [Alur, Crupi, and Malks]. An intercepting filter is essentially a decorator that wraps the
handler of the front controller. This allows you build a filter chain (or pipeline) of filters to handle
different issues such as authentication, logging, locale identification. Using filters allows you to
dynamically set up the filters to use at configuration time.
Rob Mee showed me an interesting variation of Front Controller using a two stage web handler. In this
case the web handler is further separated into a degenerate web handler and a dispatcher. The
degenerate web handler pulls the basic data out of the http parameters and hands it to the dispatcher in
such a way that the dispatcher is completely independent of the web server framework. This makes
testing easier because test code can drive the dispatcher directly without having to run in web server.
Remember that both the handler and the commands are part of the controller. As a result the commands
can (and should) choose which view to use for the response. The only responsibility of the handler is in
choosing which command to execute. Once it's done that, it plays no further part in that request.
The Front Controller is a more complicated design than the it's obvious counterpart, the Page
Controller. It therefore needs a few advantages to be worth the effort.
Only one Front Controller needs to be configured into the web server, the web handler then does the
rest of the dispatching. This simplifies the configuration of the web server, which is an advantage if the
web server is awkward to configure. With dynamic commands you can add new commands without
changing anything. It also eases porting, since you only have to register the handler in a web server
specific way.
Since you create new command object with each request, you don't have to worry about making the
command classes thread safe. This can avoid the headaches of multi-threaded programming. You do
have to make sure that you don't share any other objects, such as the model objects.
A commonly stated advantage of a Front Controller is that it allows you to factor out code that's
otherwise duplicated in the Page Controller. To be fair, however, you can also do much of this by using
a superclass Page Controller.
Since there is just one controller, you can easily enhance its behavior at run time by using decorators.
You can have decorators for authentication, character encoding, internationalization and add them either
using a configuration file or even while the server is running. ([Alur, Crupi, and Malks] describe this
approach in detail under the name Intercepting Filter.)
[Alur, Crupi, and Malks] gives a detailed description of how to implement Front Controller in Java.
The also describe Intercepting Filter which goes very well with Front Controller.
A number of Java web frameworks use this pattern. An excellent example of this is Struts
Here's a simple case of using Front Controller for the original and innovative task of displaying some
information about a recording artist. We'll use dynamic commands with a URL of the form
http://loaclhost:8080/isa/music?name=astor&command=Artist. The command parameter tells the web
handler which command to use.
We'll begin with the handler, which I've implemented as a servlet.
The logic is quite straightforward. The handler takes the command name and tries to instantiate a class
named by concatenating the command name and "Command". Once it has created the new command it
initializes it with the necessary information from the http server. In this case I've passed in what I need for
this simple example, you may well need more - such as the http session.
If you can't find a command, I've used the Special Case pattern and returned an unknown command.
As is often the case, using Special Case allows you to avoid a lot of extra error checking.
Commands share a fair bit of data and behavior. All of them need to be initialized with information from
the web server.
They can also provide common behavior, such as a forward method, and define an abstract process
command for the actual commands to override.
The command object is then very simple, at least in this case. It just implements the process method.
This involves invoking the appropriate behavior on the model objects, putting the information needed for
the view into the request, and then forwarding to a Template View.
##%%&&
Render information into HTML by embedding markers into an HTML page
Writing a program that spits out HTML is often more difficult than you would like. Although
programming languages are better at creating text than they used to be (some of us remember character
handling in Fortran and standard Pascal), creating and concatenating string constructs is still painful. If
there isn't much to do it isn't too bad - but a whole HTML page is a lot of text manipulation
With static HTML pages, those that don't change from request to request, you can use nice editors that
product the HTML pages in a wisiwig manner. Even those of us that like raw text editors find it easier to
just type in the text and tags rather than fiddle with string concatenation in a programming language.
Of course the issue is with dynamic web pages: those that take the results of something like database
queries and embed those into the HTML page. The page looks different with each result, and as a result
regular HTML editors aren't enough for the job.
The best way to work is to compose the dynamic web page the same way that you write a static page,
but put in markers that can be resolved into calls to gather dynamic information. Since the static part of
the page acts as a template for the particular response, I call this a Template View.
The basic idea of Template View is to embed markers into a static HTML page when it's written. When
the page is used to service a request, the makers are replaced by the results of some computation, such
as a database query. This way the page can be laid out in the usual manner, often using wisiwig editors,
often by people who aren't programmers. The makers then communicate with real programs to put in the
results.
There's a lot of tools that use Template View. As a result this pattern isn't about how to build one
yourself, more about how to use one effectively and what the alternative is.
There are a number of ways these markers can be placed into the HTML. One form is to use
HTML-like tags. This works well with wisiwig editors because they realize that anything between <> is
special and either ignore it, or treat it differently. If the tags follow the rules for well-formed XML you
can also use XML tools on the resulting document (providing your HTML is XHMTL, of course).
Another way to do it is to use special text markers in the body text. Wisiwig editors then treat that as
regular text, still ignoring it but probably doing annoying things like spell checking it. The advantage is
that the syntax can be easier than the clunky HTML/XML syntax.
One of the most popular forms of Template View is a server page: something like ASP, JSP, or PHP.
These actually go a step further than the basic form of a Template View in that they allow you to embed
arbitrary programming logic, referred to as scriplets, into the page. In my view, however, this feature is
actually a big problem and you're better off limiting yourself to basic Template View behavior when you
use server page technology.
The most obvious disadvantage of putting a lot of scriptlets into a page is that it eliminates the possibility
of having non-programmers edit the page. This is particularly important when you're using graphic
designers to do the page design. However the biggest problems of embedding scriptlets into the page
come from the fact that a page is poor module for a program. Even when using a object-oriented
language the page construct loses you most of the structural features that make it possible to do a
modular design: either in OO or procedural style.
Even worse, putting a lot of scriptlets into the page makes it too easy to mingle together the different
layers of an enterprise application. When domain logic starts turning up on server pages it becomes far
too difficult to structure it well, and far too easy to duplicate logic across different server pages. Indeed
the worst code I've seen in the last few years has been server page code.
The key to avoiding is to provide a regular object as a helper to each page. This helper has all the real
programming logic inside it. The page only has calls into the helper. This simplifies the page and makes it
more of a pure Template View. The resulting simplicity makes it easier for non-programmers to edit the
page and programmers to concentrate on the helper. Depending on the actual tool you are using, you
can often reduce all the templates in a page to HTML/XML tags, which keeps the page more consistent
and more amenable to tool support.
This sounds like a simple and commendable principle, but as ever there are a quite a few dirty issues
involved to make things more complicated. The simplest markers are those that get some information
from the rest of the system and put into the correct place on the page. These makers are easily translated
into calls to the helper. The calls result in text, or something that's trivially turned into text, and the engine
places the text on the page.
A more knotty issue is conditional page behavior. The simplest case is the situation where something is
only displayed if some condition is true. The easiest thing to imagine where would be some kind of
conditional tag along the lines of <IF condition = "$pricedrop > 0.1"> ...show some stuff </IF>. The
trouble with this is that when you start having conditional tags like this, you start going down the path of
turning the templates into a programming language of themselves. This leads you into all the same
problems that you get by embedding scriptlets into the page. If you need a full programming language
you might as well use scriptlets, but you know what I think of that idea!
As a result I see purely conditional tags as a bad smell, something you should try to avoid. Sometimes
you can't avoid it, but you should look to see if you can come up with something more focused than a
general purpose <IF> tag.
If you are displaying some text conditionally, one option is to move the condition into the helper. The
page then always inserts the result of the call to helper, it's just that if the condition isn't true the helper
sends back an empty string. This way the logic is all in the helper. This approach works best if there is
no markup for the returned text, or it is enough to return empty markup which gets ignored by the
browser.
An example of where this doesn't work is where you might want to highlight good selling items in a list by
putting their names in bold. In this situation we always need the names to be displayed, but sometimes
we want the special markup. One way to deal with this situation is to have the helper generate the
markup. This keeps all the logic out of the page, at the cost of moving the choice of highlighting
mechanism away from the page designer and into the programming code.
In order to keep the choice of HTML in the hands of the page design, you need some form of
conditional tag. However it's important to look beyond simple <IF> tags. A good route to go is to
consider a focused tag. So rather than have a tag that looks like.
In either case it's important that the condition be done based on a single boolean property of the helper.
Putting any more complex expression into the page is a case of putting logic into the page itself.
Another example may be something like putting information on a page that depends on the locale that the
system is running in. Consider some text that should only be shown in the united states or Canada.
Iterating over a collection presents similar issues. If you want a table where each line corresponds to a
line item on an order, you'll need a construct that allows you easily to display information for each line.
Here it's hard to avoid a general iterate over a collection tag, and indeed they usually work simply
enough to fit in quite well.
Of course the kinds of tag you have to work with are often limited by the environment that you find
yourself in. Some environments give you a fixed set of templates to work with. In which case you may
be more constrained than you would like in following these kinds of guidelines. In other environments,
however, you may have more choice in what kinds of tag to use, many of them allow you to define your
own libraries of tags.
The name Template View brings out the fact that the primary purpose of Template View is to play the
view role in Model View Controller. For many systems the Template View should only be the view. In
simpler systems it may be reasonable for Template View to play the controller role, and possibly even
the model role, although I would strive to separate model processing as much as possible. In cases
where the Template View is taking on responsibilities beyond the view, it's important to ensure that
these responsibilities are handled by the helper, not by the page. Controller and model responsibilities
involve program logic, and like all program logic this should sit in the helper.
Any template system needs extra processing by the web server. This can either be done by compiling
the page after it's created, compiling the page on it's first request, or by interpreting the page on each
request. Obviously the latter isn't a good idea if the interpretation takes a while to do.
Although server pages are one of the most common forms of Template View around these days, you
can write scripts in a Template View style. I've seen a fair bit of perl done this way. The trick, most
noticeably demonstrated by perl's CGI.pm is to avoid concatenating strings by having function calls that
output the appropriate tags to the response. This way you can write the script in your programming
language and avoid the mess of interspersing print strings with programming logic.
For implementing the view in Model View Controller the main choice is between Template View and
Transform View. The strength of Template View is that it allows you to compose the content of the
page by looking at the structure of the page itself. This seems to be easier for most people, both to do
and to learn. In particular it nicely supports the idea of a graphic designer laying out a page with a
programmer working on the helper.
Template View has two significant weaknesses. Firstly the common implementations make it too easy to
put complicated logic onto the page, thus making it hard to maintain - particularly by non-programmers.
You need good discipline to keep the page simple and display oriented, putting logic in the helper. The
second weakness of Template View is that it is harder to test than Transform View. In most
implementations Template View are designed to work within a web server and are very difficult or
impossible to test outside of a web server. Transform View implementations are much easier to hook
into a testing harness and test without a running web server.
In thinking about a view you also need to consider Two Step View. Depending on the template scheme
you have you may be able to implement Two Step View using specialized tags. However you may find it
easier to implement Two Step View based on a Transform View. If you are going to need Two Step
View you need to take that into account when making your choice.
When you're using a JSP as a view only, it will always be invoked from a controller rather than directly
from the servlet container. In this case it's important to pass to the JSP any information it will need to
figure out what to display. A good way to do this is to have the controller create a helper object and
pass that to the JSP using the http request. We'll use the simple display example from Page Controller.
As far as the Template View is concerned the important behavior is creating the helper and placing in
the request. The sever page can now reach the helper with the useBean tag.
With the helper in place we can now use it to access the information that we need to display. The model
information that the helper needs was passed to it when it was created.
We can use the helper to get appropriate information from the model. In the simplest case we can
provide a method to get some simple data, such as the artist's name.
We can then access this information by a Java expression
or a property
The choice between properties or expressions depends on who is editing the JSP. For a programmer
the expression is easy to read and more compact, but HTML editors may not be able to handle them.
Non-programmers will probably prefer the tags, since that fits in the general form of HTML and leaves
fewer room for confusing errors.
Using a helper is one way to remove awkward scriptlet code. If you want to show a list of albums for an
artist you need to run a loop. You can do this by using a scriptlet in the server page.
But frankly I find this mix of Java and HTML really hard to read. An alternative is to move the for loop
to the helper
which I find easier to follow since the amount of HTML is quite small. It also allows you to use a
property to get this list.
A third alternative available to JSP, of course, is to use a custom tag for iteration.
This is a much nicer alternative as it keeps scriptlets out of the JSP and HTML out of the helper.
In this example, I'm continuing the example I started in Page Controller. To remind you, this example
shows the scores made by batsmen in a single innings of a cricket match. For those who think that
cricket is a small noisy animal, I'll pass over the long rhapsodies about the world's most immortal sport
and boil it all down to the fact that the page displays three essential pieces of information
??An id number to reference which match it is
??Which team's scores are shown, and which innings the scores are for
??A table showing each batsman's name, his score, and shows his run rate: the number of balls he
faced divided by the runs he scored
If you don't understand what these statistics mean, don't worry about it. Cricket is full of statistics,
perhaps its greatest contribution to humanity is providing odd statistics for eccentric papers.
The discussion in Page Controller covers how a web request is handled. To sum up, the object that
acts as both the controller and the view is the aspx ASP.NET page. To avoid holding the controller
code in a scriptlet, instead you define a separate code behind class.
The page can access the methods and properties of the code behind class directly. Furthermore the
code behind can define a Page_Load method to handle the request. In this case I've defined the page
load method as a template method [Gang of Four] on a Layer Supertype
For the purposes of Template View I can ignore all but the last couple of lines of the page load. The call
to DataBine allows various page variables to be properly bound to their underlying data sources. That
will do for the simpler cases, for more complicated cases the last line calls a method in the particular
page's code behind to prepare any objects for use by the page.
The match id number, team, and innings are single values for the page, all of which came into the page as
parameters in the http request. I can provide these values by using properties on the code behind class.
With the properties defined, I can use them in the text of the page.
The table is a little more complicated, but actually works out easy in practice because of the graphical
design facilities in Visual Studio. Visual Studio provides a data grid control which can be bound to a
single table from a data set. I can do this binding in the prepareUI method that's called by the page load
method.
The batting class is a Table Module that provides domain logic for the batting table in the database. It's
data property is the data from that table, enriched by domain logic from the Table Module. In this case
the enrichment is the run rate, which is calculated rather stored in the database.
With the ASP.NET data grid you can select which columns from the table you wish to display in the
web page, together with information about the appearance of the table. In this case we can select the
name, runs, and rate columns.
The HTML for this data grid looks intimidating, but in Visual Studio you don't manipulate the HTML
directly, you manipulate it through property sheets in the development environment - as you do for much
of the rest of the page.
This ability to have web form controls on the web page that understands the ADO.NET abstractions of
data sets and data tables is the strength, and limitation, of this scheme. It's strength is that when you can
transfer information through data sets, all of this can be nicely supported through the kind of tools that
Visual Studio has. It's limitation is that it only works seamlessly when you use patterns such as Table
Module. If you have very complex domain logic, you'll find that a Domain Model becomes helpful, and
then to take advantage of the tools the Domain Model needs to create its own data set.
##%%&&
A view that process domain data element by element and transforms it into HTML
When you issue requests for data to the domain and data source layers, you'll get back all the data you
need to satisfy a request, but without the formatting you need to make a proper web page. The role of
the view in Model View Controller is to render this data into a web page. Using Transform View
means thinking of this as a transformation where you have the model's data as input and the HTML as
output.
The basic notion of Transform View is to write a program that will look at domain oriented data and
convert it to HTML. The program walks the structure of the domain data and as it recognizes each form
of domain data it writes out the particular piece of HTML for that data. If you think about this in an
imperative way you might have a method called renderCustomer that takes a customer object and
renders it into HTML. If the customer contains a bunch of orders, the renderCustomer method would
loop over the orders calling renderOrder
The key difference between Transform View and Template View is the way in which the view is
organized. A Template View is organized around the output. A transform view is organized around
separate transforms for each kind of input element. The transform is controlled by something like a
simple loop that looks at each input element, finds the appropriate transform for that element, and then
calls that transform on the input element. A typical Transform View's elements can be arranged in any
order without affecting the resulting output.
You can write a Transform View in any language, however at the moment the dominant choice for
writing Transform Views is XSLT. The interesting thing about this is that XSLT is a functional
programming language, similar to Lisp, Scheme, Haskell and other languages that never quite made it
into the IS mainstream. As such it has a different kind of structure to it. Rather than explicitly calling
routines, the language recognizes elements in the domain data and then invokes the appropriate rendering
transformations.
To carry out an XSLT transform we need to begin with some XML data. The simplest way this can
happen is if the natural return type of the domain logic is either XML or something that is automatically
transformable to XML. A good example of an automatic transform is a .NET object that can transform
itself to XML. Failing that we need to produce the XML ourselves. A good way to do this is to populate
a Data Transfer Object that can serialize itself into XML. That way the data can be assembled using a
convenient api.In simpler cases a Transaction Script can return XML directly.
The XML that's fed into the transform does not need to be a string, unless a string form is needed to
cross a communication line. It's usually quicker and easier to produce a DOM and hand that to the
transform.
Once we have the XML we pass it to XSLT engine, increasingly these are available commercially. The
logic for the transform is captured in an XSLT style sheet which we also pass to the transformer. The
transformer then applies the style sheet to the input XML to yield the output HTML, which we can write
directly to the HTTP response.
The choice between a Transform View and a Template View mostly comes down to which
environment is preferable for the team working on the view software to use. The presence of tools is a
key factor here. Increasingly there are HTML editors which you can use to write Template Views.
Tools for XSLT are, at least so far, much less sophisticated. XSLT can also be an awkward language to
master, due its functional programming style coupled with its awkward XML syntax.
One of the strengths of XSLT is its portability. It can be used with almost any web platform. You can
use the same XSLT to transform XML created from J2EE or .NET. This can help putting a common
HTML view onto data from different sources.
XSLT is also often easier if you are building a view on an XML document. Other environments usually
require you to transform the XML document into an object, or indulge in walking the XML DOM,
which is often complicated. XSLT fits naturally in an XML world.
Transform View avoids two of the biggest problems with Template View. It's easier to keep the
transform focused only on rendering HTML, this avoids having too much other logic in the view. It's also
easy to run the Transform View and capture the output for testing. This makes it easier to test the view
and you don't need a web server to run the tests.
Transform View transforms directly from domain-oriented XML into HTML. If you need to change the
overall appearance of a web site, this can lead to you having to change multiple transform programs.
Using common transforms, such as you can do with XSLT includes, helps reduce this problem. Indeed
it's much easier to call common transformations using Transform View than it is using Template View.
If you need to make global changes easily, or support multiple appearances for the same data, you might
consider Two Step View which uses a two stage process.
Setting up a simple transform involves preparing Java code to invoke the right style sheet to form the
response, and preparing the style sheet to format the response. In these cases most of the response to a
page is pretty generic, so it makes sense to use Front Controller. I'll only describe the command here,
you should look at Front Controller to see how the command object fits in with the rest of the request
response handling.
All the command object does is invoke the methods on the model to obtain an XML input document,
then pass that XML document through the XML processor.
The translation of the XML document is done by an XSLT program. Each template match matches a
particular part of the XML and produces the appropriate HTML output for the page. In this case I've
kept the formatting to a excessively simple level to just show the essentials. The following template
clauses match the basic elements of the XML file.
These template matches handle the table. The table here has alternating rows highlighted in different
colors. This is a good example of something that isn't possible with Cascading Style Sheets but is
reasonable to do with XML.
##%%&&
Turn domain data into HTML in two steps: first by forming some kind of logical page, then
rendering the logical page into HTML.
If you have a web application with many pages, you often want to have a consistent look and
organization to the site. If every page looks different, you end up with a different look and feel that users
find confusing. You may also want to make global changes to the appearance site easily.
Common web site approaches using Template View or Transform View make this difficult because
presentation decisions are often duplicated across multiple pages or transform modules. A global change
can force you to change several files.
Two Step View deals with this problem by splitting the transformation into two stages. The first
transforms the model data into a logical presentation without any specific formatting, and the second
stage converts that logical presentation with the actual formatting needed. This way you can make a
global change by altering the second stage, or support multiple output look and feels with one second
stage each.
The key to this pattern is to make the transformation to the HTML a two stage process. The first stage
assembles the information that should be displayed in a logical screen structure that is suggestive of the
display elements, yet does not contain any HTML. The second stage takes that presentation-oriented
structure and renders it into HTML
This intermediate form is a kind of logical screen. Its elements might include things like fields, headers,
footers, tables, choices, and the like. As such it's certainly presentation-oriented and certainly constrains
the screens to follow a definite style. You can think of the presentation-oriented model as one that
defines the various kinds of widgets that you can have and the data the widgets contain, but does not
specify what their appearance is in HTML.
This presentation-oriented structure is assembled by specific code written for each screen. The first
stage's responsibility is to access a domain-oriented model, either a database, a domain model, or a
domain-oriented Data Transfer Object, extract the relevant information for that screen, and then put
that information into the presentation-oriented structure.
The second stage is then turns this presentation-oriented structure into HTML. It knows about each
element in the presentation-oriented structure and how to show that element as HTML. As such a
system with many screens can be rendered to HTML by a single second stage - so all the HTML
formatting decisions are made in one place. Of course the constraint is that the resulting screen must be
derivable from the presentation-oriented structure.
There are several ways in which you can build a Two Step View. Perhaps the easiest to think about is
using two step XSLT. A single step XSLT follows the approach in Transform View. Each page has a
single XSLT style sheet that transforms the domain oriented XML into HTML. The two step approach
uses two XSLT style sheets. The first stage transforms the domain-oriented XML into a
presentation-oriented XML. The second stage style sheet then renders that presentation-oriented XML
into HTML.
Another way is to use classes. Here you define the presentation-oriented structure as a set of classes,
with a table class, a row class, etc. The first stage takes domain information and instantiates these
presentation-oriented classes into a structure that models a logical screen. Then you render these classes
into HTML, either by getting each presentation-oriented class to generate HTML for itself, or by having
a separate HTML renderer class to do the job.
Both of the above approaches are based on Transform View. You can also do a Template View
based approach. To do this you need to pick templates that are based around the idea of a logical
screen.
The template system then converts these logical tags to HTML. In such a scheme the page definition
wouldn't include any HTML, it would include these logical screen tags - as a result it would probably be
an XML document. Of course this has the disadvantage that you lose the ability to use wisiwig HTML
editors.
The key value in Two Step View comes from the separation of first and second stages, allowing you to
make global changes more easily. it helps to think of two situations: multi-appearence web application
and single-appearence web application. Multi-appearence web apps are the rarer breed, but one that's
growing. A multi-appearance web application is one where the same basic functionality is provided
through multiple organizations, and each organization wants it's own distinct look to the application. A
current example of this is airline travel sites where as you look at them you can tell from the page layout
and design that they are all variations on one base travel site. I suspect many airlines would want that
same functionality but with a distinctly different and individual appearance.
The singe-appearance case is the more common one, where there's only one organization fronting the
web app and they would like a consistent look throughout the site. That's the easiest case to consider
first.
With a single stage view (either a Template View or a Transform View you build one view module per
page of the web application.
With a Two Step View you have two stages to the view. You have one first stage module per page in
the web application and only second stage module for the entire application. Your pay-off in using Two
Step View is that any change you can make to the appearance of the site in the second stage is much
easier to make, since one change in the second stage affects the entire site.
With a multi-appearance app this advantage is compounded. You a single-stage view for each
combination of screen and appearance. So ten screens and three appearances require 30 single stage
view modules. Using Two Step View, however you can get away with ten first stages and three
second-stages. The more screens and appearances you have, the bigger the saving.
However your ability to pull this off is entirely dependent on how well you can make the
presentation-oriented structure to really serve the needs of the appearance. A design heavy site, where
each page is supposed to look different, won't work well with Two Step View because it's to find
enough commonality between the screens to get a simple enough presentation-oriented structure.
Essentially the design of the site is constrained by the presentation-oriented structure - for many sites that
is too much of a limitation.
Another limitation of Two Step View is the tools to use it. Tools are widely available for designers with
no programming skills to lay out HTML pages using Template View. Two Step View forces
programmers to write the renderer and controller objects. Thus programmers have to be involved in any
design change.
It's also true that Two Step View, with its multiple layers, presents a harder programming model to learn,
although once you are used to it it's no more difficult - and may help to reduce repetitive boilerplate
code.
This approach to a Two Step View uses a two stage XLST transformation. The first stage transforms
the domain specific XML into a logical screen XML, the second stage transforms the logical screen
XML into HTML.
The first stage XSLT processor transforms this into the following screen oriented XML.
To do this we need the following XSLT program.
The screen oriented XML is very plain. To turn it into HTML we use a second stage XSLT program.
To assemble the two stages, I've used Front Controller to help separate the code that does the work.
It's useful to compare this to the single stage approach in Transform View. Consider changing the
colors of the alternating rows. Using Transform View it would require editing every XSLT program.
With Two Step View only the single second stage XSLT program needs to be changed. While it might
be possible to use callable templates to do something similar, this will need a fair bit of XSLT gymnastics
to pull off. The down side of Two Step View is that the final HTML is very much constrained by the
screen oriented XML.
Although the XSLT route is conceptually the easiest way to think about implementing Two Step View,
there are plenty of other ways to do it. For this example I'll use JSPs and custom tags, although they are
both more awkward and less powerful than XSLT they do show how the pattern can manifest itself in
different ways.
The key rule of Two Step View is that the choosing of what to display and the HTML that displays it are
utterly separated. For this example my first stage is handled by a JSP page and its helper, while a set of
custom tags deals with the second stage.
I'm using Page Controller for the JSP page with a helper object, that much you can flick over to Page
Controller to read. For this discussion the important thing to do is look at the tags that are part of the
"2step" name space. These are the tags that I'm using to invoke the second stage. Notice that there is no
HTML on the JSP page, the only tags that are present are either the second stage tags, or bean
manipulation tags to get values out of the helper.
Each of the second stage tags has an implementation to pump out the necessary HTML for that logical
screen element. The simplest of these is the title.
For those that haven't indulged, a custom tag works by implementing hook methods that are called at the
beginning and the end of the tagged text. So this tag simply wraps it's body content with an <H1> tag.
A more complex tag, such as the field, can take an attribute. The attribute is tied into the tag class using a
setting method
With the value set, you can then use it in the output.
The table tag is the most sophisticated of the tags. As well as allowing the JSP writer to choose which
columns to put in the table, it also does highlighting of alternate rows. Similarly to my previous example,
the highlighting is done by the second stage so that a system wide change can be done globally.
The table tag takes attributes for the name of the collection property, the object on which the collection
property sits, and a comma separated list of column names.
I made a helper method to get a property out of an object. There's a good argument for using the
various classes that support Java beans, rather than just invoking a getSomething method, but this will do
for the example.
This tag doesn't have a body. When it's called it pulls the named collection out of the request property
and iterates through this collection to generate the rows of the table.
During the iteration it sets every other row to the linen background to highlight them.
To print the cells for each row, I use the column names as property values on the objects in the
collection.
Compared to the XSLT implementation, one of the biggest differences is that this solution is rather less
constraining on the uniformity of the site's layout. If an author of one page should want to slip some
individual HTML into the page it's easier to do that. Of course while this allows tweaking of design
intensive pages, it also is open to inappropriate use by people who are unfamiliar of the how the thing is
to work. Sometimes constraints help prevent mistakes, that's a trade-off a team has to decide for
themselves.
##%%&&
A centralized point for handling screen navigation and flow of an application
Some applications contain a significant amount of logic about what kind of screens to use at different
points. This may involve invoking certain screens at certain points in an application, such as the wizard
style of interaction where the user is led through a series of screens that need to be done in a certain
order. Other cases we may see screens that are only brought in under certain conditions, or choices
between different screens that depend on earlier input.
To some degree the various Model View Controller input controllers can make some of these
decisions, but as the application gets more complex this can lead to duplicated code, as several
controller for different screens need to know what to do in a certain situation.
You can remove this duplication by placing all the flow logic into an Application Controller. Input
controllers then ask the Application Controller for the appropriate commands for execution against a
model and the correct view to use depending on the context within the application.
An Application Controller has two main responsibilities: deciding which domain logic to run, and
deciding which view to the display the response with. To this the Application Controller typically holds
two structured collections of class references, one for domain commands to execute against in the
domain layer and one of views
For both the domain commands and the view, the application controller needs some way of storing
something that it can invoke. A command is a good choice, since it allows us to get hold of and run a
block of code easily. Languages that can manipulate functions can hold a reference to a function.
Another option is to hold a string that can be used to invoke a method by reflection.
The domain commands can be command objects that are part of the Application Controller layer, or
they can be references to a Transaction Script or domain object method in the domain layer.
If you are using server pages as your views, then you can use the name of a server page. If you are using
a class then a command or a string for a reflective call makes sense. You might also use an XSLT
transform, in which case the Application Controller can hold a string to reference the transform.
One decision you'll need to make is how much to separate the Application Controller from the rest of
the presentation. At the first level this manifests itself in whether the Application Controller has
dependencies into the UI machinery. This might take the form of the Application Controller directly
accessing the http session data, forwarding to a server page, or invoking methods on a rich client class.
Although I've seen direct Application Controllers, my preference is for the Application Controller to
have no links to the UI machinery. For a start this makes it possible to test the Application Controller
independently of the UI, which is a major benefit. It's also important to do this if you're going to use the
same Application Controller with multiple presentations. As a result many people like to think of the
Application Controller as an intermediate layer between the presentation and the domain.
An application can have multiple Application Controllers to handle different parts of an application.
This allows you to split complex logic up into several classes. In this case I usually see the work divided
up into broad areas of the user interface and build separate Application Controllers for each area. On
a simpler application I might only need a single Application Controller.
If you have multiple presentations - such as a web front end, a rich client, and a PDA - you may be able
to use the same Application Controller for each presentation. Don't be too eager to do this. Often
different UIs really need a different screen flow to give a really usable user interface. However reusing a
single Application Controller may reduce the development effort, and the lower effort may be worth
the cost of a more awkward user interface.
A common way of thinking about a UI is a state machine, where certain events trigger different
responses depending on the state of certain key objects in the application. In this case the Application
Controller is particularly amenable to using metadata to represent the state machine's control flow. The
metadata can either be set up by programming language calls (the simplest way) or you can store the
metadata in a separate configuration file.
You may find domain logic that's particular to one request placed in an Application Controller. As you
might suspect I'm come down pretty hard against that notion. However the boundary between domain
and application logic does get very murky. Say I'm handling insurance applications and I need to show a
separate screen of questions only if the application is a smoker. Is this application logic or domain logic?
If I have only a few such cases I'd probably put that kind of logic in the Application Controller, but if it
occurs in lots of places I'd look to design the Domain Model in such a way to drive this.
If the flow and navigation of your application is pretty simple, anyone can visit any screen in pretty much
any order, then there's little value in a Application Controller. The strength of an Application
Controller comes from definite rules about the order in which pages should be visited and different
views depending on the state of objects.
A good signal to use an Application Controller is if you find yourself having to make similar changes in
lots of different places when your application's flow changes.
Most of the ideas that underlie the writing of this pattern came from reading [Knight and Dai]. Although
these ideas aren't exactly new, I found the explanation in [Knight and Dai] remarkably clear and
compelling.
State model's are a common way of thinking about user interfaces. They are particularly appropriate
when you need to react differently to events depending on the state of some object. In this example I
have a simple state model for a couple of commands on an asset. Our leasing experts would faint at the
virulent oversimplification of this model, but it'll do to show an example of state based Application
Controller.
As far as the code is concerned our rules are this:
??When we receive a return command, and we're in the on lease state then we display a page to
capture information about the return of the asset
??A return even in the in inventory state is an error, so we show an illegal action page
??When we receive a damage command we show different pages depending on whether the asset
is in inventory or on lease.
The input controller is a Front Controller. It services the request like this.
The flow of the service method is pretty straightforward, we find the right application controller for a
given request, we then ask the application controller for the domain command, execute the domain
command, ask the application controller for a view, and finally forward to the view.
In this scheme I'm assuming a number of Application Controllers, all of which implement the same
interface.
For our commands, the appropriate Application Controller is an asset application controller. The asset
application controller uses an response class to hold the domain commands and view references. For the
domain command I use a reference to a class, for the view I use a string which the front controller will
turn into a URL for a JSP.
The application controller holds onto the responses using a map of maps indexed by the command string
and an asset status.
When asked for a domain command, the controller looks at the request to figure out the asset id, goes to
the domain to find out the status of that asset, looks up the appropriate domain command class,
instantiates it and returns the new object.
All the domain commands follow a simple interface which allows the front controller to run them.
Once the domain command has done what it needs to do then the Application Controller comes into
play again as it's asked for the view.
In this case the Application Controller doesn't return the full URL to the JSP. It returns a string that the
front controller turns into an URL. I do this to avoid duplicating the URL paths in the responses. This
also will make it easy to add further indirection later should I need it.
The Application Controller can be loaded for use with code.
Doing this from a file instead isn't rocket science, but even so I'll leave it to you.
##%%&&
An object that wraps a record data structure in an external resource, such as a row in a database
table, and adds some domain logic to that object.
An object carries both data and behavior. Much of this data is persistent, and needs to be stored to a
database. Active Record uses the most obvious approach: put data access logic into the domain object.
This way all people know how to read and write their data from the database.
The essence of a Active Record is of a Domain Model where the classes in the Domain Model match
very closely with the record structure of an underlying database. Each Active Record is responsible to
saving and loading to the database, and also any domain logic that acts upon the data. This may be all
the domain logic in the application, or you may find that some domain logic is held in Transaction Script
s with common and data-oriented code in the Active Record.
The data structure of the Active Record should exactly match that of the database: one field in the class
for each column in the table. Type the fields the way the SQL interface gives you the data - don't do any
conversion at this stage. You might consider Identity Field, but it may also be fine to leave the foreign
keys as they are. You can use views or tables with Active Record, although obviously updates through
views are usually limited. Views are particularly useful for reporting purposes.
The Active Record class typically has the following methods
??construct an instance of the Active Record from a SQL result set row
??construct a new instance for later insertion into the table
??static finder methods to wrap commonly used SQL queries and return Active Record objects
??methods to update the database and insert into the database with the data in the Active Record
??getting and setting methods for the fields
??methods that implement some pieces of business logic
The getting and setting methods can do some more intelligent things. They can convert from the SQL
oriented types to better in memory types. Also if you ask for a related table, the getting method can
return the appropriate Active Record, even if you aren't using Identity Field on the data structure (by
doing a lookup).
In this pattern the tables are a convenience, but they don't hide the fact that a relational database is
present. As a result you usually see less of the other object-relational mapping patterns present when
you're using Active Record.
It's often difficult to tell the difference between Active Record and Row Data Gateway when you are
using a Domain Model. You are using Active Record if your domain objects contain the SQL
themselves to access the database, and you're using Row Data Gateway if a separate class does this. If
you have some SQL in the domain objects and some in separate database objects then you have some
form of hybrid.
Active Record is a good choice when your domain logic is not too complex, such as create, read,
update and deletes.. Derivations and validations based on a single record work well in this structure.
In an initial design for a Domain Modelthe main choice is between Active Record and Data Mapper.
Active Record has the primary advantage of simplicity. It's easy to build Active Records and they are
easy to understand. The primary problem with them is they work well only if the Active Record objects
correspond directly to the database tables: an isomorphic schema. If your business logic is complex then
you'll soon want to use your object's mechanisms such as direct relationships, collections, and
inheritance. These don't map easily onto Active Record. Adding them piecemeal soon gets very messy,
so that's what will lead you to use Data Mapper instead.
Another argument against Active Record is the fact that it couples the object design to the database
design. This makes it more difficult to refactor either design as the project goes forward.
Active Record is a good pattern to consider if you are using Transaction Script and you are beginning
to feel some of the pain from code duplication, and difficulty in updating scripts and tables that
Transaction Script often brings. In this case you can gradually start creating Active Records and then
slowly refactor behavior into them. Often it helps to first wrap the tables as a Gateway, and then start
moving behavior so they evolve to a Active Record.
This is a simple, even simplistic, example to show how the bones of Active Record works. We begin
with a simple person class that has these fields
There's also an ID field in the superclass.
To load an object the person class acts as the finder and also performs the load. It uses static methods
on the person class.
##%%&&
An object that acts as a Gateway to a single record in a data source
Embedding database access code into in-memory objects can leave you with a fair few disadvantages.
For a start it adds complexity. If your in-memory objects have business logic of their own, adding the
database manipulation code is another aspect of complexity. Testing is awkward too, because if your
in-memory objects are tied to a database tests are slower to run due to all the database access. You
may have to access multiple databases with all those little annoying variations on their SQL.
A Row Data Gateway gives you objects that look exactly like the record in your record structure, but
can be accessed with the regular programming mechanisms of your programming language. All the
details of accessing the data source are hidden behind this interface.
A Row Data Gateway acts as an object that exactly mimics a single record, such as one database row.
Each column in the database becomes one field in the Row Data Gateway. The Row Data Gateway
will usually do any type conversion from the data source types to the in-memory types, but this type
conversion is usually pretty simple. The Row Data Gateway holds the data about a row a client can
then access the Row Data Gateway directly. The gateway acts as a good interface for each row of
data. This approach works particularly well for Transaction Scripts.
With a Row Data Gateway you're faced with the questions of where to put the find operations that
generate the Row Data Gateways. You can use static find methods, but that precludes polymorphism
should you want to substitute different finder methods for different data sources. In this case it often
makes sense to have separate finder objects. So each table in a relational database will have one finder
class and one gateway class for the results.
It's often hard to tell the difference between a Row Data Gateway and an Active Record. The crux of
the matter is whether there's any domain logic present, if so you have an Active Record. A Row Data
Gateway should contain only database access logic, and no domain logic.
The choice of Row Data Gateway often comes in two steps, firstly whether to use a gateway at all and
then whether to use Row Data Gateway or Table Data Gateway.
I use Row Data Gateway most often when I'm using a Transaction Script. In this case the Row Data
Gateway nicely factors out the database access code and also allows it to be reused easily amongst
different Transaction Scripts.
I don't use a Row Data Gateway when I'm using a Domain Model. If the mapping is simple then Active
Record does the same job without an additional layer of code. If the mapping is complex then Data
Mapper works better as it's not much more effort to write, but does a better job of decoupling the data
structure from the domain objects because the domain objects don't need to know what the layout of the
database is. Of course you can use the Row Data Gateway to shield the domain objects from the
database structure, and that is a good thing to do if you are changing the database structure when using
Row Data Gateway and don't want to change the domain logic. But doing this on a large scale leads
you to three data representations: one in the business logic, one in the Row Data Gateway, and one in
the database - and that's one too many. So I usually have Row Data Gateways that are the mirror of
the database structure.
If you use Transaction Script with Row Data Gateway you may notice that you have business logic
that's repeated across multiple scripts that would make sense in the Row Data Gateway. Moving that
logic will gradually turn your Row Data Gateway into Active Records. This is often a good thing as it
reduces duplication in the business logic.
Here's an example for Row Data Gateway. The table is a simple person table.
PersonGateway is a gateway for this table. It starts with data fields and accessors
The gateway class itself can handle updates and inserts
To pull people out of the database, we have a separate PersonFinder. This works with the gateway to
create new gateway objects
The finder uses a Registry to hold Identity Maps.
We can then use the gateways from a Transaction Script
If we want to use the Row Data Gateway from a domain object then the domain object needs to get at
the data from the gateway. Instead of copying the data to the domain object we can use the row based
gateway as a data holder for the domain object.
##%%&&
An object that acts as a Gateway to a database table
It's good to keep database access code separated from the rest of an application. A simple Table Data
Gateway holds all the SQL for accessing a single table: selects, inserts, updates and deletes. Other
routine call its manipulation routines to for all changes in the database and issue queries through it's find
routines, each of which passes back a suitable data structure.
A Table Data Gateway has a simple interface, usually consisting of several find methods to get data
from the database, together with update, insert, and delete methods. Each method maps the input
parameters into a SQL and executes the SQL against a database connection.The Table Data Gateway
is usually stateless, as its role is to push data back and forth.
The trickiest thing about a Table Data Gateway is how it returns information from a query. Even a
simple find-by-id query will return multiple data items. In environments where you can return multiple
items you can use that for a single row. However many languages only give you a single return value, and
many queries will return multiple rows.
On alternative is to return some simple data structure, such as a map. A map works, but it forces data to
be copied out of the record set that comes from the database into the map. I find that using maps to pass
around data is bad form, it defeats compile time checking and is not a very explicit interface - leading to
bugs as people misspell what's in the map. A better alternative is to use a Data Transfer Object,
another object to create - but one that may well be used elsewhere.
To save all this you can return the Record Set that comes from the SQL query. This is conceptually
messy, as ideally we would like the in-memory object to not have to know anything about the SQL
interface. It may also make it difficult to substitute the database for a file if you cannot easily create
record sets in your own code. However in many environments, such as .NET, that use Record Set
widely it's a very effective approach. A table based Table Data Gateway thus goes very well with
Table Module. If all of your updates are done through the Table Data Gateway the returned data can
be based on views rather than the actual tables, which reduces the coupling between your code and the
database.
If you're using a Domain Model you can have the Table Data Gateway return the appropriate domain
object. The problem with this is that you then have bidirectional dependencies between the gateway and
the domain objects. Since the two are closely connected that isn't necessarily a terrible thing, but it's
always something I'm reluctant to do.
Most of the time you use Table Data Gateway you'll have one Table Data Gateway for each table in
the database. For very simple cases, however, you can have a single Table Data Gateway that handles
all the methods for all the tables.
As with Row Data Gateway the decision to use Table Data Gateway is first whether to use a
Gateway approach, and then which one to use.
I find Table Data Gateway is probably the simplest database interface pattern to use, as it maps so
nicely onto a database table or record type. It also makes a natural point to encapsulate the precise
access logic of the data source. I use it least with Domain Model, because I find that Data Mapper
gives a better isolation between the Domain Model and the database, and isn't that much more complex
to use.
Table Data Gateway works particularly well with Table Module where the Table Data Gateway
produces a record set data structure for the Table Module to work on. Indeed I can't really imagine any
other database mapping approach for Table Module.
Just like Row Data Gateway, Table Data Gateway is very suitable for Transaction Scripts. The
choice between the two really boils down on how to deal with multiple rows of data. Many people like
using a Data Transfer Object, but that usually seems like more work than is worthwhile, unless the
same Data Transfer Object is used elsewhere. I tend to prefer Table Data Gateway when the result
set representation is convenient for the Transaction Script to work with.
Interestingly it often makes sense to use Table Data Gateway with Data Mapper - having the Data
Mappers talk to the database via Table Data Gateways. Although this makes little sense when
everything is hand coded, it can be very effective when if you want to use metadata for the Table Data
Gateways but prefer hand code for the actual mapping to the domain objects.
One of the benefits of using a Table Data Gateway to encapsulate data base access is that the same
interface can be used both for using SQL to manipulate the database, and for using stored procedures.
Indeed stored procedures themselves are often organized as a Table Data Gateway, that way the
actual table structure is encapsulated behind insert and update stored procedures. The find procedures in
this case can return views, which all helps to hide the underlying table structure.
[Alur, Crupi, and Malks] contains the Data Access Object pattern which is a Table Data Gateway. In
the discussion they show returning a collection of Data Transfer Objects on the query methods. It's not
clear whether they see Data Access Object as always being table based, the intent and discussion
seems to imply either Table Data Gateway or Row Data Gateway.
I've used a different name, partly because I see this pattern as a particular usage of the more general
Gateway concept, and felt the pattern name should reflect that. The other reason is that the term Data
Access Object and it's abbreviation DAO has it's own particular meaning within the Microsoft world.
Table Data Gateway is the usual form of database access in the windows world, so it makes sense to
illustrate one with C#. In doing this, however, I have to stress that this classic form of Table Data
Gateway isn't quite the way that fits in with the .NET environment since it doesn't take advantage of the
ADO.NET data set, instead it uses the Data Reader which is a cursor like interface to database records.
The data reader is the right choice to manipulating larger amounts of information where you don't want to
bring everything into memory in one go.
For the example I'm using a person gateway class that connects to a person table in a database. The
person gateway contains the finder code, returning ADO.NET's data reader to access the returned data.
Almost always you'll want to pull back a bunch of rows with a reader. On a rare occasion you might
want to get hold of an individual row of data with a method along these lines.
The update and insert methods receive the necessary data in arguments and invokes the appropriate
SQL routines.
The generic Table Data Gateway works with pretty much any kind of platform since it is nothing but a
wrapper for SQL statements. When you use .NET you'll use data sets more often, but Table Data
Gateway is still useful, although it comes in a different form.
A data set needs data adapters to load the data into the data set and do updates to the data. So I found
it useful to define a holder for the data set and adapters that load and save the data. A gateway then uses
the holder to store both the data sets and the adapters. Much of this behavior is generic, and can be
done in a superclass.
The holder stores a data set and a collection of adapters indexed by the name of the table.
The gateway stores the holder and exposes the data set for its clients.
The gateway can act on an existing holder, or create a new one.
The find behavior can work a bit differently here. Since a data set is a container for table oriented data,
and one data set can contain data from several tables. As a result it's better to load data into a data set.
To update data you manipulate the data set directly in some client code.
The gateway can have an indexer to get make it easier to get to specific rows.
The update triggers update behavior on the holder.
Insertion can be done much the same way: get a data set, insert a new row to the data table, and fill in
each column. However it can be useful to use an update method to do the insertion in one call.
##%%&&
Transfers data from a domain object to a database
Objects and relational databases have different mechanisms for structuring data. Many parts of objects,
such as collections and inheritance are not present in relational databases. When you are building an
object model with a lot of business logic it's valuable to use these mechanisms to better organize the data
and the behavior that goes with it.This leads to variant schemas: where the object schema and the
relational schema do not match up.
In this situation you still need to transfer data between the two schemas. This data transfer becomes a
complexity in its own right. If the in-memory objects know about the relational database structure, then
changes in one tend to ripple to the other.
The Data Mapper is a layer of software that acts as a mediator between the in-memory objects and the
database. It's responsibility is to transfer data between the two, and also the two layers from each other.
Using Data Mapper the in-memory objects need have no knowledge that there's even a database
present, no SQL interface code, and certainly no knowledge of the database schema. (The database
schema is always ignorant of the objects that use it.)
The separation between domain and data source is the main goal of a Data Mapper, but there are
plenty of details that have to be addressed to make it happen. There's also a lot of variety in how
different people have built their mapping layers. So much of the comments here are pretty broad, as I try
to give a general overview of what you need to separate the cat from it's skin.
We'll start with a very simple database mapper example. This is the simplest style of this layer that you
might get, but is often too simple to actually be worth doing. With simple database mapping examples
other patterns usually are simpler and thus better. If you are going to use Data Mapper at all you usually
need more complicated cases. But it's easier to explain the ideas if we start simple.
A simple case would have a person and person mapper class. To load a person from the database, a
client would call a find method on the mapper. The mapper uses an Identity Map to see if the
person is already loaded, and if not it loads the person.
Updates are similarly simple. A client asks the mapper to save a domain object. The mapper
pulls the data out of the domain object and shuttles it to the database.
The whole layer of Data Mapper can be substituted, either for testing purposes, or to allow a single
domain layer to work with different databases.
In this simple case, the mapper separates the database code away from the domain objects, thus making
the domain objects simpler as they focus on only one task. But soon other issues come into play which
suggest other patterns.
One issue that soon raises its head is how to deal with finding objects. From time to time methods in the
domain layer will need to invoke finder behavior. However if the finder behavior is defined in the
mappers this will break the dependency rule that says that the domain layer should not depend on the
mapper layer. To avoid this you can define interfaces for the finders in a separate package and
implement them in the mapper layer .
In this case the domain object calls the finder through the interface, and the finder then calls the mapper
to load the data.
This example is a gross simplification in lots of ways, but it should give you a sense of broadly what's
going on. The key element to note here are the two principle roles in the layer.
??the finder: who translates method calls into SQL queries
??the mapper: who pulls data out of the row set to create the object.
When it comes to inserts and updates, there's a new level of complexity as the database mapping layer
needs to understand what objects have changed, which new ones have been created, and which ones
have been destroyed. It also has to fit the whole workload into a transactional framework. The Unit of
Work pattern is a good way to organize this.
A simple Data Mapper would just map a database table to an equivalent in-memory class on a field to
field basis. Of course things aren't usually that simple. Mappers need a variety of strategies to handle
classes that turn into multiple fields, classes that have multiple tables, classes with inheritance, and the
joys of connecting together objects once they've been sorted out. The other patterns in this chapter deal
with these. It's usually easier to deploy these patterns with a Data Mapper, than it is with the other
organizing alternatives Gateway and Active Record
Figure 1 suggests that a single request to a finder results in a single SQL query. Often this isn't true. If we
want to load a typical order with multiple order lines, then loading orders may involve loading the order
lines as well. The request from the client will usually lead to a graph of objects being loaded, with the
mapper designer deciding exactly how much to pull back in one go. The point of this is to minimize
database queries. Thus the finders typically need to know a fair bit about the way in which the clients
typically use the objects so they can make the best choices for pulling data back.
This example also leads to cases where you load multiple classes of domain objects from a single query.
If you wanted to load orders and order lines, it will usually be faster to do a single query that joins the
orders and order lines tables. You then use the result set would to load both the order and the order line
instances.
Since objects are very interconnected, you usually have to stop pulling the data back at some point.
Otherwise you are likely to pull back the entire database with a request. Again mapping layers have
techniques to deal with this while minimizing the impact on the in memory objects using Lazy Load.
Hence the in-memory objects cannot be entirely ignorant of the mapping layer. The in-memory objects
need to know about the finders, and a few other mechanisms.
An application can have one Data Mapper or several of them. In smaller applications having a single
Data Mapper works well as it is easier to swap out for testing purposes. As a system grows, however,
the finder will get too complicated. At this point it's worth breaking up. A common habit is to create one
finder per domain class, or at least the head of a domain hierarchy. This leads to a lot of small finders,
but it's easy for a developer to find the finder they need.
As with any database find behavior the finders need to use an Identity Map in order to maintain identity
of the objects read from the database. You can either have a Registry of Identity Maps or have each
finder hold an Identity Map (providing there is only one finder per class per session).
Mappers need to get access to the fields in the domain objects. Often this can be a problem because
you need public methods to support the mappers that you don't want for domain logic. (I'm assuming
you won't commit the cardinal sin of making fields public.) There's no easy to answer to this. You could
use a lower level of visibility by packaging the mappers closer to the domain objects, such as the same
package in Java, but this messes up the bigger dependency picture, since you don't want other parts of
the system that know the domain objects to know about the mappers. You can use reflection, which
often can bypass the visibility rules of the language. Reflection is slower, but the slower speed may end
up as just rounding error compared to the time taken by the SQL call. You can use public methods, but
guard them with a status field so that they throw an exception if they are used outside the context of a
database load. If so name them in such a way that they don't get mistaken for regular getters and setters.
Tied to this is the issue of when you create the object. In essence you have two options. One is to create
the object with a rich constructor so the object is created with at least all its mandatory data. The other
is to create an empty object and then populate it with the mandatory data. In most contexts I prefer the
former since its nice to have a well formed object from the start. This also means that if you have an
immutable field you can enforce this by not providing any method to change its value.
The problem with a rich constructor is that you have to be aware of cyclic references. If you have two
objects that reference each other, each time you try to load one, it will try to load the other, which will
try to load the first object, and so on until you run out of stack space. Avoiding this needs special case
code, often using Lazy Load. Since writing this special case code is messy, it's worth trying to avoid it.
You can avoid it by creating an empty object. To do this you use a no-arg constructor to create a blank
object and insert that empty object immediately into the Identity Map. That way if you have a cycle the
Identity Map will return an object to stop the recursive loading.
Using an empty object like this means you may need some setters for values that are really immutable
when the object is loaded. A combination of a naming convention and perhaps some status checking
guards can avoid this problem. You can also avoid this problem by using reflection for data loading.
One of the decisions you need to make is how to store the information about how the field in domain
objects are mapped to columns in the database. The simplest, and often best, way to do this is with
explicit code. This requires a mapper class for each domain object. The mapper does the mapping
through assignments, and has fields (usually constant strings) to store the SQL for accessing the
database. An alternative is to use Metadata Mapping. In this approach the metadata is stored as data,
either in a class or in a separate file. The great advantage of using metadata is that all the variation in the
in mappers can be handled through data without having to write any more source code, either by using
code generation or reflective programming.
The primary reason for using Data Mapper is when you want the database schema and the object
model to evolve independently. The most common case for this is when you are using a Domain Model.
The primary benefit of Data Mapper is that when working on the domain model you can ignore the
database, both in design and within the build and testing process. The domain objects have no idea what
the database structure is, because all the correspondence is done by the mappers.
This helps you in the code because you can understand and work with the domain objects without
having to understand how they are stored in the database. You can modify either the Domain Model
and the database without having to necessarily alter the other. With complicated mappings, particularly
those involving existing databases, this is very valuable.
The price, of course, is the extra layer compared to Active Record. The test for using them is the
complexity of the business logic. If you have fairly simple business logic then you probably won't need a
Domain Model nor Data Mapper. More compiled logic leads you to Domain Model and therefore to
Data Mapper.
So I wouldn't choose Data Mapper without Domain Model, but would I use Domain Model without
Data Mapper? If the domain model is pretty simple, and the database is under the domain model
developers' control, then it's reasonable for the domain objects to access the database directly.
Effectively this puts the mapper behavior discussed here into the domain objects themselves. As things
become more complicated, however, it's better to refactor the database behavior out into a separate
layer.
Remember that you don't have to build a full featured database mapping layer. These are complicated
beasts to build in full and there are products available that do this. For most cases I recommend buying a
database mapping layer rather than building one yourself.
Here's an absurdly simple use of Data Mapper to give you a feel for the basic structure. Our example is
a simple person with an isomorphic people table.
We'll use the simple case here, where the person mapper class also implements the finder and Identity
Map. However I've added an abstract mapper Layer Supertype to indicate where I can pull out some
common behavior. Loading involves checking the object isn't already in the Identity Map, and then
pulling the data from the database.
The find behavior starts in the person mapper which wraps calls to an abstract find method to find by id.
The find method calls the load method. This method is split between the abstract and person mappers.
The abstract mapper handles checking the id, pulling the id from the data, and registering the new object
in the Identity Map
Notice that the Identity Map is checked twice, once by abstractFind and once by load. There is a
reason for this madness.
I need to check the map in the finder, because that way if the object is already there I can save myself a
trip to the database - and I always want to save myself that long hike if I can. But I also need to check in
the load because I may have queries that I can't be sure of resolving in the Identity Map. Say I want to
find everyone whose last name matches some search pattern. I can't be sure that I have all such people
already loaded, so I have to go to the database and run a query.
When I do this I may pull back some rows in the result set that correspond to people I've already
loaded. I have to ensure I don't make a duplicate, so I have to check the Identity Map again.
Writing a find method this way in each subclass that needs it involves some simple, but repetitive coding.
I can eliminate that by providing a general method.
For this to work I need an interface that wraps both the SQL string and the loading of parameters into
the prepared statement
I can then use this facility by providing a suitable implementation as an inner class.
This kind of work can be done in other places where we have repetitive statement invocation code. On
the whole I've the examples here more straight to make it easier to follow them, but if you find yourself
writing a lot of repetitive straight-ahead code then you should consider doing something similar.
To perform an update I do the JDBC stuff in the Layer Supertype and the selection of fields in subtype.
In order to allow domain objects invoke finder behavior I can separate the finder interfaces from the
mappers. I can put these finder interfaces in a separate package that is visible to the domain layer, or in
this case I can put them in the domain layer itself.
One of the most common kinds of find is one that finds an object according to a particular surrogate ID.
Much of the processing in this is quite generic, so can be handled by a suitable Layer Supertypes. All it
needs is a Layer Supertype for domain objects that know about ids.
The specific behavior for finding lies in the finder interface. This is usually best not made generic, because
you need to know what the return type is.
The finder interface is best declared in the domain package with finders held in a Registry. In this case
I've made the mapper class implement the finder interface.
The bulk of the find method is done by the mapper's Layer Supertype. This involves checking the
Identity Map to see if the object is already in memory. If not it completes a prepared statement that's
loaded in by the artist mapper and executes it.
The find part of the behavior is about getting either the existing or a new object, the load part is about
putting the data from the database into a new object.
Notice that the load method also checks the Identity Map. Although this is redundant in the this case,
the load can also be called by other finders that haven't already done this check.
In this scheme all a subclass has to do is to develop a doLoad method to load the actual data that's
needed, and return a suitable prepared statement from the findStatement method.
You can also do a find based on a query. In this case consider a database of tracks and albums.. We
want a finder that will find all the tracks on a specified album. Again the interface declares the finders.
Since this is a specific find method for this class, this find method is implemented in a specific class, such
as the track mapper class, rather than a Layer Supertype. Like any finder, there are two methods to the
implementation. One is setting up the prepared statement, the other is a method to wrap the call to the
prepared statement, and interpret the results.
The finder calls a load method for each row in the result set. This method has the responsibility of
creating the in memory object and loading it with the data. As the in the previous example, some of this
can be handled in a Layer Supertype, including checking the Identity Map to see if something is already
loaded.
There are two basic approaches for loading an object. One is to create a fully valid object with a
constructor, which is what I've done in the examples above. This results in the following loading code.
The alternative approach is to create an empty object and load it with the setters later.
You'll notice I'm using a different kind of domain object Layer Supertype here. This is because I want
to control the use of the setters. Let's say that I want the last name of a person to be an immutable field.
If this is the case I don't want to change the value of the field once it's loaded.
I can do this by adding a status field to the domain object.
The thing I don't like about this is that we now have a method in the interface that most clients of the
person class cannot use. This is an argument for the mapper using reflection to set the field, which will
completely bypass the Java's protection mechanisms.
Is the status based guard worth the trouble? I'm not entirely sure. On the one hand it will catch bugs due
to people calling update methods at the wrong time. But the question is whether the seriousness of the
bugs is worth the cost of the mechanism. At the moment I don't have a strong opinion either way.
##%%&&
Hold details of object-relational mapping in metadata.
Much of the code that deals with object-relational mapping describes how fields in the database
correspond to the field of in-memory objects. The resulting code tends to be tedious and repetitive to
write.
A Metadata Mapping allows the developers to define the mappings in a simple tabular form which can
then be processed by generic code to carry out the details of reading, inserting, and updating the data.
There are two main parts to implementing a Metadata Mapping: deciding how to represent the
metadata, and deciding how to use the metadata to carry out the data mapping operations.
The simplest way to represent the metadata is to use objects in your programming language. So to
represent the mappings for the person class, you would have a person mapper class. Unlike a explicit
Data Mapper however, this mapper class only has a method to return the map. A generic mapper,
which can be a Layer Supertype, then interrogates the map to carry the actual mapping operations.
Using the programming language may lead to a non-ideal syntax, and forces the mapping to be
maintainable by programmers; but it avoids introducing some new language or data file into the process.
Changes to the mapping have to be dealt with by deploying new classes and compiling these classes.
This compilation is only for the mapping classes and shouldn't affect the rest of the system, but it may be
a challenge in some environments that make deploying new or changes classes difficult.
The immediate alternative to using classes is to use a data file format. Any format can be used, but at the
moment the obvious one to use is XML, since the parsing and editing can use commonly available tools.
There is an overhead, compared to classes, in doing the parsing; but the maps can be shared across and
entire server process and each map only has to be parsed once per process - so the overhead turns out
to be not that considerable. Even if it's difficult to update code when the maps change, it's easy to update
XML mapping files.
Another alternative is to keep the mapping data in the database itself. This keeps it together with the
data, so if the database schema changes the mapping information is right there. It will require an
database query to get the mapping information, but again since the mapping changes rarely you can
usually cache the data for the whole process and thus only need to read the mapping tables when starting
or resetting the server process.
As well as representing the metadata, you have to choose how to make use of it. There are two main
options you can follow: code generation and reflective programming.
With code generation you write a program whose output is the source code of classes that do the
mapping based on the information in the metadata. These classes look like hand-written classes, but the
are entirely generated automatically during the build process, where they are usually generated just prior
to compilation. The resulting mapper classes are deployed with the sever code.
If you use code generation, you should make sure that the code generation step is fully integrated into
your build process with whatever build scripts you are using. The generated classes should never be
edited by hand, and thus should need to be held in source code control.
A reflective program may ask an object for a method named "setFoo", and then run an invoke method
upon the setFoo method passing in some argument. By treating methods (and fields) as data the
reflective program can read in field and method names from a file and use them to carry out the mapping.
I usually counsel against using reflection, partly because it's slow but mainly because it often causes code
that's hard to debug. But reflection is actually quite appropriate for database mapping. Since you are
reading the names of fields and methods in from a file, you are taking full advantage of the flexibility of
reflection. And although reflection is slow, it's much less of an issue because the SQL calls themselves
usually dominate the performance issue.
Code generation is a less-dynamic approach since any changes to the mapping require redeploying at
least that part of the software. However mapper changes should be pretty rare, and modern
environments make it easy to redeploy part of an application - and you can easily keep the generated
mapper classes in a separately deployable binary.
Reflective programming often suffers in speed, although the problem here depends very much on the
actual environment you are using. Also the reflection is being done in the context of a SQL call, and as
such the slower speed of using reflection may not make that much difference considering the slow speed
of the remote call.
Both approaches can be a little awkward to debug, the comparison between them depends very much
on how used developers are to generated and reflective code.
One of the challenges of metadata is that although a simple metadata scheme often works well 90% of
the time, there's often special cases that make life much more tricky. Often to handle these minority
cases you have to add a lot of complexity to metadata. A useful alternative is to handle special cases by
overriding the generic code with subclasses where the special code is handwritten. Such special case
subclasses would be subclasses of either the generated code, or the reflective routines. Since these
special cases are... well... special; it isn't easy to describe define in general how you arrange things to
support the overriding. My advice is that you handle them on a case by case basis. As you need the
overriding, alter the generated/reflective code to isolate a single method that should be overridden and
then override it in your special case.
Metadata Mapping can greatly reduce the amount of work you need to do in handling database
mapping. However there is some setup work required to prepare the framework to handle the Metadata
Mapping. Also while it's often easy to handle most cases with Metadata Mapping, you often find
exceptions that can really tangle the metadata.
It's no surprise that the commercial object-relational mapping tools use Metadata Mapping, since when
selling a product it's always worth the effort of producing a sophisticated Metadata Mapping.
If you're building your own system, you should evaluate the trade-offs yourself. Compare the amount in
adding new mappings using hand-written code and using Metadata Mapping. If you use reflection, look
into the consequences for performance, sometimes reflection causes performance issues, but often it
doesn't. Your own measurements will reveal whether it's an issue for you.
The extra work of hand coding can be greatly reduced by creating a good Layer Supertype that handles
all the common behavior. That way you should only have a few hook routines to add in for each
mapping. But usually Metadata Mapping can reduce this further.
Metadata Mapping can interfere with some refactoring, particularly if you're using automated tools. If
you change the name of a private field, then this can break an application unexpectedly. Even automated
refactoring tools won't be able to find the field name hidden in a XML data file of a map. Using code
generation is a little easier, since search mechanisms are able to find the usage. However the automated
update will get lost when you re-generate the code. So a tool can warn you of a problem, but it's up to
you to change the metadata yourself. If you use reflection, you won't even get the warning.
The above examples, like most in this book, use explicit code. While that's the easiest to follow, it does
lead to pretty tedious programming - and tedious programming is a sign that something is wrong. You
can remove a lot of tedious programming by using metadata. The metadata can be used to generate
code or it can be used in reflection. Here's a reflective example.
The first question to ask about metadata is how it's going to be kept. Here I'm keeping the metadata in
two classes. The data map corresponds to the mapping of one class to one table. This is a simple
mapping, but it will do for illustration.
The data map contains a collection of column maps that map columns in the table to fields.
This isn't a terribly sophisticated mapping. I'm just using the default Java type mappings, which means
there's no type conversion between fields and columns. I'm also forcing a one to one relationship
between tables and classes.
These structures hold the mappings, the next question is how do they get populated? For this example
I'm going to populate them with Java code in specific mapper class. While that may seem a little odd, it
still buys most of the benefit of metadata - that of avoiding repetitive code.
During construction of the column mapper, I build the link to the field. Strictly this is an optimization, so
you may not have to do this, but calculating the fields reduces the subsequent accesses by an order of
magnitude on my little laptop.
It's not much of a challenge to see how I could write a routine to load the map from an XML file, or
from a metadata database. Paltry that challenge may be, but I'll decline it and leave it to you.
Now the mappings are defined, I can make use of them. The strength of the metadata approach is that
all of the code that actually manipulates things is done in a superclass, so I don't have to write the
mapping code that I wrote in the explicit cases.
I'll begin with the find by id method.
The select is built more dynamically than the other examples, but it's still worth preparing it in a way that
allows the database session to cache it properly. If it's an issue the column list could be calculated during
construction and cached, since there's no call for updating the columns during the life of the data map.
For this example, unlike the others in this pattern, I'm using a Unit of Work to handle the database
session. There's no particular reason to use that with metadata, I'm just illustrating how that would work.
As with other example I've separated the load from the find, so that we can use the same load method
from other find methods.
This is a classic reflected program, we go through each of the column maps and use them to load the
field in the domain object. I separated the loadFields method to show how we might extend this for
more complicated cases. If we had a class and table where the simple assumptions of the metadata don't
hold, I can just override loadFields in a subclass mapper to put in arbitrarily complex code. This is a
common technique to use with metadata - providing a hook to override for more wacky cases. It's
usually a lot easier to override wacky cases with subclasses than it is to build metadata sophisticated
enough to hold a few rare special cases.
Of course, if we have a subclass, we might as well use it to avoid down casting.
For updates, I have a single update routine.
Inserts use a similar scheme.
To get back multiple objects with a query, there are a couple of routes you can take. If you want a
generic query capability on the generic mapper, you can have a query that takes a SQL where clause as
an argument.
Your alternative is to provide special case finders on the mapper subtypes.
The great advantage of the metadata approach is that I can now add new tables and classes to my data
mapping and all I have to do is to provide a loadMap method and any specialized finders that I may
fancy.
##%%&&
Ensure each object only gets loaded once by keeping every loaded object in a map. Lookup
objects using the map when referring to them
Some old proverb says that a man with two watches never knows what time it is. If two watches are
confusing, you can get in an even bigger mess with loading objects from a database. If you aren't careful
you can load the data from the same database record into two different objects. If you then update them
both you'll have an interesting time writing the changes out to the database correctly.
Related to this is an obvious performance problem. If you load the same data more than once you're
incurring an expensive cost in remote calls. So avoiding loading the same data twice doesn't just help
correctness, it can also speed up your application.
A Identity Map keeps a record of all the objects that have been read from the database in a single
business transaction. Whenever you want an object, you check the Identity Map first to see if you
already have it.
The basic idea behind the identity map is to have a series of maps of objects that have been pulled from
the database. In a simple case, with an isomorphic schema, you'll have one map per table in the
database. When you load an object from the database, you first check the map. If there's an object in
the map that corresponds to the one you're loading, then you return that. If not you go to the database,
but as you load the objects you put them into the map for future reference.
There are a number of implementation choices to worry about. As well as these Identity Maps interact
with concurrency management, so you should consider Optimistic Offline Lock.
Choice of Keys
The first thing to consider is what the key should be for the map. The obvious choice is to use the
primary key of the corresponding database table. This works well if the key a single column and is
immutable. Using a surrogate primary key fits in very well with this approach. You can then use the key
as the key in the map. The key will usually be a simple data type so the comparison behavior will work
nicely.
Explicit or Generic
You have to choose whether to make the Identity Map explicit or generic. An explicit Identity Map is
accessed with distinct methods for each kind of object you need: such as findPerson(1). A generic map
uses a single method for all kinds of objects, with a parameter to indicate which kind of object you need,
such as find("Person", 1). The obvious advantage is that you can support a generic map with a generic
and reusable object. It's easy to construct a reusable Registry that can be used for all kinds of objects
and doesn't need updating when you add a new map.
However I prefer an explicit Identity Map. For a start this gives you compile time checking in a strongly
typed language. But more than that it has all the other advantages of an explicit interface: it's easier to see
what maps are available and what they are called. It does mean adding a method each time you add a
new map, but that is a small overhead for the virtues of explicitness.
Your type of key affects the choice. You can only use a generic map if all your objects have the same
type of key. This is a good argument for encapsulating different kinds of database key behind a single
key object - see Identity Field for details.
Here the decision varies between one map per class and one map for the whole session. A single map
for the session only works if you have database-unique keys (see the discussion in Identity Field for the
trade-offs on that.) Once you have one Identity Map, the benefit is that you only have one place to go
to and no awkward decisions about inheritance.
If you have multiple maps then the obvious route is to have one map per class or per table. This works
well if your database schema and object models are the same. If they look different then it's usually
easier to base the maps on your objects rather than your tables, as the objects shouldn't really know
about the intricacies of the mapping.
Inheritance rears an ugly head here. If you have cars as a subtype of vehicle, do you have one map or
separate maps? Keeping them separate can make polymorphic references much more awkward, since
any lookup needs to know to look in both maps. As a result I prefer to use a single map for each
inheritance tree, but that means that you should also make your keys unique across the inheritance trees,
which can be awkward if you use Concrete Table Inheritance
An advantage of a single map, is that you don't have to add new identity maps when you add database
tables. However if you tie your maps to your Data Mappers (see below) it won't be any extra burden.
Identity Maps need to be put somewhere where they are easy to find. They are also tied to the process
context that you're working in. You need to ensure that each session gets it's own instance of martin, one
that's isolated from any other session's instance. As such you need to put the Identity Map on a session
specific object. If you are using Unit of Work then that's by far the best place for the Identity Maps
since the Unit of Work is the main place for keeping track of data coming in or out of the database.
If you don't have a Unit of Work then the best bet is a Registry that's tied to the session.
As I've implied here, you usually see a single Identity Map for a session, otherwise you need to provide
transactional protection for your Identity Map, which is more work than any sane developer would try
to do. However there are a couple of exceptions. The biggest one is to use an object database as a
transactional cache, even if you use a relational database for record data. While I haven't seen any
independent performance studies, the possibilities suggest it's worth taking a look at and many people I
respect are big fans of this approach to improve performance.
The second exception is for objects that are read-only in all cases. If an object can never be modified,
there's no need to worry about it being shared across sessions. In performance intensive systems it can
be very beneficial to load in all read-only data once and have available to whole process. In this case
you would have your read-only Identity Maps held in a process context and your updatable Identity
Maps at a session context. This would also apply to objects that aren't completely read-only, but are
updated so rarely that you don't mind flushing the process wide Identity Map and potentially bouncing
the server when it happens.
Even if you're inclined to have only one Identity Map you could split it into two along read-only and
updatable lines. You can avoid clients having to know which is which by providing an interface that
checks both maps.
In general you need to use an Identity Map to manage any object that is brought from a database and
modified. The key reason you need it is because you don't want a situation where you have two
in-memory objects that correspond to a single database record, where you might modify the two
records inconsistently and thus confuse the database mapping.
Another value in Identity Map is that it acts as a cache for database reads, which means you can avoid
going to the database each time you need some data.
One case where you may not need an Identity Map is for immutable objects. If you can't change an
object, then you don't have to worry about modification anomalies. But Identity Map still have
advantages. The most important of these is the performance advantages of the cache. Another is that it
helps to avoid problems where people use the wrong form of equality test, a problem that's prevalent in
Java where you can't override ==.
Another case where you don't need a Identity Map is for a dependent object. Since their persistence is
controlled by their parent, there is no need for a map to maintain identity. However although you don't
need a map, you may want to provide one if there's a need to access the object through a database key.
In this case the map is merely an index, so it's arguable whether it really counts as a map at all.
For each Identity Map we have a map field and accessors.
One of the annoyances of Java is the fact that long isn't an object, therefore you can't use a long as an
index for a map. This isn't as much of a pain as it can be, since we don't actually do any arithmetic on the
index. The one place where it is irritating is when you want to retrieve an object with a literal. You hardly
ever need to do that in production code, but you often do in test code. So I've included a getting method
that takes a long to make testing easier.
##%%&&
Save a database id field in an object to maintain identity between an in-memory object and a
database row.
Relational databases tell one row from another by using keys, in particular the primary key. However
in-memory objects don't need such a key, as the object system ensures the correct identity under the
covers (or in C++'s case with raw memory locations). Reading data from a database is all very well, but
in order to write data back, you need to tie the in-memory object system to the database.
In essence Identity Field is mind-numbingly simple. All you do is store the primary key of the relational
database table in fields of the object.
Although the basic notion of Identity Field is very simple, there are oodles of complicated issues that
come up.
Choosing your Key
The first issue is what kind of key to choose in your database? Often, of course, this isn't a choice since
you are often dealing with an existing database that already has its key structures in place. There's also a
lot of discussion and material on this in the database community. But mapping to objects does add some
concerns in the choice.
The first question is whether to use meaningful or meaningless keys. A meaningful key is something like
the US social security number to identify a person. A meaningless key is essentially a random number
that the database dreams up that's never intended for human use. The danger with a meaningful key is
that while in theory they make good keys, in practice they don't. To work at all keys need to be unique.
To work well keys need to be immutable. While assigned numbers are often supposed to be unique and
immutable, human error often makes them neither. If you mistype my SSN for my wife the resulting
record is neither unique nor immutable (assuming you'd like to fix the mistake.) While the database
should detect the uniqueness problem it can only do that after my SSN goes into the system, and of
course there's a 50% chance that that won't happen. As a result meaningful keys should be distrusted.
For small systems and/or very stable cases you may get away with it, but usually you should take a rare
stand on the side of meaninglessness.
The next issue is between simple and compound keys. A simple key is only one database field, a
compound key uses more than one field. The advantage of a compound key is that it's often easier to
use when one table makes sense in the context of another. A good example is orders and line items. A
compound key of the order number and a sequence number makes a good key for a line item. While
compound keys often make sense, there is a lot to be said for the sheer uniformity of simple keys. If you
use simple keys everywhere you can use the same code for all key manipulation. Compound keys
require special handling in concrete classes. (With code generation this isn't a problem). Compound keys
also carry a bit of meaning, so be careful about the uniqueness and particularly the immutability rule with
them.
You have to choose the type of the key. The most common operation you'll do with a key is checking
equality. So you want a type with a fast equality operation. The other important operation is getting the
next key. As a result a long integer type is often the best bet for keys. Strings can also work, but equality
checking may be slower and incrementing them is a bit more painful. Your DBA's preferences may well
decide the issue.
(Beware about using dates or times in keys. Not just are they meaningful, they also lead to problems
with portability and consistency. Dates in particular are vulnerable to this as they are often stored to
some fractional second precision, which can easily get out of sync and lead to identity problems.)
You can have keys that are unique to the table, or database wide unique keys. A table-unique key
ensures that key is unique across the table, which is what you need for a key in any case. A
database-unique key is a key that is unique across every row in every table in the database. A
table-unique key is usually fine, but a database-unique key is often easier to do, and allows you to use a
single Identity Map. Modern values being what they are, it's pretty unlikely you'll run out of numbers for
new keys. If you really insist you can reclaim keys from deleted objects with a simple database script
that compacts the key space - although running this script will require you to take the application offline.
But if you use 64 bit keys (and you might as well) you're unlikely to need it.
If you use table-unique keys, be wary of inheritance. If you are using Concrete Table Inheritance or
Class Table Inheritance life is much easier if you use keys that are unique to the hierarchy rather than
unique to each table. I still use the term table-unique, even if it should strictly be something like
"inheritance graph unique".
The size of your key may effect performance, particularly with indexes. This is very dependent on both
your database system and how many rows you have, but it's worth doing a crude check before you get
fixed into your decision.
Representing the Identity Field in an object
The simplest form of Identity Field is a simple field which matches the type of the key in the database.
So if you use a simple integral key, an integral field will work very nicely.
Compound keys are more problematic. If you have compound keys the best bet is to make a key class.
A generic key class can store a sequence of objects that act as the elements of the key. The key
behavior (I have a quota of puns per book to fill) is equality. It's also useful to get parts of the key when
you are doing the mapping to the database.
A key object is then easy to use with the association mappings and with Identity Map. As such you
might decide to avoid a key object when you are using a Transaction Script and Row Data Gateway
architecture.
If you use the same basic structure for all keys, you can do all of the key handling in a Layer Supertype.
This works particularly well if you can use an integral key field for all database tables. If you start getting
any compound keys you should use a key object in the Layer Supertype instead. Again you can put
default behavior which will work for most cases in the Layer Supertype and extend it for the exceptional
cases in the particular subtypes.
You can either have a single key class, which takes a generic list of key objects, or you can have a key
class for each domain class with explicit fields for each part of the key. I usually prefer to be explicit, but
in this case I'm not sure it buys very much. You end up with lots of small classes that don't do anything
interesting. The main benefit is that you can avoid errors due to people putting the elements of the key in
the wrong order, but that doesn't seem to be a big problem in practice.
If you are likely to import data between different database instances, then you need to remember that
you'll get key collisions unless you come up with some scheme to separate the keys between different
databases. You can solve this with some kind of key migration on the imports, but this can easily get
very messy.
Getting a new key
To create an object, you'll need to get a key. This sounds like a simple matter, but in practice it's often
quite a problem. You have three basic choices: get the database to auto-generate, use a GUID, or
generate your own.
The auto generate route should be the easiest. Each time you insert data for the database the database
takes care of generating a unique primary key.You don't have to do anything - it sounds too good to be
true and sadly it often is. Not all databases do this the same way, and many of those that do handle it in
such a way that causes problems for object-relational mapping.
The most common auto-generation method is that of declaring one field to be an auto-generated field.
In this case whenever you insert a row, this field is incremented to a new value. The problem with this
scheme is that you can't easily determine what value got generated as the key. If you want to insert an
order and several line items you need the key of the new order so you can put the value in the foreign
key for the line items. You also need this before the transaction commits so you can save everything
within the transaction. Sadly databases usually don't give you this information, so you usually can't use
this kind of auto generation on any table where you need to insert connected objects.
An alternative approach to auto-generation is a database counter, Oracle does this with its sequence.
An Oracle sequence works by sending a select statement that references a sequence, the database then
returns a SQL record set consisting of the next value in the sequence. You can set a sequence to
increment by any integer, which allows you get multiple keys at once. The sequence query is
automatically carried out in a separate transaction, so that a accessing the sequence won't lock out other
transactions inserting at the same time. A database counter like this is perfect for our needs, but it's
non-standard and not available in all databases.
A GUID (Globally Unique IDentifier) is a number generated on one machine that is guaranteed to be
unique across all machines in space and time. Often platforms give you the API to generate one. The
algorithm is an interesting one involving ethernet card addresses, time of the day in nanoseconds, chip id
numbers, and probably the number of hairs on your left wrist. All that matters is that the resulting number
is completely unique and thus a safe key. The only disadvantage is that the resulting key string is big, and
that can be an equally big problem. There's always times when someone needs to type in a key to
window or SQL expression, and long keys are hard both to type and to read. Large keys may also lead
to performance problems, particularly with indexes.
The last option is rolling your own. A simple staple for small systems is to use a table scan using the
SQL max function to find the largest key in the table, then add one to use it. Sadly this read locks the
entire table while you are doing it. As a result it works fine if inserts are rare, but if you have inserts
running concurrently with updates on the same table your performance will be toasted. You also have to
ensure you have complete isolation between transactions otherwise you can end up with multiple
transactions getting the same id value.
A better approach is to use a separate key table. This table is typically a table with two columns: a
name and next available value. If you use database-unique keys, then you'll have just one row in this
table. If you use table-unique keys then you'll have one row for each table in the database. To use the
key table, all you need to do is read that one row note the number, increment it and write it back to the
row. You can grab many keys at a time by adding a suitable number when you update the key table.
This cuts down on expensive database calls as well as reducing contention on the key table.
If you use a key table, it's a good idea to design it so the access to the key table is done in a separate
transaction to the one that updates the table you are inserting into. Say I'm inserting an order into the
orders table. To do this I'll need to lock the orders row on the key table with a write lock (since I'm
updating). That lock will last for the entire transaction that I'm in, locking out anyone else who wants a
key. For table-unique keys, this means anyone inserting into the orders table. For database-unique keys
this means anyone inserting anywhere.
By putting the access to the key table in a separate transaction, you only lock the row for that, much
shorter, transaction. The downside is that if you rollback on your insert to the orders, then the key you
got from the key table access is lost to everyone. Fortunately numbers are cheap, so that's not a big
issue. Using a separate transaction also allows you to get the id as soon as you create the in-memory
object, which is often some before you open the transaction to commit the business transaction.
Using a key table affects the choice on whether to use database-unique or table-unique keys. If you use
a table-unique key, you have to add a row to the key table every time you add a table to the database.
This is more effort, but it reduces contention on the row. If you keep your key table accesses in a
different transaction, then contention is not so much of a problem, especially if you get multiple keys in a
single call. But if you can't for the key table update to be in a separate transaction, that's a strong reason
against using database-unique keys.
It's good to separate the code for getting a new key into it's own class, as that makes it easier to build a
Service Stub for testing purposes.
You need to use Identity Field when there is a mapping between objects in memory and rows in a
database. The usual cases for this is where you are using Domain Model or a Row Data Gateway.
Conversely you don't need this if you are using Transaction Script, Table Module, or a Table Data
Gateway
A small object with value semantics, such as a money or date range object, will not have its own table.
It's better to use Embedded Value. For a complex graph of objects that doesn't need to be queried
within the relational database Serialized LOB is usually easier to write and gives faster performance.
One alternative to Identity Field is to use an Identity Map to maintain the correspondence. This can be
used for systems where you don't want to store an Identity Field in the in-memory object. The Identity
Map needs to look up both ways: give me a key for an object or an object for a key. I don't see this
done so often because usually it's easier to store the key in the object.
The simplest form of Identity Field is a integral field in the database that maps to an integral field in an
in-memory object.
An object that's been created in memory but not saved to the database will not have a value for its key.
For a .NET value object, this is a problem since .NET values cannot be null. Hence the placeholder
value.
The key is becomes important in two places: finding and insertion. For finding you need to form a query
using a key in a where clause. In .NET you may load many rows into a data set and then select a
particular one with a find operation.
Most of this behavior can live on the Layer Supertype, but you'll often need to define the find on the
concrete class just to encapsulate the downcast. Naturally you can avoid this in a language that doesn't
use compile-time typing.
With a simple integral Identity Field the insertion behavior can also be held at the Layer Supertype.
Essentially insertion involves creating the new row and using the next key for that new row. Once you
have the new row you can save the in-memory object's data to this new row.
If your database supports a database counter and you're not worried about being dependent on
database specific SQL, then you should use the counter. Even if you are worried about being dependent
on a database you should still consider it - as long as your key generation code is nicely encapsulated
you can always change it to a portable algorithm later. You could even use a strategy to use counters
where you have them and roll your own when you don't.
But for the moment lets assume we have to this the hard way. The first thing we need is a key table in the
database.
The key table contains one row for each counter that's in the database. In this case I've initialized the key
to 1. If you are pre-loading data in the database you'll need to set the counter to a suitable number. If
you want database-unique keys you'll only need the one row. If you have table-unique keys then you'll
have one row per table.
You can wrap all of your key generation code into its own class, that way it's easier to use it more
widely around one or more applications. It also makes it easier to put key reservation into its own
transaction.
We construct a key generator with its own database connection, together with information on how many
keys to take from the database at once.
We need to ensure that no auto-commit is going on since we absolutely have to have the select and
update operating in one transaction.
When we ask for a new key, the generator first looks to see if it has one cached, rather than going to the
database.
If the generator hasn't got one cached, then it needs to go to the database.
In this case we use SELECT... FOR UPDATE to tell the database to hold a write lock on the keys
table. This is an Oracle specific statement, so your mileage will vary if you're using something else. If you
can't write lock on the select you run the risk of the transaction failing should another one get in there
before you. In this case, however you can pretty safely just rerun reserveIds until you get a pristine set of
keys.
Using a simple integral key is a good simple solution, but often you need to use other types or compound
keys.
A Key Class
As soon as you need to use something else it's worth using putting together a key class. A key class
needs to be able to store multiple elements of the key and to be able to tell if two keys are equal.
The most elemental way to create a key is with an array parameter.
If you find there are common cases when you create keys with certain elements, you can add
convenience constructors. The exact convenience constructors will depend on what kinds of keys your
application has.
Don't be afraid to add these convenience methods, after all convenience is important to everyone using
the keys.
Similarly you can add accessor functions to get parts of key. The application will need to do this for the
mappings.
In this example we'll map to an order and line item tables. The order table has a simple integral primary
key, the line item primary key is a compound of the order's primary key and a sequence number.
The Layer Supertype for domain objects needs to take have a key field.
As with other examples in this book I've split the behavior into find (which gets to the right row in the
database) and load (which loads data from that row into the domain object). Both responsibilities are
affected by the use of a key object.
The primary difference between them and the other examples in this book (which use simple integral
keys) is that we have to factor out certain pieces of behavior that are overridden by those classes that
have more complex keys. For this example I'm assuming that most tables use simple integral keys but
some use something else. So as a result I've made the default case the simple integral and embedded the
behavior for that in the mapper Layer Supertype. The order class is one of those simple cases. Here's
the code for the find behavior.
For the find behavior I've extracted out the building of the find statement, since that will require different
parameters to be passed into the prepared statement. The line item is a compound key, so needs to
override that method.
As well as defining the interface for the find methods and providing a SQL string for the find statement,
the subclass needs to override the hook method to allow two parameters to go into the SQL statement.
I've also written two helper methods to extract the parts of the key information. This makes for clearer
code than just putting explicit accessors with numeric indices from the key. Such literal indices are a bad
smell.
The load behavior shows a similar structure, default behavior in the Layer Supertype for simple integral
keys, overridden for the more complex cases. In this case the order's load behavior looks like this.
The line item needs to override the hook to create a key based on two fields.
The line item also has a separate load method for use when loading all the lines for the order.
You need the special handling because the order object isn't put into the order's Identity Map until after
it's been created. Creating an empty object and inserting it directly into the Identity Field would avoid
the need for this, see [here]
In a similar way to reading data the insertion behavior has a default action for a simple integral key and
the hooks to override this for more interesting keys.
In the mapper supertype I provide an operation to act as the interface together with a template method
to do the work of the insertion.
The data from the object goes into the insert statement through two methods separating the data from
the key from the basic data of the object. I do this because I can provide a default implementation for
the key, which will work for any class, like order, that uses the default simple integral key.
The rest of the data for the insert statement is dependent on the particular subclass, so this behavior is
abstract on the superclass
The line item overrides both of these methods. Line item pulls two values out for key.
And provides its own implementation of the save statement for the rest of the data
Separating out the loading of the data into the insert statement like this is only worthwhile if most classes
use the same single field for the key. If there is more variation for handling the key, then having just one
command to insert the information is probably easier.
Coming up with the next database key is also something that I can separate into a default and an
overridden case. For the default case I can use the key table scheme that I talked about earlier on.For
the line item, we run into a problem. The line item's key uses the key of the order for the line item as part
of its composite key. However there is no reference from the line item class to the order class. So it's
impossible to tell a line item to insert itself into the database without providing the correct order as well.
This leads to the always messy approach of implementing the superclass method with an unsupported
operation exception.
Of course we can avoid this by having a back link from the line item to the order, effectively turning the
association between the two into a bidirectional association. I've chosen not to do it here to illustrate
what to do when you don't have that link.
By supplying the order, it's easy to get the order's part of the key. The next problem is to come up with
a sequence number for the order line. To find that out we need to find out what the next available
sequence number is for an order. We can do this either with a max query in SQL, or by looking at the
line items on the order in memory. For this example I'll do the latter.
This algorithm would be much nicer if I used the Collections.max method, but since we may (and indeed
will) have at least one null key the max method would fail.
Updates and Deletes
After all of that, updates and deletes are mostly harmless. Again we have an abstract method for the
assumed usual case, and an override for the special cases.
##%%&&
Save a graph of objects as by serializing them into a single large object (LOB) and store the LOB
in a database field.
Object models often contain complicated graphs of small objects. Much of the information in these
structures is not the objects but the links between the objects. Consider storing the organization
hierarchy for all your customers. An object model would quite naturally show the composition pattern to
represent organizational hierarchies. You can easily add methods that allow you to get ancestors,
siblings, descendents and other common relationships
Putting all this into a relational schema, however, get's more awkward. The basic schema is simple, an
organization table with a parent foreign key. However manipulation of the schema requires many joins,
which is both slow and awkward.
Objects don't have to be persisted as table rows related to each other. Another form of persistence is
serialization, where a whole graph of objects is written out as a single field in a table, this field then
becomes a form of memento.
There are two ways you can do the serialization: a binary (BLOB) or as textual characters (CLOB).
The BLOB is often the simplest to create since many platforms include the ability to automatically
serialize an object graph. Saving the graph is then simply a matter of applying the serialization into a
buffer and saving that buffer into the relevant field.
The advantages of the BLOB is that it's simple to program (if your platform supports it), and it uses the
minimum of space. Your database, of course, must support a binary data type to support this. Also you
have to live with the fact that you can't reconstruct the graph without the object, so the field is utterly
impenetrable to casual viewing. The most serious problem to concern with, however, is versioning. If
you change the department class, you may not be able to read all the previous serializations of that class;
since data can live in the database for a long time, this is a serious problem.
The alternative is a CLOB. In this case you serialize the department graph into a text string which carries
all the information you need. The text string can be read easily by a human viewing the row - which helps
in casual browsing of the database. However the text approach will usually need more space and you
may need to create your own parser for the textual format you use. It's also likely to be slower than a
binary serialization.
Much of these disadvantages for CLOBs can be overcome by using XML. XML parsers are commonly
available, so you don't have to write your own. Furthermore since XML is a widely supported standard,
you can take advantage of tools that become available to do further manipulations. The disadvantage
that XML does not help with is the matter of space. Indeed it makes the space issue much worse wince
XML is a very verbose format. One way to deal with that is to use a zipped XML format as your
BLOB. That loses the direct human readability, but is an option if space really is an issue.
When you're using Serialized LOB beware of identity problems. Say you wanted to use Serialized LOB
for the customer details on an order. For this don't put the customer LOB in the order table, otherwise
the customer data will be copied on every order, which makes updates a problem. (This is actually a
good thing, however, if you want to store a snapshot of the customer data as it was at the placing of the
order - avoiding temporal relationships.) If you want your customer data to be updated for each order in
the classical relational sense, you need to put the customer LOB in a customer table so many orders can
link to it. There's nothing wrong with a table that just has an id and a single LOB field for its data.
In general you need to be careful of duplicating data when using Serialized LOB. Often it's not a whole
Serialized LOB that gets duplicated, but part of a Serialized LOB that overlaps with another one. The
thing to do is to pay careful attention to the data that's stored in the Serialized LOB and be sure that this
data cannot be reached from anywhere but a single object that acts as the owner of the Serialized LOB.
Serialized LOB isn't considered as often as it might be. Using XML makes it much more attractive since
it yields a easy to implement textual approach.
The main disadvantage of the LOB is that you can't query the structure using SQL. SQL extensions are
appearing to get at XML data within a field, but that's still not the same (or portable).
Serialized LOB works best when you can chop a piece of the object model out and represent the LOB.
As such think of a LOB of a way to take a bunch of objects that aren't likely to be queried from any
SQL route outside the application. This graph can then be hooked into the SQL schema.
Serialized LOB works poorly when you have references from objects outside the LOB into objects that
are buried in the LOB. To handle this you have to come up with some form of referencing scheme that
can support references to objects inside a LOB. While this is by no means impossible, it is awkward.
Awkward enough to usually be not worth doing. Again XML, or rather XPATH, reduces this
awkwardness somewhat.
If you are using a separate database for reporting and all other SQL goes against that database, then you
can transform the LOB into a suitable table structure. The fact that a reporting database is usually
denormalized means that structures that are suitable for Serialized LOB are often also suitable for a
separate reporting database.
For this example we'll take the notion of customers and departments and show how you might serialize
all the departments into an XML CLOB. The object model of the sketch turns into the following class
structures
The database for this has only one table.
We'll treat the customer as an Active Record and illustrate writing the data with the insert behavior.
The customer has a method for serializing its departments field into a single XML DOM. Each
department also has a method for serializing itself (and its subsidiaries recursively) into a DOM. The
insert method then just takes the DOM of the departments, converts it into a string (via a utility class)
and puts it into the database. We aren't particularly concerned with the structure of the string. It's human
readable, but we aren't going to look at it on a regular basis.
Reading back is a fairly simple reversal of the process.
The load code is obviously a mirror image of the insert code. The department knows how to create itself
(and its subsidiaries) from an XML element, and the customer knows how to create the list of
departments from an XML element. The load method uses a utility class to turn the string from the
database into a utility element.
An obvious danger here is that someone may try to edit the XML by hand in the database and mess up
the XML making it unreadable by the load routine. More sophisticated tools that would support adding
a DTD or XML Schema to a field as validation would obviously help with that.
##%%&&
Map an association between objects to a foreign key reference between tables
Objects can refer to each other directly by object references. Even the simplest object oriented system
will contain a bevy of objects connected to each other in all sorts of interesting ways. To save these
objects to a database, it's vital to save these references. However since the data in these references is
specific to the specific instance of the running program, you can't just save raw data values.
This is further complicated by the fact that objects can easily hold collections of references to other
objects. Such a structure violates the first normal form of relational databases.
A Foreign Key Mapping maps an object reference to a foreign key in the database.
The obvious key to this problem is Identity Field. Each object contains the database key from the
appropriate database table. If two objects are linked together with an association, this association can
be replaced by a foreign key in the database. In the simple form of this, when you save an album to the
database, you save the ID of the artist that the album is linked to in the album record, as in Figure 1.
This is the simple case. A more complicated case turns up when you have a collection of objects. You
can't save a collection of objects in the database, so you have to reverse the direction of the reference.
So if you have a collection of tracks in the album, you use put a the foreign key of the album in the track
record as in Figure 2.
The complicated part of this occurs when you have an update. Updating implies that tracks can be
added and removed to the collection within an album. How can you tell what the alterations are to put
them to the database? Essentially you have three options, delete and insert, add a back pointer, or diff
the collection.
With delete and insert you delete all the tracks in the database that link to the album, and then insert all
the ones currently on the album. At first glance this sounds pretty appalling, especially if you haven't
changed any. But the logic is easy to implement and as such it works pretty well compared to the
alternatives. The limitation is that you can only do this if tracks are Dependent Mappings. This means
they must be owned by the album and can't be referred to outside the album.
Adding a back pointer puts a link from the track back to the album, effectively converting the association
into a bidirectional association. This changes the object model, but now you can handle the update using
the simple technique for single-valued fields on the other side.
If neither of those appeals, you can do a diff. There are two possibilities here, either diff with the current
state of the database or diff with what you read the first time. Diffing with the database involves
re-reading the collection back from the database. You then compare the collection you read from the
collection in the album. Anything on the database that isn't in the album was clearly removed, anything in
the album that isn't on the disk is clearly a new item to be added. You then have to look at the logic of
the application to decide what to do with each item.
Diffing with what you read in the first place means you have to keep what you read. It's better as it
avoids another database read. You can also diff what you first read with the database to help spot any
concurrency problems.
In the general case anything that's added to the collection needs first to be checked to see if it's a new
object, which you can do by seeing if it has a key. If there's no key, it needs to be added to the
database. This step is made a lot easier by using Unit of Work because that way any new object will be
inserted first automatically. In either case you then find the linked row in the database and update it's
foreign key to point to the current album.
For removal you have to know whether the track was moved to another album, has no album, or is
deleted altogether. If it's moved to another album it should be updated when you update the other
album. If it has no album then you need to null the foreign key. If it's deleted then it should be deleted as
things get deleted. Handling deletes is much easier if the back link is mandatory, as it is here where every
track must be on an album. That way you don't have to worry about detecting items removed from the
collection, since they will be updated when you process the album they've been added to.
If the link is immutable, meaning you can't change a track's album, then adding always means insertion
and removing always means deletion, which makes things simpler still. Usually in this case you can make
the track a Dependent Mapping.
One thing to watch out for is cycles in your links. Say you need to load an order, which has a link to a
customer (which you load). The customer has a set of payments (which you load). Each payment has
orders that it's paying for, which might include the original order you were trying to load. So you load the
order (go back to the beginning of this paragraph.)
To avoid getting lost in recursive cycles you have two choices, which boil down to how you create your
objects. Usually it's a good idea to have a creation method that includes data in the creation method that
gives you a fully formed object. If you do that then you'll need to place Lazy Load at appropriate points
to break the cycles. If you miss one you'll get a stack overflow, but if you're testing is good enough you
can manage that burden.
The other is to create empty objects and immediately put them in an Identity Map. That way when you
cycle back around the object is already loaded and you'll end the cycle. The objects you create aren't
then fully formed, but they should be fully formed by the end of the load procedure. This avoids having
to make special case decisions about the use of Lazy Load just to do a correct load.
A Foreign Key Mapping can be used for most associations between classes. The most common case
where it isn't possible to use Foreign Key Mapping is with many-to-many associations. Foreign keys
are single values and first normal form means you can't store multiple foreign keys in a single field. So
instead you need to use Association Table Mapping
If you have a collection field with no back pointer you should consider whether the many side should be
a Dependent Mapping, if so that can simplify your handling of the collection.
If the related object is a Value Object then you should use Embedded Value.
This is the simplest case, where an album has a single reference to an artist.
Figure 4 shows how you can load an album. When an album mapper is told to load a particular album it
queries the database and pulls back the result set for that object. It then queries the result set for each
foreign key field and finds that object. Then it can create the album with the appropriate found objects. If
the artist object was already in memory it would be fetched from the cache, otherwise it would be
loaded from the database in the same way.
The find operation uses abstract behavior to manipulate an Identity Map
The find operation calls a load operation to actually load the data into the album.
To update an album the foreign key value is taken from the linked artist object.
While it's conceptually clean to issue one query per table, it's often inefficient - since SQL are remote
calls and remote calls are slow. So it's often worth finding ways to gather information from multiple
tables in a single query. So I can modify the above example to use a single query to get both the album
and artist information with a single SQL call. The first alteration is that of the SQL for the find statement.
I then use a different load method that loads both the album and artist information together
There's a tension here about where to put the method that maps the SQL result into the artist object.
One the one hand it's better to put it in the artist's mapper, since it's the class that usually loads the artist.
But on the other the load method is closely coupled to the SQL and thus should stay with the SQL
query. In this case I've voted for the latter.
The case for a collection of references occurs when you have a field that is a collection. Here I'll use an
example of teams and players where we'll assume we can't make player a Dependent Mapping.
In the database this will be handled with the player record having a foreign key to the team.
The data set holder is a class that holds onto the data set in use, together with the adapters needed to
update it to the database.
For this example, we'll assume it has already been populated by some appropriate queries.
The find method calls a load to actually load the data into the new object.
To bring in the players, I execute a specialized finder on the player mapper.
To update, the team saves its own data and delegates to the player mapper to save the data into the
player table.
##%%&&
Have one class perform the mapping for another
Some objects naturally appear in the context of other objects. Tracks on an album may be loaded or
saved whenever the underlying album is loaded or saved. If these tracks are not referenced to by any
other table in the database, you can simplify the mapping procedure by having the album mapper
perform the mapping for tracks as well - treating the mapping of the tracks as a Dependent Mapping.
The basic idea behind Dependent Mapping is that one class (the dependent) relies upon some other
class (the owner) for its database persistence. Each dependent can have only one owner and must have
one owner.
This manifests itself in terms of the classes that do the mapping. In the case of Data Mapper mapper
class for the track. For an Active Record the track class won't contain any database mapping code, it
will all be in the owning album. A Row Data Gateway will have no database code in the dependent. In
a Table Data Gateway there will typically be no dependent class at all, all the handling of the dependent
is done in the owner. In most cases every time you load an owner, the dependents are loaded too. If the
dependents are expensive to load and infrequently used you can use a Lazy Load to avoid loading the
dependents until you need them.
An important property of a dependent is that it does not have an Identity Field and therefore doesn't
get stored in a Identity Map. It therefore cannot be loaded by a find method that looks up an id. Indeed
there is no finder for a dependent, all finds are done with the owner.
A dependent may itself be the owner of another dependent. In this case the owner of the first dependent
is also responsible for the persistence of the second dependent. Indeed you can have a whole hierarchy
of dependents controlled by a single primary owner.
It's usually easier for the primary key on the database to be a composite key that includes the owner's
primary key. No other table should have a foreign key into the dependent's table, unless that object is
dependent on that dependent. As a result no in-memory object, other than the owner, should have a
reference to a dependent. Strictly speaking you can relax that rule providing that the reference isn't
persisted to the database, but having a non persistent reference is itself a good source of confusion.
In a UML model, it's appropriate to use composition to show the relationship between an owner and its
dependents.
Since the writing and saving of dependents is left to the owner, and there are no outside references, this
allows updates to the dependents to be handled through deletion and insertion. So if you wish to update
the collection of dependents you can safely delete all rows that link to the owner and then reinsert all the
dependents. This saves you from having to do an analysis of what objects got added or removed from
the owner's collection.
Dependents are in many ways rather like Value Objects, although they often do not need the full
mechanics that you use in making something a Value Object (such as overriding equals). The main
difference is that from a purely in-memory point of view there is nothing special about them. The
dependent nature of the objects is only really due to the database mapping behavior.
Using Dependent Mapping complicates tracking whether the owner has changed. Any change to a
dependent needs to mark the owner as changed so that the owner will write the changes out to the
database. You can simplify this considerably by making the dependent immutable, so that any change to
a dependent needs to be done by deleting the old one and creating a new one. Although this can make
the in-memory model harder to work with, it does simplify the database mapping. While in theory the
in-memory and database mapping should be independent when you're using Data Mapper , in practice
you have to make the occasional compromise.
You use Dependent Mapping when you have an object that is only referred to by one other object.
Usually this occurs when you have one object having a collection of dependents. As such Dependent
Mapping is a good way of dealing with the awkward situation where the owner has a collection of
references to its dependents, but there is no back-pointer. Providing the many objects don't have any
need for their own identity, using Dependent Mapping makes it easier to manage their persistence.
For Dependent Mapping to work there are a number of pre-conditions.
??A dependent must have exactly one owner.
??There must be no references from any object other than the owner to the dependent
There is a school of OO design which uses the notion of entity objects and dependent objects when
designing a Domain Model. I tend to think of Dependent Mapping as a particular technique which I
use to simplify database mapping, rather than as a fundamental OO design medium. In particular I avoid
large graphs of dependents. The problem with these large graphs is that it's impossible to refer to a
dependent from outside the graph, which often leads to complex lookup schemes based around the root
owner.
I don't recommend using Dependent Mapping if you're using Unit of Work. The delete and re-insert
strategy doesn't help at all if you have a Unit of Work keeping track of things. It can also lead to
problems since the Unit of Work isn't controlling the dependents. Mike Rettig told me about an
application where a Unit of Work would keep track rows inserted for testing and then delete them all
when done. As it didn't track dependents, orphan rows would appear causing bugs in the test runs.
In this domain model an album holds a collection of tracks. Since this uselessly simple application does
not need anything else to refer to a track, it's an obvious candidate for Dependent Mapping. (Indeed
anyone would think the example is deliberately constructed for the pattern.)
This track just has a title. I've defined it as an immutable class.
The album mapper class handles all the SQL for tracks and thus defines the SQL statements that access
the tracks table.
The tracks are loaded into the album whenever the album is loaded.
When the album is updated all the tracks are deleted and reinserted.
##%%&&
Save an association as a table with foreign keys to the tables that are linked by the association.
Objects can handle multi-valued fields quite easily by using collections as field values. Relational
databases don't have this feature and are constrained to single-valued fields only. When you're mapping
a one-to-many association you can handle this using Foreign Key Mapping, essentially to use a foreign
key for the single-valued end of the association. But a many-to-many association can't do this because
there isn't a single-valued end to hold the foreign key.
The answer is the classic resolution used by relational data people for decades: create an extra table to
record the relationship. We can then use Association Table Mapping to map the multi-valued field to
this link table.
The basic idea behind Association Table Mapping is to use a link table to store the association. The
link table has only the foreign key ids for the two tables that are linked together. It has one row for each
pair of objects that are associated.
The link table has no corresponding in-memory object. As a result it has no ID. The primary key of the
link table is the compound of the two primary keys of the tables that are associated together.
To load data from the link table you, in simple terms, peform two queries. Consider loading the skills for
an employee. In this case you do, at least conceptually, queries in two stages. The first stage queries the
skillsEmployees table to find all the rows that link to the employee you want. Then for each row in the
link table you find the skill object for the related id.
If all this information is already in memory, this scheme works fine. If it isn't, this scheme can be horribly
expensive in queries, since you'll do a query for each skill that's in the link table. You can avoid this cost
by joining the skills table to the link table which allows you to get all the data in a single query, albeit at
the cost of making the mapping a bit more complicated.
Updating the link data involves many of the same issues as updating a many valued field. Fortunately the
matter is made much easier since you can in many ways treat the link table in a similar way to Dependent
Mappings. No other table should refer to the link table, so you can freely create and destroy links as
you need them.
The canonical case for Association Table Mapping is a many-to-many association, since there's not
really any alternatives for that situation.
Association Table Mapping can also be used for any other form of association. It's more complex to
use than Foreign Key Mapping, and involves an extra join - so it's not usually the right choice.
However Association Table Mapping is an option if you have tables that you need to link but can't be
altered, or if that's the way the tables are in an existing database schema.
Here's a simple example using the sketch's model. We have an employee class with a collection of skills
where each skill can appear for more than one employee.
To load an employee from the database, we need to pull in the skills using an employee mapper. Each
employee mapper class has a find method that creates an employee object. All mappers are subclasses
of the abstract mapper class that pulls together common services for the mappers.
The data set holder is a simple object that contains an ADO.NET data set and the relevant adaptors to
save it to the database.
To make this example simple, indeed simplistic, we'll assume the data set has already been loaded with
all the data we might need.
The find method calls load methods to load data for the employee.
Loading the skills is sufficiently awkward to demand a separate method to do the work.
To handle changes in skills information there is an update method on the abstract mapper.
The update method calls a save method in the subclass.
Again I've made a separate method for saving the skills.
The logic here does the simple thing of deleting all existing link table rows and creating new ones. This
saves me having to figure out which ones have been added and deleted.
One of the nice things about ADO.NET is that it allows me to discuss the basics of an object-relational
mapping without getting into the sticky details of minimizing queries. With other relational mapping
schemes you are closer to the SQL and have to take much of that into account.
For this example we'll look at reading in data for the classes in Figure 1.
If we want to load an album, we want to load all the tracks on that album. As we load each track, we
want to load all of the performers for that track.
I'll begin by looking at the logic for finding an album.
The album loads its data and also loads the tracks.
To load the tracks it uses a particular finder to find the tracks for the album.
Loading the track data occurs in the load method.
There's a separate method for loading the performers. This invokes the query on the link table.
Since this method invokes the load method on the artist mapper, it gets the list of columns to return from
the artist mapper class. That way the columns and the load method stay in sync.
##%%&&
Map an object into several fields of another object's table
There are many small objects that make sense in an OO system, that don't make sense as tables in a
database. Examples include currency aware money objects, date ranges and the like. So although the
default thinking is to save an object as a table, no sane person would want a table of money values.
An Embedded Value maps the values of an object to fields in the record of the object's owner. So in
the sketch, we have an employment object with links to a date range object and a money object. In the
resulting table the fields in those objects are mapped to fields in the employment table, rather than
making new objects themselves.
Carrying out this exercise is actually quite simple. When the owning object (the employment) is loaded
or saved, the dependent objects (the date range and the money) are loaded and saved at the same time.
The dependent classes won't have their own persistence methods, all persistence is done by the owner.
This is one of these patterns where the doing of it is very straightforward, but knowing when to use it
becomes much more complicated.
The simplest and straightforward cases for this are the clear simple Value Objects like money and date
range. Since Value Objects don't have identity you can create and destroy these easily without worrying
about such things as Identity Maps to keep them all in sync. Indeed all Value Objects should be
persisted as Embedded Value, since there you'd never want a table for them.
The grey line comes on whether it's worth storing reference objects using Embedded Value, such as an
order and shipping object. The principal question here is whether the shipping data has any relevance
outside the context of the order. One issue is the loading and saving. If you only load the shipping data
into memory when you load the order, then that's an argument for saving both into the same table.
Another question is whether you'll want to access the shipping data separately though SQL. This can be
an important issue if you are using reporting through SQL and aren't using a separate database for
reporting.
In most cases you'll only use Embedded Value on a reference object when the association between
them is single valued at both ends (a one-to-one association). Occasionally you may use it if there are
multiple candidate dependents if their number is small and fixed. Then you'll have numbered fields for
each value. This is messy table design, and is horrible to query in SQL, but it may have performance
benefits. Usually if this is the case, however, Serialized LOB is the better choice.
Since so much of the logic for deciding when to use Embedded Value is the same as for Serialized LOB
, there's the obvious matter of choosing between the two. The great advantage of Embedded Value is
that it allows SQL queries to be made against the values in the dependent object. Although using XML
as the serialization, together with XML based query add-ons to SQL may alter that in the future, at the
moment you really need to use Embedded Value if you want to use dependent values in a query. This
may be important for separate reporting mechanisms on the database
Embedded Value can only be used for fairly simple dependents. A solitary dependent, or a few
separated dependents works well. Serialized LOB however works with more complex structures,
including potentially large object sub-graphs.
Embedded Value has been called a couple of different names in its history. TOPLink refers to this
pattern as aggregate mapping. Visual Age refers to this as composer.
This is the classic example of a value object mapped with Embedded Value. We'll begin with a simple
product offering class with the following fields
In these fields the ID is an Identity Field, and the product is a regular record mapping. We'll map the
baseCost using Embedded Value. We'll do the overall mapping with Active Record to help keep things
simple.
Since we're using Active Record we need save and load routines. These routines are on the product
offering class, since it's the owner. The money class has no persistence behavior at all. These routines
are quite simple. Here is the load method.
Here's the update behavior. Again it's a simple variation on the updates.
##%%&&
An object that doesn't contain all of the data you need, but knows how to get it.
As you load data from a database into memory it's handy to design things so that as you load an object
of interest, you also load the objects that are related to that object. This makes loading easier on the
developer using the object, who otherwise has to explicitly load all the objects they need themselves.
However if you take this to its logical conclusion, you reach the point where loading one object can have
the effect of loading a huge amount of other related objects into the system, something that hurts
performance when only a few of the objects are actually needed.
A Lazy Load interrupts this loading process for the moment, leaving a marker in the object structure so
that if the data is needed it can be loaded only then. As many people know, if you're lazy about doing
things you'll win when it turns out you don't need to do them at all.
There are four main ways you can implement Lazy Load: lazy initialization, virtual proxy, value holder,
and ghost.
Lazy initialization[Beck-patterns] is the simplest one to do. The basic idea is that every access to the
field checks first to see if it's null. If it's null it calculates the value of the field before returning the field. To
make this work you have to ensure that the field is self-encapsulated; meaning that all access to the field,
even from within the class, is done through a getting method.
Using a null to signal a field that hasn't been loaded yet works well, unless null is a legal value for the
field. In this case you either need something else to signal the field hasn't been loaded, or to use a Special
Case for the null value.
Using lazy initialization is simple, but it does tend to force a dependency between the object concerned
and the database so it works best for Active Record, Table Data Gateway, and Row Data Gateway.
If you're using Data Mapper you'll need an additional layer of indirection. You can obtain this by using a
virtual proxy[Gang of Four]. A virtual proxy is an object that looks like the object that should be in the
field, but actually doesn't contain anything. Only when one of it's methods is called does it load the
correct object from the database.
The good thing about a virtual proxy is that looks exactly like the object that's supposed to be there.
However it isn't so you can easily run into a nasty identity problem. Often the virtual proxy is a different
object to the real object. Furthermore you can have more than one virtual proxy for the same real
object. All of these will have different object identities, yet they represent the same conceptual object. At
the very least you have to override the equality method and remember to use it instead of an identity
method. But without that, and discipline, you'll run into some very hard to track bugs.
In some environments the other problem with a virtual proxy is that you end up having to create lots of
them, one for each class you are proxying. You can usually avoid this in dynamically typed languages,
but with static type languages things usually get messy. Even when the platform provides handy facilities,
such as Java's proxies, they introduce other inconveniences.
These problem don't hit you if you only use virtual proxies for collections classes, such as lists. Since
collections are Value Objects , their identity doesn't matter. Additionally you only have a few collection
classes to write virtual collections for.
With domain classes you can avoid these problems by using a value holder. The value holder concept,
which I first came across in Smalltalk, is an object that wraps some other object. To get the underlying
object you ask the value holder for its value. Only on the first access does it pull the data from the
database. The disadvantages of the value holder is that the class needs to know there is a value holder
present, and you lose the explicitness of strong typing. You can avoid identity problems by ensuring that
the value holder is never passed out beyond its owning class.
A ghost is the real object, but not in its full state. When you load the object from the database it
contains just its ID. Whenever you try to access a field it loads its full state from the database. You can
think of a ghost as an object where every field is lazy initialized in one fell swoop, or as a virtual proxy
where the object is its own virtual proxy. Of course there's no need to load all the data in one go, you
may group the data into groups that are commonly used together. If you use a ghost you can put it
immediately in its Identity Map. By doing this not just will you maintain identity, you'll also avoid all
problems due to cyclic references when reading in data.
When you use a virtual proxy or a ghost the proxy/ghost doesn't need to be completely devoid of data.
If you have some data which is quick to get hold of and commonly used, it may make sense to load this
data when you load the proxy or ghost. (This is sometimes referred to as a "light object".)
Inheritance often poses a problem with Lazy Load. If you're going to use ghosts, you'll need to know
what type of ghost to create, which you often can't tell without loading the thing properly. Virtual proxies
can suffer from the same problem in static typed languages.
One danger with Lazy Load is that it can easily cause more database accesses then you need. A good
example of this ripple loading is if you fill a collection with Lazy Loads and then look at them one at a
time. This will cause you to go to the database once for each object, instead of reading them all in at
once. I've seen ripple loading cripple the performance of an application. One way to avoid ripple loading
is not to have a collection of Lazy Loads, rather make the collection itself a Lazy Load and when you
load it load all the contents. The limitation of this tactic is when the collection is very large, such as all the
IP addresses in the world. In practice these aren't usually linked up through associations in the object
model, so that doesn't happen very often. When it does you'll need a Value List Handler [missing
reference].
Lazy Load is a good candidate for aspect-oriented programming. You can put the Lazy Load behavior
into a separate aspect, which allows you change the lazy load strategy separately as well as freeing the
domain developers from having to deal with lazy load issues. I've also seen a project post-process Java
bytecode to implement Lazy Load in a transparent way.
Often you'll run into situations where different use cases work best with a different variety of laziness.
Some use cases need one subset of the object graph, others use a different subset. For maximum
efficiency you want to load the right sub-graph for the right use case.
The way to deal with this is to have separate database interaction objects for the different use cases. So
if you use Data Mapper you may have two order mapper objects, one that loads the line items
immediately, and one that loads the line items lazily. The application code chooses the appropriate
mapper depending on the use case. A variation on this is to have the same basic loader object, but defer
to a strategy object to decide the loading pattern. It's a bit more sophisticated but can be a better way to
factor the behavior.
In theory, you might want a range of different degrees of laziness, but in practice you really only need
two: a complete load and enough of a load for identification purposes in a list. Adding more usually adds
more complexity than is worth while.
Deciding when to use Lazy Load is all about deciding how much you want to pull back from the
database as you load an object, and how many database calls it will require. It's pointless to use Lazy
Load on a field that's stored in the same row as the rest of the object, because it doesn't cost any more
to bring back extra data in a call, even if the data field is quite large: such as a Serialized LOB. So it's
only worth considering Lazy Load if the field requires an extra database call to access.
In performance terms it's then about deciding when you want to take the hit of bringing back the data.
Often it's a good idea to bring everything you'll need in one call so you have it in place, particularly if it
corresponds to a single interaction with a UI. The best time to use Lazy Load is when not just does it
involve an extra call, it's also data that often isn't used when the main object is used.
Adding Lazy Load does add a little complexity to the program. So my preference is to not use Lazy
Load unless I actively think I'll need it.
The essence of lazy initialization is code like this.
In this way the first access of the products field causes the data to be loaded from the database.
The key to the virtual proxy is to provide a class that looks like the actual class you would normally use,
but actually holds a simple wrapper around the real class. So a list of products for a supplier would be
held with a regular list field.
The most complicated thing about producing a list proxy like this is setting it up so that you can provide
an underlying list that's only created when it's accessed. To do this we have to pass the code that's
needed to create the list into the virtual list when it's instantiated.
The best way to do this in Java is to define an interface for the loading behavior.
Then we can instantiate the virtual list with a loader that calls the appropriate mapper method.
During the load method we assign the product loader to the list field
The virtual list's source list is self encapsulated and evaluates the loader on first reference
I then implement the regular list methods to delegate to the source list.
This way the domain class knows nothing about how the mapper class does the Lazy Load. Indeed the
domain class isn't even aware that there is a Lazy Load.
A value holder can be used as a generic Lazy Load. In this case the domain type is aware that
something is afoot, since the product field is typed as a value holder. This fact can be hidden from clients
of the supplier by the getting method.
The value holder itself does the Lazy Load behavior. It needs to be passed the necessary code to load
it's value when it's accessed. We can do this by defining a loader interface.
A mapper can set up the value holder by creating an implementation of the loader and putting it into the
supplier object.
##%%&&
Represent an inheritance hierarchy of classes with one table per concrete class in the hierarchy.
Concrete Table Inheritance uses one database table for each concrete class in the hierarchy. Each
table contains columns for the concrete class and all its ancestors. As a result any fields in a superclass
are duplicated across the tables of the subclasses.
As with all of these inheritance schemes the basic behavior uses Inheritance Mappers.
You need to pay attention to the keys with this scheme. Punningingly the key thing to do is to ensure that
keys are not just unique to a table, but also unique to all the tables in a hierarchy. The classic case of
where you need this is if you have a collection of players and you are using Identity Field. If keys can
be duplicated between concrete tables you'll get multiple rows for a particular key value. You thus need
a key allocation system that keeps track of key usage across the tables, it also means you can't rely on
the database's primary key uniqueness mechanism.
This becomes particularly awkward if you are hooking up to databases that are used by other systems.
In many of these cases you can't guarantee key uniqueness across tables. In this situation you either have
to avoid using superclass fields or do a compound key that involves a table identifier.
An example of avoiding superclass fields is to use separate footballer, cricketer, and bowler collections
as fields instead of a player collection. You can still have a player collection in an interface, but you have
to concatenate the fields together to do this.
For compound keys you need a special key object to use as your id field for Identity Field. This key
would use both the primary key of the table and the table name to determine uniqueness.
Related to this is problems with using referential integrity in the database. Consider an object model like
Figure 1. To implement this in the database you need a link table that contains foreign key columns for
the charity function and the player. The problem is that there's no table for the player. As a result you
can't put together a referential integrity constraint for the foreign key field that takes either footballers or
cricketers. You either have to ignore the referential integrity, or use multiple link tables, one for each of
the actual tables in the database. On top of this there's also problems if you can't guarantee key
uniqueness.
If you are searching for players with a select statement, you need to look at all concrete tables to see
which ones contains the appropriate value. This leads to multiple queries to pull back a single row of
data. While this is easy to program, it can hurt performance. You don't suffer the performance hit when
you know the class you need, but you do have to prefer to use the concrete class to improve
performance.
The pattern is often referred to as something along the lines of leaf table inheritance. Some people
prefer a variation where you have one table per leaf class, instead of one table per concrete class. If you
don't have any concrete superclasses in the hierarchy this ends up as the same thing. Even if you do have
concrete superclasses the difference is pretty minor.
When figuring out how to map inheritance, Concrete Table Inheritance, Class Table Inheritance and
Single Table Inheritance are the alternatives.
The strengths of Concrete Table Inheritance are:
??Each table is self contained and doesn't have any irrelevant fields. As a result it makes good
sense when used by other applications that aren't using the objects.
??There's no joins to do when reading the data from the concrete mappers.
??Each table is only accessed when that class is accessed, which can spread the access load.
The weaknesses of Concrete Table Inheritance are
??Primary keys can be a pain to handle.
??Can't enforce database relationships to abstract classes
??If the fields on the domain classes are pushed up or down the hierarchy, you have to alter the
table definitions. You don't have to do as much alteration as with Class Table Inheritance but
you can't ignore this as you can with Single Table Inheritance.
??If a superclass field changes, you need to change each table that has this field, since the
superclass fields are duplicated across the tables.
??A find on the superclass forces you to check all the tables, which leads to multiple database
accesses (or a weird join.)
Remember that the trio of inheritance patterns can coexist in a single hierarchy. So you might use
Concrete Table Inheritance for one or two subclasses and Single Table Inheritance for the rest.
Here I'll show and implementation for the sketch. As with all the inheritance examples in this chapter, I'm
using the basic design of classes from Inheritance Mappers. Figure 2 shows the basic design.
Each mapper is linked to the database table that is the source of the data. In ADO.NET a data set holds
the data table.
The gateway class holds onto the data set with in its data property. The data can be loaded up by
supplying suitable queries.
Each concrete mapper needs to define what is the name of the table that holds its data.
The player mapper has fields for each concrete mapper.
Loading an object from the database
Each concrete mapper class has a find method that returns an object given a key value.
The abstract behavior on the superclass finds the right database row for the id, creates a new domain
object of the correct type, and uses the load method to load it up (I'll describe the load in a moment.)
The actual loading of data from the database is done by the load method, or rather by several load
methods - one for the mapper class and all its superclasses.
This logic is the logic for finding an object using a mapper for a concrete class. You can also use a
mapper for the superclass: the player mapper. It needs to find an object from whichever table it is living
in. Since all the data is already in memory in the data set, I can do it like this.
Remember this is reasonable only because the data is already in memory. If you need to go to the
database three times (or more for more subclasses) this will be slow. One way that may help is to do a
join across all the concrete tables. This allows you to access the data in one database call. However
large joins are often slow in their own right, you'll need to do some benchmarks with your own
application to find out what works and what doesn't. Also this will be an outer join, and as well as slow
the syntax for this is non-portable and often cryptic.
Updating an object
The update method can be defined on the mapper superclass.
Similar to loading, we use a sequence of save methods for each mapper class.
The player mapper needs to find the correct concrete mapper to use and then delegate the update call.
Inserting an object
Insertion is a variation on updating, the extra behavior is creating the new row - this can be done on the
superclass.
Again the player class delegates to the appropriate mapper
Deleting an object
Deletion is very straightforward. Again we have a method defined on the superclass
And a delegating method on the player mapper.
##%%&&
Represent an inheritance hierarchy of classes as a single table which has fields for all the fields of
the various classes
In this inheritance mapping scheme we have one table that contains all the data for all the classes in the
inheritance hierarchy. Each class stores the data that's relevant for that class into one row of the table.
Any columns in the database that aren't relevant for the appropriate class are left empty.
The basic mapping behavior follows the general scheme of Inheritance Mappers.
When loading an object into memory you need to know which class to instantiate. To do this you have a
field in the table that indicates which class should be used. This can be the name of the class or a code
field. A code field needs to be interpreted by some code to map it to the relevant class. This code needs
to be extended when a class is added to the hierarchy. If you embed the class name into the table you
can just use it directly to instantiate an instance. The class name, however, will take up more space and
maybe less easy to process by those using the database table structure directly, as well as more closely
coupling the class structure to the database schema.
When loading data, you read the code first to figure out which subclass to instantiate. On saving the data
the code needs be written out by the superclass in the hierarchy.
Single Table Inheritance is one of the options for mapping the fields in an inheritance hierarchy to a
relational database. The alternatives are Class Table Inheritance and Concrete Table Inheritance
The strengths of Single Table Inheritance are:
??A single table to worry about on the database
??No joins in retrieving data
??Any refactoring that pushes fields up or down the hierarchy doesn't require you to change the
database.
The weaknesses of Single Table Inheritance are:
??Fields are sometimes relevant and sometimes not, which can be confusing to people using the
tables directly.
??Columns that are only used by some subclasses lead to wasted space in the database. The
degree of how much this is actually a problem depends on the specific data characteristics and
how well the database compresses empty columns. Oracle, for example, is very efficient about
trimming wasted space particularly if you keep your optional columns to the right hand side of
the database table. Each database will have it's own tricks for this.
??The single table may end up being too large with many indexes and frequent locking. This may
hurt performance. You can avoid this by having separate index tables that either list keys of rows
that have a certain property, or copy a subset of fields relevant to an index.
??You only have a single name space for fields, so you have to be sure that you don't want to use
the same name for different fields. You can do this by using compound names that use the name
of the class as a prefix or suffix
It's important to remember that you don't need to use one form of inheritance mapping for you whole
hierarchy. It's perfectly find to map half a dozen similar classes in a single table, but to use Concrete
Table Inheritance for a couple of classes that have a lot of specific data.
I've based this code example, like the other inheritance examples, on Inheritance Mappers
Each mapper needs to be linked to a data table in an ADO.NET data set. This link can be made
generically in the mapper superclass. The gateway's data property is a data set which can be loaded by
a query.
Since there is only one table, this can be defined by the abstract player mapper.
Each class needs a type code to help the mapper code figure out what kind of player it has to deal with.
The type code is defined on the superclass and implemented in the subclasses.
The player mapper has fields for each of the three concrete mapper classes
Loading an object from the database
Each concrete mapper class has a find method to get an object from the data.
I load the data into the new object with a series of load methods, one on each class in the hierarchy.
I can also load a player through the player mapper. It needs to read the data and use the type code to
determine which concrete mapper to use.
Updating an object
The basic operation for updating an object is the same for all objects, so I can define the operation on
the mapper superclass.
The save method is similar to the load, each class defines it to save the data in that class.
The player mapper forwards to the appropriate concrete mapper
Inserting an object
Insertions are similar to updates, the only real difference is that a new row needs to be made in the table
before saving
Deleting an object
Deletes are pretty simple, defined at the abstract mapper level or in the player wrapper.
##%%&&
Represent an inheritance hierarchy of classes with one table for each class
The straightforward thing about Class Table Inheritance is that it has one table per class in the domain
model. The fields in the domain class map directly to fields in the corresponding tables.
As with the other inheritance mappings the fundamental approach of Inheritance Mappers applies.
One issue is how to link together the corresponding rows of the database tables. One scheme is to use a
common primary value, so the row of key 101 in the footballers table and the row of key 101 in the
players table correspond to the same domain object. Since the superclass table has a row for each row
in the other tables, the primary keys are going to be unique across the tables if you use this scheme. An
alternative is to let each table have its own primary and use foreign keys into the superclass table to tie
the rows together.
The biggest implementation issue with Class Table Inheritance is how to bring the data back from
multiple tables in an efficient manner. Obviously making a call for each table isn't nice since you have
multiple calls to the database. You can avoid this by doing a join across the various component tables.
However joins for more than three or four tables tend to be slow due to the way databases do their
optimizations. You also need to often query the root table first to find what kind of object you need to
load and then do a second query to get the full collection of data. To do this well you need to store a
type code in the root table, without a type code you need to search each table to get the value.
Class Table Inheritance, Single Table Inheritance and Concrete Table Inheritance are the three
alternatives to consider for inheritance mapping.
The strengths of Class Table Inheritance are:
??All columns are relevant for every row so tables are easier to understand and don't waste space
??The relationship between the domain model and the database is very straightforward
The weaknesses of Class Table Inheritance are:
??To load an object you need to touch multiple tables, which means a join or multiple queries and
sewing in memory
??Any movement of fields up or down the hierarchy causes database changes
??The supertype tables may become a bottleneck as they have to be accessed frequently
??The high normalization may make it hard to understand for ad hoc queries
You don't have to choose just one inheritance mapping pattern for one class hierarchy. You could use
Class Table Inheritance for the classes at the top of the hierarchy and a bunch of Concrete Table
Inheritance for those lower down.
A number of IBM texts refer to this as Root-Leaf mapping [Brown et al]
Here's an implementation for the sketch. Again I'll follow the familiar (if perhaps a little tedious) theme of
players and the like using Inheritance Mappers
Each class needs to define the table that holds the data for the class and a type code for the class.
Unlike the other inheritance examples, this one does not have a overridden table name, since we have to
have the table name for this class even when the instance is an instance of the subclass.
Loading an object
If you've been reading the other mappings, you'll know the first step is the find method on the concrete
mappers.
The abstract find method looks for a row matching the key and, if successful, creates a domain object
and calls the load method on that object.
There is one load method for each class which loads the data defined by that class.
As with the other sample code, but more noticeably in this case, I'm relying on the fact that the
ADO.NET data set has brought the data from the database and cached it into memory. This allows me
to make several accesses to the table based data structure without a high performance cost. If you are
going directly to the database, you'll need to reduce that load. For this example you might do this by
creating a join across all the tables and manipulating that.
The player mapper determines which kind of player it needs to find, and then delegates the correct
concrete mapper.
Updating an object
The update method appears on the mapper superclass
It's implemented through a series of save methods, one for each class in the hierarchy.
The player mapper's update method overrides the general method to forward to the correct concrete
mapper.
Inserting an object
The method for inserting an object is declared on the mapper superclass. It has two stages, first creating
new database rows to hold the data, secondly using the save methods to update these blank rows with
the necessary data.
Each class inserts a row into its table
The player mapper delegates to the appropriate concrete mapper
Deleting an object
To delete an object, each class deletes a row from the corresponding table in the database.
The player mapper, again wimps out of all the hard work and just delegates to the concrete mapper.
##%%&&
A set of mappers to handle inheritance hierarchies
When you are mapping from an object-oriented inheritance hierarchy in memory to a relational database
you need to minimize the amount of code needed to save and load the data to database. You also want
to provide both abstract and concrete mapping behavior that allows you to save or load a superclass or
a subclass.
Although the details of this behavior varies with your inheritance mapping scheme (Single Table
Inheritance, Class Table Inheritance, Concrete Table Inheritance) the general structure works the
same for all of them.
You can organize the mappers using a hierarchy so that each domain class has a mapper that saves and
loads the data for that domain class. That way when the mapping changes, you have one point where
you can change the mapping. This approach works well for concrete mappers that know how to map
the concrete objects in the hierarchy. There are times, however, where you also need mappers for the
abstract classes in the hierarchy. These can be implemented with mappers that are actually outside of the
basic hierarchy but delegate to the appropriate concrete mappers.
To best understand how this works, I'll start with the concrete mappers. In the sketch the concrete
mappers are the mappers for footballer, cricketer, and bowler. The basic behavior of the mappers
include the find, insert, update, and delete operations.
The finder methods are declared on the concrete subclasses because they will return a concrete class, so
the find method on BowlerMapper should return a Bowler not an abstract class. Common OO
languages cannot let you change the declared subtype of a method, so it's not possible to inherit the find
operation and still declare a specific return type. You can, of course, return an abstract type but that
forces the user of the class to downcast - which is best to avoid. (A language with run time typing avoids
this problem.)
The basic behavior of the find method is to find the appropriate row in the database, instantiate an object
of the correct type (a decision that's made by the subclass) and then load the object with data from the
database. The load method is implemented by each mapper in the hierarchy and the mapper loads the
behavior for its corresponding domain object. So the bowler mapper's load method loads the data
specific to the bowler class, calls the superclass method to load the data specific to the cricketer, which
calls its superclass method, etc.
The insert and update methods operate in a similar way using a save method. Here you can define the
interface on the superclass, indeed on a Layer Supertype. The insert method creates a new row and
then saves the data from the domain object using the save hook methods. The update method just saves
the data using the save hook methods. The save hook methods operate similarly to the load hook
methods, with each class storing its specific data and calling the superclass save method.
This scheme makes it quite easy to write the appropriate mappers to save the specific information
needed for a particular part of the hierarchy. The next step is to support loading and saving an abstract
class, in this example a player. While a first thought is to put appropriate methods on the superclass
mapper, that actually gets awkward because while concrete mapper classes can just use the abstract
mapper's insert and update methods, the player mapper's insert and update needs to override these to
call a concrete mapper instead. The result is one of those combinations of generalization and
composition that twists your brain cells into an appalling knot.
So I prefer to separate them into two classes. The abstract player mapper is the one whose
responsibility is to load and save the specific player data to the database. This is a class that is abstract
and whose behavior is just used by the concrete mapper objects. A separate player mapper class is
used for its interface. It provides a find method and overrides the insert and update methods. For all of
these its responsibility is to figure out which concrete mapper should handle the task and delegate to it.
Although this broad scheme makes sense for each type of inheritance mapping, the details do vary with
the exact mapping scheme, so it's not possible to show a code example for this case. You can find good
examples in each of the inheritance mapping patterns: Single Table Inheritance, Class Table
Inheritance, and Concrete Table Inheritance.
This general scheme makes sense for any inheritance based database mapping. The alternatives involve
such things as duplicating superclass mapping code amongst the concrete mappers or folding the player's
interface into the abstract player mapper class. The former is a heinous crime and latter is possible but
leads to a messy and confusing player mapper class. So on the whole its hard to think of a good
alternative to this pattern.
##%%&&
An object that represents a database query
SQL can be an involved language, and many developers are not particularly familiar with it. Furthermore
to form queries, you need to know what the database schema looks like. You can avoid this by creating
specialized finder methods that hide the SQL inside parameterized methods, but that makes it difficult to
form more ad hoc queries. It also leads to duplication in the SQL statements, should the database
schema change.
A Query Object is an interpreter that is a structure of objects, but is able to form itself into a SQL
query. You can create this query by referring to classes and their fields, rather than tables and columns.
That way those who write the queries can do so independently of the database schema and changes to
the schema can localized into a single place.
A Query Object is an application of the interpreter geared to represent a SQL query. The primary roles
of the Query Object is to allow a client to form queries of various kinds, and then to turn that object
structure into the appropriate SQL string.
In order to represent any query, you need quite a flexible Query Object. Often, however, applications
can make do with a lot less than the full power of SQL, in which case you build a simpler Query Object.
While it won't be able to represent anything, it can satisfy your particular needs and if you need to
enhance it, it's usually no more work to enhance when you need more capability than it is to create a fully
capable Query Object right from the beginning. As a result you should only create a minimally functional
Query Object for your current needs, and evolve it as your needs grow.
A common feature of Query Object is that they can represent queries in the language of the in-memory
objects rather than the database schema. So instead of using table and column names, you can use
object and field names. While this isn't important if your objects and database have the same structure, it
can be very useful if you get variations between the two. In order to perform this change of view, the
Query Object needs to know how the database structure maps to the object structure, so this capability
really needs Metadata Mapping.
If you have multiple databases to work with, you can design your Query Object so that it can produce
different SQL depending on which database the query is running against. At it's simplest level, this can
take into account the annoying differences in SQL syntax that keep cropping up. At a more ambitious
level this can use different mappings to cope with the same classes being stored in different database
schemas.
A particularly sophisticated use of Query Object is using it to eliminate redundant queries against a
database. At its simplest level you can look to see if you have run the same query earlier on in the
session. If so then you can use the query to select objects from the Identity Map and avoid a trip to the
database. A more sophisticated approach can detect whether one query is a particular case of an earlier
query, such as a query that is the same as an earlier one but with an additional clause linked with an
AND.
Exactly how to do these more sophisticated features is beyond the scope of this book. But these are the
kind of features that OR mapping tools may provide.
A variation on the Query Object that Cocobase does, is to allow a query to be specified by an example
domain object. So you might have a Person object whose last name is set to "Fowler" but all other
attributes are set to null. You can then treat this domain object as a query by example that's processed in
a similar way to the interpreter style Query Object, that would return all people in the database whose
last name is "Fowler".
Query Objects are a pretty sophisticated pattern to put together, so most projects don't use them if they
have a hand built data source layer. You only really need them when you are using Domain Model and
Data Mapper, you also really need Metadata Mapping to make real use of them.
Even then they aren't always necessary. Many developers are comfortable with SQL. You can hide
much of the details of the database schema behind specific finder methods.
The advantages of Query Object come with more sophisticated needs: keeping database schema
encapsulated, supporting multiple databases, supporting multiple schemas, and optimizing to avoid
multiple queries. Although some projects with a particularly sophisticated data source team might want
to build these themselves, most people that use Query Object do so with a commercial tool. My
inclination is that if you want these capabilities, you almost always better off buying a tool.
Having said all that, you may find that limited Query Object may fulfill your needs and not be that
difficult to build on a project that wouldn't justify a fully featured Query Object. The trick to this is paring
down the functionality of the Query Object and being determined to not build more than you actually
use.
This is a simple example of a Query Object, rather less than would be useful for most situations, but
enough to give you and idea of what a Query Object is about.
This Query Object is able to query a single table based on set of criteria that and'ed together, (or in
slightly more technical language it handles a conjunction of elementary predicates.)
The Query Object is set up using the language of domain objects rather than that of the table structure.
So a query knows the class the query is for and a collection of criteria that'll correspond to the clauses of
a where clause.
A simple form of criteria is one that takes a field, a value, and a SQL operator to compare them.
To make it easier to create the right criteria, I can provide an appropriate creation method.
This allows me to find everyone with dependents by forming a query such as
I can form a query that asks for all people with dependents by creating a query for person and adding a
criteria
That's enough for to describe the query, the query then needs to execute by turning itself into a SQL
select. In this case I assume my mapper class supports a method that will find objects based on a string
which is a where clause.
Here I'm using a Unit of Work that holds mappers indexed by the class and a mapper that uses
Metadata Mapping
To generate the where clause, the query iterates through the criteria and gets each one to print itself out
and tied them together with ANDs
As well as criteria with simple SQL operators, we can create more complex criteria classes that do a
little more. So consider a case insensitive pattern match query, such as finding all people whose last
names start with 'f'. We can form a query object for all people with dependents whose name starts with
'f'.
This uses a different criteria class that forms a more complex clause into the where statement.
##%%&&
Maintains a list of objects that are affected by a business transaction and coordinates the writing
out of changes and resolution of concurrency problems.
When you're pulling data in and out of a database, it's important to keep track of what you've changed,
otherwise you won't get stuff written back into the database. Similarly you have to insert new objects
you create and remove any object you've deleted.
You could change the database with each change to your object model, but this can lead to lots of very
small calls to the database, which ends up being very slow. Furthermore it requires you to have a
transaction open for the whole interaction - which is impractical if you have a business transaction that
spans multiple requests. This situation is even worse if you need to keep track of objects you've read so
you can avoid inconsistent reads.
A Unit of Work keeps track of everything you do during a business transaction that can have
ramifications to the database. When you are done, it figures out all everything that needs to be done to
alter the database as a result of all the work.
The obvious things that cause you to deal with the database are changes: new object created and
existing ones updated or deleted. Unit of Work is an object that keeps track of these things. As soon as
you start doing something that may affect a database, you create a Unit of Work to keep track of the
changes. Every time you create, change, or delete an object you tell the Unit of Work. You can also let
the Unit of Work know about objects you've read. This way the Unit of Work can check for
inconsistent reads by verifying that none of the objects changed on the database during the business
transaction.
The key thing about Unit of Work is that when it comes time to commit, the Unit of Work decides
what to do. It opens a transaction, does any concurrency checking (using Pessimistic Offline Lock or
Optimistic Offline Lock and writes changes out to the database. Application programmers never
explicitly call methods to update the database. This way they don't have to keep track of what's
changed, nor do they need to worry about how referential integrity affects the order in which they need
to do things.
Of course for this to work the Unit of Work needs to know what objects it should keep track of. You
can do this either by the caller doing it or by getting the object to tell the Unit of Work.
With caller registration the user of an object needs to remember to register the object with the Unit of
Work for changes. Any objects that aren't registered won't get written out on commit. Although this
allows forgetfulness to cause trouble, it does give flexibility in allowing people to make in-memory
changes that they don't want written out - although we would argue that is going to cause far more
confusion that would be worthwhile. It's better to make an explicit copy for that purpose.
With object registration the onus is removed from the caller. The usual trick here is to place
registration methods in object methods. Loading an object from the database registers the object as a
clean object, the setting methods cause the object to be registered as dirty. For this scheme to work the
Unit of Work needs to either be passed to the object or to be in a well known place. Passing the Unit
of Work around is tedious, but it's usually no problem to have the Unit of Work present in some kind of
session object.
Even object registration leaves something to remember, that is the developer of the object needs to
remember to add a registration call in the right places. The consistency becomes habitual, but is still an
awkward bug when it's missed.
This is a natural place for code generation to generate appropriate calls, but that only works when you
can clearly separate generated and non-generated code. This turns out to be particularly suited to
aspect-oriented programming. I've also come across post-processing of the object files to pull this off. In
this example a post-processor examined all the Java .class files, looked for the appropriate methods and
inserted registration calls into the byte code. This kind of finicking around feels dirty, but it separates the
database code from the regular code. Aspect-oriented approach will do this more cleanly with source
code, and as aspect oriented programming tools become more commonplace I would expect to see this
strategy being used.
Another technique I've seen is unit of work controller, which the TOPLink product uses. Here the Unit
of Work handles all reads from the database, and registers clean objects whenever they are read. Rather
than marking objects as dirty the Unit of Work takes a copy at read time, and then compares the object
at commit time. Although this adds overhead to the commit process, it allows a selective update of only
those fields that were actually changed, as well as avoiding registration calls in the domain objects. A
hybrid approach is to take copies only of objects that are changed, while this requires registration, it
supports selective update and greatly reduces the overhead of the copy if there many more reads than
updates.
Creating an object is often a special time to consider caller registration. It's not uncommon for people to
want to create objects that are only supposed to transient. A good example of this is in testing domain
objects where the tests will run much faster without database writes. Caller registration can make this
apparent. However there are other solutions, such as providing a transient constructor that doesn't
register with the unit or work, or better still providing a Special Case Unit of Work that does nothing
with a commit.
Another area where a Unit of Work can be helpful is in update order when a database uses referential
integrity. Most of the time you can avoid this issue by ensuring that the database only checks referential
integrity when the transaction commits, rather than with each SQL call. Most databases allow it, and if
available there's no good reason not to do it. If you can't then the Unit of Work is the natural place to
sort out the update order. In smaller systems this can be done with explicit code that contains details
about which tables to write first based on the foreign key dependencies. In a larger application it's better
to use metadata to figure out which order to write to the database. How you do that is beyond the scope
of this book, and a common reason to use a commercial tool. If you have to do it yourself I'm told the
key to the puzzle is a topological sort.
You can use a similar technique to minimize deadlocks. If every transaction uses the same sequence of
tables to edit, then you greatly reduce the chances of getting deadlocks. The Unit of Work is an ideal
place to hold a fixed sequence of table writes so that you always touch the tables in the same order.
Objects need to be able to find their current Unit of Work. A good way to do this is to use a thread
scoped Registry. Another approach is to pass the Unit of Work around to objects that need it, either in
method calls or by passing in the Unit of Work when you create an object. In either case make sure you
can't get more than one thread getting access to a Unit of Work, there lies the way to madness.
Unit of Work makes an obvious point to handle batch updates. The idea behind a batch update is to
send multiple SQL commands as a single unit so that they can be processed in a single remote call. This
is particularly important for updates where many updates, inserts, and deletes are often sent in rapid
succession. Different environments provide different levels of support for batch updates. JDBC has a
batch update facility which allows you to add individual statements to another statement to batch them
up. If you don't have this you can mimic this capability by building up a string that has multiple SQL
statements and then submitting that string as one statement, [missing reference] describes an example of
this for the Microsoft platforms. However if you do this, check to see if it interferes with statement
pre-compilation.
Unit of Work works with any transactional resource, not just databases, so you can also use it to
coordinate with message queues and transaction monitors.
The Unit of Work in .NET is done by the disconnected data set, which is a slightly different Unit of
Work to the classical variety. Most Unit of Work I've come across register and track changes to
objects. .NET reads data from the database into a data set, which is a series of objects arranged like the
tables, rows and columns of the database. The data set is essentially an in-memory mirror image of the
result of one or SQL queries. Each DataRow has the concept of a version (current, original, proposed)
and a state (unchanged, added, delete, modified). This information, together with the fact that the data
set mimics the database structure, makes it straightforward to write changes out to the database.
The fundamental problem that Unit of Work deals with is keeping track of the various objects that
you've manipulated, so that you know which objects you need to consider to synchronize your
in-memory data with the database. If you are able to do all your work within a system transaction, then
the only objects you need to worry about are those you alter. Although Unit of Work is generally the
best way of doing this, there are alternatives.
Perhaps the simplest alternative is to explicitly save any object whenever you alter it. The problem here
is that you may get many more database calls than you would like, since if you alter one object in three
different points in your work, then you get three calls rather than one call in its final state.
To avoid multiple database calls, you can leave all your updates to the end. To do this you need to keep
track of all the objects that have changed. You can do this with variables in your code, but this soon
becomes unmanageable once you have more than a few. Such variables often can work fine with a
Transaction Script, but is very difficult with a Domain Model.
Rather than keep objects in variables you can give each object a dirty flag which you set when the
object changes. Then you need to find all the dirty objects at the end of your transaction and write them
out. The value of this technique hinges on how easy it is to find them. If all your objects are in a single
hierarchy, then you can traverse the hierarchy and write out any object that's been changed. But a more
general object network, such as a Domain Model, is harder to traverse.
The great strength of Unit of Work is that it keeps all this information in one place. Once you have Unit
of Work working for you, you really don't have to remember to do much in order to keep track of your
changes. Unit of Work also is a firm platform for more complicated situations, such as handling business
transactions that span several system transactions using Optimistic Offline Lock and Pessimistic
Offline Lock.
Here's a unit of work that can track all changes for a given business transaction and then commit them to
the database when instructed to do so. Our domain layer has a Layer Supertype, DomainObject, with
which the unit of work will interact. To store the change set we will use three lists to store new, dirty,
and removed domain objects.
The registration methods maintain the state of these lists. These methods must perform basic assertions
such as checking that an id is not null or that a dirty object is not being registered as new.
Notice that registerClean() doesn't do anything here. A common practice is to place an Identity Map
within a unit of work. An Identity Map is necessary most anytime you are storing domain object state in
memory, such as in this unit of work, as multiple copies of the same object will result in undefined
behavior. Were an Identity Map in place registerClean() would put the registered object into the map.
Likewise registerNew() would put a new object in the map and registerRemoved() would remove a
deleted object from the map. Without the Identity Map you have the option of not including
registerClean() in your Unit of Work. I've seen implementations of registerClean() that remove changed
objects from the dirty list but partially rolling back changes is always tricky. Be careful when reversing
any state in the change set.
commit() will locate the Data Mapper for each object and invoke the appropriate mapping method.
updateDirty() and deleteRemoved() are not shown but they would behave in similar fashion to
insertNew(), exactly as would expect.
Not included in this Unit of Work is tracking of any objects we've read and would like to check for
inconsistent read errors upon commit. This is addressed in Optimistic Offline Lock
Next, we need to facilitate object registration. First, each domain object needs to find the Unit of Work
serving the current business transaction. Since the Unit of Work will be needed by the entire domain
model passing it around as a parameter is probably unreasonable. As each business transaction executes
within a single thread we can associate the Unit of Work with the currently executing thread using the
java.lang.ThreadLocal class. Keeping things simple we'll add this functionality by using static methods on
our Unit of Work class. If we already had some sort of session object associated to the business
transaction execution thread we should place the current Unit of Work on that session object rather
than add the management overhead of another thread mapping. Besides, the Unit of Work would
logically belong to the session anyway.
We can now provide our abstract domain object marking methods to register itself with the current Unit
of Work:
Concrete domain objects then need to remember to mark themselves new and dirty where appropriate.
Not shown is that registration of removed objects can be handled by a remove() method on the abstract
domain object and if you've implemented registerClean() your Data Mapper will need to register any
newly loaded object as clean.
The final piece is to register and commit the Unit of Work where appropriate. This can be done in either
explicit or implicit fashion. Here's what explicit Unit of Work management looks like:
Beyond the simplest of applications implicit Unit of Work management is more appropriate to avoid
repetitive, tedious coding. Here's a servlet Layer Supertype that registers and commits the Unit of Work
for its concrete subtypes. Subtypes will implement handleGet() rather than override doGet(). Any code
executing within handleGet() will have a Unit of Work with which to work.
The above servlet example is obviously a bit simplistic in that system transaction control is skipped. And
if you were using Front Controller you would be more likely to wrap Unit of Work management
around your commands rather than doGet(). Similar wrapping approaches can be taken with just about
any execution context.
##%%&&
Prevent conflicts between concurrent business transactions, by detecting a conflict and rolling
back the transaction.
Often a business transaction executes across a series of system transactions. Once outside the confines
of a single system transaction, we cannot depend upon our database manager alone to ensure that the
business transaction will leave the record data in a consistent state. Data integrity is at risk once two
sessions begin to work on the same records. With two sessions editing the same data lost updates are
quite possible. With one session editing data that another is reading an inconsistent read becomes likely.
Optimistic Offline Lock solves this problem by validating that the changes about to be committed by
one session don't conflict with the changes of another session. A successful pre-commit validation is, in a
sense, obtaining a lock indicating it's OK to go ahead with the changes to the record data. So long as the
validation and the updates to the record data occur within a single system transaction the business
transaction will display consistency.
Whereas Pessimistic Offline Lock assumes that the chance of session conflict is high and therefore
limits the system's concurrency, Optimistic Offline Lock assumes that the chance of conflict is rather
low. The expectation that session conflict isn't likely allows multiple users to work with the same data at
the same time.
An Optimistic Offline Lock is obtained by validating that in the time since a session loaded a record
another session has not altered that record. An Optimistic Offline Lock can be acquired at any time but
is valid only during the system transaction in which it is obtained. So in order that a business transaction
not corrupt record data it must acquire an Optimistic Offline Lock for each member of its change set
during the system transaction in which it applies changes to the database.
The most common implementation is to associate a version number to each record in your system. When
a record is loaded that version number is maintained by the session along with all other session state.
Getting the Optimistic Offline Lock is a matter of comparing the version stored in your session data to
the current version in the record data. Once the verification succeeds, all changes, including an increment
of the version, can be committed. The version increment is what prevents inconsistent record data. A
session with an old version cannot acquire the lock.
With an RDBMS data store the verification is a matter of adding the version number to the criteria of
any SQL statements used to update or delete a record. A single SQL statement can both acquire the
lock and update the record data. The final step is for the business transaction to inspect the row count
returned by the SQL execution. A row count of 1 indicates success. A row count of 0 indicates that the
record has been changed or deleted. With a row count of 0 the business transaction must rollback the
system transaction to prevent any changes from entering the record data. At this point the business
transaction must either abort or attempt to resolve the conflict and retry.
In addition to a version number for each record, storing information as to who last modified a record and
when can be quite useful when managing concurrency conflicts. When informing a user of a failed update
due to a concurrency violation a proper application will provide a message as to who altered the record
in question and when. Note that it is a bad idea to use the modification timestamp rather than a version
count for your optimistic checks. System clocks are simply too unreliable. Even more so if you are
coordinating across multiple servers.
An alternative implementation that sometimes comes up is for the where clause in the update to include
every field in the row. The advantage of this is that you can use this without using some form of version
field, which can be handy if you can't alter the database tables to add a version field. The problem is that
this complicates the update statement with a potentially rather large where clause. This may also be a
performance impact, although this depends on how clever the database is about using the primary key
index.
Often, implementation of Optimistic Offline Lock is left at including the version in UPDATE and
DELETE statements. This fails to address the problem of an inconsistent read. Suppose a billing system
that creates charges and calculates appropriate sales tax. A session creates the charge and then looks up
the customer's address to calculate the tax on that charge. But while the charge generation session is in
progress a separate customer maintenance session edits the customer's address. As local tax rates are
dependent upon location the tax rate calculated by the charge generation session might be invalid. But
since the charge generation session did not make any changes to the address the conflict will not be
detected.
There is no reason why Optimistic Offline Lock cannot be used to detect an inconsistent read. In the
example above the charge generation session needs to recognize that its correctness is dependent upon
the value of the customer's address. The session should perform a version check on the address as well.
This could be done by adding the address to your change set or maintaining a separate list of items on
which to perform a version check. The later requires a bit more work to setup but results in code that
more clearly states its intent. If you are checking for a consistent read simply by re-reading the version
rather than an artificial update be especially aware of your system transaction isolation level. The version
re-read will only work with repeatable read or stronger isolation. Anything weaker requires an increment
of the version.
A version check might be overkill for certain inconsistent read problems. Often a transaction will be
dependent only upon the presence of a record or maybe the value of only one of its fields. In such a case
it may improve your system's liveness to check these types of conditions rather than the version, as
fewer concurrent updates will result in the failure of competing business transactions. The better you
understand your concurrency issues the better you can manage them in your code.
The Coarse-Grained Lock can help with certain inconsistent read conundrums by treating a group of
objects as a single lockable item. A final option is to simply execute all of the steps of the problematic
business within a long running transaction. The ease of implementation might prove worth the resource hit
of using a few long transactions here and there.
Where detection of an inconsistent read gets a bit difficult is when your transaction is dependent upon
the results of a dynamic query rather than the reading of specific records. Its possible that you could
save the initial results and compare them to the results of the same query at commit time as a means of
obtaining an Optimistic Offline Lock.
As with all locking schemes Optimistic Offline Lock by itself does not provide adequate solutions for
some of the trickier concurrency and temporal issues in a business application. We cannot stress enough
that in a business application concurrency management is as much a domain issue as it is a technical one.
Is the customer address scenario above really a conflict? It might be OK that I calculated the sales tax
with an older version of the customer. Which version should I actually be using? This is a business issue.
Or consider a collection. What if two sessions simultaneously add items to a collection. The typical
Optimistic Offline Lock scheme will not prevent this. But this might very well be a violation of business
rules.
A system using Optimistic Offline Lock that we all should be familiar with is a source code
management system. When an SCM system detects a conflict between programmers it usually can figure
out the correct merge and retry the commit. A quality merge strategy makes the use of Optimistic
Offline Lock very powerful. Not only is the system's concurrency quite high but users rarely have to
redo any work. Of course, the big difference between an SCM system and an enterprise business
application is that an SCM system must implement only one type of merge while an enterprise business
system might have hundreds of merges to implement. Some might be of such complexity that they are not
worth the cost of coding. Others might be of such value to the business that the merge should by all
means be coded. Despite rarely being done the merging of business objects is quite possible. Merging
business data is a pattern unto its own so I will leave it at that rather than butcher the topic, but do
understand the power that it adds to using the Optimistic Offline Lock.
Optimistic Offline Lock only lets us know if a business transaction will commit during the last system
transaction but it's occasionally useful to know earlier on if a conflict has occurred. To do this you can
provide a checkCurrent method that looks to see if anyone else has updated the data. Such a check is
not a guarantee that you won't get a conflict, but it may be worthwhile to stop a complicated process if
you can tell in advance that it certainly won't commit
Optimistic concurrency management is appropriate when the chance of conflict between any two
business transactions in a system is low. If conflicts are likely it is not user-friendly to announce a conflict
only when the user has finished his work and is ready to commit. Eventually the user will assume the
failure of business transactions and stop using the system. Pessimistic Offline Lock is more appropriate
when the chance of conflict is high or the expense of a conflict is unacceptable.
As optimistic locking is much easier to implement and not prone to the same defects and runtime errors
as a Pessimistic Offline Lock, consider using it as the default approach to business transaction conflict
management in any system you build. Where needed, Pessimistic Offline Lock works well as a
complement to Optimistic Offline Lock. So rather than asking when to use an optimistic approach to
conflict avoidance, ask when is the optimistic approach alone is not good enough. The correct approach
to concurrency management will maximize concurrent access to data while minimizing the number of
conflicts.
##%%&&
Prevent conflicts between concurrent business transactions by allowing only one business
transaction to access data at once.
Offline concurrency involves manipulating data for a business transaction that spans multiple requests.
The simplest approach to use is to have a system transaction open for the whole business transaction.
Sadly this often does not work well because transaction systems are not geared to work with these long
transactions. When you can't use long transactions you have to use multiple system transactions. At that
point you're left to your own devices to manage concurrent access to your data.
The first approach to try is Optimistic Offline Lock. However Optimistic Offline Lock has its
problems. If several people access the same data within a business transaction, then one of them will
commit with no problems but the others will conflict and fail. Since the conflict is only detected at the end
of the business transaction, the victims will do all the work of the transaction only to find the whole thing
fails - wasting their time. If this happens a lot on lengthy business transactions the system can soon
become very unpopular.
Pessimistic Offline Lock prevents conflicts by attempting to avoid them altogether. Pessimistic Offline
Lock forces a business transaction to acquire a lock on a piece of data before it starts to use that data.
This way most of the time, once you begin a business transaction you can be pretty sure you'll complete
it without being bounced by concurrency control.
Implementing Pessimistic Offline Lock involves three phases: determining what type of locks you need,
building a lock manager, and defining procedures for a business transaction to use locks. Additionally, if
using Pessimistic Offline Lock as a complement to Optimistic Offline Lock it becomes necessary to
determine which record types to lock.
Looking at lock types, the first option is to have an exclusive write lock. That is, require only that a
business transaction acquire a lock in order to edit session data. This avoids conflict by not allowing two
business transactions to simultaneously make changes to the same record. What this locking scheme
ignores is the reading of data. If it is not critical that a view session have the absolute most recent data
this strategy will suffice.
If it becomes critical that a business transaction always have the most recent data regardless of its
intention to edit then use the exclusive read lock. Using an exclusive read lock requires that a business
transaction acquire a lock to simply load the record. This strategy clearly has the potential of severely
restricting the concurrency of your system. For most enterprise systems the exclusive write lock will
afford much more concurrent record access than this strategy.
There is a third strategy combining more specialized forms of read and write locks that will provide the
more restrictive locking of the exclusive read lock while also providing the increased concurrency of the
exclusive write lock. This third strategy, the read/write lock, is a bit more complicated than the first
two. The relationship of the read and write locks is the key to getting the best of both worlds:
??Read and write locks are mutually exclusive. A record cannot be write locked if any other
business transaction owns a read lock on that record. A record cannot be read locked if any
other business transaction owns a write lock on that record.
??Concurrent read locks are acceptable. The existence of a single read lock prevents any business
transaction from editing the record, so there is no harm in allowing any number of sessions as
readers once one is allowed.
Allowing multiple read locks is what increases the concurrency of the system. The downside of this
scheme is that it's a bit nasty to implement and presents more of a challenge for domain experts to wrap
their heads around when they are modeling the system.
In choosing the correct lock type you are looking to maximize system concurrency, meet business needs,
and minimize code complexity. In addition, the locking strategy must be understood by the domain
modelers and analysts. Locking is not just a technical problem as the wrong lock type, simply locking
every record, or locking the wrong types of records can result an ineffective Pessimistic Offline Lock
strategy. An ineffective Pessimistic Offline Lock strategy is one that doesn't prevent conflict at the
onset of the business transaction or degrades the concurrency of your system such that your multi-user
system seems more like single-user system. The wrong locking strategy cannot be saved by a proper
technical implementation. In fact, it's not a bad idea to include Pessimistic Offline Lock in your domain
model.
Once you have decided upon your lock type define your lock manager. The lock manager's job is to
grant or deny any request by a business transaction to acquire or release a lock. For a lock manager to
do its job it needs to know about what's being locked as well as the intended owner of the lock, the
business transaction. It's quite possible that your concept of a business transaction isn't something that
can be uniquely identified. This makes it a bit difficult to pass a business transaction to the lock manager.
In this case take a look at your concept of a session as you are more likely to have a session object at
your disposal. So long as business transactions execute serially within a session the session will serve as
a fine Pessimistic Offline Lock owner. The terms session and business transaction are fairly
interchangeable. The code example should shed some light on the idea of a session as lock owner.
The lock manager shouldn't consist of much more than a table that maps locks to owners. A simple lock
manager might wrap an in-memory hash table. Another option is to use a database table to store your
locks. You must have one and only one lock table, so if it's in memory be sure to use a singleton lock
manager. If your application server is clustered an in-memory lock table will not work unless it is pinned
to a single server instance. The database-based lock manager is probably more appropriate once you
are in a clustered application server environment.
The lock, whether implemented as an object or as SQL against a database table, should remain private
to the lock manager. Business transactions should interact only with the lock manager, never with a lock
object.
It is now time to define the protocol by which a business transaction must use the lock manager. This
protocol must include what to lock, when to lock, when to release a lock, and how to act when a lock
cannot be acquired.
The answer to what depends upon when so let's look first at when. Generally, the business transaction
should acquire a lock before loading the data. There is not much point in acquiring a lock without a
guarantee that you will have the latest version of the item once it's locked. But, since we are acquiring
locks within a system transaction there are circumstances where the order of the lock and load won't
matter. Depending upon your lock type if you are using serializable or repeatable read transactions the
order in which you load objects and acquire locks might not matter. Another option might be to perform
an optimistic check on an item after you acquire the Pessimistic Offline Lock. The rule is that you
should be very sure that you have the latest version of an object after you have locked it. This usually
translates to acquiring the lock before loading the data.
So, what are we locking? While we are locking objects or records or just about anything, what we
usually lock is the id, or primary key, that we use to find those objects. This allows us to obtain the lock
before loading the object. Locking the object works fine so long as this doesn't force you to break the
rule that an object must be current after you acquire its lock.
The simplest rule for releasing locks is to release them when the business transaction completes.
Releasing a lock prior to completion of a business transaction might be allowable, dependent upon your
lock type and your intention to use that object again within the transaction. Unless you have a very
specific reason to release the lock, such as a particularly nasty system liveness issue, stick to releasing all
locks upon completion of the business transaction.
The easiest course of action for a business transaction when it is unable to acquire a lock is to abort. The
user should find this acceptable since using Pessimistic Offline Lock should result in failure happening
rather early in the transaction. The developer and designer can certainly help the situation by not waiting
until late in the transaction before acquiring a particularly contentious lock. If at all possible acquire all of
your locks before the user begins work.
For any given item that you intend to lock access to the lock table must by serialized. With an
in-memory lock table it's easiest to simply serialize access to the entire lock manager with whatever
constructs your programming language provides. If you need concurrency greater than this affords be
aware you are entering complex territory.
If the lock table is stored in a database the first rule is, of course, interact with the lock table within a
system transaction. Take full advantage of the serialization capabilities that a database provides. With the
exclusive read and exclusive write lock types serialization is simply a matter of having the database
enforce a uniqueness constraint on the column storing the lockable item's id. Storing read/write locks in a
database makes things a bit more difficult since read/write lock logic requires reads of the lock table in
addition to inserts. Thus, it becomes imperative to avoid inconsistent reads. A system transaction with
isolation level of serializable will provide ultimate safety as we are guaranteed not to have inconsistent
reads. But using serializable transactions throughout our system might get us into performance trouble.
Using a separate serializable system transaction for lock acquisition and a less strict isolation level for
other work might ease the performance problem. Still another option is to investigate whether a stored
procedure might help with lock management. Concurrency management can be tough so don't be afraid
to defer to your database at key moments.
The serial nature of lock management screams performance bottleneck. A large consideration is lock
granularity. The fewer locks required the less of a bottleneck you will have. A Coarse-Grained Lock
can address lock table contention.
When using a system transaction pessimistic locking scheme, such as 'SELECT FOR UPDATE...' or
entity EJBs, deadlock is a distinct possibility. Deadlock is a possibility because these locking
mechanisms will wait until a lock becomes available. Suppose two users need resources A and B. If one
gets the lock on A and the other on B both transactions might sit and wait forever for the other lock.
Given that we're spanning multiple system transactions waiting for a lock does not make much sense. A
business transaction might take 20 minutes. Nobody wants to wait for those locks. This is good. Coding
for a wait involves timeouts and quickly gets complicated. So simply have your lock manager throw an
exception as soon as a lock is unavailable. This removes the burden of coping with deadlock.
A final requirement is managing lock timeouts for lost sessions. If a client machine crashes in the middle
of a transaction the lost business transaction is unable to complete and release any owned locks. This is
an even bigger deal for a web application where sessions are regularly abandoned by users. Ideally you
will have a mechanism managed by your application server rather than your application available for
handling timeouts. Web application servers provide an HTTP session that can be utilized for timeout
purposes. Timeouts can be implemented by registering a utility object that releases all locks for the
session when the HTTP session becomes invalid. Another option would be to associate a timestamp
with each lock and consider any lock older than a certain age invalid.
Pessimistic Offline Lock is appropriate when the chance of conflict between concurrent sessions is
high. A user should never have to throw away work. Locking is also quite appropriate when the cost of
a concurrency conflict is too high, regardless of the likelihood of that conflict. Locking every entity in a
system will almost surely create tremendous data contention problems so remember that Pessimistic
Offline Lock is very complementary to Optimistic Offline Lock and only use Pessimistic Offline Lock
where it is truly required.
If you have to use Pessimistic Offline Lock you should also consider a long transaction. Although long
transactions are never a good thing, in some situations they may work out to be no more damaging than
Pessimistic Offline Lock, and they are much easier to program. Do some load testing before you
choose.
Do not use these techniques if your business transactions fit within a single system transaction. Many
system transaction pessimistic locking techniques ship with the application and database servers you are
already using. Among these techniques are the 'SELECT FOR UPDATE' SQL statement for database
locking and the entity EJB for application server locking. Why worry about timeouts, lock visibility and
such when there is no need. Understanding these types of locking can certainly add a lot of value to your
implementation of Pessimistic Offline Lock. But understand that the inverse is not true! What you have
read here has not prepared you to write a database manager or transaction monitor. The offline locking
techniques presented in this book are all dependent upon your system having a real transaction monitor.
##%%&&
Allow framework or layer supertype code to acquire offline locks.
Key to any locking scheme is that there are no gaps in its use. A developer's forgetting to write a single
line of code that acquires a lock can render an entire offline locking scheme useless. Failing to retrieve a
read lock where other transactions use write locks means you might not get up-to-date session data.
Failing to use a version count properly can result in unknowingly writing over someone's changes.
Generally, if an item might be locked anywhere it must be locked everywhere. Ignoring its application's
locking strategy allows a business transaction to create inconsistent data. Not releasing locks will not
corrupt your record data but will eventually bring productivity to a halt. As offline concurrency
management is a difficult concept to test such errors might go undetected by all of your test suites.
One solution is to not allow developers to make such a mistake. Locking tasks that cannot be
overlooked should be handled implicitly by the application rather than explicitly by developers. The fact
that most enterprise applications make use of some combination of framework, Layer Supertype, and
code generation provides us with ample opportunity to facilitate Implicit Lock.
Implementing Implicit Lock is a matter of factoring your code such that any locking mechanics that
absolutely cannot be skipped can be carried out by your application framework. For lack of a better
word we'll use the word framework to mean any combination of Layer Supertype, framework classes,
and any other 'plumbing' code. Code generation tools are another avenue to enforce proper use of a
locking strategy. OK, this is by no means a ground-breaking idea. You are very likely to head down this
path once you have coded the same locking mechanics a few times over for in your application. But we
have seen it done poorly often enough that it merits a quick look.
The first step is to assemble a list of what tasks are mandatory for a business transaction to work within
your locking strategy. For Optimistic Offline Lock the list will include items such as storing a version
count for each record, including the version in update SQL criteria, and storing an incremented version
when changing the record. The Pessimistic Offline Lock list will include items along the lines of
acquiring any lock necessary to load a piece of data, typically the exclusive read lock or the read portion
of the read/write lock, and the release of all locks when the business transaction or session completes.
Note that the Pessimistic Offline Lock list did not include acquiring any lock only necessary for editing
a piece of data. This is the exclusive write lock and the write portion of the read/write lock. Yes, this is
indeed mandatory should your business transaction wish to edit the data. However, implicitly acquiring
these locks would present a couple of difficulties. First, the only points where we might implicitly acquire
a write lock, such as the registration of a dirty object within a Unit of Work, offer us no promise that the
transaction will abort as soon as the user begins to work should the locks be unavailable. The application
cannot figure out on its own when is a good time to acquire these locks. A transaction not failing rapidly
conflicts with an intent of Pessimistic Offline Lock: that a user not have to perform work twice.
Second, and just as important, is that these are the types of locks that most greatly limit the system's
concurrency. Avoiding Implicit Lock here helps us think about how we are impacting concurrency by
forcing the issue out of the technical arena and into the business domain. But still we must enforce that
locks necessary for writing are acquired before changes are committed. What your framework can do is
assure that a write lock has already been obtained before committing any changes. Not having acquired
the lock by commit time is a programmer error and the code should at least throw an assertion failure.
We'd advise skipping the assertion and throwing a concurrency exception here as you really don't want
any such errors in your production system when assertions are turned off.
A word of caution with using the Implicit Lock. While Implicit Lock allows developers to ignore a
large part of locking mechanics it does not allow them to ignore consequences. For example, if using
Implicit Lock with a pessimistic locking scheme that waits for locks the developers still need to think
about deadlock possibilities. The danger with Implicit Lock is that business transactions can fail in
unexpected fashion once developers stop thinking about locking.
Making it work is a matter of determining the best way to get your framework to implicitly carry out
locking mechanics. Please see Optimistic Offline Lock for samples of implicit handling of that lock
type. Below is an example of an implicit Pessimistic Offline Lock. The possibilities for a quality Implicit
Lock implementation are far too numerous to demonstrate here.
Implicit Lock should be used except in the simplest of applications that have no concept of framework.
The risk of a single forgotten lock is too great.
##%%&&
Lock a set of related objects with a single lock
Often groups of objects can be edited as a group. Perhaps you have a customer and its set of
addresses. When using the application it makes sense that if you want to lock any one of these items,
you should lock all of them. Having a separate lock for each individual object presents a number of
challenges. The first problem is that anyone manipulating these objects has to write code that can find all
the objects in the group in order to lock them. This is easy enough for a customer and its addresses but
gets tricky as you have more locking groups. And what if the groups get complicated? And where is this
behavior when your framework is managing lock acquisition? Then, if your locking strategy requires that
an object be loaded in order that it be locked, such as with Optimistic Offline Lock, locking a large
group will present a performance problem. And if using Pessimistic Offline Lock a large lock set is a
management headache and increases lock table contention.
A Coarse-Grained Lock is a single lock that covers many objects. Not just does this simplify the
locking action itself, it also means that you don't have load all the members of a group in order to lock
them.
The first step to implementing Coarse-Grained Lock is to create a single point of contention for locking
a group of objects. This allows that only one lock be necessary for locking the entire set. Then provide
the shortest path possible to finding that single lock point in order to minimize the group members which
must be identified and possibly loaded into memory in the process of obtaining that lock.
With the Optimistic Offline Lock, having each item in a group share a version creates the single point of
contention. And this means sharing the same version, not an equal version. Incrementing this version will
lock the entire group with a shared lock. Setup your model to point every member of the group at the
shared version and you have certainly minimized the path to the point of contention.
Figure 1: Sharing a version
Using a shared Pessimistic Offline Lock will require that each member of the group share some sort of
lockable token. The Pessimistic Offline Lock must then be acquired on this token. As Pessimistic
Offline Lock is often used as a complement to Optimistic Offline Lock a shared version object makes
an excellent candidate for the role of lockable token.
Figure 2: Locking a shared version
Eric Evans and David Siegel[missing reference] define an aggregate as a cluster of associated objects
that we treat as a unit for data changes. Each aggregate has a root that provides the only access point to
members of the set and a boundary that defines what gets included in the set. This aggregate has the
characteristics that call for a Coarse-Grained Lock since working with any member of the aggregate
requires locking all members. Locking an aggregate yields an alternative to a shared lock that we'll call a
root lock. This works by locking the root and making all lock usage within the aggregate use that root
lock. This approach gives us a single point of contention.
Using a root lock as a Coarse-Grained Lock makes it necessary to implement navigation to the root in
your object graph. This allows a locking mechanism, upon receiving a request to lock any object in the
aggregate, to navigate to the root and lock it instead. This navigation can be accomplished in a couple of
fashions. You can maintain a direct navigation to the root for each object in the aggregate, or you can
use a sequence of intermediate relationships. For example, in a hierarchy, the obvious root is the top
level parent. You can link the descendents to the top level parent directly, or you can give each node a
link to its immediate parent and navigate that structure to reach the root. In a large graph the later
strategy might cause performance problems as each parent must be loaded in order to determine
whether it has a parent. Be sure to use a Lazy Load when loading the objects that make up the path to
your root. This will not only prevent objects from being loaded before they are needed but will prevent
an infinite mapping loop should you be mapping a bidirectional relationship. Be wary of the fact that if
Lazy Load for a single aggregate occur across multiple system transactions you may end up with an
aggregate built from inconsistent parts. That, of course, is not good.
Figure 3: Locking the root
Note that a shared lock also works for locking an aggregate as locking any object in the aggregate will
simultaneously lock the root.
Both the shared lock and root lock implementation of Coarse-Grained Lock have their trade-offs.
When using a relational database the shared lock carries the burden that most all of your selects will
require a join to the version table. On the other hand, loading objects while navigating to the root could
be a performance hit as well. The root lock and Pessimistic Offline Lock perhaps make an odd
combination. By the time you navigate to the root and lock it you may need to reload a few objects to
guarantee their freshness. And, as always, building a system against a legacy data store will place
numerous constraints on your implementation choice. Locking implementations abound. The number of
subtleties even more numerous. Be sure to arrive at an implementation that suits your needs.
The most obvious place to use a Coarse-Grained Lock is to satisfy business requirements. This is the
case when locking an aggregate. Consider a lease object that owns a collection of assets. It probably
does not make business sense that one user edits the lease while another user simultaneously edits an
asset. Locking either the asset or the lease ought to result in the lease and all of its assets being locked.
A very positive outcome of using Coarse-Grained Lock is the decreased cost of acquiring and releasing
locks. This is certainly a legitimate motivation for using Coarse-Grained Lock. The shared lock can be
used beyond the concept of Evans' aggregate. But be careful when working from non-functional
requirements such as performance. Beware of creating unnatural object relationships in order to facilitate
Coarse-Grained Lock.
##%%&&
Store session state on the client
Even the most server-oriented designs need at least a little Client Session State, if only to hold a session
identifier. For some applications you can consider putting all of the session data on the client. In this case
the client sends the full set of session data with each request, and the server sends back the full session
state with each response. This allows the server to be completely stateless.
Most of the time you'll want to use Data Transfer Object to handle the data transfer. The Data
Transfer Object can serialize itself over the wire and thus allow even complex data to be transmitted.
The client also needs to store the data. If our client is a rich client application - it can do this within its
own structures. These could be the fields in the rich client interface - although I would drink Budweiser
rather than do that. A set of non-visual objects often makes a better bet. This could be the Data
Transfer Object itself, or a domain model. Either way it's not usually a big problem.
If we have a HTML interface then things get a bit more complicated. There are three common ways to
do it: URL parameters, hidden fields, and cookies.
URL Parameters are the easiest to work with for a small amount of data. Essentially it means that all
the URLs on any response page add the session state as parameters to the URL. The clear limit to doing
this is that the size of an URL is limited. However if you only have a couple of data items this works well,
so it's a popular choice for something like a session id. Some platforms will do automatic URL rewriting
to add a session id. Changing the URL may be a problem with bookmarks, so that's an argument against
using it for consumer sites.
A hidden field is a field sent to the browser that isn't displayed on the web page. You get it by using a
tag of the form <INPUT type = "hidden">. To make hidden fields work you serialize your session state
into the hidden field when you make a response and read it back in again on each request. You'll need to
make a format for putting the data in the hidden field. XML is an obvious standard choice, but of course
is rather wordy. You can also encode the data into some text based encoding scheme. Remember that a
hidden field is only hidden from the displayed page, anyone can look at the data by looking at the page
source.
Beware if you have a mixed site that has some older or fixed web pages. You can lose all the session
data if you navigate to these pages.
The last, and sometimes controversial choice, is cookies. Cookies are sent back and forth
automatically. Just like a hidden field you can use them by serializing the session state into the cookie.
You are limited in size to how big the cookie can be. Another issue is that many people don't like
cookies, as a result they turn them off. If they turn them off then your site will stop working. More and
more sites are dependent on cookies now, so that will happen less often, and certainly isn't a problem
for a purely in-house system. Cookies also are no more secure than anything else, so assume prying of
all kinds can happen.
Cookies also only work within a single domain name, so if your site is separated into different domain
names the cookies won't travel between them.
Some platforms can detect whether cookies are enabled, and if not they can use URL rewriting. This
can make it very easy for very small amounts of data.
Client Session State contains a number of advantages. In particular it reacts well in supporting stateless
server objects with maximal clustering and fail-over resiliency. Of course if the client fails, all is lost; but
often the user would expect that anyway.
The arguments against Client Session State vary exponentially with the amount of data involved. With
just a few fields, everything works nicely. With large amounts of data then the issues of where to store
the data, and the time cost of transferring everything with every request start becoming prohibitive. This
is especially true if your stars include an http client.
There's also the security issue. Any data sent to the client is vulnerable to being looked at and altered.
Encryption is the only way to stop this, but encrypting and decrypting with each request will add some
performance burden. Without encryption you have to be sure you aren't sending anything you would
rather hide from prying eyes. Fingers can pry too, so don't assume that what got sent out is the same as
what gets sent back. Any data coming back will need to be completely revalidated.
You'll almost always have to use Client Session State for session identification. Fortunately this should
be just one number, which won't burden any of the above schemes. You should still be concerned about
session stealing, which is what happens when a malicious user changes his session id to see if he can
snag someone else's session.
##%%&&
Store session date as committed data in the database.
When a call occurs from the client to the server, the server object will first pulls the data required for the
request from the database. The server object then does the work it needs to do and saves all the data
required back to the database.
In order to pull information from the database, the server object will need some information about the
session - requiring at least a session ID number to be stored on the client. Usually, however, this
information is nothing more than the appropriate set of keys that's needed to find the appropriate amount
of data in the database.
The data that's involved will typically involve a mix of session data, that's only local to the current
interaction, and committed data that's relevant to all interactions.
One of the key issues to consider here is how to deal with the fact that session data is usually considered
local to the session and shouldn't affect other parts of the system until the session as a whole is
committed. So if you are working on an order in a session and you want to save it's intermediate state to
the database, you usually need to handle it differently to an order that is confirmed at the end of a
session. This is because that often, you don't want pending orders to appear in queries that are run
against the database for such things as book availability, revenue that day, and the like.
So how do you separate the session data? One route is to simply add a field to each database row that
may have session data.The simplest form of this is to just have a boolean field isPending. However a
better way is to store a session id in this field. This makes it much easier to find all the data for a
particular session. All queries that want only record data now need to be modified with a clause
sessionID is not NULL, or use a view that filters out that data.
Using a session ID field ends up being a very invasive solution because all applications that touch the
record database need to know about the meaning of the field to avoid getting session data. Views will
sometimes do the trick and remove the invasiveness, but they often impose other costs of their own.
A second alternative is to use a separate set of pending tables. So if you have an orders and an order
lines tables in your database, you would add tables for pending orders and pending order lines. While
session data is pending you save it to the pending table and when it becomes record data you save it to
the real tables. This removes much of the invasiveness problem, but you'll need to add the appropriate
table selection logic to your database mapping code, which will certainly add some complications.
Often the record data will have integrity rules that aren't really true for pending data. In this case the
pending tables allow you to not have the rules when you don't want them, but to enforce them when you
do. Validation rules also will typically not be applied when saving pending data. You may face different
validation rules depending on where you are in the session - but this will typically appear in server object
logic.
If you use pending tables, they should be pretty much exact clones of the real tables. That way you can
keep your mapping logic as similar as possible. So use the same field names between the two tables.
However do add a session id field to the pending tables so that you can easily find all the data for a
session.
You'll need a mechanism to clean out the session data if a session is cancelled or abandoned. Using a
session ID makes this very straightforward as you can find all data with that session ID and delete it. If
users abandon the session without telling you, you'll need some kind of timeout mechanism. A daemon
that runs every few minutes can look for old session data. This implies a table in the database that keeps
a track of the time of the last interaction with the session.
Rollback is made much more complicated by updates. If you update an existing order in a session that
allows a rollback of the whole session, how do you perform the rollback? One option is to not allow
cancellation of a session like this. Any updates to existing record data become part of the record data at
the end of the request. This is simple and often fits the users' view of the world. The alternative is
awkward whether you use pending fields or pending tables. Pending tables can be easier as you can
copy all the data that may be modified into pending tables, modify it there, and commit it back to the
record tables at the end of the session. You can do a similar approach with a pending field, but only if
the session ID becomes part of the key so that you can keep the old and new IDs in the same table at
the same time - which can get very messy.
If you are going to use separate pending tables that are only read by objects that handle a session, then
there may be little point in tabularizing the data. Then it's better to use a Serialized LOB. At this point
we've crossed the boundary into a Server Session State
You can avoid all of the hassles of pending data by not having any. If you design your system so that all
data is considered to be record data, then you avoid all issues of pending data. This isn't always
possible, of course, and sometimes it's done in such an awkward way that the designers would be better
off with thinking about explicit pending data. But if you have the option it makes Database Session State
a lot easier to work with.
Database Session State is one alternative to handling session state and should be compared with Server
Session State and Client Session State.
The first aspect to consider with this pattern is performance. You'll gain by allowing yourself to use
stateless objects on the server, thus enabling pooling and easy clustering. However the cost you'll pay is
the time to pull the data in and out of the database with each request. You can reduce this cost by
caching the server object. This will save you having to read the data out of the database whenever the
cache is hit. You'll still pay the write costs.
The second main issue is that programming effort, most of which centers around handling session state. If
you have no session state and are able to save all your data as record data in each request - then this
pattern is an obvious choice as you lose nothing in either effort or performance (if you cache your server
objects).
In a choice between Database Session State and Server Session State the biggest issue may be in how
easy it is to support clustering and fail over with Server Session State in your particular application
server. Getting clustering and fail over to work with Database Session State is usually more
straightforward, at least with the regular solutions.
##%%&&
Keep the session state in an active process in memory that can be serialized into a memento if the
memory resources are needed or the session migrates.
The simplest form of this pattern occurs when a session object is held in memory on an application
server. In this case you can simply have some kind of map in memory which holds these session objects
keyed by a session id - and then all the client needs to do is to give the session id and the session object
can be retrieved from the map to process the request.
This simple scenario assumes, of course, that the application server carries enough memory to be able to
perform this task. It also assumes that there is only one application server, i.e. no clustering, and that if
the application server fails then it is appropriate for the session to be abandoned and all work so far in
the session lost the great bit-bucket in the sky.
For many application this set of assumptions is actually not a problem. However for others it may be
problematic. But there are ways of dealing with cases when the assumptions are no longer valid, and
these introduce common variations that add complexity to this essentially simple pattern.
The first issue is that of dealing with memory resources held by the session objects - which indeed is the
common objection to Server Session State. The answer, of course, is to not keep them in memory but
instead serialize all the session state to a memento. This presents two questions: in what form do you
persist the Server Session State and where do you persist the Server Session State.
The form to use is usually as simple a form as possible, since the accent of Server Session State is it's
simplicity in programming. Several platforms provide a simple binary serialization mechanism that allows
you serialize a graph of objects quite easily. Another route is to serialize into another form. A common
one to consider these days is a textual form, fashionably as an XML file.
The binary one is usually easier, since it requires little programming, while the textual form will usually
require at least a little code. Binary serliazations also require less disk space - and although total disk
space will rarely be a concern, large serialized graphs will take longer to activate into memory.
There are two common issues with binary serialization. Firstly the serialized form is not human readable -
which is a problem iff humans want to read it. Secondly there may be problems with versioning. If you
modify a class, by say adding a field, after you've serialized it; you may not be able to read it back. Of
course not many sessions are likely to span an upgrade of the server software - unless of course it's a
7/24 server where you may have a cluster of machines running: some upgraded and some not.
This brings us to question of where to store the Server Session State. An obvious possibility is on the
application server itself, either in the file system or in a local database. This is the simple route - but may
not support efficient clustering or support fail over. In order to support these the passivated Server
Session State needs to be somewhere generally accessible, such as shared server. This will now support
clustering and fail over, at the cost of a longer time to activate the server - although caching may well
eliminate much of this cost.
This line of reasoning may lead to the ironic route of storing the serialized Server Session State in the
database using a session table indexed by the session id. This table would require a Serialized LOB to
hold the serialized Server Session State. Database performance varies when it comes to handling large
objects, so the performance aspects of this one is very database dependent.
At this point we are right at the boundary between Server Session State and Database Session State
.The boundary between these patterns is completely arbitrary - but I've drawn the line at the point where
you convert the data in the Server Session State into tabular form.
If you're storing the Server Session State in a database, then you'll have to worry about handling
sessions that have gone away, especially in a consumer application. One route is to have a daemon that
looks for aged sessions and deletes them, but this can lead to a lot of contention on the session table.
Kai Yu told about an approach he used with success. In this case they partitioned the session table into
twelve database segments. Every two hours they would rotate the segments, deleting everything in the
oldest segment and directing all inserts to the newly empty segment. While this did mean that any session
that was active for twenty-four hours got unceremoniously dumped, that's sufficiently rare to not be a
problem.
Although all of these variations get more and more effort to do, the good news is that increasingly
application servers are supporting these capabilities automatically. So it may well be that these are things
that the application server vendors can worry their ugly little heads about.
The two most common techniques for Server Session State are using the http session and using a
stateful session bean. The http session is a simple route and leads to the session data being stored by the
web server. In most cases this leads to server affinity and can't cope with fail over. Some vendors are
implementing a shared http session capability which allows you to store http session data in a database
that's available to all application servers. (You can also do this manually, of course.)
The other common route is to use a stateful session bean, which requires an EJB server. The EJB
container handles all persistence and passivation, so this makes it very easy to program to. The main
disadvantage is that the specification doesn't ask application servers to avoid server affinity. However
some application servers do provide this kind of capability, for example IBM's WebSphere can serialize
a stateful session bean into a BLOB in DB2, which allows multiple application servers to get at its state.
Another alternative is to use an entity bean. On the whole I've been pretty dismissive of entity beans, but
you can use an entity bean to store a Serialized LOB of session data. This is pretty simple and is less
likely to run into many of the issues that usually surround entity beans.
Server Session State are easy to implement with the built in session state capability. By default .NET
stores session data in the server process itself. You can also adjust the storage using a state service,
which can reside on the local machine or on any other machine on the network.By using a separate state
service you can reset the web server and still retain the session state. You make the change between
in-process state and a state service in the configuration file, so you don't have to change the application.
The great appeal of Server Session State is its simplicity. In a number of cases you don't have to do any
programming at all to make this work. Whether you can get away with that depends on if you can get
away with the in-memory implementation, and if not how much help your application server platform
gives you.
Even without that you may well find that the effort you do need is little. Serializing a BLOB to a database
table may turn out to be much less effort than converting the server objects to tabular form.
Where the programming effort does come into play is in the session maintenance. Particularly if you have
to roll your own support to enable clustering and fail-over that may work out to be more trouble than
your other options, particularly if you don't have much session data to deal with, or if your session data is
easily converted to tabular form.
##%%&&
Provides a coarse grained facade on fine grained objects to improve efficiency over a network.
In an object-oriented model, you do best with small objects that have small methods. This gives you lots
of opportunity for control and substitution of behavior, and to use good intention revealing naming to
help an application be easier to understand. So if you have an address object you would store and
retrieve each piece of information individually. You have separate fields for the city, state, and zip code.
You have separate getting methods to get this data, and separate setting methods to update this
information. This allows the class to control updates easily, deciding to do special validation on a zip or
determining a city automatically if the address only has a zip code. Other behaviors, such as determining
if two addresses are within a few miles of each other, are also done on a fine grained basis. This allows
clients to clearly ask for exactly the information and behavior they want.
One of the consequences of this fine grained behavior is that there is usually a lot of interaction between
objects, and that interaction usually requires lots of method invocations. Within a single address space,
this is no great issue. Method calls are cheap these days, and clever compilers and VMs can eliminate
even this small cost. In the very few cases where it does make a difference, it's a small job to optimize.
But this happy state does not exist when you make calls between processes. Remote calls are much
more expensive because there is a lot more to do. Data may have to be marshaled, security may need to
be checked, packets may need to be routed through switches. If the two processes are running on
machines on opposite sides of the globe, the speed of light may be a factor. The brutal truth is that any
inter-process call is orders of magnitude more expensive than an in-process call - even if both processes
are on the same machine. Such a performance effect cannot be ignored, even for us believers in lazy
optimization.
As a result any object that's intended to be used as a remote object needs a coarse grained interface that
minimizes the amount of calls needed to get something done. If you want to change the city, state, and
zip of a remote address; you want to do this in one call. Not just does this affect your method calls, it
also affects your objects. Rather than ask for an order and it's order lines individually, you need to
access and update the order and order lines in a single call. This affects your entire object structure.
This coarse grained interfaces come with a price. You give up the clear intention and fine-grained control
that you get with small objects and small methods. Programming becomes more difficult and your
productivity slows.
You can't rely on clever middleware to solve your problem. I've seen many sellers of distributed objects
say that you can just take a regular object model and add CORBA (for instance) to make the whole
thing distributed in a simple step.They point out that with their powerful software you only pay the price
of a remote call if the call is actually remote. So you can program fine grained objects and only pay a
performance cost should you actually use them remotely.
Frankly, that doesn't wash. The point is that if you use a fine grained object remotely, it's performance
will suck. If you use coarse-grained objects inside a process you lose productivity. There's no point
taking a fine grained object and making it remotable just in case you want to call it remotely. Either it's a
remote object or it isn't. The fundamental issue is that you need to have fine grained objects within a
process and coarse grained objects between processes.
A Remote Facade is a coarse-grained facade over a web of fine-grained objects. None of the
fine-grained objects have a remote interface and the Remote Facade does not contain any domain logic.
Remote Facade tackles the distribution problem by the standard OO approach of separating distinct
responsibilities into different objects, and as a result has become the standard pattern for this problem.
Firstly I recognize that fine-grained objects are the right answer for complex logic - so I ensure that any
complex logic is placed in fine-grained objects which are designed to collaborate within a single process.
To allow them to be accessed efficiently remotely I make a separate facade object to act as a remote
interface. The facade, as the name implies, is merely a thin skin that switches from a coarse grained to a
fine grained interface.
In a simple case, like an address object, a Remote Facade would replace all the getting and setting
methods of the regular address object with one getter and one setter: often referred to as bulk
accessors. When a client calls a bulk setting method, the address facade reads the data from the setting
method and calls the individual accessors on the real address object (see Figure 1). The facade does
nothing more than this. This way all the logic of validation and computation stays on the address object
where it can factored cleanly and be used by other fine-grained objects.
In a more complex case a single Remote Facade may act as a remote gateway for many fine-grained
object. An order facade may be used to get and update information for an order, all its order lines, and
maybe even some customer data as well.
In transferring information in bulk like this, you need to transfer this information in a form that can easily
be moved over the wire. If your fine-grained classes are present on both sides of the connection, and
these objects are serializable, then you can transfer the objects directly by making a copy. In this case a
getAddressData method would create a copy of the original address object. The setAddressData would
receive an address object and use this address to update the data of the actual address object. (This
assumes that the original address object needs to preserve its identity and thus can't be just replaced
with the new address.)
Often, however you can't do this. You may not want to duplicate your domain classes on multiple
processes. Often it's difficult to serialize a segment of a domain model due to its complicated relationship
structure. The client may not want the whole model, just a simplified subset of it. In these cases it makes
sense to use a Data Transfer Object as the basis of the transfer.
In the sketch, I've shown a Remote Facade that corresponds to a single domain object. While this is not
uncommon and is easy to understand, it isn't the most usual case. A single Remote Facadewould have a
number of methods each designed to pass on information from several objects. So getAddressData and
setAddressData would be methods defined on a class like CustomerService which would also have
methods along the lines of getPurchasingHistory and updateCreditData.
The question of granularity is one of the most tricky questions that comes up with Remote Facade.
Some people like to make fairly small Remote Facades, such as one per use case. I prefer a coarser
grained structure with much fewer Remote Facades. For even a moderate sized application I might have
just a single Remote Facade. Even for a large application I may only have half a dozen Remote Facade
s. This means that each Remote Facade has a lot of methods, but since these methods are small I don't
see this as a problem.
You design a Remote Facade based on the needs of a particular client usage. The most common
example would be the need to view and update information through a user interface. In this case you
might have a single Remote Facade for a family of screens with one bulk accessor method for loading
and saving the data for each screen. Pressing buttons on a screen, to change a order's status, would
invoke command methods on the facade. Quite often you'll have different methods on the Remote
Facade that do pretty much the same thing on the underlying objects. This is common and reasonable.
The facade is designed to make life simpler for external users, not the internal system, so if the client
process thinks of it as a different command, it is a different command - even if it all goes to the same
internal command.
Remote Facade can be stateful or stateless. A stateless Remote Facade can be pooled, and this can
improve resource usage and improve performance, especially in a B2C situation. However if the
interaction involves state across a session, then it needs to store session state somewhere using Client
Session State or Database Session State or a some implementation of Server Session State. A stateful
Remote Facade can hold onto its own state, which makes for an easy implementation of Server
Session State. This may lead to performance issues when you have thousands of simultaneous users.
As well as providing a coarse-grained interface, several other responsibilities naturally go to the Remote
Facade. The methods of a Remote Facade are a natural point to apply security. An access control list
can say which users can invoke calls on which methods of a Remote Facade. The Remote Facade
methods also are a natural point to apply transactional control. A Remote Facade method can start a
transaction, do all the internal work and then commit the transaction at the end. Each call makes a good
transaction since you don't want a transaction open when return goes back to the client, since
transactions aren't built to be efficient for such long running cases.
Use Remote Facade whenever you need remote access to a fine-grained object model. By using
Remote Facade you gain the advantages of a coarse grained interface while still keeping the advantage
of fine grained objects. This gives you the best of both worlds.
The most common case of using this is between a presentation and a Domain Model where they may
run on different processes. You'll get this between a swing UI and server domain model, or with a
servlet and a server object model if the application and web servers are different processes.
If all your access is within a single process, then you don't need this kind of conversion. So I wouldn't
use this pattern to communicate between a client Domain Model and its presentation nor between a
CGI script and Domain Model that are running all in one web server. You don't tend to see Remote
Facade used with a Transaction Script, since a Transaction Script is inherently a coarser grained
structure.
Remote Facade's imply a synchronous, that is a remote procedure call style of distribution. Often you
can greatly improve the responsiveness of an application by going with an asynchronous, message based
style of remote communication. Indeed there are many compelling advantages for an asynchronous
approach. Sadly discussion of asynchronous patterns is outside the scope of this book.
[Alur, Crupi, and Malks] discusses Session Facade in detail in the context of J2EE. Also take a look at
Kyle Brown's Session Facade paper.
If you are working with the Enterprise Java platform, then a good choice for a distributed facade is a
session bean. Session beans are remote objects, they may be stateful or stateless. In this example I'll run
a bunch of plain old Java objects inside an EJB container and access them remotely through a session
bean that's designed as a Remote Facade. Session Bean's aren't particularly complicated, so this
example should make sense even if you haven't done any work with them before.
I feel the need for a couple of side notes here. Firstly I've been surprised by how many people seem to
believe that you can't run plain objects inside an EJB container in Java. I hear the question "are the
domain objects Entity Beans?". They can be entity beans (that's a separate question), but they don't have
to be. Simple Java objects work just fine - as in this example.
My other side note is just to point out that this is not the only way to use a session bean. It can be used
to host Transaction Scripts. I'll look at that usage elsewhere, in this case I'm using it as a Remote
Facade.
In this example I'll look at remote interfaces to accessing information about music albums. The Domain
Model consists of fine grained objects that represent an artist, and album, and tracks. Surrounding this
are several other packages that provide the data sources for the application Figure 2.
Figure 2: Packages the remote interfaces
ABC Amber CHM Converter Trial version, http://www.processtext.com/abcchm.html
The dto package contains Data Transfer Objects that help us move data over the wire to the client.
They have simple accessor behavior and also the ability to serialize themselves in binary or XML textual
formats. In the remote package are assembler objects that will move data between the domain objects
and the Data Transfer Objects. If you're interested in how this works see the discussion of Data
Transfer Object.
To explain the facade I'll assume I can move data back and forth into Data Transfer Objects and
concentrate on the remote interfaces. A single logical Java session bean has three actual classes. Two of
these make up the remote API (and in fact are Java interfaces) the other is the class that implements the
API. The two interfaces are the AlbumService itself and the home object: AlbumHome. The home
object is used by the naming service to get access to the distributed facade, that's a detail on EJB that I'll
skip over here.Our interest is in the Remote Facade itself: which is AlbumService. It's interface is
declared in the API package to be used by the client, and is just a list of methods.
You'll notice that even in this short example, I see methods for a two different classes in the Domain
Model: artist and album. I also see minor variations on the same method. Methods have variants that use
either the Data Transfer Object or an XML string to move data into the remote service. This allows the
client to choose which form to use depending on the nature of the client and of the connection. As you
can see, for even a small application this can lead to many methods on AlbumService.
Fortunately the methods themselves are very simple. Here are the ones for manipulating albums.
As you can see, each method really does nothing more than delegate to another object, so each method
is only a line or two in length (ignoring the noise of the try/catch blocks). This small example illustrates
pretty nicely what a distributed facade should look like: a long list of very short methods with very little
logic in them. The facade then is nothing more than a packaging mechanism - which is as it should be.
We'll just finish with a few words on testing. It's very useful to be able to do as much testing as possible
in a single process. In this case I can do this by writing tests for the session bean implementation directly:
these can be run without deploying to the EJB container.
This is one of the jUnit tests to be run in-memory. It shows how I can create an instance of the session
bean outside the container and run tests on it. This allows for a faster testing turn-around time.
I was talking over this book with Mike Hendrickson, my editor at Addison-Wesley. Ever alert to the
latest buzzwords, he asked me if I'd got anything about web services in it. I'm actually loath to rush to
every fashion in these books, after due to the languid pace of book publishing any latest fashion that I
write about will seem quaint by the time you read it. But it's a good example of how core patterns so
often keep their value even with the latest technological flip-flops.
At it's heart a web service is nothing more than an interface for remote usage. As such the basic advice
of Remote Facade still holds: build your functionality in a fine-grained manner, and then layer a Remote
Facade over the fine-grained model in order to handle web services.
For the example, I'll the same basic problem as I described above, concentrating just on the request for
information about a single album. Figure 3 shows the various classes that take part. They fall into the
familiar groups: Album Service is the Remote Facade, two Data Transfer Objects, three objects in a
Domain Model and an assembler to pull data from the Domain Model into the Data Transfer Objects.
Figure 3: Classes for the album web service
The Domain Model is absurdly simple, indeed for this kind of problem you'd be better off using a Table
Data Gateway to create the Data Transfer Objects directly - but that would rather spoil the example
of showing a Remote Facade over a domain model.
I use Data Transfer Objects for passing the data over the wire. These are just data holders which
flatten the structure for the purposes of the web service.
Since this is .NET, I don't need to write any code to serialize and restore into XML. The .NET
framework comes with the appropriate serializer class to do the job.
Since this is a web service, I also need to declare the structure of the Data Transfer Objects in WSDL.
The Visual Studio tools will generate the WSDL for me, and I'm a lazy kind of guy, so I'll let it do that.
Here's the XML Schema definition that corresponds to the Data Transfer Objects.
Being XML, it's a particularly verbose form of data structure definition, but it does the job.
To get the data from the Domain Model to the Data Transfer Object I need an assembler.
The last piece we need is the service definition itself. This comes first from the C# class.
Of course, this isn't the real interface definition - that comes from the WSDL file. Here are the relevant
bits
As expected, WSDL is rather more garrulous than your average politician, but unlike so many of them -
it does get the job done. I can now invoke the service by sending a SOAP message of the form
The important thing to remember about this example isn't the cool gyrations with SOAP and .NET, but
the fundamental layering approach. Design an application without distribution, then layer the distribution
ability on top of that with Remote Facades and Data Transfer Objects.
##%%&&
An object which acts purely as a carrier of data in order to reduce method calls.
When you're working with a remote interface, such as a Remote Facade, you need to be able to send
and receive a large amount of information in a single call. One way to do this is to use lots of parameters.
However this is often awkward to program and indeed it's often impossible, such as with languages like
Java that only return a single return value.
So the way to deal with this is to create a Data Transfer Object which can hold all the data for the call.
This object usually needs to be serializable to go across the connection.
(Many people in the Sun community use the term Value Object for this pattern. I use Value Object to
mean something else, see the discussion there for the background to this.)
In many ways, a Data Transfer Object is one of those objects our mothers told us never to write. A
Data Transfer Object is often little more than a bunch of fields and getters and setters for these fields.
The value of this usually hateful beast is that it allows you to move several pieces of information over a
network in a single call - a trick that's essential for distributed systems.
When using a Data Transfer Object whenever a remote object needs some data, it asks for a suitable
Data Transfer Object. The Data Transfer Object will usually carry much more data than the specific
item that the remote object wants from this request, but should carry all the data that the remote object
will need for a while. Due to the latency costs of remote calls, it's better to err on the side of sending too
much data, rather than run into a situation where multiple calls are needed.
A single Data Transfer Object will usually contain for more than just a single server object. A Data
Transfer Object will usually aggregate data from all the server objects that the remote object is likely to
want data from. So if a remote object requests data about an order object, the returned Data Transfer
Object will contain data from the order, customer, the line items, the products on the line items, the
delivery information, all sorts of stuff.
As a result it usually makes sense to design the Data Transfer Object around the needs of a particular
client. As a result you'll often see Data Transfer Objects corresponding to web pages or GUI screens.
Similarly you may see multiple Data Transfer Objects for an order, depending on the particular screen,
of course if different presentations require similar data, then it makes sense to use a single Data
Transfer Object to handle them all.
A related question to consider is whether to use a single Data Transfer Object for a whole interaction,
or to use different Data Transfer Objects for each request and response. Using different Data
Transfer Objects makes it easier to see what data is transferred in each call, but leads to a lot of Data
Transfer Objects. The single Data Transfer Object is less work to write, but makes it harder to see
how each call transfers information. I'm inclined to use a single Data Transfer Object if there's a lot of
commonality over the data, but I don't hesitate to use different Data Transfer Objects if a particular
request suggests it. It's one of those things where you can't really make a blanket rule, so I might use one
Data Transfer Object for most of the interaction and use different ones for a couple of requests and
responses.
A similar question is whether to have a single Data Transfer Object for both request and response, or
separate Data Transfer Objects for each. Again there's no blanket rule. If the data in each case is
pretty similar, then I'd use one. If they are very different I'd use two.
The fields in a Data Transfer Object are usually fairly simple. They are either primitives, simple classes
like strings and dates, or other Data Transfer Objects. This is for a couple of reasons. Firstly they have
to be serializable into whatever transport format is being used. Secondly they need to be understood by
both sides of the wire.
Some people like to make Data Transfer Objects immutable. In this scheme you receive one Data
Transfer Object from the client and create and send back a different object, even if it's the same class.
Others will alter the request Data Transfer Object. I don't have any strong opinions either way. On the
whole I prefer a mutable Data Transfer Object because that way it's easier to put the data in gradually,
even if you do make a new object for the response. Some arguments in favor of immutable Data
Transfer Object are due to the naming confusion with Value Object.
A common form for Data Transfer Object is that of a record set. A record set is a set of tabular
records, exactly what you get back from a SQL query. Indeed a record set is the Data Transfer Object
for a SQL database. Often architectures use this form further through the design. Many GUI tools are
designed to work with record sets, so providing a record set to such tools makes a lot of sense. A
number of platforms, in particular those with an origin in two-layer client server systems, provide the
ability for other layers to create record sets. In this way a domain model could generate a record set of
data to transfer to a client, that client treats the record set as if it was coming directly from SQL. The
record set can be entirely created by the domain logic, or more likely it's generated from a SQL query
and modified by the domain logic before it's passed on to the presentation.
Other than simple getters and setters, the Data Transfer Object is also usually responsible for serializing
itself into some format that will go over the wire. Which form to choose depends on what runs on both
sides of the connection, what can run over the connection itself, and the ease of doing the serialization. A
number of platforms provide built in serialization for simple objects. For example Java has a built-in
binary serialization and .NET has built in binary and XML serializations. If there is a built in serialization
this usually works right out of the box as Data Transfer Object are simple structures that don't need to
deal with the kinds of complexities that you run into with objects in a domain model. As a result I always
use the automatic mechanism if I can.
If you don't have an automatic mechanism already available, you can usually create one yourself. In
several cases I've seen code generators that take a simple record descriptions and generate appropriate
classes to hold the data, provide accessors, and to read and write serializations of that data. The
important thing to remember is to make the generator only as complicated as you actually need: don't try
to put in features that you think you may need. Often it's a good idea to write the first classes by hand,
and use the hand written classes to help you write the generator.
You also have to choose a mechanism that both ends of the connection will work with. If you control
both ends, then you can choose the easiest one that will work. If you don't have both ends, you may be
able to provide a connector at the end you don't own. Then you can use a simple Data Transfer Object
on both sides of the connection and use the connector to adapt to the foreign component.
One of the most common issues to face is whether to use a text or binary serialization form. Text
serializations have the advantage that they are easy to read to see what's being communicated. XML is a
popular choice since you can easily get tools to create and parse XML documents. The big
disadvantage with text is that it more bandwidth to send the same data: something that is particularly true
of XML.
An important factor for serialization is dealing with the synchronization of a the Data Transfer Object
on each side of the wire. In theory whenever the server changes the definition of the Data Transfer
Object the client will update as well. In practice, however, this may not happen. Accessing a server with
an out of date client is always going to lead to some kinds of problem, but the serialization mechanism
can make the problems more or less painful. With a pure binary serialization of a Data Transfer Object
the result will be that communication of that Data Transfer Object will be entirely lost, since any change
to the structure of the Data Transfer Object will usually cause the an error on deserialization. Even an
innocuous change, such as adding an optional field, will have this effect. As a result direct binary
serialization can introduce a lot of fragility into the communication lines.
Other serialization schemes can avoid this. The XML serialization can usually be written in such a way
that makes the classes more tolerant of these kinds of changes. Another approach is to use a more
tolerant binary approach. Serializing the data using a dictionary is one way to do this. Although I don't
like using a dictionary as the Data Transfer Object, it can be a useful way of doing a binary serialization
of the data, since that introduces some tolerance into the synchronization.
Assembling a Data Transfer Object from domain objects
A Data Transfer Object does not know about how to connect with the domain objects. This is
because the Data Transfer Object is something that should be deployed on both sides of the
connection, so I don't want the Data Transfer Object to be dependent on the domain object. Similarly
I don't want the domain objects to be dependent of the Data Transfer Object since the structure of the
Data Transfer Object will change when I alter interface formats. As a general rule I want to keep the
domain model independent of the external interfaces.
As a result of this I like to have a separate assembler object that is responsible for creating a Data
Transfer Object from the domain model and updating the domain model from a Data Transfer Object.
The attributes of the data transfer object will either be simple value objects or other data transfer
objects. Any structure between data transfer objects should be a very simple graph structure - usually a
hierarchy - as opposed to the more complicated graph structures that you see in a Domain Model
When I choose a structure for a Data Transfer Object I prefer to base the structure on the needs of the
client of the Data Transfer Object. So if I'm presenting information in a web page, I'll design the Data
Transfer Object around the structure of the web page. This may well mean that you'll have multiple
Data Transfer Objects for different presentations. This is quite reasonable. Whichever way I do it I
need to do transformation from the structure of the domain model to the structure of the presentation -
and the assembler is in the best position to do this because it is able to take advantage of the full richness
of the domain model's associations and behavior to construct the Data Transfer Object. I make an
exception for simple variations, so if one presentation shows a subset of data of another I would have
them share the same Data Transfer Object.
I may also find multiple assemblers sharing the same Data Transfer Object. A common case for this is
when you have different update semantics in different scenarios using the same data.
Use a Data Transfer Object whenever you need to transfer multiple items of data between two
processes in a single method call.
There are some alternatives to Data Transfer Object, although I'm not a fan of them.One is to not use
an object at all, but simply to use a setting method with many arguments or a getting method with several
pass by reference arguments. This runs into a few problems. Many languages, such as Java, only allow
one object as a return value. So although this can be used for updates, it can't be used for retrieving
information without playing games with callbacks. Even when it is possible the Data Transfer Object
makes an excellent boundary point for testing, allowing you run tests in one process without connecting
to other processes. It also allows you to queue calls, rather like a command object, and encapsulates the
serialization approach.
Another alternative is to use a collection class to hold the data. I've seen arrays used for this - but I
discourage that because the array indices obscure the code. The best collection to use is a dictionary,
since you can use meaningful strings as keys. The problem with a dictionary is you lose the advantage of
an explicit interface and strong typing. It can be worth using for ad hoc cases when you don't have a
generator to hand, as it's easier to manipulate a dictionary than to write a Data Transfer Object by
hand. However with a generator, I think you're better off with an explicit interface, especially when you
consider that it is being used as communication protocol between different components.
In particular it's worth creating a Data Transfer Object when you want to use XML to communicate
between components. The XML DOM is a pain in the neck to manipulate, and it's much better to use a
Data Transfer Object that encapsulates the DOM, especially since it's so easy to generate the Data
Transfer Object.
Another common purpose for a Data Transfer Object is to act as a common source of data for various
different components in different layers. Each component takes the Data Transfer Object, makes some
changes to it, and then passes on to the next layer. The use of Record Set in COM and .NET is a good
example of this, where each layer knows how to manipulate record set based data, whether it comes
directly from a SQL database, or has been modified by other layers. .NET expands on this by providing
a built in mechanism to serialize record sets into XML.
For this example I'll use the domain model in Figure 2. The data I want to transfer is the data about these
linked objects and the structure for the data transfer objects is the one in Figure 3
The data transfer objects simply the structure a good bit. The name of the artist is collapsed into the
album DTO and the performers for a track are represented as an array of strings. This is typical of the
kind of collapsing of structure you see for a data transfer object. There are two data transfer objects
present, one for the album and one for each track. In this case I don't need a transfer object for the artist
as all the data is present on one of the other two. I only have the track as a transfer object because there
are several of them in the album and each one can contain more than one data item.
Here's the code to write a Data Transfer Object from the domain model.
Updating the model from the Data Transfer Object is usually a good bit more involved. For this
example there is a difference between creating a new album and updating an existing one. Here's the
creation code.
Reading the DTO involves quite a few decisions. The noticeable one here is how to deal with the artist
names as they come in. In this case our requirements are that artists should already be in the registry
when I create the album, so if I can't find an artist this is an error. A different create method might decide
to create artists when they are mentioned in the Data Transfer Object.
For this example I have a different method for updating an existing album.
When you do updates you can decide to either update the excising domain object or to destroy the
existing domain object and replace it with a new one. The question here is whether you have other
objects referring to the object you want to update. So in this code I'm updating the album since I have
other objects referring to the existing album and tracks. However for the title and performers of a track I
just replace the objects that are there.
Another question is that if an artist changes, is this changing the name of the existing artist, or changing
the artist that the album is linked to. Again these questions have to settled on a use case by use case
basis, and I'm handling it by linking to a new artist.
In this example I've used the native binary serialization, this means I have to be careful that the Data
Transfer Object classes on both sides of the wire are kept in sync. If I make a change to the data
structure of the server Data Transfer Object and don't change the client, then I'll get errors in the
transfer. I can make the transfer more tolerant by using a Map as my serialization.
Now if I add a field to the server and use the old client, then although the new field won't be picked up
by the client, the rest of the data will transfer correctly.
Of course, writing the serialization and deserialization routines like this is tedious work. I can avoid much
of this tedium by using a reflective routine such as this on the Layer Supertype.
This kind of routine will handle most cases pretty well (although you'll have to add extra code to handle
primitives.)
Once I have the data structure for the Data Transfer Object, I need to decide how to serialize it. In
Java you get binary serialization for free by simply using a marker interface. This works completely
automatically for a Data Transfer Object so it's our first choice. However often it's necessary to use a
text based serialization so I can easily send it over http. So for this example I'll use XML.
For this example, I'm using JDOM, since that makes working with XML much easier than using the
W3C standard interfaces. For each Data Transfer Object class I write methods to read and write an
XML element to represent that class.
Of course these only create the elements in the XML DOM. To perform the serialization I need to read
and write text. Since the track is only transferred in the context of the album, I only need to write this
code for the album.
##%%&&
An object that encapsulates access to an external system or resource
Interesting software rarely lives in isolation. Even the most carefully pure object-oriented system often
has to deal with things that aren't objects: relation database tables, CICS transactions, XML data
structures.
When accessing external resources like this, you'll usually get an API for that resource. However these
APIs are naturally going to be somewhat complicated as they take the nature of the resource into mind.
Anyone who needs to understand that resource needs to understand that API: whether it be JDBC and
SQL for relational databases, or the w3c or JDOM APIs for XML. Not just does this make the
software harder to understand, it also makes it much harder to change should you shift some data from a
relational database to an XML message at some point in the future.
The answer is so common, that it's hardly worth stating. Wrap all the special API code into a class
whose interface looks like a regular object. Other objects access the resource through this Gateway,
and the Gateway object translates the simple method calls into the appropriate specialized API.
In reality this is a very simple wrapper pattern. Take the external resource. Consider what the
application needs to do with this resource. Create a simple API for your usage and use the Gateway to
translate to the external source.
One of the key uses for a Gateway is to make a good point to apply a Service Stub. Often you'll find
you may alter the design of the Gateway to make it easier to apply a Service Stub. Don't be afraid to
do this - well placed Service Stubs can make a system much easier to test and that will make the system
easier to write.
Keep a Gateway as simple as you can. Focus on the essential roles of adapting the external service and
providing a good point for stubbing. Keep the Gateway as minimal as possible yet able to handle these
tasks. Any more complex logic should be in clients of the Gateway.
Often it's a good idea to use code generation to create Gateways. From a definition of the structure of
the external resource, you can generate a Gateway class to wrap it. You might use relational meta data
to create a wrapper class for a relational table, or an XML schema or DTD to generate code for a
Gateway for XML. The resulting Gateway are dumb but they do the trick, other objects can carry out
more complicated manipulations.
Sometimes a good strategy is to build the Gateway in terms of more than one object. The obvious form
is to use two objects: a back-end and a front-end. The back-end acts as a minimal overlay to the
external resource and does not simplify the API of the external resource at all. The front end then
transforms the awkward API into one that's more convenient for your application to use. Use this
approach if the wrapping of the external service and the adaptation to your needs are both reasonably
complicated. That way each responsibility is handled by a single class. Conversely if the wrapping of the
external service is simple, then one class can handle that and any adaptation that's needed.
You should consider this whenever you have an awkward interface to something that feels external.
Rather than let the awkwardness spread through the whole system, use a Gateway to contain it. There's
hardly any downside to making the Gateway, and the code elsewhere in the system becomes much
easier to read.
Using Gateway usually makes a system easier to test by giving you a clear point to deploy Service Stub
s. So even if the external system's interface is fine, a Gateway is useful as a first move to make in
applying Service Stub.
A clear benefit of Gateway is that it also makes it easier for you to swap out one kind of resource for
another. Any change in resource means you only have to alter the Gateway class and the change doesn't
ripple through the rest of the system. Gateway is a simple and powerful form of protected variation. In
many cases reasoning about this flexibility is the focus of a discussion about whether to use Gateway or
not. However don't forget that even if you don't think the resource is ever going to change, you can
benefit simply because of the simplicity and testability that Gateway gives you.
When you have a couple of subsystems like this, then another choice to decouple them is to use a
mediator. However mediator is much more complicated to use than Gateway. As a result I find that the
majority of external resource access is done using a Gateway.
I must admit that I've struggled a fair bit with whether to make this a new pattern as opposed to
referencing existing patterns such as facade and adaptor. I've separated from these other patterns
because I think there's a useful distinction to be made.
??While facade also simplifies a more complex API, a facade is usually done by the writer of the
service for general use, while a Gateway is written by the client for their particular use. In
addition a facade always implies a different interface to what it's covering, while a Gateway may
copy the wrapped facade entirely, being use for substitution or testing purposes
??Adapter alters an implementation's interface to match another interface which you need to work
with. With Gateway there usually isn't a an existing interface, although you might use an adaptor
to map an implementation to a an existing Gateway interface. In this case the adaptor is part of
the implementation of the Gateway.
I was talking about this pattern with my colleague, Mike Rettig, and he described how he's used this
pattern to handle interfaces with EAI (Enterprise Application Integration) software. We decided that this
would be a good inspiration for an example of Gateway.
To keep things to the usual level of ludicrous simpleness well just build a gateway to an interface that just
sends a message using the message service. The interface is just a single method.
The first argument is a string indicating the type of the message, the second is the arguments of the
message. The messaging system allows you to send any kind of message, so it needs a generic interface
like this. When you configure the message system in your use of it, you specify the types of messages the
system will send, and the number and types of arguments for the message. So we might configure the
confirm message with the string "CNFRM" and have arguments for an id number as a string, an integer
amount and a string for the ticker code. The messaging system will check the types of the arguments for
us and generate an error if we send a wrong message or the right message with the wrong arguments.
All this is laudable, and necessary, flexibility but the generic interface is awkward to use because it fails
to be explicit. You can't tell by looking at the interface what the legal message types are, nor what
arguments are needed for a certain message type. What we need instead is an interface with methods
like this.
That way if we want a domain object to send a message, it can do so like this
Here the name of the method tells us what message we are sending, and the arguments are typed and
given names. This is a much easier method to call than the generic method. This is the role of the
gateway to make a more convenient interface. It does mean that every time we add or change a
message type in the messaging system we need to change the gateway class. But we would have to
change the calling code anyway, and at least this way the compiler can help us find clients and check for
errors.
There's another problem. When we get an error with this interface it tells us by giving us a return error
code. A return code of 0 indicates success, anything else is an error. Different numbers indicate different
errors. While this is a natural way for a C programmer to work, it isn't the way Java does things. In Java
you throw an exception to indicate an error. So the gateway methods should throw exceptions rather
than return error codes.
The full range of possible errors is something that we will naturally ignore. Instead I'll focus on just two:
sending a message with an unknown message type, and sending a message where one of the arguments
is null. The return codes are defined in the messaging system's interface.
The two errors have a significant difference. The unknown message type error indicates an error in the
gateway class, since any client is only calling a fully explicit method, clients should never generate this
error. Clients might pass in a null, however, and thus see the null parameter error. The null parameter
error is not a checked exception since it indicates a programmer error - not something that you would
write a specific handler for. The gateway could actually check for nulls itself, but if the messaging system
is going to raise the same error it probably isn't worth it.
As a result the gateway has to do both the translation from the explicit interface to the generic interface,
and translate the return codes into exceptions.
So far, it's hard to see the point of the doSend method. It's there for another key role for a gateway -
testing. We can test objects that use the gateway without the message sending service being present. To
do this we need to create a message gateway stub. In this case the gateway stub is a subclass of the real
gateway and overrides doSend .
Capturing the number of messages sent is a simple way of helping us test that the gateway works
correctly with tests like these.
You usually set up the gateway so classes can find it from a well known place. Here I've used a static
environment interface. Testing code initializes it to use the gateway stub while in regular use you use the
real gateway.
In this case I've used a subclass of the gateway to stub the messaging service. Another route is to
subclass (or re-implement) the service itself. For testing you'd then connect the gateway to the sending
service stub. This works if reimplementation of the service is not too difficult. You always have the
choice of stubbing the service or stubbing the gateway. In some cases it's even useful to stub both of
them, using the stubbed gateway for testing clients of the gateway and the stubbed service to test the
gateway itself.
##%%&&
An in-memory representation of tabular data
In the last twenty years, the dominant way to represent data in a database is the tabular form of relational
database. Backed by database companies big and small, and a fairly standard query language, almost
every new development I see uses relational data.
On top of this came a wealth of tools to help build UI's quickly. These data aware UI frameworks rely
on the fact that the underlying data is relational, and provide UI widgets of various kinds that make it
easy to view and manipulate this data with hardly any programming.
The dark side of these environments, was the fact that while they made display and simple updates
ridiculously easy, they had no real facilities to place any business logic. Any validations beyond the "is
this a valid date", and any business rules or computations have no good place to go. Either they get
jammed into the database as stored procedures or they get mingled with UI code.
The idea of the Record Set is to have your cake and eat it, by providing an in memory structure that
looks exactly like the result of a SQL query, but can be generated and manipulated by other parts of the
system.
A Record Set is usually something that you won't build yourself. Usually it's provided by a vendor of the
software platform you are working with. Examples include the Data Set of ADO.NET and the row set
of JDBC 2.0.
The first essential element of a Record Set is that it looks exactly the same as the result of a database
query. This way you can use the classical two-tier approach of issuing a query and throwing the data
directly into a UI with all the ease that these two-tier tools give you. The second essential element is that
you can easily build a Record Set yourself, or take a Record Set that is the result of a database query
and easily manipulate it with domain logic code.
Although platforms often give you Record Set you can create them yourself. The problem is that there
isn't that much point without the data-aware UI tools, which you would also need to create yourself.
However it's fair to say that the approach of building a Record Set structure as a list of maps, which is
common in dynamically typed scripting languages, is a fair example of Record Set.
The ability to disconnect the Record Set is very valuable. A disconnected Record Set is one that is
separated from it's link to the data source. This allows you to pass the Record Set around a network
without having to worry about database connections. Furthermore if you can then easily serialize the
Record Set it can also act as a Data Transfer Object for an application.
Disconnection raises the question of what happens when if you update the Record Set. Increasingly
platforms are allowing the Record Set to be a form of Unit of Work. In this way you can take the
Record Set, modify it, and then return it to the data source to be committed. A data source can typically
use Optimistic Offline Lock to see if there's any conflicts, and if not write the changes out to the
database.
To my mind the value of Record Set comes from having an environment that relies on it as a common
way of manipulating data. If you have a lot of UI tools that use Record Set, then that's a compelling
reason to use them yourself. If you have such an environment, then that's a big reason to use Table
Module to organize your domain logic. In this style you get a Record Set from the database, pass it to a
Table Module to calculate derived information, pass it to a UI for display and editing, pass it back to a
Table Module for validation, and that commit updates to the database.
In many ways the appearance of the tools that make Record Set so valuable occurred because of the
ever-presence of relational databases and SQL. There hasn't really been an alternative common
structure and query language. But now, of course, there is XML which has a widely standardized
structure and a query language in XPath. Thus I think it's likely that we'll tools appear that use a
hierarchic structure in the same way that tools now use Record Set. In this case this is perhaps really a
particular case of a more generic pattern: something like Generic Data Structure. But I'll leave thinking
about that pattern until then.
##%%&&
A small simple object, like a money or date range, whose equality is not based on identity.
In dealing with object systems of various kinds, I've found it useful to distinguish between reference
objects and Value Objects. Of these two a Value Object is usually a small object, similar to the
primitive types present in many languages that are not purely object-oriented.
Defining the difference between a reference object and Value Object is often a tricky thing. In a broad
sense we like to think of it that Value Object are small objects, such as a Money object or a date; while
reference objects are larger things like orders and customers. Such a definition is handy, but annoyingly
informal.
The key difference between a reference and value object lies in how they deal with equality. A reference
object uses identity as the basis for equality. This may be the identity within the programming system,
such as the built in identity of OO programming languages; or it may be some kind of ID number, such
as the primary key in a relational database.
A Value Object bases its notion of equality on field values within the class. So two date objects may be
the same if their day, month, and year values are the same.
The difference manifests itself in how you deal with them. Since Value Objects are small and easily
created, they are often passed around by value instead of by reference. You don't really care about how
many March 18 2001 objects there are in your system. Nor do you care if two objects share the same
physical date object, or whether they have different yet equal copies.
Most languages don't have any special facility for value objects. In these cases for value objects to work
properly it's a very good idea to make any Value Object immutable. That is once created, none of its
fields should change. The reason for this is to avoid aliasing bugs. An aliasing bug occurs when two
objects share the same value object and one of the owners changes the values in the value. So if Martin
has a hire date of Mar 18 and we know Cindy was hired on the same day, we may set Cindy's hire date
to be the same as Martin's. If Martin then changes the month in his hire date to May, Cindy's hire date
changes too. Whether it's correct or not, it isn't what people would expect. Usually with small values like
this people would expect to change a hire date by replacing the existing date object with a new object.
Making Value Objects immutable fulfills that expectation.
A pleasant exception to this is .NET which has a first class treatment of Value Object. In C# objects
are marked as Value Object by declaring them as a struct instead as a class.
Value Objects shouldn't be persisted as complete records. Instead use Embedded Value or Serialized
LOB. Since Value Objects are small Embedded Value is usually the best choice, since it also allows
SQL querying using the data in the Value Object.
If you are doing a lot of binary serializing you may find that optimizing the serialization of Value Objects
can have a big impact on improving performance, particularly in languages like Java that don't have
special treatment for Value Objects.
For an example of a Value Object see Money.
Treat something as a value object when you are basing equality on something other than an identity. It's
worth considering this for any small object that is easy to create.
##%%&&
Represents a monetary value
A pretty large proportion of computers in this world manipulate money. As such it's always puzzled me
that Money isn't actually a first class data type in any mainstream programming language. That lack of a
type causes problems. The most obvious problem surrounds currencies. If all your calculations are done
in a single currency, then this isn't a huge problem, but once you involve multiple currencies then you
wasn't avoid adding your dollars to your yen without taking the currencies into account. The more subtle
problem is with rounding. Monetary calculations are often rounded to the smallest currency unit. When
you do this it's easy to lose pennies (or your local equivalent) due to rounding errors.
The good thing about object-oriented programming is that you can fix these problems by creating a
Money class that handles these issues. Of course it's still surprising that non of the mainstream base class
libraries actually do this
The basic idea is to have a Money class with fields for the numeric amount and the currency. You can
store the amount as either an integral type or a fixed decimal type. The decimal type is easier for some
manipulations and the integral type for others. You should absolutely avoid any kind of floating point
type, as that will introduce the kind of rounding problems that Money is intended to avoid. Most of the
time people want monetary values rounded to the smallest complete unit, such as cents in the dollar.
There are times, however, when fractional money units are needed. It's important to make it clear what
kind of money you are working with, especially in an application where you use both kinds. It makes
sense to have two different types for the two cases as they behave quite differently under arithmetic.
Money is a Value Object, so it should have its equality and hash code operations overridden to be
based on the currency and amount.
Money needs arithmetic operations so that you can use money objects as easily as you use numbers. But
arithmetic operations for money have some important differences to money operations in numbers. Most
obviously any addition or subtraction needs to be currency aware, so you can react if you try to add
together monies of different currencies. The simplest, and most common, response is to treat adding of
disparate currencies as an error. In some more sophisticated situations you can use Ward Cunningham's
idea of a Money Bag. (A Money Bag is an object that contains monies of multiple currencies together in
one object. This object can then participate in calculations just like any money object. You can also
value a money bag into a currency.)
Multiplication and division end up being more complicated due to rounding problems. When you multiply
money you'll do it with a scalar. If you want to add 5% tax to a bill you multiply by 0.05. So you'll see
multiplication by regular numeric types.
The awkward complication comes with rounding, particularly when allocating money between different
places. Here's Matt Foemmel's simple conundrum. Suppose I have a business rule that says that I have
to allocate the whole amount of a sum of money to two accounts: 70% to one and 30% to another. I
have 5 cents to allocate. If I do the math I end up with 3.5 cents and 1.5 cents. Whichever way I round
these I get into trouble. If I do the usual rounding to nearest then 1.5 becomes 2 and 3.5 becomes 4. So
I end up gaining a penny. Rounding down gives me 4 cents and rounding up gives me six cents. There's
no general rounding scheme I can apply to both that will avoid losing or gaining a penny.
I've seen various approaches to this.
??Perhaps the most common is to ignore the problem, after all it's only a penny here and there.
However this tends to make accountants understandably nervous.
??The simplest rule to follow is that when you are allocating you always do the last allocation by
subtracting from what you've allocated so far. While this avoids losing pennies you can get a
cumulative amount of pennies on the last allocation.
??You can allow the users of a money class to declare the rounding scheme when they call the
method. This would allow a programmer to say that the 70% case rounds up and the 30%
rounds down. This can get more complicated when you are allocating across ten accounts
instead of two. You also have to remember to do this. To help encourage people to remember
I've seen some money classes force a rounding parameter into the multiply operation. Not just
does this force the programmer to think about what rounding they need, it also might remind
people about what tests to write. But this gets messy if you have a lot tax calculations that all
round the same way.
??My favorite solution is to have an allocator function on the Money. The parameter to the
allocator is a list of numbers, representing the ratio to be allocated. (so it would look something
like aMoney.allocate([7,3])). It then returns a list of monies. The allocator guarantees no pennies
get dropped by scattering pennies across the allocated monies in a way that looks
pseudo-random from the outside. The allocator is my favorite but has faults: you have to
remember to use it and if you have precise rules about where the pennies go they are difficult to
enforce.
The fundamental issue here is between using multiplication to determine proportional charge (such as a
tax charge) and using multiplication to allocate a sum of money across multiple places. Multiplication
works well for the former, but an allocator works better for the latter. The important thing is to consider
your intent whenever you want to use multiplication or division on a monetary value.
You may want to convert from one currency to another with a method along the line of
aMoney.convertTo(Currency.DOLLARS). The obvious way to do this is to look up an exchange rate
and to multiply by it. While this works in many situations, there are cases where it doesn't - again due to
rounding. The Euro conversion rules between the fixed Euro currencies had specific roundings applied
that made simple multiplication not work. As a result it's wise to have a convertor object to encapsulate
the algorithm.
Comparison operations allow you to sort monies. Like the addition operation, conversions need to be
currency aware. You can either choose to throw an exception if you compare different currencies, or
you can do a conversion.
One of the most useful results of using a money object is that it can encapsulate the printing behavior.
This makes it much easier to provide good display on user interfaces and reports. A money class can
also parse a string to provide a currency aware input mechanism - which again is very useful for the user
interface.
Storing a money in a database always raises a question, since databases also don't seem to understand
that money is important (although their vendors do.) The obvious route to take is to use Embedded
Value. This results in storing a currency for every money. This can be overkill, for instance an account
may have all its entries be in pounds. In this case you may store the currency on the account and alter
the database mapping to pull the account's currency whenever you load entries.
I pretty much use Money for all numeric calculation in object-oriented environments. The primary reason
is to encapsulate the handling of rounding behavior, which helps reduce the problems of rounding errors.
Another reason to use Money is to make multi-currency work much easier. The most common objection
to using Money is performance, although I've only rarely heard of cases where using Money makes any
noticeable difference - and even then the encapsulation often makes tuning easier.
The first decision is what data type to use for the amount. If anyone needs convincing that a floating point
number is a bad idea, ask them to run this code.
With floats safely disposed of, the choice lies between fixed point decimals and integers. In Java this
boils down to BigDecimal, BigInteger and long. Using an integral value actually makes the internal math
easier, and if we use long we get to use primitives and thus have readable math expressions.
Since I'm using an integral amount, the integral is the amount of the smallest base unit, which I refer to as
cents in the code since that is as good a name as any. By using a long we will get an overflow error if the
number gets too big. If you give us $92,233,720,368,547,758.09 we'll write you a version that uses
BigInteger.
It's useful to provide constructors from various numeric types
Different currencies have different fractional amounts. Java's 1.4 version has a currency class that will tell
you the number of fractional digits in a class. We could determine how many minor units there are in a
major unit by raising ten to the power, but that's such a pain to do in Java that the array is easier (and
probably quicker). We're prepared to live with the fact that this code breaks if someone uses four
fractional digits.
Although most of the time you'll want to use money operation directly, there are occasions when you'll
need access to the underlying data
You should always question yourself if you find yourself using the accessors, almost always there will be
a better way that won't break encapsulation. One example that I couldn't avoid was database mapping,
as in Embedded Value
If you use one currency very frequently for literal amounts, then it can be useful to provide a helper
constructor.
As Money is a Value Object you'll need to define equals..
And wherever there's an equals there should be a hash
We'll start going through arithmetic with addition and subtraction.
Note the use of a private factory method here that doesn't do the usual conversion into the cent-based
amount. We'll use that a few times inside the Money code itself.
With addition defined, subtraction is easy.
The base method for comparison is compare
Although that's all you get on most Java classes these days, we find code is more readable with the
others such as these.
Now we feel ready to look at multiplication. We're providing a default rounding mode as well as
allowing you to set one yourself.
If you want to allocate a sum of money amongst many targets, and you don't want to lose cents, then
you'll want an allocation method. The simplest one allocates the same amount (almost) amongst a
number of targets.
A more sophisticated allocation algorithm can handle any ratio.
##%%&&
A stand-in implementation of an external service
Business systems are quite often dependent upon access to third party services such as credit scoring,
tax rate lookups, or a pricing engine. These services are often not only third party, but also provided by
a remote or external resource. Any developer who has built such an application can speak to the
frustration of being dependent upon resources completely out of his control. Feature delivery, reliability,
and performance are all unpredictable.
At the very least such issues will slow the development process. Developers will sit around waiting for
the service to come back on line or maybe put some hacks into the code to compensate for yet to be
delivered features. Much worse, and quite likely, such dependencies will lead to periods in time when
tests cannot execute. When tests cannot run the development process is broken.
What is needed is a stand-in implementation of this service. One whose development is completely
within the control of the application team and one that runs locally, fast, and in-memory. This stand-in
code is often referred to as a Service Stub.
The first step is to define access to this service with an interface. If your service provider has already
provided an interface for your platform be sure to read the note at the bottom of this section on interface
design. Once the interface is defined and an implementation or two has been coded it's simply a matter
of using the Plugin pattern to provide the proper implementations of services to your application at
runtime.
As with most all design the key principal to stub implementation is keeping it simple. Since the primary
reason behind writing a stub is to speed and ease the development process, writing a complex stub
defeats its own purpose.
Let's walk through the thought process of stubbing a sales tax service. Code examples are provided
below. This service provides state sales tax amount and rate, given an address, product type, and sales
amount. The first question to ask when stubbing is 'What's the simplest possible means of providing this
service?' The answer is to write a 2 or 3 line implementation that uses a flat tax rate for all requests.
But tax laws aren't that simple. Certain products are exempt from taxation in certain states. As such rules
are part of tax law we're dependent upon our tax service for such information. And we have a lot of
functionality that is dependent upon whether taxes are charged so we need to accommodate the
possibility of tax exemption in our service. Now ask what is the simplest means of adding this to the
stub? It's probably to add an if statement and simply for a specific combination of address and product.
The number of lines of code in our stub can probably still be counted on one hand.
If you're building one of those fantastically large back office applications with 10,000+ functions points
and as many unit tests you might even add some more dynamic functionality to your stub. We might wish
to add a method to the service that allows tests to setup exemptions for themselves. We could even
provide reset functionality for test cleanup.
Even with the addition of dynamic functionality the stub is incredibly simple. We've only added a map
and some simple lookup code. If you've ever dealt with state and local sales tax laws you know they are
beyond complicated. A switch to a flat tax would indeed put tens of thousands of accountants and
lawyers out of work! Keep the stub simple enough so that it doesn't break under its own weight. If the
stub is too simple to possibly work it's probably a good stub. Always remember that the primary
purpose of Service Stub is to speed up development.
The last, more dynamic stub above brings up an interesting question regarding the dependency between
test cases and a Service Stub. The stub relies upon a method for adding exemptions that is not in the
service interface. This implies that the test case has some prior knowledge as to the type of the tax
service that will be loading during its execution. This defeats the purpose of using the Plugin pattern to
load the stub implementation.
Two solutions come to mind: The first is to add the test methods to the public interface. Throw
assertions or system exceptions in the production implementation of these methods. If you're writing to a
3rd party interface you will need to extend the interface.
Another alternative is possible if your tests cases don't require that the stub be configured and reset
between each case. Simply have the stub configure itself. If a developer adds a test case she must make
sure that all of the available stubs can provide appropriate behavior for this test case. This avoids
compilation dependencies and seems a reasonable compromise to the logical dependency problem.
There is no way around the fact that some coordination is required between your stubs and your tests.
A note on interface definition:
Perhaps the service provider supplies a well designed SDK for your particular platform and this work
has already been done for you. Otherwise you'll have to do it yourself. I've seen instances where the
application developers might someday -- 'in theory' -- switch vendors for a particular service, therefore
despite the existence of a perfectly good interface supplied by a vendor the application team goes ahead
and defines its own interface. The application is providing its own definition of the service. This is
over-design. It's too much work and results in unreadable code and a debugging nightmare. Writing to
the vendor interface is clean enough. A switch in vendors, if the services are in fact similar, will require
merely a simple refactoring of the code that calls that interface.
Deciding to use Service Stub is a simple task. Ask yourself these questions to determine whether to use
Service Stub
??Is my dependence upon this remote or third party service slowing down my development
process?
??Does this service make testing laborious?
??Can I write an extremely simple stand-in implementation of this service that would suffice for
testing purposes?
If you find yourself answering yes by all means go ahead and write the stub. Just never forget that a stub
is meant to make your life easier rather than more difficult.
Our sales tax service is being used by the A/R subsystem of a lease management system. Within this
system billing schedules store information as to when to generate charges for each leased asset. Each
night a charge generation process inspects each billing schedule and generates appropriate charges. For
each charge a call to the tax service is made in order to generate any tax charges.
What is the point of such a long winded description of a leasing A/R subsystem? It's that a Service Stub
makes no sense unless you're actually testing functionality that is dependent upon that service. Here we
have a charge generation service that is creating extra charges if taxes ought to be charged. Our A/R
subsystem has lots of behavior dependent upon whether tax charges were created. But our tax system is
a web service and we're having massive downtime problems. Sounds like an ideal candidate for a stub.
Since we've decided to use a tax service that has been deployed as a web service the first thing we need
to do is write a Java interface to define the service. While we've been given a nice WSDL interface that's
not something we want to write to from our domain layer so we'll define our own Java interface:
Note the use of Plugin to load the service implementation.
The application code that we're working on here is charge generation and we want to test that the
generator will return a single charge if a product is exempt from taxation and an additional state sales tax
charge if the product is not exempt. We also want to test the returned charges are of the correct type.
So let's write our test:
Given these tests we can use the 'middle of the road' stub approach from above and write a service
implementation that looks for a particular product and location to determine exemption status:
As we discussed above there are a few trade-offs to think about when approaching the test case to stub
dependency issue. Here we have values hard coded in both the test case and the stub. There's no way
around this when testing so just choose a sensible approach.
##%%&&
A type that acts as the supertype for all types in its layer
Often all the objects in a layer will have some common methods that you don't want to have duplicated
throughout the system. So you can move all of this behavior into a common Layer Supertype.
Layer Supertype is a simple idea that leads to a very short pattern. All you need is a superclass for all
the objects in a layer. For example all the domain objects in a Domain Model might have a superclass
called Domain Object. Common features, such as the storage and handling of Identity Fields can go
there. Similarly all Data Mappers in the mapping layer may have a superclass which can rely on the fact
that all the domain objects have a common superclass.
If you have more than one kind of object in a layer, then it's useful to have more than one Layer
Supertype.
Use Layer Supertype when you have common features from all objects in a layer. Often I do this
automatically now, because I so often make use of common features.
Domain objects can have a common superclass for id handling.
##%%&&
A subclass which provides special behavior for particular cases.
Null's make awkward things in object-oriented programs because they defeat polymorphism. Usually
you can invoke foo freely on a variable reference of a given type without worrying about whether the
item in the type is of the exact type or a subclass. With a strongly typed language you can even have the
compiler check that the call is correct. However since a variable can contain null, you may run into a
runtime error by invoking a message on null, which will get you a nice friendly stack trace.
If it's possible for a variable to be null, you have to remember to surround it with null test code so you
remember do the right thing if a null is present. Often the right thing is same in lots of contexts, so you
end up writing similar code in lots of places - which is the sin of duplicate code.
Nulls are a common problem of this kind, but others crop up regularly. In number systems you have to
deal with infinity, which has special rules for things like addition that break the usual invariants of real
numbers. One of my earliest examples in business software was a utility customer who wasn't fully
known - referred to as "occupant". All of these imply altered behavior to the usual behavior of the type.
Instead of returning null, or some odd value, return a Special Case that has the same interface as what
the caller expects.
The basic idea is to create a subclass to handle the Special Case. So if you have a customer object and
you want to avoid null checks, you make a null customer object. Take all of the methods for customer
and override them in the Special Case to provide some harmless behavior. Then whenever you would
have a null put in an instance of null customer instead.
There's usually not any reason to distinguish between different instances of null customer, so you can
often implement a Special Case with a flyweight [Gang of Four]. This isn't true all the time. For a utility
you can accumulate charges against an occupant customer even you can't do much of the billing, so it's
important to keep your occupants separate.
People often use a null to mean different things. A null customer may mean there isn't a customer or it
may mean that there is a customer but we don't know who it is. Rather than just use a null customer,
consider having separate Special Cases for missing customer and unknown customer.
A common way for a Special Case to override methods is to return another Special Case. So if you
ask an unknown customer for his last bill, you may well get an unknown bill.
Use Special Case whenever you have multiple places in the system that do the same behavior after a
conditional check for a particular instance of this class, or same behavior after a null check.
I haven't seen Special Case written up as a pattern yet, but Null Object has been written up in [Woolf].
If you'll pardon the unresistable punnery I see Null Object as special case of Special Case.
Here's a simple example of using Special Case for its common use as a null object.
We have a regular employee.
The features of the class could be overridden by a null employee
Notice how when you ask a null employee for its contract you get a null contract back.
The default values here avoid a lot of null tests if the null tests end up with the same null values. The
repeated null values are handled by the null object by default. You can also test for nullness explicitly
either by giving customer an isNull method or by using a type test for a marker interface.
##%%&&
A well known object that other objects can use to find common objects and services
Most of the movement between objects is done by following references in fields. If you want to find all
the orders for a customer, you'll most often start with the customer object and use a method on the
customer to get the orders. However in some cases you won't have an appropriate object to start with.
You may know the customer's id number, but not have a reference to the customer. In this case you
need to you use some kind of lookup method - a finder. But the question just continues to ask itself -
how do you get to the finder?
A Registry is essentially some kind of global object, or at least it looks like a global object - even if it
isn't as global as it may appear.
As with any object, you have to think about the design of a Registry in terms of interface and
implementation. And like many objects the two are quite different while people often make the mistake
of thinking they should be the same.
The first thing to think of is the interface, and for Registrys my preferred interface is static methods. A
static method on a class is easy to find anywhere in an application. Furthermore you can encapsulate any
logic you like within the static method, including delegation to other methods which can either be static or
instance methods.
However just because your methods are static does not mean your data should be in static fields. Indeed
I almost never use static fields unless they are constants.
Before you decide on how to hold your data, you have to think about the scope of the data. The data
for a Registry can vary with different execution contexts. Some data is global across an entire process,
some global across a thread, some global across a session. Different scopes call for different
implementations, however different scopes don't call for different interfaces. The application programmer
need not know whether a call to a static method yields process scoped or thread scoped data. You can
have different Registrys for different scopes, but you can also have a single Registry where different
methods are at different scopes.
If your data is common to a whole process, then a static field is an option. However I rarely use static
mutable data because they don't allow for substitution. It is often extremely useful to be able to substitute
a Registry for a particular purpose, especially for testing (Plugin is a good way to do this.)
So for process scoped Registry the usual option is a singleton[Gang of Four]. The Registry class
contains a single static field that holds an instance of the Registry. Often when people use a singleton
they make the caller of the singleton explicitly access the underlying data
(Registry.soleInstance.getFoo()), but I prefer to have a static method that hides the singleton object from
me (Registry.getFoo()). This works particularly well since C based languages allow static methods to
access private instance data.
Singletons are quite widely used in single-threaded applications, but can be quite a problem for
multi-threaded applications. This is because it's too easy to have multiple threads manipulating the same
object in unpredictable ways. You may be able to solve this with synchronization, but the difficulty of
writing the synchronization code is likely to drive you into an insane asylum before you get all the bugs
out. So I don't recommend using a singleton for mutable data in a multi-threaded environment. A
singleton does work well for immutable data, anything that can't change isn't going to run into thread
clash problems. So something like a list of all states in the US makes a good candidate for a process
scoped Registry. Such data can be loaded when a process starts up and then never needs changing, or
may be updated rarely with some kind of process interrupt.
A common kind of Registry data is thread scoped data. A good example of this might be a database
connection. In this case many environments give you some form of thread specific storage that you can
use, such as Java's thread local. Another technique is to have a dictionary keyed by thread whose value
is an appropriate data object. A request for a connection would then result in a lookup in that dictionary
by the current thread.
The important thing to remember about using thread-scoped data is that it looks no different from using
process-scoped data. I can still use a method such as Registry.getDbConnection(), which is just the
same form as if I'm accessing process-scoped data.
A dictionary lookup is also a technique that you can use for session-scoped data. Here you need a
session id, but this could be put into a thread scoped registry when a request begins. Any subsequent
accesses for session data can then look the data up in a map that's keyed by session using the session id
that's held in thread specific storage.
Some applications may have a single Registry, some may have several. Registrys are usually divided up
by system layer, or by execution context. My preference is to divide them up by how they are used,
rather than how they are implemented.
Despite the encapsulation of a method a Registry is still global data, and as such is something I'm
uncomfortable using. I find I almost always see some form of Registry in an application, but I always try
to access objects through regular inter-object references rather than resorting to the Registry. So you
should always use a Registry as a means of last resort.
There are alternatives to using a Registry at all. One alternative is to pass around any widely needed
data in parameters. The problem with this is that it leads to parameters that are added to method calls
where they aren't needed by the called method, only needed for some other method that's called several
layers deep in the call tree. Passing a parameter round when it's not needed 90% of the time is the
trigger that leads me to use a Registry instead.
Another alternative I've seen to a Registry is to add a reference to the common data to objects when
they are created. Although this leads to an extra parameter in a constructor, at least it's only used by the
constructor. It's still more trouble than it's often worth, but if you have data that's only used by a subset
of classes, this technique does allow you to restrict things that way.
One of the problems with a Registry is that every time you add a new piece of data you have to modify
the Registry. This leads to some people preferring to use a map as their holder of global data. I prefer
the explicit class because it keeps the methods explicit - so there's no confusion about what key you use
to find something. With an explicit class you can just look at the source code or generated
documentation to see what's available, with a map you have to find places in the system where data is
read or written to the map to find out what key is used - or rely on documentation which quickly
becomes stale. An explicit class also allows you to keep type safety in a statically typed language as well
as to encapsulate the structure of the Registry, which allows you to refactor it as the system grows. A
bare map also is unencapsulated, which makes it harder to hide the implementation, which is particularly
awkward if you have to change the execution scope of the data.
So there are times when it's right to use a Registry - but remember that any global data is always guilty
until proven innocent.
Consider an application that reads data from a database and then munges on the data to turn it into
information. Well imagine a fairly simple system that uses Row Data Gateways to access the data. With
such a system it's useful to have finder objects to encapsulate the database queries. These finders are
best made as instances because that way we can substitute them to make a Service Stub for testing
purposes. To get hold of the finders, we'll need a place to put them: a Registry is the obvious choice.
A singleton registry is a very simple example of the singleton pattern [Gang of Four]. You have a static
variable for the single instance.
Everything that's stored on the registry is stored on the instance.
To make access easier, however, I make the public methods static.
I can re-initialize the registry by simply creating a new sole instance.
If I want to use Service Stubs for testing, I use a subclass instead.
The finder Service Stub just returns hard coded instances of the person Row Data Gateway
I put a method on the registry to initialize it in stub mode, but by keeping all the stub behavior in the
subclass I can separate all the code required for testing.
The simple example above won't work for a multi-threaded application where different threads need
their own registry. Java provides Thread Specific Storage [Schmidt] - variables that are local to a
thread, helpfully called thread local variables. You can use these to create a registry that's unique for a
thread.
The Registry needs to be set up with methods to acquire and release the registry. You would typically
do this on a transaction or session call boundary.
We can then store person finders just as before.
Calls from the outside then wrap their use of a registry in the begin and end methods.
##%%&&
The Layers architectural pattern helps to structure applications that
can be decomposed into groups of subtasks in which each group of
subtasks is at a particular level of abstraction.
Example Networking protocols are probably the best-known example of layered
architectures. Such a protocol consists of a set of rules and conventions that describe how computer programs communicate across
machine boundaries. The format, contents, and meaning of all
messages are defined. All scenarios are described in detail, usually by
giving sequence charts. The protocol specifies agreements at a variety
of abstraction levels, ranging from the details of bit transmission to
high-level application logic. Therefore designers use several sub protocols and arrange them in layers. Each layer deals with a specific
aspect of communication and uses the services of the next lower
layer. The International Standardization Organization (ISO) defined
the following architectural model, the OSI 7-Layer Model Pan921:
Layer 7 Provides miscellaneous protocols
for common activities
Layer 6 Structures information
and attaches semantics
Layer 5 Provides dialog control and
synchronization facilities
Layer 4 Breaks messages into packets
and guarantees delivery
Layer 3 Selects a route
from sender to receiver
Layer 2 Detects and corrects errors
in bit sequences
Layer 1 Transmits bits: velocity.
bit-code, connection, etc.
Architectural Patterns
A layered approach is considered better practice than implementing
the protocol as a monolithic block. since implementing conceptually different issues separately reaps several benefits, for example aiding
development by teams and supporting incremental coding and
testing. Using semi-independent parts also enables the easier exchange of individual parts at a later date. Better implementation
technologies such as new languages or algorithms can be incorporated by simply rewriting a delimited section of code.
While OSI is an important reference model, TCP/IP, also known as the
'Internet protocol suite', is the prevalent networking protocol. We use
TCP/IP to illustrate another important reason for layering: the reuse
of individual layers in different contexts. TCP for example can be used
'as is' by diverse distributed applications such as telnet or ftp.
Context A large system that requires decomposition.
Problem Imagine that you are designing a system whose dominant
characteristic is a mix of low- and high-level issues, where high-level
operations rely on the lower-level ones. Some parts of the system
handle low-level issues such as hardware traps, sensor input,
reading bits from a file or electrical signals from a wire. At the other
end of the spectrum there may be user-visible functionality such as
the interface of a multi-user 'dungeon' game or high-level policies
such as telephone billing tariffs. A typical pattern of communication
flow consists of requests moving from high to low level, and answers
to requests, incoming data or notification about events traveling in
the opposite direction.
Such systems often also require some horizontal structuring that is
orthogonal to their vertical subdivision. This is the case where several
operations are on the same level of abstraction but are largely in dependent of each other. You can see examples of this where the word
'and' occurs in the diagram illustrating the OS1 7-layer model.
The system specification provided to you describes the high-level
tasks to some extent, and specifies the target platform. Portability to
other platforms is desired. Several external boundaries of the system
are specified a priori, such as a functional interface to which your
system must adhere. The mapping of high-level tasks onto the platform is not straightforward, mostly because they are too complex to
be implemented directly using services provided by the platform.
In such a case you need to balance the following forces:
Late source code changes should not ripple through the system.
They should be confined to one component and not affect others.
Interfaces should be stable. and may even be prescribed by a standards body.
Parts of the system should be exchangeable. Components should
be able to be replaced by alternative implementations without
affecting the rest of the system. A low-level platform may be given
but may be subject to change in the future. While such fundamental changes usually require code changes and recompilation.
reconfiguration of the system can also be done a t run-time using
an administration interface. Adjusting cache or buffer sizes are
examples of such a change. An extreme form of exchangeability
might be a client component dynamically switching to a different
implementation of a service that may not have been available at
start-up. Design for change in general is a major facilitator of
graceful system evolution.
It may be necessary to build other systems at a later date with the
same low-level issues a s the system you are currently designing.
Similar responsibilities should be grouped to help understand ability and maintainability. Each component should be coherent if one component implements divergent issues its integrity may be
lost. Coupling and coherence are conflicting at times.
There is no 'standard' component granularity.
Complex components need further decomposition.
Crossing component boundaries may impede performance, for
example when a substantial amount of data must be transferred
over several boundaries, or where there are many boundaries to
cross.
The system will be built by a team of programmers, and work has
to be subdivided along clear boundaries-a requirement that is
often overlooked a t the architectural design stage.
Solution From a high-level viewpoint the solution is extremely simple.
Structure your system into an appropriate number of layers and
place them on top of each other. Start at the lowest level of
abstraction-call it Layer 1. This is the base of your system. Work
your way up the abstraction ladder by putting Layer J on top of Layer
J-1until you reach the top level of functionality-call it Layer N.
Note that this does not prescribe the order in which to actually design
layers, it just gives a conceptual view. It also does not prescribe
whether an individual Layer J should be a complex subsystem that
needs further decomposition, or whether it should just translate
requests from Layer J + l to requests to Layer J - 1 and make little
contribution of its own. It is however essential that within an individual layer all constituent components work at the same level of
abstraction.
Most of the services that Layer J provides are composed of services
provided by Layer J -1. In other words, the services of each layer
implement a strategy for combining the services of the layer below in
a meaningful way. In addition, LayerJ's services may depend on other
services in Layer J.
Structure An individual layer can be described by the following CRC card:
The main structural characteristic of the Layers pattern is that the
services of Layer J are only used by Layer J+1-there are no further
direct dependencies between layers. This structure can be compared
with a stack. or even an onion. Each individual layer shields all lower
layers from direct access by higher layers. highest level of abstract "on
Layer N-1
Layer 1 lowest level of abstraction
Examining individual layers in more detail may reveal that they are
complex entities consisting of different components. In the following
figure, each layer consists of three components. In the middle layer
two components interact. Components in different layers call each
other directly-other designs shield each layer by Incorporating a
unified interface. In such a design, component-2.1 no longer calls
component-1.1 directly, but calls a Layer 1 interface object that
forwards the request instead. In the Implementation section, we
discuss the advantages and disadvantages of direct addressing.
Dynamics The following scenarios are archetypes for the dynamic behavior of
layered applicators. This does not mean that you will encounter
every scenario in every architecture. In simple layered architectures
you will only see the first scenario. but most layered applications
involve Scenarios I and 11. Due to space limitations we do not give
object message sequence charts in this pattern.
Scenario I Is probably the best-known one. A client Issues a request
to Layer N. Since Layer N cannot carry out the request on its own. it
calls the next Layer N - 1 for supporting subtasks. Layer N- I provides
these. In the process sending further requests to Layer N-2. and so
on until Layer I 1sreached. Here, the lowest-level services are finally
performed. If necessary, replies to the different requests are passed
back up from Layer 1to Layer 2, from Layer 2 to Layer 3, and so on
until the final reply arrives at Layer N. The example code in the
Implementation secUon illustrates this.
A characteristic of such top-down communication Is that Layer J
often translates a single request from Layer J+1 Into several requests
to Layer J - 1. This is due to the fact that Layer J is on a higher level of
abstraction than Layer J-1 and has to map a high-level service onto
more prlmlUve ones.
ScenarioII illustrates bottom-up communication-a chain of actions
starts at Layer 1, for example when a device driver detects input. The
driver translates the input into an Internal format and reports it to
Layer 2. which starts interpreting it, and so on. In this way data
moves up through the layers until It arrives at the highest layer. While
top-down information and control flow are often described as
'requests'. bottom-up calls can be termed 'notifications'.
As mentioned in Scenario I, one top-down request often fans out to
several requests in lower layers. In contrast. several bottom-up nonfictions may either be condensed into a single notification higher In
the structure. or remain in a I :I relationship.
Scenario III describes the situation where requests only travel
through a subset of the layers. A top-level request may only go to the
next lower level N- 1 lf this level can satisfy the request. An example
of this is where level N- 1acts as a cache. and a request from level N
can be satisfied without being sent all the way down to Layer 1 and
from here to a remote server. Note that such caching layers maintain
state information, while layers that only forward requests are often
stateless. Stateless layers usually have the advantage of being
simpler to program, particularly with respect to re-entrance.
Scenario N describes a situation similar to Scenario 111. An event is
detected in Layer 1, but stops at Layer 3 instead of traveling all the
way up to Layer N. In a communication protocol, for example, a resend request may arrive from an impatient client who requested data
some time ago. In the meantime the server has already sent the
answer, and the answer and the re-send request cross. In this case,
Layer 3 of the server side may notice this and intercept the re-send
request without further action.
Scenario V involves two stacks of N layers communicating with each
other. This scenario is well-known from communication protocols
where the stacks are known a s 'protocol stacks'. In the following
diagram, Layer N of the left stack issues a request. 'The request moves
down through the layers until it reaches Layer 1, is sent to Layer 1of
the right stack, and there moves up through the layers of the right
stack. The response to the request follows the reverse path until it
arrives a t Layer N of the left stack.
For more details about protocol stacks, see the Example Resolved
section, where we discuss several communication protocol issues
using TCP/IP a s an example.
Implementation The following steps describe a step-wise refinement approach to the
definition of a layered architecture. This is not necessarily the best
method for all applications-often a bottom-up or 'yo-yo' approach is
better. See also the discussion in step 5.
Not all the following steps are mandatory-it depends on your
application. For example, the results of several implementation steps
can be heavily influenced or even strictly prescribed by a standards
specification that must be followed.
1 Define the abstraction criterion for grouping tasks into layers. This
criterion is often the conceptual distance from the platform.
Sometimes you encounter other abstraction paradigms, for example
the degree of customization for specific domains, or the degree of
conceptual complexity. For example, a chess game application may
consist of the following layers, listed from bottom to top:
Elementary units of the game, such as a bishop
Basic moves, such as castling
Medium-term tactics, such as the Sicilian defense
Overall game strategies
In American Football these levels may correspond respectively to
linebacker, blitz, a sequence of plays for a two-minute drill, and finally
a full game plan.
In the real world of software development we often use a mix of
abstraction criterions. For example, the distance from the hardware
can shape the lower levels, and conceptual complexity governs the
higher ones. An example layering obtained using a mixed-mode
layering principle like this is a s follows, ordered from top to bottom:
User-visible elements
Specific application modules
Common services level
Operating system interface level
Operating system (being a layered system itself, or structured
according to the Microkernel pattern (171))
Hardware
Determine the number of abstraction levels according to your
abstraction criterion. Each abstraction level corresponds to one layer
of the pattern. Sometimes this mapping from abstraction levels to
layers is not obvious. Think about the trade-offs when deciding
whether to split particular aspects into two layers or combine them
into one. Having too many layers may impose unnecessary overhead,
while too few layers can result in a poor structure.
Name the layers and assign tasks to each of them The task of the
highest layer is the overall system task, as perceived by the client. The
tasks of all other layers are to be helpers to higher layers. If we take
a bottom-up approach. then lower layers provide an infrastructure on
which higher layers can build. However, this approach requires considerable experience and foresight in the domain to find the right
abstractions for the lower layers before being able to define specific
requests from higher layers.
Specify the services. The most important implementation principle is
that layers are strictly separated from each other, in the sense that
no component may spread over more than one layer. Argument,
return, and error types of functions offered by LayerJ should be built in types of the programming language, types defined in Layer J, or
types taken from a shared data definition module. Note that modules
that are shared between layers relax the principles of strict layering.
I t is often better to locate more services in higher layers than in lower
layers. This is because developers should not have to learn a large set ,
of slightly different low-level primitives--which may even change
during concurrent development. Instead the base layers should be
kept 'slim' while higher layers can expand to cover a broader
spectrum of applicability. This phenomenon is also called the
'inverted pyramid of reuse'.
Refine the layering. Iterate over steps 1to 4. It is usually not possible
to define an abstraction criterion precisely before thinking about the
implied layers and their services. Alternatively, it is usually wrong to
define components and services first and later impose a layered
structure on them according to their usage relationships. Since such
a structure does not capture an inherent ordering principle, it is very
likely that system maintenance will destroy the architecture. For
example. a new component may ask for the services of more than one
other laver. violating the principle of strict levering.
Pattern
The solution is to perform the first four steps several times until a
natural and stable layering evolves. 'Like almost all other kinds of
design, finding layers does not proceed in an orderly, logical way, but
consists of both top-down ahd bottom-up steps, and certain amount
of inspiration.. .' [Joh95]. Performing both top-down and bottom-up
steps alternately is often called 'yo-yo' development, mentioned at the
start of the Implementation section.
specific an interface for each layer. If Layer J should be a 'black box'
r Layer J+1, design a flat interface that offers all Layer J's services,
and perhaps encapsulate this interface in a Facade object [GHJV95].
The Known Uses section describes flat interfaces further. A 'white box' approach is that in which J+1 sees the internals of Layer
J. The last figure in the e section shows a 'gray-box'
approach, a compromise between black and white box approaches.
Here Layer J+1 is aware of the fact that Layer J consists of three
components, and addresses them separately, but does not see the
internal workings of individual components.
Good design practice tells u s to use the black-box approach whenever
possible, because it supports system evolution better than other
approaches. Exceptions to this rule can be made for reasons of
efficiency, or a need to access the innards of another layer. The latter
occurs rarely, and may be helped by the Reflection pattern (193).
which supports more controlled access to the internal functioning of
a component. Arguments over efficiency are debatable, especially
when inclining can simply do away with a thin layer of indirection.
structure individual layers. Traditionally, the focus was on the proper
relationships between layers, but inside individual layers there was
often free-wheeling chaos. When an individual layer is complex it
should be broken into separate components. This subdivision can be
helped by using finer-grained patterns. For example, you can use the
Bridge pattern [GHJV95] to support multiple implementations of
services provided by a layer. The Strategy pattern [GHJV95] can
support the dynamic exchange of algorithms used by a layer.
8 Speech the communication between adjacent layers. The most often
used mechanism for inter-layer communication is the push model.
When LayerJ invokes a service of LayerJ-1,any required information
is passed as part of the service call. The reverse is known as the pull
model and occurs when the lower layer fetches available information
Layers
from the higher layer at its own discretion. The Publisher-Subscriber
(339) and Pipes and Filters patterns (53)give details about push and
pull model information transfer. However, such models may intro duce additional dependencies between a layer and its adjacent higher
layer. If you want to avoid dependencies of lower layers on higher
layers introduced by the pull model, use callbacks, a s described in
the next step.
9 Decouple adjacent layers. There are many ways to do this. Often an
upper layer is aware of the next lower layer, but the lower layer is
unaware of the identity of its users. This implies a one-way coupling
only: changes in Layer J can ignore the presence and identity of Layer
J+1 provided that the interface and semantics of the Layer J services
being changed remain stable. Such a one-way coupling is perfect
when requests travel top-down, as illustrated in Scenario 1,as return
values are sufficient to transport the results in the reverse direction.
For bottom-up communication, you can use callbacks and still
preserve a top-down one-way coupling. Here the upper layer registers
callback functions with the lower layer. This is especially effective
when only a fixed set of possible events is sent from lower to higher
layers. During start-up the higher layer tells the lower layer what
functions to call when specific events occur. The lower layer
maintains the mapping from events to callback functions in a
registry. The Reactor pattern [Sch94] illustrates a n object-oriented
implementation of the use of callbacks in conjunction with event
demultiplexing. The Command pattern [GHJV95] shows how to
encapsulate callback functions into first-class objects.
You can also decouple the upper layer from the lower layer to a certain
degree. Here is an example of how this can be done using object oriented techniques. The upper layer is decoupled from specific
implementation variants of the lower layer by coding the upper layer
against an interface. In the following C++ code, this interface is a base
class; The lower-level implementations can then be easily exchanged.
even at run-time. In the example code, a Layer 2 component talks to
a Level 1 provider but does not know which implementation of Layer
1 it is talking to. The 'wiring' of the layers is done here in the main
program, but will usually be factored out into a connection management component. The main program also takes the role of the
client by calling a service in the top layer.
For communicating stacks of layers where messages travel both up
and down, it is often better explicitly to connect lower levels to higher
levels. We therefore again introduce base classes, for example classes
and additionally Llparent , parent , and LlPeer . Class LlParent
provides the interface by which level 1 classes access the next higher
layer, for example to return results, send confirmations or pass data
streams. An analogous argument holds for L a p a r e n t . LlPeer
provides the interface by which a message is sent to the level 1 peer
module in the other stack. A Layer 1 implementation class therefore
inherits from two base classes: LIProvider and LlPeer. A second level implementation class inherits from LIProvider and LlParent,
as it offers the services of Layer 2 and can serve a s the parent of a
Layer 1 object. A third-level implementation class finally inherits
from L 3 P r o v i d e r and L 2 P a r e n t .
If your programming language separates inheritance and sub typing
at the language level, as for example Sather [Om0931and Java [AG96]
do, the above base classes can be transformed into interfaces by
pushing data into subclasses and implementing all methods there.
10 Design a n error-handling strategy. Error handling can be rather
expensive for layered architectures with respect to processing time
and, notably, programming effort. A n error can either be handled in
the layer where it occurred or be passed to the next higher layer. In
the latter case, the lower layer must transform the error into an error
description meaningful to the higher layer. As a rule of thumb, try to
handle errors at the lowest layer possible. This prevents higher layers
from being swamped with many different errors and voluminous
error-handling code. As a minimum, try to condense similar error
types into more general error types, and only propagate these more
general errors. If you do not do this, higher layers can be confronted
with error messages that apply to lower-level abstractions that the
higher layer does not understand. And who hasn't seen totally cryptic
error messages being popped u p to the highest layer of all-the user?
Architectural Patterns
Example The most widely-used communication protocol, TCP/IP, does not
Resolved strictly conform to the OSI model and consists of only four layers: TCP
and IP constitute the middle layers, with the application at the top
and the transport medium at the bottom. A typical configuration, that
for the UNIX ftp utility, is shown below:
TCP/IP has several interesting aspects that are relevant to our
discussion. Corresponding layers communicate in a peer-to-peer
fashion using a virtual protocol. This means that, for example, the two
TCP entities send each other messages that follow a specific format.
From a conceptual point of view, they communicate using the dashed
line labeled TCP protocol' in the diagram above. We refer to this
protocol a s 'virtual' because in reality a TCP message traveling from
left to right in the diagram is handled first by the IP entity on the left.
This IP entity treats the message as a data packet, prefixes it with a
header, and forwards it to the local Ethernet interface. The Ethernet
interface then adds its own control information and sends the data
over the physical connection. On the receiving side Lhe local Ethernet
and IP entities strip the Ethernet and IP headers respectively. The
TCP entity on the right-hand side of the diagram then receives the
TCP message from its peer on the left as if it had been delivered over
the dashed Line.
A notable characteristic of TCP/IP and other communication proto cols is that standardizing the functional interface is a secondary
concern, partly driven by the fact that TCP/IP implementations from
different vendors differ from each other intentionally. The vendors
usually do not offer single layers, but full implementations of the
protocol suite. As a result, every TCP implementation exports a fixed
Layers
set of core functions but is free to offer more, for example to increase
flexibility or performance. This looseness has no impact on the application developer for two reasons. Firstly, different stacks understand
each other because the virtual protocols are strictly obeyed. Secondly,
application developers use a layer on top of TCP, or its alternative,
UDP. This upper layer has a fixed interface. Sockets and TLI are
examples of such a fixed interface.
Assume that we use the Socket API on top of a TCP/IP stack. The
Socket API consists of system calls such a s bind ( ) , listen ( ) or
read 0.The Socket implementation sits conceptually on top of
TCP/UDP, but uses lower layers a s well, for example IP and ICMP.
This violation of strict layering principles is worthwhile to tune performance, and can be justified when all the communication layers from
sockets to IP are built into the OS kernel.
The behavior of the individual layers and the structure of the data
packets flowing from layer to layer are much more rigidly defined in
TCP/IP than the functional interface. This is because different
TCP/IP stacks must understand each other-they are the workhorses
of the increasingly heterogeneous Internet. The protocol rules de scribe exactly how a layer behaves under specific circumstances. For
example, its behavior when handling an incoming re-transmit message after the original has been sent is exactly prescribed. The data
packet specifications mostly concern the headers and trailers added
to messages. The size of headers and trailers is specified, a s well a s
the meaning of their subfields. In a header, for example, the protocol
stack encodes information such a s sender, destination, protocol
used, time-out information, sequence number, and checksums. For
more information on TCP/IP, see for example ISte901. For even more
detail, study the series started in [Ste94].
Variants Relaxed Layered System This is a variant of the Layers pattern that
is less restrictive about the relationship between layers. In a Relaxed
Layered System each layer may use the services of all layers below it,
not only of the next lower layer. A layer may also be partially opaque this means that some of its services are only visible to the next higher
layer,while others are visible to all higher layers. The gain of flexibility
and performance in a Relaxed Layered System is paid for by a loss of
maintainability. This is often a high price to pay, and you should consider carefully before giving in to the demands of developers asking
Architectural Patterns
for shortcuts. We see these shortcuts more often in infrastructure
systems, such as the UNIX operating system or the X Window System,
than in application software. The main reason for this is that infra structure systems are modified less often than application systems,
and their performance is usually more important than their maintainability.
Layering Through Inheritance. This variant can be found in some
object-oriented systems and is described in [BuCa96]. In this variant
lower layers are implemented as base classes. A higher layer re questing services from a lower layer inherits from the lower layer's
implementation and hence can issue requests to the base class
services. An advantage of this scheme is that higher layers can modify
lower-layer services according to their needs. A drawback is that such
an inheritance relationship closely ties the higher layer to the lower
layer. If for example the data layout of a C++ base class changes, all
subclasses must be recompiled. Such unintentional dependencies
introduced by inheritance are also known as the fragile base class
problem.
Known Uses Virtual Machines. We can speak of lower levels as a virtual machine
that insulates higher levels from low-level details or varying
hardware. For example, the Java Virtual Machine (JVM) defines a
binary code format. Code written in the Java programming language
is translated into a platform-neutral binary code, also called byte codes, and delivered to the JVM for interpretation. The JVM itself is
platform-specific-there are implementations of the JVM for different
operating systems and processors. Such a two-step translation
process allows platform-neutral source code and the delivery of
binary code not readable to humans1, while maintaining platform independency.
APIs. An Application Programming Interface is a layer that encapsulates lower layers of frequently-used functionality. An API is usually
a flat collection of function specifications, such as the UNIX system
calls. 'Flat' means here that the system calls for accessing the UNIX
file system, for example, are not separated from system calls for storage allocation-you can only know from the documentation to which
1. The Java bytecodes can be transformed into an ASCII representation that is a
kind of object-oriented assembler code. This code can be read, but only with some pain!
Layers
group open ( ) or sbrk ( ) belong. Above system calls we find other
layers, such as the C standard library [KR881 with operations like
p r i n t f ( ) or f open ( ) .These libraries provide the benefit of portability between different operating systems, and provide additional
higher-level services such as output buffering or formatted output.
They often carry the liability of lower efficiency2, and perhaps more
tightly-prescribed behavior, whereas conventional system calls would
give more flexibility-and more opportunities for errors and conceptual mismatches, mostly due to the wide gap between high-level
application abstractions and low-level system calls.
Information Systems(IS)from the business software domain often
use a two-layer architecture. The bottom layer is a database that
holds company-specific data. Many applications work concurrently
on top of this database to fulfill different tasks. Mainframe interactive
systems and the much-extolled Client-Server systems often employ
this architecture. Because the tight coupling of user interface and
data representation causes its share of problems, a third layer is
introduced between them-the domain layer-which models the
conceptual structure of the problem domain. As the top level still
mixes user interface and application, this level is also split, resulting
in a four-layer architecture. These are, from highest to lowest:
See [Fow961for more information on business modeling.
Windows NT [Cus93]. This operating system is structured according
to the Microkernel pattern (171). The NT Executive component
corresponds to the microkernel component of the Microkernel
pattern. The NT Executive is a Relaxed Layered System, as described
in the Variants section. It has the following layers:
System services: the interface layer between the subsystems and
the NT Executive.
2. Input/output buffering in Mgher layers is often intended to have the inverse
effect-better performance than undisciplined direct use of lower-level system calls.
@ Resource management layer: this contains the modules Object
Manager, Security Reference Monitor, Process Manager, 1/0
Manager, Virtual Memory Manager and Local Procedure Calls.
@ Kernel: this takes care of basic functions such a s interrupt and exception handling, multiprocessor synchronization, thread scheduling and thread dispatching.
@ HAL (Hardware Abstraction Layer):this hides hardware differences
between machines of different processor families.
@ Hardware
Windows NT relaxes the principles of the Layers pattern because the
Kernel and the 1/0 manager access the underlying hardware directly
for reasons of efficiency.
on sequences The Layers pattern has several
Reuse of layers. If an individual layer embodies a well-defined
abstraction and has a well-defined and documented interface, the
layer can be reused in multiple contexts. However, despite the higher
costs of not reusing such existing layers, developers often prefer to
rewrite this functionality. They argue that the existing layer does not
fit their purposes exactly, layering would cause high performance
penalties-and they would do a better job anyway. An empirical study
hints that black-box reuse of existing layers can dramatically reduce
development effort and decrease the number of defects [ZEWH95].
Support for standardization. Clearly-defined and commonly-accepted
levels of abstraction enable the development of standardized tasks
and interfaces. Different implementations of the same interface can
then be used interchangeably. This allows you to use products from
different vendors in different layers. A well-known example of a
standardized interface is the POSIX programming interface [IEEE88].
Dependencies are kept local. Standardized interfaces between layers
usually confine the effect of code changes to the layer that is changed.
Changes of the hardware, the operating system, the window system,
special data formats and so on often affect only one layer, and you can
adapt affected layers without altering the remaining layers. This sup ports the portability of a system. Testability is supported as well,
since you can test particular layers independently of other components in the system.
Layers
Exchangeability.Individual layer implementations can be replaced by
semantically-equivalent implementations without too great an effort.
If the connections between layers are hard-wired in the code, these
are updated with the names of the new layer's implementation. You
can even replace a n old implementation with an implementation with
a different interface by using the Adapter pattern for interface adaptation [GHJV95].The other extreme is dynamic exchange, which you
can achieve by using the Bridge pattern IGHJV951, for example, and
manipulating the pointer to the implementation at run-time.
Hardware exchanges or additions are prime examples for illustrating
exchangeability. A new hardware 1/0 device, for example, can be put
in operation by installing the right driver program-which may be a
plug-in or replace a n old driver program. Higher layers will not be affected by the exchange. A transport medium such as Ethernet could
be replaced by Token Ring. In such a case, upper layers do not need
to change their interfaces, and can continue to request services from
lower layers as before. However, if you want to be able to switch
between two layers that do not match closely in their interfaces and
services, you must build a n insulating layer on top of these two layers. The benefit of exchangeability comes at the price of increased
programming effort and possibly decreased run-time performance.
The Layers pattern also imposes liabilities:
Cascades of changing behavior:A severe problem can occur when the
behavior of a layer changes. Assume for example that we replace a 10
Megabit/sec Ethernet layer at the bottom of our networked application and instead put IP on top of 155 Megabit/sec ATM~.Due to
limitations with 1/0 and memory performance, our local-end system
cannot process incoming packets fast enough to keep up with ATM's
high data rates. However, bandwidth-intensive applications such as
medical imaging or video conferencing could benefit from the full
speed of ATM. Sending multiple data streams in parallel is a high level solution to avoid the above limitations of lower levels. Similarly,
IP routers, which forward packets within the Internet, can be layered
3. ATM (Asynchronous Transfer Mode) provides much higher data rates (ranging
from 155Mbps to 2.4Gbps) and functionality (such as quality of service guarantees)
than conventional low-speed networks such as Ethernet and Token Ring. In addition.
ATM can emulate the behavior of Ethernet in a LAN,which allows it to be integrated
seamlessly into existing networks. See IHHS941 for more Information on ATM.
Architectural Patterns
to run on top of high-speed ATM networks via multi-CPU systems that
perform IP packet processing in parallel [PST96].
In summary, higher layers can often be shielded from changes in low er layers. This allows systems to be tuned transparently by collapsing
lower layers and/or replacing them with faster solutions such as
hardware. The layering becomes a disadvantage if you have to do a
substantial amount of rework on many layers to incorporate an apparently local change.
Lower efficiency. A layered architecture is usually less efficient than,
say, a monolithic structure or a 'sea of objects'. If high-level services
in the upper layers rely heavily on the lowest layers, all relevant data
must be transferred through a number of intermediate layers, and
may be transformed several times. The same is true of all results or
error messages produced in lower levels that are passed to the highest
level. Communication protocols, for example, transform messages
from higher levels by adding message headers and trailers.
Unnecessary work. if some services performed by lower layers per form excessive or duplicate work not actually required by the higher
layer, this has a negative impact on performance. Demultiplexing in
a communication protocol stack is an example of this phenomenon.
Several high-level requests cause the same incoming bit sequence to
be read many times because every high-level request is interested in
a different subset of the bits. Another example is error correction in
fde transfer. A general purpose low-level transmission system is writ ten first and provides a very high degree of reliability, but it can be
more economical or even mandatory to build reliability into higher
layers, for example by using checksums. See ISRC841 for details of
these trade-offs and further considerations about where to place
functionality in a layered system.
Difficulty of establishing the correct granularity of layers. A layered
architecture with too few layers does not fully exploit this pattern's
potential for reusability, changeability and portability. On the other
hand, too many layers introduce unnecessary complexity and
overheads in the separation of layers and the transformation of
arguments and return values. The decision about the granularity of
layers and the assignment of tasks to layers is difficult, but is critical
for the quality of the architecture. A standardized architecture can
Layers
only be used If the scope of potential client applications fits the
defined layers.
See Also Composite Message. Aamod Sane and Roy Campbell (SC95bldescribe
an object-oriented encapsulation of messages traveling through
layers. A composite message is a packet that consists of headers,
payloads, and embedded packets. The Composite Message pattern is
therefore a variation of the Composite pattern (GHJV951.
A Microkernel architecture (171) can be considered as a specialized
layered architecture. See the discussion of Windows NT in the Known
Uses section.
The PAC architectural pattern (145) also emphasizes levels of
increasing abstraction. However, the overall PAC structure is a tree of
PAC nodes rather than a vertical line of nodes layered on top of each
other. PAC emphasizes that every node consists of three components,
presentation, abstraction, and control, while the Layers pattern does
not prescribe any subdivisions of an individual layer.
##%%&&
The Pipes and Filters architectural pattern provides a structure for
systems that process a stream of data. Each processing step is
encapsulated in a filter component. Data is passed through pipes
between adjacent filters. Recombining filters allows you to build
families of related systems.
Example Suppose we have defined a new programming language called Mocha
[Modular Object Computation with Hypothetical Algorithms). Our
task is to build a portable compiler for this language. To support
existing and future hardware platforms we define an intermediate
language AuLait [Another Universal Language for Intermediate
Translation) running on a virtual machine Cup (Concurrent Uniform
Processor). Cup will be implemented by an interpreter or platform specific backbends. The AuLait interpreter simulates Cup in software.
A backend will translate AuLait code into the machine instructions of
a specific processor for best performance.
Conceptually, translation from Mocha to AuLait consists of the
phases lexical analysis, syntax analysis, semantic analysis, intermediate-code generation (AuLait), and optionally intermediate-code
optimization iASU86). Each stage has well-defined input and output
data. The input to the compilation process is a sequence of ASCII
characters representing the Mocha program. The final stage in our
system-whether backend or interpreter-takes the binary AuLait
code as its input.*
Context Processing data streams.
Problem Imagine you are building a system that must process or transform a
stream of input data. Implementing such a system as a single
component may not be feasible for several reasons: the system has to
be built by several developers, the global system task decomposes
naturally into several processing stages, and the requirements are
likely to change.
You therefore plan for future flexibility by exchanging or reordering
the processing steps. By incorporating such flexibility, it is possible
to build a family of systems using existing processing components.
The design of the system-especially the interconnection of processing steps-has to consider the following forces:
Future system enhancements should be possible by exchanging
processing steps or by recombination of steps, even by users.
Small processing steps are easier to reuse in different contexts
than large components.
Non-adjacent processing steps do not share information.
Different sources of input data exist, such as a network connection
or a hardware sensor providing temperature readings, for example.
It should be possible to present or store final results in various
ways.
Explicit storage of intermediate results for further processing in
files clutters directories and is error-prone, if done by users.
You may not want to rule out multi-processing the steps, for
example running them in parallel or quasi-parallel.
4. Any similarities in names, persons, or events are coincidental and unintended.
Whether a separation into processing steps is feasible strongly
depends on the application domain and the problem to be solved. For
example, an interactive, event-driven system does not split into
sequential stages.
Solution The Pipes and Filters architectural pattern divides the task of a
system into several sequential processing steps. These steps are
connected by the data flow through the system-the output data of a
step is the input to the subsequent step. Each processing step is
implemented by a filter component. A filter consumes and delivers
data incrementally-in contrast to consuming all its input before
producing any output-to achieve low latency and enable real parallel
processing. The input to the system is provided by a data source such
as a text file. The output flows into a data sink such as a file, terminal,
animation program and so on. The data source, the filters and the
data sink are connected sequentially by pipes. Each pipe implements
the data flow between adjacent processing steps. The sequence of
filters combined by pipes is called a processing pipeline.
Structure Filter components are the processing units of the pipeline. A filter
enriches, refines or transforms its input data. It enriches data by
computing and adding information, refines data by concentrating or
extracting information, and transforms data by delivering the data in
some other representation. A concrete filter implementation may
combine any of these three basic principles.
The activity of a filter can be triggered by several events:
@ The subsequent pipeline element pulls output data from the filter.
The previous pipeline element pushes new input data to the filter.
@ Most commonly,the filter is active in a loop, pulling its input from
and pushing its output down the pipeline.
The first two cases denote so-called passive filters, whereas the last
case is an active filter5.An active filter starts processing on its own as
a separate program or thread. A passive filter component is activated
by being called either as a function (pull) or as a procedure (push).
5. Note that all UNIX filters are active by this definition. Passive filters may be an
unfamiliar concept. We introduce It to show that the Pipes and Filters pattern can be
implemented without the overhead of context switches and data transfers and still
remain a viable concept.
Pipes denote the connections between filters, between the data source
and the first filter,and between the last filter and the data sink. If two
active components are joined, the pipe synchronizes them. This
synchronization is done with a first-in- first-out buffer. If activity is
controlled by one of the adjacent filters, the pipe can be implemented
by a direct call from the active to the passive component. Direct calls
make filter recombination harder, however.
The data source represents the input to the system, and provides a
sequence of data values of the same structure or type. Examples of
such data sources are a file consisting of lines of text, or a sensor
delivering a sequence of numbers. The data source of a pipeline can
either actively push the data values to the first processing stage, or
passively provide data when the first filter pulls.
The data sink collects the results from the end of the pipeline. Two
variants of the data sink are possible. An active data sink pulls
results out of the preceding processing stage, while a passive one
allows the preceding filter to push or write the results into it.
In our Mocha compiler we use the UNIX tools lex and yacc to
implement the first two stages of the compiler [ASU86). Both tools
generate functions-yylex ( 1 and yyparse ( ) -for embedding in a
program. The function yyparse ( ) actively controls the frontend of
our compiler. It calls yylex ( 1 whenever further input tokens are
needed. The connection to the other frontend stages consists of many
procedure calls embedded in the grammar action rules, and not just
simple data flow. Such embedded calls are more efficient than treating an explicit abstract syntax tree representation and passing it
along a pipe. The backbends and interpreter run as separate programs
to allow exchangeability. They are connected via a UNIX pipe to the
frontend. 0
Dynamics The following scenarios show different options for control flow
between adjacent filters. Assume that Filter 1 computes function f 1
on its input data and Filter 2 function f 2. The first three scenarios
show passive filters that use direct calls to the adjacent pipeline
components, with different components controlling the activity-no
explicit pipe components therefore exist. The last scenario shows the
commonest case, in which all filters are active, with a synchronizing
pipe between them.
Architectural Patterns
Scenario I shows a push pipeline in which activity starts with the
data source. Filter activity is triggered by writing data to the passive
filters.
Scenario 11shows a pull pipeline. Here control flow is started by the
data sink calling for data.
Scenario 111 shows a mixed push-pull pipeline with passive data
source and sink. Here the second filter plays the active role and starts
the processing.
Scenario lV shows a more complex but typical behavior of a Pipes
and Filters system. All filters actively pull, compute. and push data in
a loop. Each filter therefore runs in its own thread of control, for
example as a separate process. The filters are synchronized by a
buffering pipe between them. For simplicity we assume that the pipe
buffers only a single value. This scenario also shows how you can
achieve parallel execution using filters.
The following steps occur in this scenario:
Fi1t er2 tries to get new data by reading from the pipe. Because no
data is available the data request suspends the activity of
Fi1ter2-the buffer is empty.
Filter l pulls data from the data source and performs function f 1.
Filter 1 then pushes the result to the pipe.
Filter 2 can now continue, because new input data is available.
Filter l can also continue, because it is not blocked by a full
buffer within the pipe.
Filter 2 computes f 2 and writes its result to the data sink.
In parallel with Filter 2's activity, Filter l computes the next
result and tries to push it down the pipe. This call is blocked
because Filter 2 is not waiting for data-the buffer is full.
Filter 2 now reads new input data that is already available from
the pipe. This releases Filter l so that it can now continue its
processing.
Implementation Implementing a Pipes and Filters architecture is straightforward. You
can use a system service such as message queues or UNIX pipes for
pipe connections, or other options like the direct call implementation,
as described in steps 3 through 6 below. The design decisions in these
steps are closely interrelated, so you may make them in an order
other than that given here. The implementation of the data source
and data sink is not addressed explicitly, because it follows the
guidelines for pipes or filters closely.
1 Divide the system's task into a sequence of processing stages. Each
stage must depend only on the output of its direct predecessor. All
stages are conceptually connected by the data flow. If you plan to
develop a family of systems by exchanging processing stages, or if you
are developing toolbox of components, you can consider alternatives
or recombinations for some processing stages at this point.
r In our Mocha compiler the primary separation is between the
AuLait-creating frontend and the backends. Further structuring of
the frontend gives us the stages of scanner, parser, semantic analyzer
and code generator. We decide not to construct an abstract syntax
tree explicitly, to be passed from the parser to the semantic analyzer.
Instead we embed calls to the semantic analyzer (sa)and code generator (cg)into yacc's grammar rules:
This means that we need to build a filter consisting of the parser,
semantic analyzer and code generator stages. The scanner, as a
separate filter component, remains passive until called by the parser.
We thus link the function yylex ( ) into our frontend program. D
Define the data format to be passed along each pipe. Defining a
uniform format results in the highest flexibility because it makes
recombination of filters easy. In most UNLX filter programs the data
format is line-structured ASCII text. This may however impose an
efficiency penalty. For example, a textual representation of floating point numbers may be too inefficient to pass along a pipe, because
repeated conversion between ASCII and floating-point representations and back is needed. If you both want flexibility and apt for
different data representations, you can create transformation filter
components to change the data between semantically-equivalent representations.
You must also define how the end of input is marked. If a system
service is used for pipe connections an end-of-input error condition
may be sufficient. For other pipe implementations you can use a
special data value to mark the end of input. The values zero, -1, $,
control-D,or control-Zare favored examples of end-of-input markers.
s The input to our frontend is a Mocha program, in the form of a
stream or file of ASCII characters. The tokens passed from scanner to
parser are denoted by integer values. The function yylex ( 1 returns
either the ASCII code of a character scanned, or a code beyond the
ASCII range for tokens, such as a Mocha keyword. The end of input
is marked by the value zero. The data format used between the
frontend and the backends or interpreter is provided by the definition
of the AuLait byte codes. Ll
3 Decide how to implement each pipe connection This decision directly
determines whether you implement the filters as active or passive
components. Adjacent pipes further define whether a passive filter is
triggered by push or pull of data. The simplest case of a pipe connection is a direct call between adjacent filters to push or pull a data
Architectural Patterns
value, a s shown in the first three scenarios of the Dynamics section.
If you use a direct call between filters, however, you have to change
your code whenever you want to recombine or reuse filter components. Such filters are also harder to develop and test in isolation, due
to the need for test frames to call the filter components.
Using a separate pipe mechanism that synchronizes adjacent active
filters provides a more flexible solution. If all pipes use the same
mechanism, arbitrary recombination of filters becomes possible. A
pipe supplies a first-in-first-out buffer to connect adjacent filters that
produce and consume unequal amounts of data per computation.
Many operating systems provide inter-process communication
services such a s queues or pipes that you can use to connect active
filter programs. If such services are not available, you can implement
filters a s separate threads, and pipes a s queues that synchronize
producers and consumers of data.
Because we want flexibility at the backend of our Mocha
compiler, we use the UNIX pipe mechanism between frontend and
backends. This also allows u s to store the intermediate results of our
compilation-the AuLait code-in a file for further analysis or
translation by another backend. 0
4 Design and implement the filters. The design of a filter component is
based both on the task it must perform and on the adjacent pipes.
You can implement passive filters a s a function, for pull activation, or
as a procedure for push activation. Active filters can be implemented
either a s processes or a s threads in the pipeline program.
The cost of a context switch between processes, and the need to copy
data between address spaces, may heavily impact performance. The
buffer size of the pipes is an additional parameter you should take
into account. A small buffer gives the worst case when combined with
the most context switches. You can achieve high flexibility with small
active filter components at the price of an overhead for many context
switches and data transfers.
If you want to be able to reuse filters easily, it is vital to control their
behavior in some way. Several techniques are available for passing
parameters to filters. UNIX filter programs, for example, allow many
options to be passed on the command line. An alternative method is
to use a global environment or repository that is available to filters
when they execute. This can be supported by the operating system,
the shell or a configuration file. You should think carefully about the
trade-off between the flexibility of a filter and its ease of use. As a rule
of thumb, a filter should do one thing well.
Our Mocha frontend reads program source code from standard
input and creates an AuIait program on its standard output. The
stages within the frontend communicate by direct calls. We also cre ate an optimizer for AuLait running as a separate filter program. The
AuLait interpreter can be viewed as a data sink,whereas the intended
backends are additional filter stages producing object code as output.O
5 Design the error handling. Because pipeline components do not share
any global state, error handling is hard to address and is often
neglected. As a minimum, error detection should be possible. UNIX
defines a specific output channel for error messages, stderr, that is
used by most provided filter programs for this purpose. Such an
approach can denote errors in input data, resource limitations and so
on. A single error channel may however mix error messages from
different components in a non-obvious and unpredictable way when
filters run in parallel.
If a filter detects errors in its input data, it can ignore input until some
clearly marked separation occurs. For example, a filter may skip to
the next line of input if a line is expected to contain a numerical value
and does not. This approach is helpful if incorrect input data is
possible and inaccurate results can be tolerated.
It is hard to give a general strategy for error handling with a system
based on the Pipes and Filters pattern. For example, consider the
case in which a pipeline has consumed three-quarters of its input.
already produced half of its output data, and some intermediate filter
crashes. In many systems the only solution is to restart the pipeline
and hope that it will complete without failure.
Resynchronization of the pipeline can be a goal of an advanced
system in which filters process data incrementally. One option is to
introduce special marker values to tag the input data stream. These
markers are passed unchanged to the output. You can then restart
the pipe at the correct stage of the input to continue processing from
a failure. Another option is to use pipes to buffer data that has
already been consumed, and then use it to restart the pipeline if a
filter crashes.
In our simple compiler we send errors to the standard error
channel. The parser is designed to skip tokens when it detects a
syntax error until the scanner recognizes the ';' statement separator.
This is done for example in the following grammar rule, which ignores
syntax errors if they occur in an 'import' statement.
In this rule yacc's special token error matches all unrecognized
tokens until a semi colonies found. The statement yyerrok is a special
action that resets the parser into normal mode after the occurrence
of a syntax error. 0
6 Set u p the processing pipeline. If your system handles a single task
you can use a standardized main program that sets up the pipeline
and starts processing. This type of system may benefit from a direct call pipeline, in which the main program calls the active filter to start
processing.
You can increase flexibility by providing a shell or other end-user facility to set up various pipelines from your set of filter components.
Such a shell can support the incremental development of pipelines by
allowing intermediate results to be stored in files, and supporting files
as pipeline input. You are not restricted to a text-only shell such as
those provided by UNIX, and could even develop a graphical environment for visual creation of pipelines using 'drag and drop' interaction.
u Our compilers set up by a UNIX shell command that establishes the compilation or interpreter pipeline:
Mocha is the frontend program, the optimizer is called o p t a u l a i t ,
and the backends follow the naming convention a u l a it2machine.
The interpreter is named cup after the virtual machine it imp1ements.Q
Example We did not follow the Pipes and Filters pattern strictly in our Mocha
resolved compiler by implementing all phases of the compiler a s separate filter
programs connected by pipes. We did this for performance reasons,
and also because, in contrast to the third force, these phases do share
a global state-the symbol table. It is sometimes possible to remove
the need for shared global states by passing global information along
the pipeline as additional data. However, this involves more complex
data structures and an increase in pipeline data volume, imposing a
performance penalty. Where the data being processed consists of
simple types such a s lines of text, such complex additional data
structures have to be encoded and decoded by each filter.
We combined the first four compiler phases into a single program
because they all access and modify the symbol table. This allows u s
to implement the pipe connection between the scanner and the parser
as a simple function call. Backends, interpreters or a debugger would
also benefit from access to the symbol table. We therefore follow the
example of many existing compilers by encoding some of the symbol
table information, such as names and source line numbers, into the
binary code for debugging purposes. Note that such symbol table
information can greatly increase the size of compiled programs.
Variants Tee and join pipeline systems. The single-input single-output filter
specification of the Pipes and Filters pattern can be varied to allow
filters with more than one input and/or more than one output. Processing can then be set up a s a directed graph that can even contain
feedback loops. The design of such a system, especially one with
feedback loops, requires a solid foundation to explain and
understand the complete calculation-a rigorous theoretical analysis
and specification using formal methods are appropriate, to prove that
the system terminates and produces the desired result. If we restrict
ourselves to simple directed acyclic graphs, however, it is still possible
to build useful systems. The UNIX filter program tee. for example,
allows you to write data passed along a pipe to a file or to another
'named' pipe. Some filter programs allow the use of files or named
pipes a s input, a s well as standard input. For example, to build a
sorted list of all lines that occur more than once In a text file. we can
construct the following shell program:
Known Uses UNIX [Bac86] popularized the Pipes and Filters paradigm. The
command shells and the availability of many filter programs made
this approach to system development popular. As a system for
software developers, frequent tasks such as program compilation and
documentation creation are done by pipelines on a 'traditional' UNIX
system. The flexibility of UNIX pipes made the operating system a
suitable platform for the binary reuse of filter programs and for
application integration.
CMS Pipelines IHRV951 is an extension to the operating system of
IBM mainframes to support Pipes and Filters architectures. The implementation of CMS pipelines follows the conventions of CMS, and
defines a record as the basic data type that can be passed along pipes,
instead of a byte or ASCII character. CMS Pipelines provides a reuse
and integration platform in the same way as UNIX. Because the CMS
operating system does not use a uniform I/O-model in the same way
as UNIX, CMS Pipelines defines device drivers that act a s data
sources or sinks, allowing the handling of specific I/O-devices within
pipelines.
LASSPTools [Set951 is a toolset to support numerical analysis and
graphics. The toolset consists mainly of filter programs that can be
combined using UNIX pipes. It contains graphical input devices for
analog input of numerical data using knobs or sliders, filters for
numerical analysis and data extraction, and data sinks that produce
animations from numerical data streams.
Consequences The Pipes and Filters architectural pattern has the following benefits:
No intermediate files necessary, but possible. Computing results
using separate programs is possible without pipes, by storing
intermediate results in files. This approach clutters directories, and
is error-prone if you have to set up your processing stages every time
you run your system. In addition, it rules out incremental and
parallel computation of results. Using Pipes and Filters removes the
need for intermediate files,but allows you to investigate intermediate
data by using a T-junction in your pipeline.
Flexibility by filter exchange. Filters have a simple interface that
allows their easy exchange within a processing pipeline. Even if filter
components call each other directly in a push or pull fashion,
exchanging a filter component is still straightforward. In our compiler
example a scanner generated with lex can easily be replaced by a
more efficient hand-coded function yylex ( 1 that performs the same
task. Filter exchange is generally not possible at run-time due to
incremental computation in the pipeline.
Flexibility by recombination.This major benefit, combined with the reusability of filter components, allows you to create new processing
pipelines by rearranging filters or by adding new ones. A pipeline
without a data source or sink can be embedded a s a filter within a
larger pipeline. You should aim to tune the system platform or surrounding infrastructure to support this flexibility,such a s is provided
by the pipe mechanism and shells in UNIX.
Reuse filter components. Support for recombination leads to easy
reuse of filter components. Reuse is further enhanced if you
implement each filter a s an active component, while the underlying
platform and shell allow easy end-user construction of pipelines.
Rapid prototyping of pipelines. The preceding benefits make it easy to
prototype a data-processing system from existing filters. After you
have implemented the principal system function using a pipeline you
can optimize it incrementally. You can do this, for example, by
developing specific filters for time-critical processing stages, or by re implementing the pipeline using more efficient pipe connections. Your
prototype pipeline can however be the final system if it performs the
required task adequately. Highly-flexible filters such a s the UNIX
tools send and ask reinforce such a prototyping approach.
Efficiency by parallel processing. It is possible to start active filter
components in parallel in a multiprocessor system or a network. If
each filter in a pipeline consumes and produces data incrementally
they can perform their functions in parallel.
Applying the Pipes and Filters pattern imposes some liabilities:
Sharing state information is expensive or inflexible. If your processing
stages need to share a large amount of global data, applying the Pipes
and Filters pattern is either inefficient or does not provide the full
benefits of the pattern.
Efficiency gain by parallel processing is often a n illusion. This is for
several reasons:
The cost for transferring data between filters may be relatively high
compared to the cost of the computation carried out by a single
filter.This is especially true for small filter components or pipelines
using network connections.
Some filters consume all their input before producing any output,
either because the task, such a s sorting, requires it or because the
filter is badly coded, for example by not using incremental processing when the application allows it.
Context-switching between threads or processes is generally an
expensive operation on a single-processor machine.
Synchronization of filters via pipes may stop and start filters often,
especially when a pipe has only a small buffer.
Data transformation overhead. Using a single data type for all filter
input and output to achieve highest flexibility results in data
conversion overheads. Consider a system that performs numeric
calculations and uses UNIX pipes. Such a system must convert ASCII
characters to real numbers, and vice-versa, within each filter. A
simple filter, such a s one that adds two numbers, will spend most of
its processing time doing format conversion.
Error handling. As we explained in step 5 of the Implementation
section, error handling is the Achilles' heel of the Pipes and Filters
pattern. You should at least define a common strategy for error
reporting and use it throughout your system. A concrete error recovery or error-handling strategy depends on the task you need to
solve. If your intended pipeline is used in a 'mission-critical' system
and restarting the pipeline or ignoring errors is not possible, you
should consider structuring your system using alternative architectures such a s Layers 
See Also The Layers pattern (31) is better suited to systems that require
reliable operation, because it is easier to implement error handling
than with Pipes and Filters. However, Layers lacks support for the
easy recombination and reuse of components that is the key feature
of the Pipes and Filter pattern.
##%%&&
The Blackboard architectural pattern is useful for problems for which
no deterministic solution strategies are known. In Blackboard several
specialized subsystems assemble their knowledge to build a possibly
partial or approximate solution.
Example Consider a software system for speech recognition. The input to the
system is speech recorded as a waveform. The system not only
accepts single words, but also whole sentences that are restricted to
the syntax and vocabulary needed for a specific application, such as
a database query. The desired output is a machine representation of
the corresponding English phrases. The transformations involved
require acoustic-phonetic, linguistic. and statistical expertise. For
example. one procedure divides the waveform into segments that are
meaningful in the context of speech, such as At the other
end of the processing sequence, another procedure checks the syntax
of candidate phrases. Both procedures work In different domains.
This diagram is mostly taken from [EHLR88]. The input is the
waveform at the bottom, and the output consists of the phrase 'are
any by Feigenbaum'.
For the moment we assume that there is no consistent algorithm that
combines all the necessary procedures for recognizing speech-we
discuss this topic further in the Consequences section. To make matters worse, the problem is characterized by the ambiguities of spoken
language, noisy data, and the individual peculiarities of speakers
such as vocabulary, pronunciation, and syntax.
Context An immature domain in which no closed approach to a solution is
known or feasible.
Problem The Blackboard pattern tackles problems that do not have a feasible
deterministic solution for the transformation of raw data into high level data structures, such as diagrams, tables or English phrases.
Vision, image recognition, speech recognition and surveillance are
examples of domains in which such problems occur. They are characterized by a problem that, when decomposed into sub problems,
spans several fields of expertise. The solutions to the partial problems
require different representations and paradigms. In many cases no
predetermined strategy exists for how the 'partial problem solvers'
should combine their knowledge. This is in contrast to functional de composition, in which several solution steps are arranged so that the
sequence of their activation is hard-coded.
In some of the above problem domains you may also have to work
with uncertain or approximate knowledge. Each transformation step
can also generate several alternative solutions. In such cases it is
often enough to find an optimal solution for most cases, and a
suboptimal solution, or no solution, for the rest. The limitations of a
Blackboard system therefore have to be documented carefully, and if
important decisions depend on its results, the results have to be
verified.
The following forces influence solutions to problems of this kind:
A complete search of the solution space is not feasible in a reason able time. For example, if you consider phrases of up to ten words
using a vocabulary of a thousand words, the number of possible
permutations of words is in the order of 1000.
Since the domain is immature, you may need to experiment with
different algorithms for the same subtask. For this reason, individual modules should be easily exchangeable.
There are different algorithms that solve partial problems. For
example, the detection of phonetic segments in the waveform is
unrelated to the generation of phrases based on words and word
sequences.
Input, as well as intermediate and final results, have different
representations, and the algorithms are implemented according to
different paradigms.
An algorithm usually works on the results of other algorithms.
Uncertain data and approximate solutions are involved. For example, speech often includes pauses and extraneous sounds. These
significantly distort the signal. The process of interpretation of the
signal is also error-prone. Competing alternatives for a recognition
target may occur at any stage of the process. For example, it is hard
to distinguish between 'till' and 'tell'. The words 'two' and 'too' even
have the same pronunciation, as do many others in English.
Employing disjoint algorithms induces potential parallelism. If
possible you should avoid a strictly sequential solution.
Artificial Intelligence (Al)systems have been used with some success
for such complex non-deterministic problems. In the 'classical' expert
system structure, the input to the system and intermediate results
are kept in working memory. The memory contents are used by an
inference engine in conjunction with the knowledge base to infer new
intermediate results. Such manipulation steps are repeated until
some completion condition is fulfilled.
This type of expert system structure is inadequate for a speech
recognition system. There are three reasons for this:
All partial problems are solved using the same knowledge representation. However,the components involved in the speech recognition
process work on fields of knowledge that differ as widely as the segmentation of a waveform and the parsing of candidate phrases.
They therefore require different representations.
The expert system structure provides only one inference engine to
control the application of knowledge. Different partial problems
with different representations require separate inference engines.
In a 'classical' expert system, control is implicit in the structure of
the knowledge base, for example in the ordering of the rules in a
rule-based system. This is consistent with the view of many A1
systems that 'problem solving is search' and 'knowledge prunes
and directs search'. This implies that you search in the search tree
for nodes that include the solution for your problem, and use items
of knowledge to guide your way from the root of the search tree where all solutions are possible-to a single leaf.
For the speech recognition problem, the view 'problem solving is
experts assembling their knowledge' is more suitable. In other
words, fragments of knowledge have to be applied at an opportune
time, rather than in a predetermined order.
Solution The idea behind the Blackboard architecture is a collection of
independent programs that work cooperatively on a common data
structure. Each program is specialized for solving a particular part of
the overall task, and all programs work together on the solution.
These specialized programs are independent of each other. They do
not call each other, nor is there a predetermined sequence for their
activation. Instead, the direction taken by the system is mainly
determined by the current state of progress. A central control
component evaluates the current state of processing and coordinates
the specialized programs. This data-directed control regime is
referred to as opportunistic problem solving. It makes experimentation
with different algorithms possible, and allows experimentally-derived
heuristics to control processing.
During the problem-solving process the system works with partial
solutions that are combined, changed or rejected. Each of these
solutions represents a partial problem and a certain stage of its
solution, The set of all possible solutions is called the solution space,
and is organized into levels of abstraction. The lowest level of solution
consists of an internal representation of the input. Potential solutions
of the overall system task are on the highest level.
The name 'blackboard was chosen because it is reminiscent of the
situation in which human experts sit in front of a real blackboard and
work together to solve a problem. Each expert separately evaluates
the current state of the solution, and may go up to the blackboard at
any time and add, change or delete information. Humans usually
decide themselves who has the next access to the blackboard. In the
pattern we describe, a moderator component decides the order in
which programs execute if more than one can make a contribution.
Structure Divide your system into a component called blackboard, a collection
of knowledge sources, and a control component.
The blackboard is the central data store. Elements of the solution
space and control data are stored here. We use the term vocabulary
for the set of all data elements that can appear on the blackboard. The
blackboard provides a n interface that enables all knowledge sources
to read from and write to it.
At1 elements of the solution space can appear on the blackboard. For
solutions that are constructed during the problem solving process
and put on the blackboard, we use the terms hypothesis or
blackboard entry. Hypotheses rejected later in the process are
removed from the blackboard.
A hypothesis usually has several attributes, for example its
abstraction level, that is, its conceptual distance from the input.
Hypotheses that have a low abstraction level have a representation
that is still similar to input data representation, while hypotheses
with the highest abstraction level are on the same abstraction level as
the output. Other hypothesis attributes are the estimated degree of
truth of the hypothesis or the time interval covered by the hypothesis.
It is often useful to specify relationships between hypotheses, such a s
'part-of or 'in-support-of.
The solution space for the speech recognition example consists
of acoustic-phonetic and linguistic speech fragments. The levels of
abstraction are signal parameters, acoustic-phonetic segments,
phones, syllables, words, and phrases.
The degree of truth for a syllable is estimated by the quality of the
match between the ideal phone sequences for that syllable and the
hypothesized phones.
The waveform of the acoustic signal is recorded on a time axis that
corresponds to the X-axis in the figure on page 71.Every solution has
an attribute that specifies the interval on the X-axis that it describes.
The blackboard can be viewed as a three-dimensional problem space
with the time line for speech on the X-axis, increasing levels of
abstraction on the Y-axis and alternative solutions on the Z-axis
Knowledge sources are separate, independent subsystems that solve
specific aspects of the overall problem. Together they model the
overall problem domain. None of them can solve the task of the
system alone-a solution can only be built by integrating the results
of several knowledge sources.
In the speech recognition system we specify solutions for the
following partial problems: defining acoustic-phonetic segments, and
creating phones, syllables, words and phrases. For each of these
partial problem we define one or several knowledge sources. One
knowledge source at the word level, for example, may create words
from adjacent syllables, while another source on the same level
verifies words that depend on neighboring words.
Note that the transformation from waveform to phrase is not
necessarily a strictly sequential process. The complete waveform is
not necessarily first transformed into segments, all segments into
phones, then into syllables and words, and phrases then built. A
portion of the waveform may have been transformed into words.
another may have been rejected at the word level back to the phone
level, and a third may not be analyzed at all until enough evidence on
the phrase level exists to tackle it.
Knowledge sources do not communicate directly-they only read from
and write to the blackboard. They therefore have to understand the
vocabulary of the blackboard. We explore the ramifications of this in
the Implementation section.
Often a knowledge source operates on two levels of abstraction. If a
knowledge source implements forward reasoning, a particular
solution is transformed to a higher-level solution. A knowledge source
that reasons backwards searches at a lower level for support for a
solution, and may refer it back to a lower level if the reasoning did not
give support for the solution.
Each knowledge source is responsible for knowing the conditions
under which it can contribute to a solution. Knowledge sources are
therefore split into a condition-part and an action-part.The condition part evaluates the current state of the solution process, as written on
the blackboard, to determine if it can make a contribution. The
action-part produces a result that may cause a change to the
blackboard's contents.
Our speech recognition system has diverse knowledge sources
that transform several hypotheses at the same level and with
contiguous time intervals to a single hypothesis on the next higher
level. For example, a phrase is built from a selection of words that
together span the time interval corresponding to the phrase. Other
knowledge sources predict new hypotheses at the same level. For
example, one knowledge source predicts possible words that might
syntactically precede or follow a given phrase. We also define a
knowledge source that verifies the predicted hypotheses based on
information at the next lower level. This calculates the consistency
between a predicted word and the set of segments that span the same
time interval. 
The control component runs a loop that monitors the changes on the
blackboard and decides what action to take next. It schedules
knowledge source evaluations and activations according to a knowledge application strategy.The basis for this strategy is the data on the
blackboard.
The strategy may rely on control knowledge sources. These special
knowledge sources do not contribute directly to solutions on the
blackboard, but perform calculations on which control decisions are
made. Typical tasks are the estimation of the potential for progress,
or the computational costs for execution of knowledge sources. Their
results are called control data and are put on the blackboard as well.
Theoretically, it is possible that the blackboard can reach a state at
which no knowledge source is applicable. In this case, the system
fails to deliver a result. In practice, it is more likely that each
reasoning step introduces several new hypotheses, and that the
number of possible next steps 'explodes'. The problem is therefore to
restrict the alternatives to be taken rather than to find an applicable
knowledge source.
A special knowledge source or a procedure in the control component
determines when the system should halt, and what the final result is.
The system halts when an acceptable7 hypothesis is found, or when
the space or time resources of the system are exhausted.
The following figure illustrates the relationship between the three
components of the Blackboard architecture. The blackboard
component defines two procedures: inspect and update. Knowledge
sources call inspect to check the current solutions on the
blackboard. update is used to make changes to the data on the
blackboard.
The Control component runs a loop that monitors changes on the
blackboard and decides what actions to take next. We call the procedure responsible for this decision nextsource ( ).
Dynamics The following scenario illustrates the behavior of the Blackboard
architecture. It is based on our speech recognition example:
The main loop of the Control component is started.
Control calls the nextsource ( ) procedure to select the next
knowledge source.
nextsource ( ) first determines which knowledge sources are potential contributors by observing the blackboard. In this example
we assume the candidate knowledge sources are Segmentation.
Syllable Creation and Word Creation.
nextsource0 invokes the condition-part of each candidate
knowledge source. In the example, the condition-parts of Segmentation. Syllable Creation and Word Creation inspect the blackboard
to determine if and how they can contribute to the current state of
the solution.
The Control component chooses a knowledge source to invoke, and
a hypothesis or a set of hypotheses to be worked on. In the example
the choice is made according to the results of the condition parts.
In other cases the selection is also based on control data. It applies
the action-part of the knowledge source to the hypotheses. In our
speech recognition example, assume that Syllable Creation is the
most promising knowledge source. The action-part of Syllable
Creation inspects the state of the blackboard, creates a new
syllable and updates the blackboard.
To implement the Blackboard pattern, carry out the following steps:
Define the problem:
Specify the domain of the problem and the general fields of
knowledge necessary to find a solution.
Scrutinize the input to the system. Determine any special properties of the input such as noise content or variations on a theme that is, does the input contain regular patterns that change slowly
over time?
Define the output of the system. Specify the requirements for
correctness and fail-safe behavior. If you need an estimation of the
credibility of the results, or if there are cases in which the system
should ask the user for further resources, record this.
Detail how the user interacts with the system.
s The fields of knowledge important for a system in the domain of
speech recognition are acoustics, linguistics and statistics. The input
is a sequence of acoustic signals from a speaker. The data is noisy. If
the system allows the speaker to repeat a phrase several times, the
input contains 'variations on a theme', a s described above. The desired output is a written English phrase corresponding to the spoken
phrase. When used for a database query interface, the system can tolerate occasional misinterpretations. If we have to repeat a query in,
say, 10%of cases, the system can still be useful. CS
2 Define the solution space for the problem We distinguish intermediate
and top-level solutions on one hand, and partial and complete solutions on the other. A top-level solution is at the highest abstraction
level. Solutions at other levels are intermediate solutions. A complete
solution solves the whole problem, whereas a partial solution solves
part of the problem. Note that complete solutions can belong to inter mediate levels, and a partial solution may be top-level.
In speech recognition, complete top-level solutions are phrases
that are correct with respect to a defined vocabulary and syntax.
Complete intermediate solutions are sequences of acoustic-phonetic
or linguistic elements that describe the whole spoken phrase. Parts of
solutions are the elements themselves.
So, perform the following steps:
Specify exactly what constitutes a top-level solution.
List the different abstraction levels of solutions.
Organize solutions into one or more abstraction hierarchies.
Find subdivisions of complete solutions that can be worked on
independently, for example words of a phrase or regions of a
picture or area.
Divide the solution process into steps:
Define how solutions are transformed into higher-level solutions.
Describe how to predict hypotheses at the same abstraction level.
Detail how to verify predicted hypotheses by finding support for
them in other levels.
Specify the kind of knowledge that can be used to exclude parts of
the solution space.
To transform solutions on the syllabic level to solutions on the
word level, we provide a dictionary that associates a syllable with all
the words whose pronunciation contains the syllable.
Syntactic and statistical knowledge is useful when pruning the
search for word sequences. For example, the heuristic that an adjective is normally followed by another adjective or a noun can be used
to cut down computing time.
4 Divide the knowledge into specialized knowledge sources with certain
subtasks. These subtasks often correspond to areas of specialization.
There may be some subtasks for which the system defers to human
specialists for decisions about dubious cases, or even to replace a
missing knowledge source. Knowledge sources must be complete in
the following sense: for most of the input phrases, at least one
possible sequence of knowledge source activations that leads to an
acceptable solution should exist.
Examples of knowledge sources are segmentation, phone creation, syllable creation, word creation, phrase creation, word
prediction and word verification. Li
5 Define the vocabulary of the blackboard.Elaborate your first definition
of the solution space and the abstraction levels of your solutions. Find
a representation for solutions that allows all knowledge sources to
read from and contribute to the blackboard. This does not mean that
each knowledge source must understand every blackboard entry, but
each knowledge source must be able to decide whether it can use a
blackboard entry. If necessary, provide components that translate
between blackboard entries and the internal representations within
knowledge sources. This allows knowledge sources to be easily
exchanged, to be independent of each other's representation and
paradigms, and at the same time use each other's results.
In our speech recognition example, each hypothesis has a uniform attribute-value structure. Some attributes must be included in
all hypotheses, while others are optional. The element name, the abstraction level and the time interval covered by the hypothesis are
among the required attributes. The estimated degree of truth is optional. For example, the blackboard may contain the following entry:
Depending on the abstraction level, each knowledge source can
decide if it is able to work on a hypothesis or not. The knowledge
source responsible for segmentation, for example, does not understand the symbols '+' and 'I ' in the blackboard entry shown here. It
knows, by reading the value of the attribute abstraction level, that the
hypothesis is a phrase, so it does not check the other attributes.
To evaluate the contents of the blackboard, the Control component
must be able to understand it. The vocabulary of the blackboard
cannot therefore be defined once, but evolves In concert with the
definition of knowledge sources and the Control component. At some
point during design the vocabulary must stabilize, to allow the
development of stable interfaces to the knowledge sources.
6 Specify the control of the system The Control component implements
an opportunistic problem-solving strategy that determines which
knowledge sources are allowed to make changes to the blackboard.
The aim of this strategy is to construct a hypothesis that is acceptable
as a result. But when is a hypothesis acceptable? Since the
correctness of a hypothesis is not verifiable In a strict sense, our goal
is to construct the most credible complete, top-level solution possible
in the solution space.
The credibility of a hypothesis is the likelihood that it is correct. We
estimate the credibility of a hypothesis by considering all plausible
alternatives to it, and the degree of support each alternative receives
from the input data. The credibility rating is, for example, a number
on a scale ranging from 0 to 100. A hypothesis is acceptable if it is
top-level and complete and if its assessed credibility reaches a
threshold value, for example 85.To find a n acceptable hypothesis, the
system eliminates hypotheses with a low credibility, and detects
mutually-supportive clusters of hypotheses that are consistent with
the input data.
In the simplest case the control strategy consults the condition-part
of all knowledge sources whenever the blackboard is changed, and
picks one of the applicable knowledge sources for activation at
random. However, this strategy usually is too inefficient, a s progress
toward an acceptable hypothesis is slow. The design of a good control
strategy is the most difficult part of the system design. It often
consists of a tedious process of trying combinations of several
mechanisms and partial strategies. The Strategy pattern IGHJV951 is
useful here to support an exchange of control strategies, even at run time. Sophisticated control strategies may be implemented by a
dedicated knowledge-based system.
The following mechanisms optimize the evaluation of knowledge
sources, and so increase the effectiveness and performance of the
control strategy:
Classifying changes to the blackboard into two types. One type
specifies all blackboard changes that may imply a new set of applicable knowledge sources, the other specifies all blackboard
changes that do not. After changes of the second type, the Control
component chooses a knowledge source without another invocation of all condition-parts.
Associating categories of blackboard changes with sets of possibly
applicable knowledge sources.
Focusing of control. The focus contains either partial results on the
blackboard that should be worked on next, or knowledge sources
that should be preferred over others.
Creating a queue in which knowledge sources classified a s applicable wait for their execution. By using a queue, you save valuable
information about knowledge sources rather than discarding it after each change to the blackboard.
Control strategies use heuristics to determine which of the applicable
knowledge sources to activate. Heuristics are rules based on experience and guesses. Keep in mind that good heuristics work often, but
not always. Here are some examples of heuristics that can be used by
control strategies:
Prioritizing applicable knowledge sources. The basis for such a
priority calculation is the evaluation of the condition-parts of
knowledge sources, and possibly other information such as the
potential for making progress using a knowledge source, and the
costs of its application. The Control component may consider the
contributions of knowledge-sources to decide about prioritization.
In this case it must execute the action-parts of all applicable
knowledge sources before it can decide which should make a
change to the blackboard. If the system uses a queue, the priority
of each knowledge source is stored with its entry. A change to the
blackboard may result in a change in priorities or the removal of
knowledge sources from the queue.
Preferring low-level or high-level hypotheses. If this is the only
strategy used, the control strategy is no longer opportunistic, but
rather implements forward- or backward-chaining.
Preferring hypotheses that cover large parts of the problem.
'Island driving'. This strategy involves assuming that a particular
hypothesis is part of an acceptable solution, and is considered as
an 'island of certainty'. Knowledge source activations that work on
this hypothesis are then preferred over others, which removes the
need to search constantly for alternative hypotheses with higher
priorities.
If the control component displays complex and independent
subtasks, define one control knowledge source for each of these
subtasks. Treat them like other knowledge sources. For example, the
priority calculation for applicable knowledge sources can itself be
implemented as a dedicated control knowledge source.
7 Implement the knowledge sources. Split the knowledge sources into
condition-parts and action-parts according to the needs of the Control component. To maintain the independency and exchangeability of
knowledge sources, do not make any assumptions about other knowledge sources or the Control component.
You can implement different knowledge sources in the same system
using different technologies. For example, one may be a rule-based
system, another a neural net and a third a set of conventional
functions. This implies that the knowledge sources themselves may
be organized according to diverse architectural or design patterns.
For example, one knowledge source may be designed using the Layers
pattern (31), while another may be structured according to the
Reflection pattern (193).If you intend to develop your system using
object-oriented technology, but your knowledge sources are implemented using another paradigm, it makes sense to 'wrap' them
using the Facade pattern (GHJV951.
Variants Production System This architecture is used in the OPS language
[FMcD77]. In this variant subroutines are represented as condition action rules, and data is globally available in working memory.
Condition-action rules consist of a left-hand side that specifies a
condition, and a right-hand side that specifies an action. The action
is executed only if the condition is satisfied and the rule is selected.
The selection is made by a 'conflict resolution module'. A Blackboard
system can be regarded as a radical extension of the original production system formalism: arbitrary programs are allowed for both
sides of the rules, and the internal complexity of the working memory
is increased. Complicated scheduling algorithms are used for conflict resolution.
Repository.This variant is a generalization of the Blackboard pattern.
The central data structure of this variant is called a repository. In a
Blackboard architecture the current state of the central data
structure, in conjunction with the Control component, finally
activates knowledge sources. In contrast, the Repository pattern does
not specify an internal control. A repository architecture may be
controlled by user input or by an external program. A traditional
database, for example, can be considered as a repository. Application
programs working on the database correspond to the knowledge
sources in the Blackboard architecture.
Examples of repository systems that are not Blackboard systems are
given in lSG961: 'Programming environments are often organized as a
collection of tools together with a shared repository of programs and
program fragments. Even applications that have been traditionally
viewed as pipeline architectures, may be more accurately interpreted
as repository systems...' Compilers, for example, have traditionally
been described and sometimes also been implemented as pipelines8.
Modern compilers have a repository that holds shared information
such as symbol tables and abstract syntax trees. The compilation
phases correspond to knowledge sources operating on the repository
This architecture enables incremental problem solving:
The scanner reads an identifier that is not yet defined.
The parser recognizes the syntactical unit described by the
identifier.
The code generator then jumps in and creates the corresponding
machine code, if any.
Known uses HEARSAY-11. The first Blackboard system was the HEARSAY-I1
speech recognition system from the early 1970's. It was developed as
a natural language interface to a literature database. Its task was to
answer queries about documents and to retrieve documents from a
collection of abstracts of Artificial Intelligence publications. The
inputs to the system were acoustic signals that were semantically
interpreted and then transformed to a database query. (EM881gives
a detailed description and retrospective view of the project. Selected
aspects of HEARSAY-I1 also serve as the running example of this
pattern. The following paragraphs discuss its control aspects.
In HEARSAY-11, the condition-part of a knowledge source identifies a
configuration of hypotheses on the blackboard appropriate for action
by the knowledge source. This subset is called the stimulus frame. For
example, the condition-part of the knowledge source that generates
phrase hypotheses looks for contiguous word or phrase hypotheses.
Condition-parts also calculate a formal description of the likely action
that the knowledge source will perform, called the response frame.
For example, a response frame for a word hypothesizer based on
syllables indicates that its action will be to generate hypotheses at the
word level, and that the interval covered by the hypothesis on the X axis will include at least the stimulus frame.
The control component of HEARSAY-I1consists of the following:
The focus of control database, which contains a table of primitive
change Qpes of blackboard changes, and those condition-parts
that can be executed for each change type. Examples of primitive
change types are 'new syllable' or 'new word created bottom-up'-
indicating that a new word appeared on the blackboard and it was
inferred using hypotheses on lower levels.
The scheduling queue, which contains pointers to condition- or
action-parts of knowledge source.
The monitor, which keeps track of each change made to the blackboard. The monitor inserts pointers to applicable condition-parts
into the scheduling queue based on the corresponding primitive
change types. If a condition-part is actually executed and the calculated response frame is not empty, a pointer to the matching
action-part is placed in the scheduling queue.
The scheduler, which uses experimentally-derived heuristics to
calculate priorities for the condition- and action-parts waiting in
the scheduling queue. This estimation is based on the specific
stimulus and response frames. It also takes into account overall
blackboard state information, such as which out of several competing hypotheses in the same X-axis interval has highest support
from hypotheses on lower levels. The scheduler finally selects the
condition- or action-part with the highest priority for execution.
The designers of HEARSAY-I1 combined several problem-solving
techniques for their knowledge application strategy. The first is a
bottom-up approach in which interpretations are synthesized directly
from the data, working up the abstraction hierarchy. The second is a
top-down strategy, in which hypotheses at lower levels are produced
recursively until a sequence of hypotheses on the lowest level is
produced that can be tested against the original input. Orthogonal to
those approaches, HEARSAY-I1 employs a 'generate-and-test' strategy, in which a knowledge source generates hypotheses, and their
validity is evaluated by another knowledge source.
HASP/SIAP. The HASP system was designed to detect enemy
submarines. In this system, hydrophone arrays monitor a sea area by
collecting sonar signals. A Blackboard system interprets these signals
[Nii86]. HASP is an event-based system in the sense that the
occurrence of a particular event implies that new information is
available. The blackboard is used as a 'situation board' that evolves
9. The scheduling queue does not implicitly determine the sequence of elements to
be removed, as a LIFO- or FIFO-queue does. Instead the Scheduler determines the
sequence by repeatedly calculating priorities. Therefore, according to our terminology,
the HEARSAY-I1'scheduling queue' is a container and not a queue.
over time. Since information is collected continuously, there is
information redundancy as well as new and different information.
HASP deals with multiple input streams. Besides the low-level data
from hydrophones, it accepts high-level descriptions of the situation
gathered from intelligence or other sources.
CRYSALIS. This system was designed to infer the three-dimensional
structure of protein molecules from X-ray diffraction data [Ter88].
The system introduces several features to the Blackboard
architecture. The blackboard is divided into several parts called
panels. Each panel has its own vocabulary and hierarchy. It is
possible to restrict access to certain panels by knowledge sources.
CRYSALIS uses a data panel and a hypothesis panel. Knowledge
sources are organized into levels. Only the lowest level contains
knowledge sources that actually create and modify hypotheses. The
other levels consist of control knowledge sources. CRYSALIS was the
first Blackboard system to use rule-based systems for control.
TRICERO. This system monitors aircraft activities. It extends the
Blackboard architecture to distributed computing [Wi184]. Four
complete, independent expert systems for partial problems were
designed to run on four separate machines.
Generalizations. Between 1977 and 1984 application-oriented
Blackboard systems were generalized to produce frameworks
intended to ease building Blackboard applications. However, no
standard way to do this emerged.
A recent project called 'Software Understanding System',
described in ITHG941, is particularly interesting from our point of
view as software pattern authors. The aim of SUS is to support
understanding of software, and the search for reusable assets. In a
matching process the system compares patterns from a pattern base
to the system under analysis. SUS incrementally builds a 'pattern
map' of the analyzed software that then can be viewed.
Example In the following we present an excerpt of the processing steps that
Resolved HEARSAY-I1 performs to understand the phrase 'Are any by
Feigenbaum and Feldman?', as described in 1EHLR881.
To briefly characterize the knowledge sources that are activated in the
example:
RPOL runs as a high-priority task immediately after any knowledge
source activity that creates a new hypothesis.RPOL uses rating
information on the new hypothesis, as well as rating information
on hypotheses to which the new hypothesis is connected, to
calculate an overall rating for the new hypothesis.
PREDICT works on a phrase and generates predictions of all words
that can immediately precede or follow the phrase In the language.
VERIFY tries to verify the existence of, or reject, a predicted word,
in the context of the phrase that predicts it. If the word is verified
a confidence rating must also be generated for it. This is done by
the knowledge source RPOL.
CONCAT accomplishes the generation of a phrase from a verified
word and its predicting phrase. The extended phrase includes a
rating that is based on the ratings of the predicting phrase, and the
verified word. If a verified word is already associated with some other phrase, CONCAT tries to parse that phrase with the predicting
phrase. If successful, a phrase hypothesis is created which represents the merging of the two phrases.
We have simplified the original description for ease of understanding,
and have omitted explicit executions of the condition-parts of knowledge sources. Executions of RPOL are also omitted. An execution of
the VERIFY knowledge source often immediately follows the execution
of the PREDICT knowledge source. The two knowledge source executions are therefore combined into one step.
To help you understand the following sequence of processing steps
and the figure, here is an explanation of the notation we have used:
The number in brackets behind a word or phrase denotes its
credibility rating.
The rating of a hypothesis is not the only parameter the Scheduler
uses to assign priorities to waiting knowledge source activations. In
particular, the length of a hypothesis is also important. The phrase
If all ten word predictions preceding the phrase had been rejected, the
phrase hypothesis itself would also be rejected.
The phrase happens to be a complete sentence, and is therefore a
candidate for the interpretation of the spoken input.
In the figure that follows, an arc points from one hypothesis to another if one hypothesis is derived from the other in a single processing
step. The arc is labeled with the number of the processing step.
Dashed arcs point to hypotheses that were already on the blackboard
before step 17.
Consequences The Blackboard.approach to problem decomposition and knowledge
application helps to resolve most of the forces listed in the problem
section:
Experimentation. In domains in which no closed approach exists and
a complete search of the solution space is not feasible, the Blackboard
pattern makes experimentation with different algorithms possible,
and also allows different control heuristics to be tried.
Support for changeability and maintainability. The Blackboard
architecture supports changeability and maintainability because the
individual knowledge sources, the control algorithm and the central
data structure are strictly separated. However, all modules can
communicate via the blackboard.
Reusable knowledge sources. Knowledge sources are independent
specialists for certain tasks. A Blackboard architecture helps in
making them reusable. The prerequisites for reuse are that
knowledge source and the underlying Blackboard system understand
the same protocol and data, or are close enough in this respect not to
rule out adaptors for protocol or data.
Support for fault tolerance and robustness. In a Blackboard
architecture all results are just hypotheses. Only those that are
strongly supported by data and other hypotheses survive. This provides tolerance of noisy data and uncertain conclusions.
The Blackboard pattern has some liabilities:
Difficulty of testing. Since the computations of a Blackboard system
do not follow a deterministic algorithm, its results are often not
reproducible. In addition, wrong hypotheses are part of the solution
process.
No good solution is guaranteed.Usually Blackboard systems can solve
only a certain percentage of their given tasks correctly.
Difficulty of establishing a good control strategy. The control strategy
cannot be designed in a straightforward way, and requires a n
experimental approach.
Low Efficiency. Blackboard systems suffer from computational
overheads in rejecting wrong hypotheses. If no determinist algorithm exists, however, low efficiency is the lesser of two evils when
compared to no system at all.
High development effort. Most Blackboard systems take years to
evolve. We attribute this to the ill-structured problem domains and
extensive trial-and-error programming when defining vocabulary,
control strategies and knowledge sources.
No support for parallelism. The Blackboard architecture does not
prevent the use of a control strategy that exploits the potential
parallelism of knowledge sources. It does not however provide for
their parallel execution. Concurrent access to the central data on the
blackboard must also be synchronized.
To summarize, the Blackboard architecture allows an interpretative
use of knowledge. It evaluates alternative actions, chooses the best
for the current situation, and then applies the most promising
knowledge source. The expense for such deliberation can be justified
so long as no adequate explicit algorithm is available for the problem.
When such an algorithm emerges, it usually provides higher
performance and effectiveness. The Blackboard architecture
consequently lends itself best to immature domains in which
experimentation is helpful. After research and the gaining of
experience, better algorithms may evolve that allow you to use a more
efficient architecture.
This occurred in the domain of speech recognition. For example, in
the HARPY system, a successor to HEARSAY-11, most of the
knowledge is precompiled into a unified structure that represents all
possible spoken phrases IEHLR881. All inter-level substitutions, such
a s segment to phone. phone to word, and word to phrase are compiled
into a single enormous finite-state Markov network. An interpreter
then compares segments of the spoken phrase with this structure to
find a network path that most closely approximates the segmented
speech signal. The search technique used, called beam search,
combined with word lattices, is a heuristic form of dynamic
programming. Acoustical and linguistic knowledge are no longer
combined via a blackboard, but rather by a 'maximum likelihood'
computation. A window slides over the input and continuously
appends new results to the output. This allows speech recognition to
be used in a real-time fashion. [Mar951 gives a recent update on
simpler speech recognition products. For more details on speech
recognition, see [HAJSO],[Rab86]and [Rab89].
##%%&&
The Broker architectural pattern can be used to structure distributed
software systems with decoupled components that interact by remote
service invocations. A broker component Is responsible for
coordinating communication, such as forwarding requests. as well as
for transmitting results and exceptions.
Example Suppose we are developing a city information system (CIS)designed
to run on a wide area network. Some computers in the network host
one or more services that maintain information about events, restaurants. hotels, historical monuments or public transportation.
Computer terminals are connected to the network. Tourists throughout the city can retrieve information in which they are interested from
the terminals using a World Wide Web IWWW)browser. This front-end
software supports the on-line retrieval of information from the appropriate servers and its display on the screen. The data is distributed
across the network. and is not all maintained in the terminals.
We expect the system to change and grow continuously, so the
individual services should be decoupled from each other. In addition.
the terminal software should be able to access services without
having to know their location. This allows us to move, replicate, or
migrate services. One solution is to install a separate network that
connects all terminals and servers, leading to an intranet system.
Such an approach, however, has several disadvantages: not every
information provider wants to connect to a closed intranet, and even
more importantly, available services should also be accessible from
all over the world. We therefore decide to use the Internet as a better
means of implementing the CIS system.
Context Your environment is a distributed and possibly heterogeneous system
with independent cooperating components.
Problem Building a complex software system as a set of decoupled and inter operating components, rather than as a monolithic application,
results in greater flexibility, maintainability and changeability. By
partitioning functionality into independent components the system
becomes potentially distributable and scalable.
However, when distributed components communicate with each
other, some means of inter-process communication is required. If
components handle communication themselves, the resulting system
faces several dependencies and limitations. For example, the system
becomes dependent on the communication mechanism used, clients
need to know the location of servers, and in many cases the solution
is limited to only one programming language.
Services for adding, removing, exchanging, activating and locating
components are also needed. Applications that use these services
should not depend on system-specific details to guarantee portability
and interoperability, even within a heterogeneous network.
From a developer's viewpoint, there should essentially be no differ ence between developing software for centralized systems and
developing for distributed ones. An application that uses an object
should only see the interface offered by the object. It should not need
to know anything about the implementation details of an object, or
about its physical location.
Use the Broker architecture to balance the following forces:
Components should be able to access services provided by others
through remote, location-transparent service invocations.
You need to exchange, add, or remove components at run-time.
The architecture should hide system- and implementation-specific
details from the users of components and services.
Solution Introduce a broker component to achieve better decoupling of clients
and servers. Servers register themselves with the broker, and make
their services available to clients through method interfaces. Clients
access the functionality of servers by sending requests via the broker.
A broker's tasks include locating the appropriate server, forwarding
the request to the server and transmitting results and exceptions
back to the client.
By using the Broker pattern, an application can access distributed
services simply by sending message calls to the appropriate object.
instead of focusing on low-level inter-process communication. In
addition, the Broker architecture is flexible,in that it allows dynamic
change, addition, deletion, and relocation of objects.
The Broker pattern reduces the complexity involved in developing
distributed applications, because it makes distribution transparent
to the developer. I t achieves this goal by introducing an object model
in which distributed services are encapsulated within objects. Broker
systems therefore offer a path to the integration of two core
technologies: distribution and object technology. They also extend
object models from single applications to distributed applications
consisting of decoupled components that can run on heterogeneous
machines and that can be written in different programming
languages.
Structure The Broker architectural pattern comprises six types of participating
components: clients. servers, brokers, bridges, client-side proxies and
server-side proxies.
A server1' implements objects that expose their functionality through
interfaces that consist of operations and attributes. These interfaces
are made available either through an interface definition language
(IDL) or through a binary standard. The Implementation section
contains a comparison of these approaches. Interfaces typically
10. In this pattern description servers are responsible for implementing services. In
an object-oriented approach every services realized by one or more objects.We use the
term server object to emphasize the fact that such a server appears to other
components as an object in the object-oriented sense.
textural Patterns
group semantically-related functionality. There are two kinds of
servers:
Servers offering common services to many application domains.
Servers implementing specific functionality for a single application
domain or task.
b The servers in our CIS example comprise WUW servers that
provide access to HTML (Hypertext Markup Language) pages. VWWV
servers are implemented as http daemon processes (hypertext
transfer protocol daemon) that wait on specific ports for incoming re quests. When a request arrives at the server, the requested document
and any additional data is sent to the client using data streams. The
HTML pages contain documents as well as CGI (Common Gateway interface) scripts for remotely-executed operations on the network
host-the remote machine from which the client received the HTML page. A CGI script may be used to allow the user fill out a form and
submit a query, for example a search request for vacant hotel rooms.
To display animations on the client's browser, Java 'applets'
are integrated into the HTML documents. For example, one of these
Java applets animates the route between one place and another on a
city map. Java applets run on top of a virtual machine that is part of
the TNWWbrowser. CGI scripts and Java applets differ from each other: CGI scripts are executed on the server machine, whereas Java
applets are transferred to the WWW browser and then executed on
the client machine.
Clients are applications that access the services of at least one server.
To call remote services, clients forward requests to the broker. After
an operation has executed they receive responses or exceptions from
the broker.
The interaction between clients and servers is based on a dynamic
model, which means that servers may also act as clients. This
dynamic interaction model differs from the traditional notion of
Client-Server computing in that the roles of clients and servers are
not statically defined. From the viewpoint of an implementation, you
can consider clients a s applications and servers a s libraries-though
other implementations are possible. Note that clients do not need to
know the location of the servers they access. This is important,
because it allows the addition of new services and the movement of
existing services to other locations, even while the system is running.
In the context of the Broker pattern, the clients are the available
WWW browsers. They are not directly connected to the network.
Instead, they rely on Internet providers that offer gateways to the
Internet, such as CompuServe. WWW browsers connect to these
workstations, using either a modem or a leased line. When connected
they are able to retrieve data streams from http servers, interpret
this data and initiate actions such as the display of documents on the
screen or the execution of Java applets. 
A broker is a messenger that is responsible for the transmission of
requests from clients to servers, as well as the transmission of
responses and exceptions back to the client. A broker must have
some means of locating the receiver of a request based on its unique
system identifier. A broker offers APIs (Application Programming
Interfaces) to clients and servers that include operations for
registering servers and for invoking server methods.
When a request arrives for a server that is maintained by the local
broker1', the broker passes the request directly to the server. If the
server is currently inactive, the broker activates it. All responses and
exceptions from a service execution are forwarded by the broker to the
client that sent the request. If the specified server is hosted by
another broker, the local broker finds a route to the remote broker
11. In this pattern description we distinguish between local and remote brokers. A
local broker is running on the machine currently under consideration. A remote broker
is running on a remote network node.
and forwards the request using this route. There is therefore a need
for brokers to interoperate.
Depending on the requirements of the whole system, additional
services-such a s name services 12 or marshaling support may be
integrated into the broker.
A broker in our CIS example is the combination of an Internet
gateway and the Internet infrastructure itself. Every information
exchange between a client and a server must pass through the
broker. A client specifies the information it wants using unique
identifiers called URLs (Universal Resource Locators). By using these
identifiers the broker is able to locate the required services and to
route the requests to the appropriate server machines. When a new
server machine is added, it must be registered with the broker.
Clients and servers use the gateway of their Internet provider as an
interface to the broker.
Client-side proxies represent a layer between clients and the broker.
This additional layer provides transparency, in that a remote object
appears to the client a s a local one. In detail, the proxies allow the
hiding of implementation details from the clients such as:
12. Name services provide associations between names and objects. To resolve a
name, a name service determines which server is associated with a given name. In the
context of Broker systems, names are only meaningful relative to a name space.
13. Marshaling is the semantic-invariant conversion of data into a machine independent format such as ASN.1 (Abstract Syntax Notation or ONC XDR [external
Data Representation). Unmarshaling performs the reverse transformation.
Broker
The inter-process communication mechanism used for message
transfers between clients and brokers.
The creation and deletion of memory blocks.
The marshaling of parameters and results.
In many cases, client-side proxies translate the object model specified
as part of the Broker architectural pattern to the object model of the
programming language used to implement the client.
Sewer-side proxies are generally analogous to Client-side proxies. The
difference is that they are responsible for receiving requests,
unpacking incoming messages, unmarshaling the parameters, and
calling the appropriate service. They are used in addition for
marshaling results and exceptions before sending them to the client.
When results or exceptions are returned from a server,the Client-side
proxy receives the incoming message from the broker, unmarshals
the data and forward it to the client.
r In our CIS example the WWW browsers and httpd servers such
as Netscape provide built-in capabilities for communicating with the
gateway of the Internet provider, so we do not need to worry about
proxies in this case.
 Bridges are optional components used for hiding implementation
details when two brokers interoperate. Suppose a Broker system runs
on a heterogeneous network. If requests are transmitted over the
14. We call these components Bridges following the terminology of the OMG in the
CORBA 2 specification.
Architectural Patterns
network, different brokers have to communicate independently of the
different network and operating systems in use. A bridge builds a
layer that encapsulates all these system-specific details.
w Bridges are not required in our CIS example, because all http
servers and WWW browsers implement the protocols necessary for
remote data exchange such as http (hypertext transfer protocol) or
ftp (file transfer protocol). 
There are two different kinds of Broker systems: those using direct
communication and those using indirect communication. To achieve
better performance, some broker implementations only establish the
initial communication link between a client and a server, while the
rest of the communication is done directly between participating
components-messages, exceptions and responses are transferred
between client-side proxies and server-side proxies without using the
broker as an intermediate layer. This direct communication approach
requires that servers and clients use and understand the same
protocol. In this pattern description we focus on the Indirect Broker
variant, where all messages are passed through the broker. The
Client-Dispatcher-Server pattern (323) describes the important
aspects of the direct variant of the Broker pattern.
Our CIS example implements the indirect communication
variant, because browsers and servers can only collaborate using
Inter-net gateways. There is one place in CIS however where we use
the direct communicator variant instead4ava applets loaded from
the network may connect directly to the WWW server from which they
came using a socket connection. 
The following diagram shows the objects involved in a Broker system:
Dynamics This section focuses on the most relevant scenarios in the operation
of a Broker system.
Scenario I illustrates the behavior when a server registers itself with
the local broker component:
The broker is started in the initialization phase of the system. The
broker enters its event loop and waits for incoming messages.
The user, or some other entity, starts a server application. First, the
server executes its initialization code. After initialization is complete, the server registers itself with the broker.
The broker receives the incoming registration request from the
server. It extracts all necessary information from the message and
stores it into one or more repositories. These repositories are used
to locate and activate servers. An acknowledgment is sent back.
After receiving the acknowledgment from the broker, the server
enters its main loop waiting for incoming client requests.
Scenario 11 illustrates the behavior when a client sends a request to
a local server. In this scenario we describe a synchronous invocation.
in which the client blocks until it gets a response from the server. The
broker may also support asynchronous invocations, allowing clients
to execute further tasks without having to wait for a response.
The client application is started. During program execution the
client invokes a method of a remote server object.
The client-side proxy packages all parameters and other relevant
information into a message and forwards this message to the local
broker.
The broker looks up the location of the required server in its repositories. Since the server is available locally, the broker forwards the
message to the corresponding server-side proxy. For the remote
case, see the following scenario.
The server-side proxy unpacks all parameters and other information, such as the method it is expected to call. The server-side
proxy invokes the appropriate service.
After the service execution is complete, the server returns the
result to the server-side proxy, which packages it into a message
with other relevant information and passes it to the broker.
The broker forwards the response to the client-side proxy.
The client-side proxy receives the response, unpacks the result and
returns to the client application. The client process continues with
Its computation.
Scenario IIl illustrates the interaction of different brokers via bridge
components:
Broker A receives an incoming request. It locates the server responsible for executing the specified service by looking it u p in the
repositories. Since the corresponding server is available at another
network node, the broker forwards the request to a remote broker.
The message is passed from Broker A to Bridge A. This component
is responsible for converting the message from the protocol defined
by Broker A to a network-specific but common protocol understood
by the two participating bridges. After message conversion, Bridge
A transmits the message to Bridge B.
Bridge B maps the incoming request from the network-specific
format to a Broker B-specific format.
Broker B performs all the actions necessary when a request
arrives, as described in the &st step of this scenario.
To implement this pattern, carry out the following steps:
 Define an object model, or use an existing model. Your choice of object
model has a major impact on all other parts of the system under
development. Each object model must specify entities such as object
names, objects, requests, values, exceptions, supported types, type
extensions, interfaces and operations. In this first step you should
only consider semantic issues. If the object model has to be
extensible, prepare the system for future enhancements. For
example, specify a basic object model and how it can be refined
systematically using extensions. More information on this topic is
available in [OMG921.
The description of the underlying computational model is a key issue
in designing an object model. You need to describe definitions of the
state of server objects, definitions of methods. how methods are
selected for execution and how server objects are generated and
destroyed. The state of server objects and their method implementations should not be directly accessible to clients. Clients may
only change or read the server's state indirectly by passing requests
to the local broker. With this separation of interfaces and server
implementations the so-called 'remoting' of interfaces becomes
possible-clients use the client-side proxies as server interfaces that
are completely decoupled from the server implementations, and thus
from the concrete implementations of the server interfaces.
2 Decide which kind of component-interoperability the system should
offer. You can design for interoperability either by specifying a binary
standard or by introducing a high-level interface depiction language
[IDL). An IDL file contains a textual description of the interfaces a
server offers to its clients. The binary approach needs support from
your programming language. For example, binary method tables are
available in Microsoft Object Linking and Embedding [OLE) Bro94j.
These tables consist of pointers to method implementations, and
enable clients to call methods indirectly using pointers. Access to
OLE objects is only supported by compilers or interpreters that know
the physical structure of these tables.
In contrast to the binary approach, the IDL approach is more flexible
in that an IDL mapping may be implemented for any programming
language. Sometimes both approaches are used in combination, as in
IBM's System Object Model [SOM) [Cam94].
Architectural Patterns
An IDL compiler uses an IDL file as input and generates programming-language code or binary code. One part of this generated code
is required by the server for communicating with its local broker,
another part is used by the client for communicating with its local
broker. The broker may use the IDL specification to maintain type
information about existing server implementations.
Whenever interoperability is provided as a binary standard, every
semantic concept of the object model must be associated with a
binary representation. However, if you supply an interface definition
language for interoperability, you can map the semantic concepts to
programming language representations. For example, object handles
may be represented by C++ pointers and data types may be mapped
to appropriate C++ types.
One question remains--when should a Broker system expose interfaces with an interface definition language, and when by a binary
standard? The rationale for the first approach is to gain more fled ability for the broker's implementation-every implementation of the
Broker architecture may define its own protocol for the interaction
between the broker and other components. It is the task of the IDL to
provide a mapping to the local broker protocol. When following a
binary approach, you need to define binary representations such as
method tables for invoking remote services.This often leads to greater
efficiency, but requires all brokers to implement the same kind of
protocol when communicating with clients and servers.
Specify the APIs the broker component provides for collaborating with
clients and servers. On the client side, functionality must be available
for constructing requests, passing them to the broker and receiving
responses. Decide whether clients should only be able to invoke
server operations statically, allowing clients to bind the invocations at
compile-time. If you want to allow dynamic invocations15 of servers
as well, this has a direct impact on the size or number of APIs. For
example, you need some way of asking the broker about existing
server objects. You can implement this with the help of a meta-level
schema, as described in the Reflection pattern (193).
15. Dynamic invocations are method calls that are dynamically constructed at run time using API functions as well as type information. In contrast. static invocations are
hard-coded into the source code.
You have to offer operations to clients, so that they are capable of
constructing requests at run-time. The server implementations use
API functions primarily for registering with the broker. Brokers use
repositories to maintain the information. These repositories may be
available as external files, so that servers can register themselves
before system start-up. Another approach is to implement the
repository as an internal part of the broker component. Here, the
broker must offer an API that allows servers to register at run-time.
Since the broker needs to identify these servers when requests arrive,
an appropriate identification mechanism is necessary. In other
words, the broker component is responsible for associating server
object identifiers with server object implementations. The server-side
API of the broker must therefore be able to generate system-unique
identifiers.
If clients, servers and the broker are running as distinct processes.
the API functions need to be based on an efficient mechanism for
inter-process communication between clients, servers and the local
broker.
4 Use proxy objects to hide implementation details from clients and
sewers. On the client side, a local proxy (263)object represents the
remote server object called by the client. On the server side, a proxy
is used for playing the role of the client. Proxy objects have the
following responsibilities:
Client-side proxies package procedure calls into messages and
forward these messages to the local broker component. In addition,
they receive responses and exceptions from the local broker and
pass them to the calling client. You must specify an internal
message protocol for communication between proxy and broker to
support this.
Server-side proxies receive requests from the local broker and call
the methods in the interface implementation of the corresponding
server. They forward server responses and exceptions to the local
broker after packaging them, according to an internal message
protocol.
Note that proxies are always part of the corresponding client or server
process.
Proxies hide implementation details by using their own inter-process
communication mechanism to communicate with the broker component. They may also implement the marshaling and unmarshaling of
parameters and results into/from a system-independent format.
If you follow the IDL approach for interoperability, proxy objects are
automatically available, because they can be generated by a n IDL
compiler. If you use a binary approach, the creation and deletion of
proxy objects can happen dynamically.
5 Design the broker component in parallel with steps 3 and 4. In this
step we describe how to develop a broker component that acts a s a
messenger for every message passed from a client to a server and vice versa. To increase the performance of the whole system, some
implementations do not transmit messages via the broker. In these
systems most of the work is done by the proxies, while the broker is
still responsible for establishing the initial communication link
between clients and servers. A direct communication between client
and server is only possible when both of them can use the same
protocol. We call such systems Direct CommunicationBroker systems
(seeVariants section).
During design and implementation, iterate systematically through
the following steps:
5.1 Specify a detailed on-the-wire protocol for interacting with client-side
proxies and server-side proxies. Plan the mapping of requests,
responses, and exceptions to your internal message protocol. In an
on-the-wire protocol, the internal message protocol handles the
mapping of higher-level structures such a s parameter values, method
names and return values to corresponding structures specified by the
underlying inter-process communication mechanism.
5.2 A local broker must be available for every participating machine in the
network. If requests, responses or exceptions are transferred from
one network node to another, the corresponding local brokers must
communicate with each other using a n on-the-wire protocol. Use
bridges to hide details such as network protocols and operating
system specifics from the broker. The broker must also maintain a
repository to locate the remote brokers or gateways to which it
forwards messages. You may encode the routing information for
finding remote brokers as a part of the server or client identifier.
Broadcast communication is another (potentially inefficient) way to
locate the network node where a server or client resides.
5.3 When a client invokes a method of a server, the Broker system is
responsible for returning all results and exceptions back to the original client. In other words, the system must remember which client
has sent the request. In the Direct Communication variant (see the
Variants section) there is no need to remember the originator of an in vocation, because the client and the server are directly connected
through a communication channel. In Indirect Broker systems you
can choose between different means of remembering the sender of a
request. For example, you may specific the client's address as an additional, invisible parameter of the request or message.
5.4 If the proxies (see step 4) do not provide mechanisms for marshaling
and unmarshaling parameters and results, you must include that
functionality in the broker component.
5.5 If your system supports asynchronous communication between
clients and servers, you need to provide message buffers within the
broker or within the proxies for the temporary storage of messages.
5.6 Include a directory service for associating local server identifiers with
the physical location of the corresponding servers in the broker. For
example, if the underlying inter-process communication protocol is
based on TCP/IP, you could use an Internet port number as the
physical server location.
5.7 When your architecture requires system-unique identifiers to be
generated dynamically during server registration, the broker must
offer a name service for instantiating such names.
5.8 If your system supports dynamic method invocation (see step 3),the
broker needs some means for maintaining type information about
existing servers. A client may access this information using the
broker APIs to construct a request dynamically. You can implement
such type information by instantiating the Reflection pattern (193).In
this, metaobjects maintain type information that is accessible by a
metaobject protocol.
5.9 Consider the case in which something fails. In a distributed system
two levels of errors may occur:
A component such a s a server may run into an error condition.
This is the same kind of error you encounter when executing
conventional non-distributed applications.
The communication between two independent processes may fail.
Here the situation is more complicated, since the communicating
components are running asynchronously.
Plan the broker's actions when the communication with clients, other
brokers or servers fails. For example, some brokers resend a request
or response several times until they succeed. If you use an at-most once semantic 16, you have to make sure that a request is only
executed once even if it is resent. Do not forget the case in which a
client tries to access a server that either does not exist, or which the
client is not allowed to access. Error handling is an important topic
when implementing a distributed system. If you forget to handle errors in a systematic way, testing and debugging of client applications
and servers becomes an extremely tedious job.
6 Develop IDL compilers. Whenever you implement interoperability by
providing an interface definition language, you need to build a n IDL
compiler for every programming language you support. An IDL
compiler translates the server interface definitions to programming
language code. When many programming languages are in use, it is
best to develop the compiler as a framework that allows the developer
to add his own code generators.
Example Our example CIS system offers different kinds of services. For
Resolved example, a separate server workstation provides all the information
related to public transport. Another server is responsible for
collecting and publish information on vacant hotel rooms. A
tourist may be interested in retrieving information from several
16. When supporting at-most-once semantics your system has to guarantee that
any request either fails, or is executed only once. If you Implement other semantics
instead such as at-least-once,the same request may be resent and executed several
times. This strategy is only applicable to idempotent services, where overall consistency
is not damaged by executing a service more than once. A typical example of an
idempotent service is a function that assigns an initial value to a variable.
hotels, so we decide to provide this data on a single workstation.
Every hotel can connect to the workstation and perform updates.
A tourist is capable of booking hotel rooms on-line from anywhere in
the Internet using CGI scripts. Payments for hotel reservations are
charged on-line by credit card. For security reasons we include encryption mechanisms for such transactions. Additional httpd servers
are available to provide extra services such as flight booking or train
reservations, the ordering of tickets or the retrieval of information
about museums and other places of interest.
Each CIS terminal executes a WWW browser. This allows us to use
inexpensive PCs and Internet PCs as terminals. The httpd servers run
on fast UNM and Windows NT workstations to guarantee short
response times.
Variants Direct Communication Broker System. You may sometimes choose to
relax the restriction that clients can only forward requests through
the local broker for efficiency reasons. In this variant clients can
communicate with servers directly. The broker tells the clients which
communication channel the server provides. The client can then
establish a direct link to the requested server. In such systems, the
proxies take over the broker's responsibility for handling most of the
communication activities. A similar argument applies to off-board
communication: here clients address the remote broker directly,
using bridges when appropriate, as opposed to sending requests to
their local broker for forwarding to the remote server's broker.
Message Passing Broker S y s t e m This variant is suitable for systems
that focus on the transmission of data, instead of implementing a
Remote Procedure Call abstraction17. Using this variant, servers use
the type of a message to determine what they must do, rather than
offering services that clients can invoke. In this context, a message is
a sequence of raw data together with additional information that
specifies the type of a message, its structure and other relevant
attributes.
Trader S y s t e m A client request is usually forwarded to exactly one
uniquely-identified server. In some circumstances, services and not
17. Brokers offering RPC (Remote Procedure Call) interfaces are typically built
using message-passing interfaces.
servers are the targets to which clients send their requests. In a
Trader system, the broker must know which server(s) can provide the
service, and forward the request to an appropriate server. Client-side
proxies therefore use service identifiers instead of server identifiers to
access server functionality.The same request might be forwarded to
more than one server implementing the same service.
Adapter Broker System. You can hide the interface of the broker
component to the servers using an additional layer, to enhance
flexibility.This adapter layer is a part of the broker and is responsible
for registering servers and interacting with servers. By supplying
more than one adapter, you can support different strategies for server
granularity and server location. For example, if all the server objects
accessed by an application are located on the same machine and are
implemented as library objects, a special adapter could be used to
link the objects directly to the application.Another example is the use
of an object-oriented database for maintaining objects. Since the
database is responsible for providing methods and storing objects,
there may be no need to register objects explicitly. In such a scenario,
you could provide a special database adapter. See also IOMG92).
Callback Broker System Instead of implementing an active
communication model in which clients produce requests and servers
consume them, you can also use a reactive model. The reactive model
Is event-driven, and makes no distinction between clients and
servers. Whenever an event arrives, the broker invokes the callback
method of the component that is registered to react to the event. The
execution of the method may generate new events that in turn cause
the broker to trigger new callback method invocations. For more
details on this variant, see [Sch94].
There are several ways of combining the above variants. For example,
you can implement a Direct Communication Broker system and
combine it with the Trader variant. In such a system an incoming
client request causes the broker to select one server among those that
provide the requested service. The broker then establishes a direct
link between the client and the selected server.
Known Uses CORBA. The Broker architectural pattern was used to specify the
Common Object Request Broker Architecture (CORBA)defined by the
Object Management Group. CORBA is an object-oriented technology
for distributing objects on heterogeneous systems. An interface
definition language is available to support the interoperability of
client and server objects [OMG92]. Many CORBA implementations
realize the Direct CommunicationBroker System variant, for example
IONA Technologies' Orbix IIona951.
IBM SOM/DSOM. [Cam941 represents a CORBA-compliant Broker
system. In contrast to many other CORBA implementations, it
implements interoperability by combining the CORBA interface
definition language with a binary protocol. SOM's binary approach
supports subclassing from existing binary parent classes. You can
implement a class in SOM in one programming language and derive
a subclass from it in another language.
Microsoft's OLE 2.x technology provides another example of the use
of the Broker architectural pattern. While CORBA guarantees inter operability using an interface definition language, OLE 2.x defines a
binary standard for exposing and accessing server interfaces IBro941.
The World Wide Web is the largest available Broker system in the
world. Hypertext browsers such as HotJava, Mosaic, and Netscape
act as brokers and WWW servers play the role of service providers.
ATM-P. We implemented the Message Passing Broker System variant
[ATM93]in a Siemens in-house project to build a telecommunication
switching system based on ATM (AsynchronousTransfer Mode).
Consequences The Broker architectural pattern has some important benefits:
Location Transparency. As the broker is responsible for locating a
server by using a unique identifier, clients do not need to know where
servers are located. Similarly, servers do not care about the location
of calling clients, as they receive all requests from the local broker
component.
Changeability and extensibility of components. If servers change but
their interfaces remain the same, it has no functional impact on
clients. Modifying the internal implementation of the broker, but not
the APIs it provides, has no effect on clients and servers other than
performance changes. Changes in the communication mechanisms
used for the interaction between servers and the broker, between
clients and the broker, and between brokers may require you to
recompile clients, servers or brokers. However, you will not need to
change their source code. Using proxies and bridges is an important
reason for the ease with which changes can be implemented.
Portability of a Broker system. The Broker system hides operating system and network system details from clients and servers by using
indirection layers such a s APIs, proxies and bridges. When porting is
required, it is therefore sufficient in most cases to port the broker
component and its APIs to a new platform and to recompile clients
and servers. Structuring the broker component into layers is recommended, for example according to the Layers architectural pattern
(31).If the lower-most layers hide system-specific details from the
rest of the broker, you only need to port these lower-most layers,
instead of completely porting the broker component.
Interoperability between different Broker systems. Different Broker
systems may interoperate if they understand a common protocol for
the exchange of messages. This protocol is implemented and handled
by bridges, which are responsible for translating the broker-specific
protocol into the common protocol, and vice versa.
Reusability. When building new client applications, you can often
base the functionality of your application on existing services. Sup pose you are going to develop a new business application. If components that offer services such as text editing, visualization, printing,
database access or spreadsheets are already available, you do not
need to implement these services yourself. It may instead be sufficient
to integrate these services into your applications.
The Broker architectural pattern imposes some liabilities:
Restricted efficiency. Applications using a Broker implementation are
usually slower than applications whose component distribution is
static and known. Systems that depend directly on a concrete mechanism for inter-process communication also give better performance
than a Broker architecture, because Broker introduces indirection
layers to enable it to be portable, flexible and changeable.
Lower fault tolerance. Compared with a non-distributed software system, a Broker system may offer lower fault tolerance. Suppose that a
server or a broker fails during program execution. All the applications
that depend on the server or broker are unable to continue success fully. You can increase reliability through replication of components.
The following aspect gives benefits as well as liabilities:
Testing and Debugging. A client application developed from tested
services is more robust and easier itself to test. However, debugging
and testing a Broker system is a tedious job because of the many
components involved. For example, the cooperation between a client
and a server can fail for two possible reasons--either the server has
entered an error state, or there is a problem somewhere on the
communication path between client and server.
See also The Forwarder-Receiver pattern (307) encapsulates inter-process
communication between two components. On the client side a
forwarder receives a request and addressee from the client and
handles the mapping to the IPC (inter-process communication)
facility used. The receiver on the server side unpacks and delivers the
message to the server. There is no broker component in this pattern.
It is simpler to implement and results in smaller implementations
than the Broker pattern, but is also less flexible.
The Proxy pattern (263)comes in several flavors, the remote case
being one of them. A remote proxy is often used in conjunction with
a forwarder. The proxy encapsulates the interface and remote address
of the server. The forwarder takes the message and transforms it into
IPC-level code.
The Client-DispatcherServerpattern (323)is a lightweight version of
the Direct Communication Broker variant. A dispatcher allocates,
opens and maintains a direct channel between client and server.
The Mediator design pattern IGHJV951replaces a web of inter-object
connections by a star configuration in which the central mediator
component encapsulates collective behavior by defining a common
interface for communicating with objects. As with the Broker pattern,
the Mediator pattern uses a hub of communication, but it also has
several major differences. The Broker pattern is a large-scale infra structure paradigm-it is not used for building single applications,
but rather serves as a platform for whole families of applications. It is
not restricted to processing local computation, and dispatches and
monitors requests without regard to the sender or the content of the
request. In contrast, the Mediator pattern encapsulates application
semantics by checking what a request is about and possibly where it
came from--only then does it decide what to do. It may return a message to the sender, fulfill the request on its own, or involve more than
one other component.
##%%&&
The Model-View-Controller architectural pattern (MVC) divides an
interactive application into three components. The model contains
the core functionality and data. Views display information to the user.
Controllers handle user input. Views and controllers together
comprise the user interface. A change-propagation mechanism
ensures consistency between the user interface and the model.
Example Consider a simple information system for political elections with
proportional representation. This offers a spreadsheet for entering
data and several kinds of tables and charts for presenting the current
results. Users can interact with the system via a graphical interface.
All information displays must reflect changes to the voting data
immediately.
It should be possible to integrate new ways of data presentation, such
as the assignment of parliamentary seats to political parties, without
major impact to the system. The system should also be portable to
platforms with different 'look and feel' standards, such as
workstations running Motif or PCs running MicrosoftWindows 95.
126 Architectural Patterns
Context Interactive applications with a flexible human-computer interface.
Problem User interfaces are especially prone to change requests. When you
extend the functionality of an application,you must modify menus to
access these new functions. A customer may call for a specific user
interface adaptation, or a system may need to be ported to another
platform with a different 'look and feel' standard. Even upgrading to
a new release of your windowing system can imply code changes. The
user interface platform of long-lived systems thus represents a
moving target.
Different users place conflicting requirements on the user interface.
A typist enters information into forms via the keyboard. A manager
wants to use the same system mainly by clicking icons and buttons.
Consequently, support for several user interface paradigms should be
easily incorporated.
Building a system with the required flexibility is expensive and error prone if the user interface is tightly interwoven with the functional
core. This can result in the need to develop and maintain several
substantially different software systems, one for each user interface
implementation. Ensuing changes spread over many modules. The
following forces influence the solution:
The same information is presented differently in different windows,
for example, in a bar or pie chart.
The display and behavior of the application must reflect data
manipulations immediately.
Changes to the user interface should be easy, and even possible at
run-time.
Supporting different 'look and feel' standards or porting the user
interface should not affect code in the core of the application.
Solution Model-View-Controller (MVC]was first introduced in the Smalltalk-80
programming environment [KPSS]. MVC divides an interactive
application into the three areas: processing, output, and input.
The model component encapsulates core data and functionality. The
model is independent of specific output representations or input
behavior.
View components display information to the user. A view obtains the
data from the model. There can be multiple views of the model.
Each view has an associated controller component. Controllers receive
input, usually as events that encode mouse movement, activation of
mouse buttons, or keyboard input. Events are translated to service
requests for the model or the view. The user interacts with the system
solely through controllers.
The separation of the model from view and controller components
allows multiple views of the same model. If the user changes the
model via the controller of one view, all other views dependent on this
data should reflect the changes. The model therefore notifies all views
whenever its data changes. The views in turn retrieve new data from
the model and update the displayed information. This change propagation mechanism is described in the Publisher-Subscriber
pattern (339).
Structure The model component contains the functional core of the application.
It encapsulates the appropriate data, and exports procedures that
perform application-specific processing. Controllers call these procedures on behalf of the user. The model also provides functions to
access its data that are used by view components to acquire the data
to be displayed.
The change-propagation mechanism maintains a registry of the
dependent components within the model. All views and also selected
controllers register their need to be informed about changes. Changes
to the state of the model trigger the change-propagation mechanism.
The change-propagation mechanism is the only link between the
model and the views and controllers.
View components present information to the user. Different views
present the information of the model in different ways. Each view
defines an update procedure that is activated by the change propagation mechanism. When the update procedure is called, a view
retrieves the current data values to be displayed from the model, and
puts them on the screen.
During initialization all views are associated with the model, and
register with the change-propagation mechanism. Each view creates
a suitable controller. There is a one-to-one relationship between views
and controllers. Views often offer functionality that allows controllers
to manipulate the display. This is useful for user-triggered operations
that do not affect the model, such a s scrolling.
The controller components accept user input a s events. How these
events are delivered to a controller depends on the user interface platform. For simplicity, let us assume that each controller implements
a n event-handling procedure that is called for each relevant event.
Events are translated into requests for the model or the associated
view.
If the behavior of a controller depends on the state of the model, the
controller registers itself with the change-propagation mechanism
and implements an update procedure. For example, this is necessary
when a change to the model enables or disables a menu entry.
An object-oriented implementation of MVC would define a separate
class for each component. In a C++ implementation, view and
controller classes share a common parent that defines the update
interface. This is shown in the following diagram. In Smalltalk, the
class Object defines methods for both sides of the change propagation mechanism. A separate class Observer is not needed.
Observer
In our example system the model holds the cumulative votes for
each political party and allows views to retrieve vote numbers. It
further exports data manipulation procedures to the controllers.
We define several views: a bar chart, a pie chart and a table. The chart
views use controllers that do not affect the model, whereas the table
view connects to a controller used for data entry. 
You can also use the MVC pattern to build a framework for interactive
applications, as within the Smalltalk-80 environment [KP88].Such a
framework offers prefabricated view and controller subclasses for
frequently-used user interface elements such as menus, buttons, or
lists. To instantiate the framework for an application, you can
combine existing user interface elements hierarchically using the
Composite pattern IGHJV951.
Dynamics The following scenarios depict the dynamic behavior of MVC. For
simplicity only one view-controller pair is shown in the diagrams.
Scenario I shows how user input that results in changes to the model
triggers the change-propagation mechanism:
* The controller accepts user input in its event-handling procedure,
interprets the event, and activates a service procedure of the
model.
* The model performs the requested service.This results in a change
to its internal data.
* The model notifies all views and controllers registered with the
change-propagation mechanism of the change by calling their
update procedures.
* Each view requests the changed data from the model and re displays itself on the screen.
Each registered controller retrieves data from the model to enable
or disable certain user functions. For example, enabling the menu
entry for saving data can be a consequence of modifications to the
data of the model.
* The original controller regains control and returns from its event handling procedure.
Scenario XI shows how the MVC triad is initialized. This code is usually located outside of the model, views and controllers, for example
in a main program. The view and controller initialization occurs similarly for each view opened for the model. The following steps occur:
The model instance is created, which then initializes its internal
data structures.
A view object is created. This takes a reference to the model as a
parameter for its initialization.
The view subscribes to the change-propagation mechanism of the
model by calling the attach procedure.
The view continues initialization by creating its controller. It passes
references both to the model and to itself to the controller's
initialization procedure.
The controller also subscribes to the change-propagation
mechanism by calling the attach procedure.
After initialization, the application begins to process events.
Implementation Steps 1 through 6 below are fundamental to writing a n MVC-based
application. Steps 7 through 10clescribe additional topics that result
in higher degrees of freedom, and lend themselves to highly flexible
applications or application frameworks.
Separate human-computer interaction from core functionality.Analyze
the application domain and separate the core functionality from the
desired input and output behavior. Design the model component of
your application to encapsulate the data and functionality needed for
the core. Provide functions for accessing the data to be displayed.
Decide which parts of the model's functionality are to be exposed to
the user via the controller, and add a corresponding interface to the
model.
The model in our example stores the names of the political parties and the corresponding votes in two lists of equal length18. Access
to the lists is provided by two methods, each of which creates an iterator. The model also provides methods to change the voting data.
Implement the change-propagation mechanism. Follow the Publisher Subscriber design pattern (339)for this, and assign the role of the
publisher to the model. Extend the model with a registry that holds
references to observing objects. Provide procedures to allow views and
18.An associative array with party names as keys and votes as the information
would be a more realistic implementation but would bloat the example code.
controllers to subscribe and unsubscribe to the change-propagation
mechanism. The model's notify procedure calls the update procedure
of all observing objects. All procedures of the model that change the
model's state call the notify procedure after a change is performed.
Proper C++ usage suggests that one should define an abstract
class Observer to hold the update interface. Both views and
controllers inherit from Observer. The Model class from step 1 is
extended to hold a set of references to current observers, and two
methods, a t t a c h ( ) and d e t a c h ( ) , to allow observing objects to
subscribe and unsubscribe. The method notify ( ) will be called by
methods that modify the state of the model.
Our implementation of the method n o t i f y ( ) iterates over all
Observer objects in the registry and calls their update method. We do
not provide a separate function to create an iterator for the registry,
because it is only used internally.
The methods change vote ( ) and clear votes ( ) call notify ( ) after
the voting data is changed. 
3 Design and implement the views. Design the appearance of each view.
Specify and implement a draw procedure to display the view on the
screen. This procedure acquires the data to be displayed from the
model. The rest of the draw procedure depends mainly on the user
interface platform. It would call, for example, procedures for drawing
lines or rendering text.
Implement the update procedure to reflect changes to the model. The
easiest approach is to simply call the draw procedure. The draw
procedure goes ahead and fetches data needed for the view. For a
complex view requiring frequent updates, such a straightforward
implementation of update can be inefficient. Several optimization
strategies exist in this situation. One is to supply additional
parameters to the update procedure. The view can then decide if a redraw is needed. Another solution is to schedule, but not perform, the
re-draw of the view when it is likely that further events also require it.
The view can then be redrawn when no more events are pending.
In addition to the update and draw procedures, each view needs an
initialization procedure. The initialization procedure subscribes to
the change-propagation mechanism of the model and sets up the
relationship to the controller, a s shown in step 5. After the controller
is initialized, the view displays itself on the screen. The platform or
the controller may require additional view capabilities, such a s a
procedure to resize a view window.
b For all the views used by the election system we define a common
base class View. The relationships to model and controller are
represented by two member variables with corresponding access
methods. The constructor of View establishes the relationship to the
model by subscribing to the change-propagation mechanism. The
destructor removes it again by unsubscribing. View also provides a
simple non-optimized update ( ) implementation.
The class definition of Bar Chart view demonstrates a specific view of
our system. It redefines draw ( ) to show the voting data as a bar
chart. 
4 Design and implement the controllers.For each view of the application,
specify the behavior of the system in response to user actions. We
assume that the underlying platform delivers every action of a user as
an event. A controller receives and interprets these events using a
dedicated procedure. For a non-trivial controller, this interpretation
depends on the state of the model.
The initialization of a controller binds it to its model and view and
enables event processing. How this is achieved depends on the user interface platform. For example, the controller may register its event handling procedure with the window system as a callback.
b Most views in our example do not require any specific event
processing-they are only used for display. We therefore define a base
class Controller with an empty handleEvent 0 method. The
constructor attaches the controller to its model and the destructor
detaches it again.
We omit a separate controller initialization method, because the
relationship to the view and the model is already set up by its
constructor. D
Calling the functional core closely links a controller with the model.
since the controller becomes dependent on the application-specific
model interface. If you plan to modify functionality, or if you want to
provide reusable controllers and therefore would like the controller to
be independent of a specific interface. apply the Command Processor
(277)design pattern. The model takes the role of the supplier of the
Command Processor pattern. The command classes and the
command processor component are additional components between
controller and model. The MVC controller has the role of controller in
Command Processor.
Model-View-Controller 137
5 Design and implement the view-controller relationship.A view typically
creates its associated controller during its initialization. When you
build a class hierarchy of views and controllers, apply the Factory
Method design pattern [GHJV95] and define a method
makecontroller ( 1 in the view classes. Each view that requires a
controller that differs from its superclass redefines the factory
method.
In our C++ example the View base class implements a method
initialize( ) that in turn calls the factory method
make controller ( ) . We cannot put the call to make controller( )
into the constructor of the View class. because then a subclass'
redefined make controller ( ) would not be called a s desired. The
only View subclass that requires a specific controller is TableView.
We redefine make controller ( ) to return a TableController to
accept data from the user.
6 Implement the set-up of MVC. The set-up code first initializes the
model, then creates and initializes the views.After initialization, event
processing is started, typically in a Loop, or with a procedure that
includes a loop, such as XtMainLoop ( ) from the X Toolkit. Because
the model should remain independent of specific views and
controllers, this set-up code should be placed externally, for example,
in a main program.
In our simple example the main function initializes the model
and several views. The event processing delivers events to the controller of the table view, allowing the entry and change of voting data.
7 Dynamic view creation If the application allows dynamic opening and
closing of views, it is a good idea to provide a component for managing
open views. This component, for example, can also be responsible for
terminating the application after the last view is closed. Apply the
View Handler (291) design pattern to implement this view
management component.
8 'Pluggabkgcontrollers.The separation of control aspects from views
supports the combination of different controllers with a view. This
flexibility can be used to implement different modes of operation,
such as casual user versus expert, or to construct read-only views
using a controller that ignores any input. Another use of this
separated is the integration of new input and output devices with an
application. For example, a controller for an eye-tracking device for
disabled people can exploit the functionality of the existing model and
views, and is easily incorporated into the system.
r In our example only the class TableView supports several
controllers. The default controller Tab1eController allows the user
to enter voting data. For display-only purposes, TableView can be
configured with a controller that ignores all user input. The code
below shows how a controller is substituted for another controller.
Note that set controller returns the previously-used controller
object. Here the controller object is no longer used and so it is deleted
immediately.
9 Infrastructure for hierarchical views and controllers. A framework
based on MVC implements reusable view and controller classes. This
is commonly done for user interface elements that are applied
frequently, such as buttons, menus, or text editors. The user
interface of an application is then constructed largely by combining
predefined view objects. Apply the Composite pattern [GHJV95] to
create hierarchically composed views. If multiple views are active
simultaneously, several controllers may be interested in events at the
same time. For example, a button inside a dialog box reacts to a
mouse click, but not to the letter 'a' typed on the keyboard. If the
parent dialog view also contains a text field, the 'a' is sent to the
controller of the text view. Events are distributed to event-handling
routines of all active controllers in some defined sequence. Use the
Chain of Responsibility pattern IGHJV951 to manage this delegation
of events. A controller will pass an unprocessed event to the controller
of the parent view or to the controller of a sibling view if the chain of
responsibility is set up properly.
10 Further decoupling from system dependencies. Building a framework
with an elaborate collection of view and controller classes is
expensive. You may want to make these classes platform
independent. This is done in some Smalltalk systems. You can
provide the system with another level of indirection between it and the
underlying platform by applying the Bridge pattern [GHJV95].Views
use a class named display as an abstraction for windows and
controllers use a sensor class.
The abstract class display defines methods for creating a window,
drawing lines and text, changing the look of the mouse cursor and so
on. The sensor abstraction defines platform-independent events, and
each concrete sensor subclass maps system-specific events to
platform-independent events. For each platform supported,
implement concrete display and sensor subclasses that encapsulate
system specifics.
The design of the abstract classes display and sensor is non-trivial,
because it impacts both the efficiency of the resulting code, and the
efficiency with which the concrete classes can be implemented on the
different platforms. One approach is to use sensor and display
abstractions with only the very basic functionality that is provided
directly by all user-interface platforms. The other extreme is to have
display and sensor offer higher-level abstractions. Such classes need
greater effort to port, but use more native code from the user interface platform. The first approach leads to applications that look
similar across platforms, while the second results in applications that
conform better to platform-specific guidelines.
Variants Document-View. This variant relaxes the separation of view and
controller. In several GUI platforms, window display and event
handling are closely interwoven. For example, the X Window System
reports events relative to a window. You can combine the
responsibilities of the view and the controller from MVC in a single
component by sacrificing exchangeability of controllers. This kind of
structure is often called a Document-View architecture [App89],
[Gam91], [Kru96]. The document component corresponds to the
model in MVC, and also implements a change-propagation
mechanism. The view component of Document-View combines the
responsibilities of controller and view in MVC, and implements the
user interface of the system. As in MVC, loose coupling of the
document and view components enables multiple simultaneous
synchronized but different views of the same document.
Known Uses Smalltalk [GR83].The best-known example of the use of the Model View-Controller pattern is the user-interface framework in the
Smalltalk environment [LP911, IKP88).MVC was established to build
reusable components for the user interface. These components are
shared by the tools that make up the Smalltalk development
environment. However, the MVC paradigm turned out to be useful for
other applications developed in Smalltalk as well. The VisualWorks
Smalltalk environment supports different 'look and feel' standards by
decoupling view and controllers via display and sensor classes, as
described in implementation step 10.
MF'C [Kru96]. The Document-View variant of the Model-View Controller pattern is integrated in the Visual C++ environment-the
Microsoft Foundation Class Library-for developing Windows
applications.
ET++[Gam91]. The application framework ET++also uses the Document-View variant. A typical ET++-based application implements its
own document class and a corresponding view class. ET++ establishes 'look and feel' independence by defining a class Windowport
that encapsulates the user interface platform dependencies, in the
same way as do our display and sensor classes.
Consequences The application of Model-View-Controller has several benefits:
Multiple views of the same model. MVC strictly separates the model
from the user-interface components. Multiple views can therefore be
implemented and used with a single model. At run-Ume, multiple
views may be open at the same time, and views can be opened and
closed dynamically.
Synchronized views. The change-propagation mechanism of the
model ensures that all attached observers are notified of changes to
the application's data at the correct time. This synchronizes all
dependent views and controllers.
'Pluggable'views and controllers. The conceptual separation of MVC
allows you to exchange the view and controller objects of a model.
User interface objects can even be substituted at run-time.
Exchangeability of 'look and feel'. Because the model is independent
of all user-interface code, a port of an MVC application to a new
platform does not affect the functional core of the application. You
only need suitable implementations of view and controller
components for each platform.
Framework potential. It is possible to base an application framework
on this pattern, as sketched in implementation steps 7 through 10.
The various Smalltalk development environments have proven this
approach.
The liability of MVC are a s follows:
Increased complexity. Following the Model-View-Controller structure
strictly is not always the best way to build an interactive application.
Gamma [Gamgl] argues that using separate model, view and
controller components for menus and simple text elements increases
complexity without gaining much flexibility.
Potential for excessive number of updates. If a single user action
results in many updates, the model should skip unnecessary change
notifications. It may be that not all views are interested in every
change-propagated by the model. For example, a view with an
iconized window may not need an update until the window is restored
to its normal size.
Intimate connection between view and controller. Controller and view
are separate but closely-related components, which hinders their
individual reuse. It is unlikely that a view would be used without its
controller, or vice-versa, with the exception of read-only views that
share a controller that ignores all input.
Close coupling of views and controllers to a d e l . Both view and
controller components make direct calls to the model. This implies
that changes to the model's interface are likely to break the code of
both view and controller. This problem is magnified if the system uses
a multitude of views and controllers. You can address this problem by
applying the Command Processor pattern (277),as described in the
Implementation section, or some other means of indirection.
Inefficiency of data access in view. Depending on the interface of the
model, a view may need to make multiple calls to obtain all its display
data. Unnecessarily requesting unchanged data from the model
weakens performance if updates are frequent. Caching of data within
the view improves responsiveness.
Inevitability of change to view and controller when porting. All
dependencies on the user-interface platform are encapsulated within
view and controller.However,both components also contain code that
is independent of a specific platform. A port of an MVC system thus
requires the separation of platform-dependent code before rewriting.
In the case of an MVC framework or a large composed application, an
additional encapsulation of platform dependencies may be required.
Difficulty of using MVC with modern user-interface took. If portability
is not an issue, using high-level toolkits or user interface builders can
rule out the use of MVC. It is usually expensive to retrofit toolkit
components or the output of user interface layout tools to MVC.
Additional wrapping would be the minimum requirement. In addition,
many high-level tools or toolkits define their own flow of control and
handle some events internally, such as displaying a pop-up menu or
scrolling a window. Finally, a high-level user interface platform may
already interpret events and offer callbacks for each kind of user
activity. Most controller functionality is therefore already provided by
the toolkit, and a separate component is not needed.
See Also The Presentation-Abstraction-Control pattern (145)takes a different
approach to decoupling the user-interface aspects of a system from
its functional core. Its abstraction component corresponds to the
model in MVC, and the view and controller are combined into a
presentation component. Communication between abstraction and
presentation components is decoupled by the control component. The
interaction between presentation and abstraction is not limited to
calling an update procedure, as it is within MVC.
##%%&&
The Presentation-Abstraction-Control architectural pattern (PAC)
defines a structure for interactive software systems in the form of a
hierarchy of cooperating agents. Every agent is responsible for a
specific aspect of the application's functionality and consists of three
components: presentation, abstraction, and control. This subdivision
separates the human-computer interaction aspects of the agent from
its functional core and its communication with other agents.
Example Consider a simple information system for political elections with proportional representation. This offers a spreadsheet for entering data
and several kinds of tables and charts for presenting current standings. Users interact with the software through a graphical interface.
Different versions, however, adapt the user interface to specific
needs. For example, one version supports additional views of the
data, such as the assignment of parliament seats to political parties.
Context Development of an interactive application with the help of agents.
19. In the context of this pattern an agent denotes an information-processing
component that includes event receivers and transmitters, data structures to maintain
stale, and a processor that handles incoming events, updates its own state. and
that may produce new events IBaCo911. Agents can be as small a s a single object, but
also as complex as a complete software system. We use the terms agent and PAC agent
a s synonyms in this pattern description.
Problem Interactive systems can often be viewed as a set of cooperating agents.
Agents specialized in human-computer interaction accept user input
and display data. Other agents maintain the data model of the system
and offer functionality that operates on this data. Additional agents
are responsible for diverse tasks such as error handling or
communication with other software systems. Besides this horizontal
decomposition of system functionality, we often encounter a vertical
decomposition. Production planning systems (PPS), for example,
distinguish between production planning and the execution of a
previously specified production plan. For each of these tasks separate
agents can be defined.
In such an architecture of cooperating agents, each agent is specialized for a specific task, and all agents together provide the system
functionality. This architecture also captures both a horizontal and
vertical decomposition. The following forces affect the solution:
Agents often maintain their own state and data. For example, in a
PPS system, the production planning and the actual production
control may work on different data models, one tuned for planning
and simulation and one performance-optimized for efficient
production. However, individual agents must effectively cooperate
to provide the overall task of the application. To achieve this, they
need a mechanism for exchanging, data, messages, and events.
Interactive agents provide their own user interface, since their
respective human-computer interactions often differ widely. For
example, entering data into spreadsheets is done using keyboard
input, while the manipulation of graphical objects uses a pointing
device.
Systems evolve over time. Their presentation aspect is particularly
prone to change. The use of graphics, and more recently, multimedia features, are examples of pervasive changes to user
interfaces. Changes to individual agents, or the extension of the
system with new agents, should not affect the whole system.
Solution Structure the interactive application as a tree-like hierarchy of PAC
agents. There should be one top-level agent, several intermediate level agents, and even more bottom-level agents. Every agent is
responsible for a specific aspect of the application's functionality, and
consists of three components: presentation, abstraction, and control.
The whole hierarchy reflects transitive dependencies between agents.
Each agent depends on all higher-level agents up the hierarchy to the
top-level agent.
The agent's presentation component provides the visible behavior of
the PAC agent. Its abstraction component maintains the data model
that underlies the agent, and provides functionality that operates on
this data. Its control component connects the presentation and
abstraction components, and provides functionality that allows the
agent to communicate with other PAC agents.
The top-level PAC agent provides the functional core of the system.
Most other PAC agents depend or operate on this core. Furthermore,
the top-level PAC agent includes those parts of the user interface that
cannot be assigned to particular subtasks, such as menu bars or a
dialog box displaying information about the application.
Bottom-level PAC agents represent self-contained semantic concepts
on which users of the system can act, such as spreadsheets and
charts. The bottom-level agents present these concepts to the user
and support all operations that users can perform on these agents,
such as zooming or moving a chart.
Intermediate-level PAC agents represent either combinations of, or
relationships between, lower-level agents. For example, an intermediate-level agent may maintain several views of the same data, such
as a floor plan and an external view of a house in a CAD system for
architecture.
Our information system for political elections defines a top-level
PAC agent that provides access to the data repository underlying the
system. The data repository itself is not part of the application. At the
bottom level we specify four PAC agents: one spreadsheet agent for
entering data, and three view agents for each type of diagram for
representing the data. The application has one intermediate-level
PAC agent. This coordinates the three bottom-level view agents and
keeps them consistent. The spreadsheet agent is directly connected
to the top-level PAC agent. Users of the system only interact with
bottom-level agents.
Structure The main responsibility of the top-level PAC agent is to provide the
global data model of the software. This is maintained in the
abstraction component of the top-level agent. The interface of the
abstraction component offers functions to manipulate the data model
and to retrieve information about it. The representation of data within
the abstraction component is media-independent. For example, in a
CAD system for architecture, walls, doors, and windows are
represented in centimeters or inches that reflect their real size, not in
pixels for display purposes. This media-independency supports
adaptation of the PAC agent to different environments without major
changes in its abstraction component.
The presentation component of the top-level agent often has few
responsibilities. It may include user-interface elements common to
the whole application. In some systems, such as the network traffic
manager [TS93],there is no top-level presentation component at all.
The control component of the top-level PAC agent has three
responsibilities:
It allows lower-level agents to make use of the services of the top level agents, mostly to access and manipulate the global data
model. Incoming service requests from lower-level agents are
forwarded either to the abstraction component or the presentation
component.
It coordinates the hierarchy of PAC agents. It maintains
information about connections between the top-level agent and
lower-level agents. The control component uses this information to
ensure correct collaboration and data exchange between the top level agent and lower-level agents.
It maintains information about the interaction of the user with the
system. For example, it may check whether a particular operation
can be performed on the data model when triggered by the user. It
may also keep track of the functions called to provide history or
undo/redo services for operations on the functional core.
s In our example information system for political elections, the
abstraction component of the top-level PAC agent provides an
application-specific interface to the underlying data repository. It
implements functions for reading and writing election data. It also
implements all functions that operate on the election data, such as
algorithms for calculating projections and seat distributions. It
further includes functions for maintaining data, such as those for
updating and consistency checking. The control component organizes
communication and cooperation with lower-level agents, namely the
view coordinator and spreadsheet agents. This top-level PAC agent
does not include a presentation component.
Bottom-levelPAC agents represent a specific semantic concept of the
application domain, such as a mailbox in a network traffic management system [TS93]or a wall in a mobile robot system [Cro85]. This
semantic concept may be as low-level as a simple graphical object
such as a circle, or as complex as a bar chart that summarizes all the
data in the system.
The presentation component of a bottom-level PAC agent presents a
specific view of the corresponding semantic concept, and provides
access to all the functions users can apply to it. Internally, the
presentation component also maintains information about the view,
such as its position on the screen.
The abstraction component of a bottom-level PAC agent has a similar
responsibility as the abstraction component of the top-level PAC
agent, maintaining agent-specific data. In contrast to the abstraction
component of the top-level agent, however, no other PAC agents
depend on this data.
The control component of a bottom-level PAC agent maintains
consistency between the abstraction and presentation components,
thereby avoiding direct dependencies between them. It serves as an
adapter and performs both interface and data adaptation.
The control component of bottom-level PAC agents communicates
with higher-level agents to exchange events and data. Incoming
events-such a s a 'close window' request-are forwarded to the presentation component of the bottom-level agent, while incoming data
is forwarded to its abstraction component. Outgoing events and data,
for example error messages, are sent to the associated higher-level
agent.
Concepts represented by bottom-level PAC agents, such as the bar
and pie charts in the example, are atomic in the sense that they are
the smallest units a user can manipulate. For the election system this
means that users can only operate on the bar chart a s a whole, for
instance by changing the scaling factor of the y-axis. They cannot, for
example, resize a n individual bar of a bar chart.
Bottom-level PAC agents are not restricted to providing semantic
concepts of the application domain. You can also specify bottom-level
agents that implement system services. For example, there may be a
communication agent that allows the system to cooperate with other
applications and to monitor this cooperation.
r Consider a bar-chart agent in our information system for
political elections. Its abstraction component saves the election data
presented in the chart, and maintains chart-specific information
such as the order of presentation for the data. The presentation
component is responsible for displaying the bar chart in a window,
and for providing all the functions that can be applied to it, such as
zooming, moving, and printing. The control component serves as a
level of indirection between the presentation and abstraction
components. The control component is also responsible for the bar chart agent's communication with the view coordinator agent. 
Intermediate-LevelPAC agents can fulfill two different roles: composition and coordination When, for example, each object in a complex
graphic is represented by a separate PAC agent, an intermediate-level
agent groups these objects to form a composite graphical object. The
intermediate-level agent defines a new abstraction, whose behavior
encompasses both the behavior of its components and the new characteristics that are added to the composite object. The second role of
an intermediate-level agent is to maintain consistency between lower level agents, for example when coordinating multiple views of the
same data.
The abstraction component maintains the specific data of the inter mediate-levelPAC agent. The presentation component implements its
user interface. The control component has the same responsibilities
of the control components of bottom-level PAC agents and of the top level PAC agent.
Our example information system for political elections defines
one intermediate-level PAC agent. Its presentation component
provides a palette that allows users to create views of the election
data, such as bar or pie charts. The abstraction component maintains
data about all currently-active views, each of which is realized by its
own bottom-level agent. The main responsibility of the control
component is to coordinate all subordinate agents. It forwards
incoming notifications about data model changes taking place in the
top-level agent to the bottom-level agents, and organizes their update.
It also includes functionality to create and delete bottom-level agents
The following OMT diagram illustrates the PAC hierarchy of the
information system for political elections. However, it only lists those
functions that are necessary for controlling and coordinating the PAC
hierarchy, or which are accessible to other agents or tithe user.
We keep the interfaces of PAC agents small by applying the Composite
Message pattern ISC95bl. All incoming service requests. events. and
data are handled by a single function called receiveMsg ( ) . This
interprets messages and routes them to their intended recipient,
which may be the abstraction or presentation components of the
agent, or of another agent. Similarly,the function sendMsg ( ) is used
to pack and deliver service requests, events, and data to other agents.
Another approach would be to provide an agent-specific interface that
includes all the services the agent offers. The consequences of both
these approaches are discussed in the implementation section.
The internal structure of a PAC agent is shown below, using the bar chart agent from our example:
Dynamics We will illustrate the behavior of a PAC architecture with two
scenarios, both based on our election system example.
Scenario I describes the cooperation between different PAC agents
when opening a new bar-chart view of the election data. The scenario
also includes a more detailed description of the internal behavior of
the bar-chart agent. It is divided into five phases:
A user asks the presentation component of the view coordinator
agent to open a new bar chart.
The control of the view coordinator agent instantiates the desired
bar-chart agent.
The view coordinator agent sends an 'open' event to the control
component of the new bar-chart agent.
The control component of the bar-chart agent first retrieves data
from the top-level PAC agent. The view coordinator agent mediates
between bottom and top-level agents. The data returned to the bar chart agent is saved in its abstraction component. Its control
component then calls the presentation component to display the
chart.
The presentation component creates a new window on the screen,
retrieves data from the abstraction component by requesting it
from the control component, and finally displays it within the new
window.
There are obvious optimizations possible here, such as caching top level data in the view coordinator, or calling the bottom-level
presentation component first and then storing the data. At this point,
however, our emphasis is on explaining the basic ideas of the pattern.
Scenario 11 shows the behavior of the system after new election data
is entered, providing a closer look at the internal behavior of the top level PAC agent. It has five phases:
The user enters new data into a spreadsheet. The control
openview (barchart)
component of the spreadsheet agent forwards this data to the top level PAC agent.
The control component of the top-level PAC agent receives the data
Control
and tells the top-level abstraction to change the data repository
accordingly. The abstraction component of the top-level agent asks
its control component to update all agents that depend on the new
data. The control component of the top-level PAC agent therefore
notifies the view coordinator agent.
Abstraction
The control component of the view coordinator agent forwards the
change notification to all view PAC agents it is responsible for
coordinating.
As in the previous scenario, all view PAC agents then update their
data and refresh the image they display.
Implementation To implement a PAC architecture, carry out the following ten steps,
repeating any step or group of steps as necessary.
1 Define a model of the application. Analyze the problem domain and
map it onto an appropriate software structure. Do not consider the
distribution of components to PAC agents when performing this step.
Concentrate on finding a proper decomposition and organization of
the application domain. To this end, answer the following questions:
Which services should the system provide?
Which components can fulfill these services?
What are the relationships between components?
How do the components collaborate?
What data do the components operate on?
How will the user interact with the software?
Follow an appropriate analysis method when specifying the model.
2 Define a general strategy for organizing the PAC hierarchy. At this
point we have not yet defined individual agents, but can specify
general guidelines for organizing the hierarchy of cooperating agents.
One rule to follow is that of 'lowest common ancestor'. When a group
of lower-level agents depends on the services or data provided by
another agent, we try to specify this agent a s the root of the subtree
formed by the lower-level agents. As a consequence only agents that
provide global services rise to the top of the hierarchy. For example,
all agents in the election system depend on the central data
repository. This is therefore provided by the top-level PAC agent. If
only a fraction of all agents depend on the repository, we would try to
group them into a subtree and define an agent holding the repository
at the root of that subtree.
A second aspect to consider is the depth of the hierarchy. Most PAC
architectures comprise several intermediate levels of PAC agents. In
the Mobile Robot system [Cro85],for example, bottom-level agents are
composed to environments which again are composed to workspaces
-this is covered in more detail in the description of the Mobile Robot
system in the Known Uses section. The deeper the hierarchy, the
better it often reflects the decomposition of an application into self contained concepts. On the other hand, deep hierarchies tend to be
inefficient at run-time, and also hard to maintain. Finding the
appropriate decomposition of a system into PAC agents is important
to be able to gain the benefits of this architecture.
3 Specify the top-level PAC agent. Identify those parts of the analysis
model that represent the functional core of the system. These are
mostly components that maintain the global data model of the
system, and components directly operating on this data. Identify also
all user interface elements that are common to the whole application,
such a s menu bars or dialogs with information about the system. All
components identified in this step will be part of the top-level agent.
4 Specify the bottom-levelPAC agents. Identify those components of the
analysis model that represent the smallest self-contained units of the
system on which the user can perform operations or view presentations. In our example system, these units are the various diagrams
and charts presenting election data, and the spreadsheet for entering
this data.
For each of these units, identify those components that provide the
human-computer interaction associated with them. The bar chart in
our example requires a window in which the diagram is displayed,
and functionality to manipulate the diagram, such a s zooming and
printing. Each semantic concept such a s a bar chart and its user
interface components together form a separate bottom-level agent.
5 Speech bottom-level PAC agents for system services. Often an
application includes additional services that are not directly related
to its primary subject. In our example system we define an error
handler. Other systems may provide services for communicating with
other systems or for configuration purposes. Each of these services,
including their human-computer interaction, can be implemented as
a separate bottom-level agent [BaCoS11..
6 Specify intermediate-level PAC agents to compose lower-level PAC
agents. Often, several lower-level agents together form a higher-level
semantic concept on which users can operate.
In the mobile robot system described in [Cro85],several wall, place,
and route PAC agents form an environment. Users of the system can
specify new environments, and missions for robots within environments. Environments are displayed on the screen, and users perform
actions such as scrolling and zooming on these presentations. An environment is therefore a higher-level concept with its own functionality and human-computer interaction. Such concepts are implemented a s separate agents. They provide their own human-computer interaction, and operate on their constituent lower-level agents.
Our election example does not provide semantic concepts above
individual charts, diagrams, and spreadsheets. Therefore we do not
define PAC agents for composing other PAC agents. 0
7 Speech intermediate-level PAC agents to coordinate lower-level PAC
agents. Many systems offer multiple views of the same semantic
concept. For example, in text editors you find 'layout' and 'edit" views
of a text document. When the data in one view changes, all other
views must be updated. Such coordination components, which you
may have identified when modeling the analysis model, provide their
own human-computer interaction; for example, menu entries and
associated callback functions. The view coordinator agent of our
example system is such an intermediate-level agent. To implement
agents that coordinate multiple views you may apply the View
Handler pattern (291).
Note that views are not the only aspect of an application that must be
coordinated. The network traffic management system described in the
Known Uses section [TS93],for example, implements an agent that
coordinates the different concurrent jobs the system performs in a
telecommunication network.
8 Separate core functionality from human-computer interaction. For
every PAC agent, introduce presentation and abstraction components. All components that provide the user interface of the agent,
such as graphical images presented to the user, presentation-specific
data like screen coordinates, or menus, windows, and dialogs form
the presentation part. All components that maintain core data or
operate on them form the abstraction.
You can provide a unified interface to the abstraction and presentation components of a PAC agent by applying the Facade pattern
[GHJV951.The control component exports those parts of the abstraction and presentation interfaces that other components can use.
For some PAC agents it may be hard to specify presentation or
abstraction parts. For example, top-level PAC agents often do not
provide a presentation component. [BaCoSl] suggest the implementation of the top-level presentation as a general geometry manager
that maintains spatial relationships between the presentation components of lower-level PAC agents. You can apply the Command
Processor pattern (277)to further organize the presentation component. This allows you to schedule user requests for deferred or prioritized execution, and to provide agent-specific undo/redo services.
Some abstraction components, especially those in lower-level agents,
often operate on data provided by other PAC agents. In this case, you
may either not specify an abstraction component, or design the application such that the abstraction component.just serves as a data
cache. In the first case, you save all the effort of implementing
components to keep replica data, and the functionality to keep these
replica consistent. In the latter case,you save additional communication effort between PAC agents, for example when refreshing a view
after a window is moved.
Finally, introduce the control component to mediate between the
abstraction and presentation components, and to avoid direct
dependencies between them. The control component is implemented
as an Adapter [GHJV95]. It links the presentation and abstraction
components together by performing interface and data adaptation
between them. In this step, do not consider the parts of the control
component that deal with the communication between the agent and
other PAC agents. That is a different role of the control component.
and should therefore be separated from the mediation between the
agent internal abstraction and presentation components.
r To illustrate this step in our example, we refine the bar-chart
agent from the example, as described in the Structure section. The
abstraction component keeps a copy of the election data displayed in
the bar chart.
The presentation component is structured into components that
provide the functionality of windowing, menus, dialogs, and of
maintaining presentation-specific data. To shield clients from this
structure we provide a Facade (GHJV951.
The control component of the pie chart PAC agent is simple. It just
forwards data read requests from the presentation component to the
abstraction component. Communication with higher-level agents is
handled in the next step.
9 Provide the external interface. To cooperate with other agents, every
PAC agent sends and receives events and data. Implement this
functionality as part of the control component.
Within an agent, incoming events or data are forwarded to their
intended recipient. The recipient may be the abstraction or the
presentation component of the agent, but may also be lower or
higher-level agents. For example, the view coordinator agent of our
information system regularly receives change notifications from the
top-level PAC agent and forwards them to the view agents. It also
receives requests from lower-level agents that are forwarded to the
top-level agent. In other words, the control component is a mediator you may use the Mediator pattern [GHJV95] to implement this role.
One way of implementing communication with other agents is to
apply the Composite Message pattern [SC95b].This keeps the inter face of an agent small. It also allows agents to be independent of the
specific interfaces of other agents, and also of particular data formats,
marshaling, unmarshaling, fragmentation and re-assembling
methods. Applying the Composite Message pattern requires, however,
that the control component interprets incoming messages. It must
decide what to do with them-calling the abstraction or presentation
components, or forwarding the message to another agent. This
functionality is usually very complex and hard to implement.
A second option is to provide a public interface that offers every
service of an agent as a separate function. These functions 'know' how
to handle data and events when called. Compared to the Composite
Message solution, this reduces the inner complexity of the control
component, but introduces additional dependencies between
agents-they depend on the specific interfaces of other agents. In
addition, in this approach the interface of an agent can 'explode'. For
example, an intermediate-level agent must offer all the functions of
the top-level agent that are called by its associated lower-level agents.
Vice versa, the intermediate-level agent must offer all the services of
its associated lower-level agents that are called by the top-level agent.
The interface of an agent may become complex and hard to maintain
as a result.
A PAC agent can be connected to other PAC agents in a flexible and
dynamic way by using registration functionality,as introduced by the
Publisher-Subscriber pattern (339).For example, if a new instance of
the bar-chart agent in our election system is created. it is dynamically
registered with the view coordinator agent.
If a PAC agent depends on data or information maintained by other
PAC agents, you should provide a change-propagation mechanism.
Such a mechanism should involve all agents and all levels of the
hierarchy and work in both directions. When changes to data occur
within an agent. its abstraction component starts the change propagation. The control component forwards change notifications to all
dependent PAC agents. but often also to the presentation component.
Incoming change notifications from other agents cause the abstraction and presentation components to update their internal states.
One way to implement such a change-propagation mechanism is to
use the Publisher-Subscriber pattern (339).Another way is to integrate change propagation with the general functionality for sending
and receiving events, messages, and data: see the example code
below.
The interface for these communication and cooperation functions
should be the same for all PAC agents. This supports re-configuration
and reuse of PAC agents, and the extension of the application with
new PAC agents.
Link the hierarchy together. After implementing the individual PAC
agents you can build the final PAC hierarchy. Connect every PAC
agent with those lower-level PAC agents with which it directly
cooperates.
Provide the PAC agents that dynamically create and delete lower-level
PAC agents with rationality to dynamically extend or reduce the
PAC hierarchy. For example, the view coordinator agent in our
information system creates a new view PAC agent if the user wants to
own a particular view, and deletes this agent when the user closes
the window in which the view is displayed.
Variants Many large applications-especially interactive ones-are multi-user
systems. Multi-tasking is thus a major concern when designing such
software systems. The following two variants of PAC address this
force.
PAC agents as active objects. Many applications, especially interactive
ones, benefit from multi-threading. The mobile robot system (Cro851
is an example of a multi-threaded PAC architecture. Every PAC agent
can be implemented as an active object that lives in its own thread of
control. Design patterns like Active Object and Half-Sync/Half-Async
[Sch95]can help you implement such an architecture.
PAC agents as processes. To support PAC agents located in different
processes or on remote machines, use proxies (263) to locally
represent these PAC agents and to avoid direct dependencies on their
physical location. Use the Forwarder-Receiver pattern (307) or the
Client-Dispatcher-Server pattern (323) to implement the inter process communication (IPC)between PAC agents.
Since IPC is inefficient, you can also consider organizing coherent
subtrees of the PAC hierarchy within different processes. Agents that
cooperate closely in carrying out a particular task are then located
within the same process. IPC between PAC agents is minimized, and
is only necessary for coordinating different subtrees, as well as for
accessing the services of the top-level PAC agent.
Known Uses Network TrafficManagement.This system is described in [TS93].I t
displays the traffic in telecommunication networks. Every fifteen
minutes all monitored switching units report their current traffic
situation to a control point where the data is stored. analyzed and
displayed. This helps with identification of potential bottlenecks and
in preventing traffic overload. The system includes functions for:
Gathering traffic data from switching units.
Threshold checking and generation of overflow exceptions.
Logging and routing of network exceptions.
Visualization of traffic flow and network exceptions.
Displaying various user-configurable views of the whole network.
Statistical evaluations of traffic data.
Access to historic traffic data.
System administration and configuration.
The design and implementation of the system follows the
Presentation-Abstraction-Control pattern. Every function of the
system is represented by its own bottom-level PAC agent. There are
dedicated agents for each view of the network, for the jobs the system
can perform, and for the additional services the system offers, such
as mail or help. Three intermediate-level PAC agents coordinate these
bottom-level PAC agents, one for each of the three categories of
application functionality: view, jobs, and additional services. In the
diagram below, they are denoted by the agents NetEnv, JobEnv, and
RegieEnv. An additional intermediate PAC agent organizes user
sessions. The top-level PAC agent coordinates individual user
sessions, and communicates with the functional core of the system.
The core is implemented separately from the PAC hierarchy, probably
because it incorporates legacy software. The PAC agent hierarchy of
the system is dynamic. If, for example, a user starts a new session, a
corresponding UISession agent is created and registered with the top level agent. At the end of the session this agent is deleted.
Mobile Robot. This system [Cro85] allows an operator to interact
with a mobile robot that navigates within a - closed and hazardous
environment consisting of walls, equipment and people, either
intruders or accident victims. The robot navigates using its own
sensors and information from the system operator. The software
allows the operator to:
Provide the robot with a description of the environment it will work
in, places in this environment, and routes between places.
Subsequently modify the environment.
Specify missions for the robot.
Control the execution of missions.
Observe the progress of missions.
Each wall, route and place within an environment is represented by
its own bottom-level PAC agent. These agents together visualize the
environment. Environments are represented by intermediate-level
PAC agents. They control the constituent wall, route and place PAC
agents. The control users can exert on an environment is
implemented in a 'palette' PAC agent, which is also at the bottom level
of the hierarchy. The environment PAC agent and the palette PAC
agent form a workspace for the robot. This workspace is represented
by its own intermediate-level PAC agent. To support multiple views of
the same environment, a multi-workspace PAC agent coordinates the
different views of the same workspace. The PAC agent at the top level
of the hierarchy encapsulates the functional core of the application,
which is a rule-based intelligent supervisor for navigating and
controlling the robot.
Consequences The Presentation-Abstraction-Control architectural pattern has
several benefits:
Separation of concerns. Different semantic concepts in the application
domain are represented by separate agents. Each agent maintains its
own state and data, coordinated with, but independent of other PAC
agents. Individual PAC agents also provide their own human-computer interaction. This allows the development of a dedicated data
model and user interface for each semantic concept or task within the
application, independently of other semantic concepts or tasks.
Support for change and extension.Changes within the presentation or
abstraction components of a PAC agent do not affect other agents in
the system. This allows you to individually modify or tune the data
model underlying a PAC agent, or to change its user interface, for
example from command shells to menus and dialogs.
New agents are easily integrated into an existing PAC architecture
without major changes to existing PAC agents. All PAC agents
communicate with each other through a pre-defined interface. In
addition, existing agents can dynamically register new PAC agents to
ensure communication and cooperation. To add, for example, a new
view PAC agent to our information system for political elections, we
only need to extend the presentation of the view coordinator PAC
agent with an appropriate palette field that allows users to create this
new view. The functionality for handling this new PAC agent, for
registering it with the view coordinator PAC agent, and for
propagating changes and events to it is already available.
Support for multi-tasking. PAC agents can be distributed easily to
different threads, processes, or machines. Extending a PAC agent
with appropriate IPC functionality only affects its control component.
Multi-tasking also facilitates multi-user applications. For example, in
our information system a newscaster can present the latest projection
while data entry personnel update the data base with new election
data. All that is necessary is for the shared data repository, or its
control component, to take care of serialization or synchronization.
The liabilities of this pattern are as follows:
Increased system complexity. The implementation of every semantic
concept within an application as its own PAC agent may result in a
complex system structure. For example, if every graphical object such
as a circle or square within a graphics editor is implemented as its
own PAC agent, the system would drown in a sea of agents. Agents
must also be coordinated and controlled, which requires additional
coordination agents. Think carefully about the level of granularity of
your design, and where to stop refining agents into more and more
bottom-level agents.
Complex control component. In a PAC system, the control components
are the communication mediators between the abstraction and presentation parts of an agent, and between different PAC agents. The
quality of the control component implementations is therefore crucial
to an effective collaboration between agents, and therefore for the
overall quality of the system architecture. The individual roles of con 
See also
control components should be strongly separated from each other. The
implementation of these roles should not depend on specific details of
other agents, such as their concrete names or physical locations in a
distributed system. The interface of the control components should
be independent of internal details, to ensure that an agent's collaborators do not depend on the specific interface of its presentation or
abstraction components. It is the responsibility of the control component to perform any necessary interface and data adaptation.
Efficiency. The overhead in the communication between PAC agents
may impact system efficiency. For example, if a bottom-level agent
retrieves data from the top-level agent, all intermediate-level agents
along the path from the bottom to the top of the PAC hierarchy are
involved in this data exchange. If agents are distributed, data transfer
also requires IPC, together with marshaling, unmarshaling,
fragmentation and re-assembling of data.
These are serious potential pitfalls. We take them into account in the
following discussion about when to use, and when not to use, the
Presentation-Abstraction-Control pattern.
Applicability. The smaller the atomic semantic concepts of an application are, and the greater the similarity of their user interfaces, the
less applicable this pattern is. For example, a graphical editor in
which every individual object in a document is represented by its own
PAC agent will probably result in a complex fine-grain structure
which is hard to maintain. On the other hand, if the atomic semantic
concepts are substantially larger, and require their own human computer interaction, PAC provides a maintainable and extensible
structure with clear separation of concerns between different system
tasks.
The Model-View-Controller pattern (125)also separates the functional
core of a software system from information display and user input
handling. MVC, however, defines its controller as the entity
responsible for accepting user input and translating it into internal
semantics. This means that MVC effectively divides the user accessible part-the presentation in PAC-into view and control. It
lacks mediating control components. Furthermore, MVC does not
separate self-reliant subtasks of a system into cooperating but
loosely- coupled agents.
##%%&&
The Microkernel architectural pattern applies to software systems
that must be able to adapt to changing system requirements. It
separates a minimal functional core from extended functionality and
customer-specific parts. The microkernel also serves as a socket for
plugging in these extensions and coordinating their collaboration.
Example Suppose we intend to develop a new operating system for desktop
computers called Hydra. Our development team has elaborated a list
of design goals to achieve this. One requirement is that this
innovative operating system must be easily portable to the relevant
hardware platforms, and must be able to accommodate future
developments easily. It must also be able to run applications written
for other popular operating systems such as NeXTSTEP,Microsoft
Windows and UNIX System V. A user should be able to choose which
operating system he wants from a pop-up menu before starting an
application. Hydra will display all the applications currently running
within its main window:
To emulate all these operating systems, Hydra will integrate special
servers that implement specific views of Hydra's functional core. A
view denotes a layer of abstraction built on top of the core
functionality. The emulation of Microsoft Windows by a server
process is an example of such a view. Since several new technologies
such as multimedia, pen-based computing and the World Wide Web
are likely to increase in importance, Hydra should be designed for
their easy integration, as well as for adaptation, evolution and
enhancement of its overall functionality.
Context The development of several applications that use similar
programming interfaces that build on the same core functionality.
Problem Developing software for an application domain that needs to cope
with a broad spectrum of similar standards and technologies is a non-trivial task. Well-known examples are application platforms such as
operating systems and graphical user interfaces20. Such systems
often have a long life-span, sometimes ten years or more. Over time
periods of this length, new technologies emerge and old ones change.
The following forces therefore need particular consideration when
designing such systems:
The application platform must cope with continuous hardware and
software evolution.
The application platform should be portable, extensible and adaptable to allow easy integration of emerging technologies.
The success of such application platforms further depends on their
capability to run applications written for existing standards. To
support a broad range of applications, there is a need for more than
one view of the functionality of the underlying application platform.
In other words, an application platform such as an operating system
or a database should also be able to emulate other application
platforms that belong to the same application domain.
20. In the existing literature Microkernel systems have mainly been described in
relation to the design of operating systems. Nonetheless, we believe this pattern is also
applicable to several other domains, for example that of financial applications or
database systems [Woo96]. Due to the wide knowledge available about implementing
operating systems using microkernels, our example will focus on this specific domain.
For example, Hydra is designed to run applications that were
originally developed for popular operating systems such as Microsoft
Windows or OS/2 Warp. CI
This leads to the following forces:
The applications in your domain need to support different, but
similar, application platforms. 8
The applications may be categorized into groups that use the same
functional core in different ways, requiring the underlying
application platform to emulate existing standards.
An application platform that provides the functional core of a domain
is an exclusive resource for its clients. To avoid performance problems and to guarantee scalability, your solution must take an
additional force into account:
The functional core of the application platform should be separated
into a component with minimal memory size, and services that
consume as little processing power as possible.
Solution Encapsulate the fundamental services of your application platform in
a microkernel component. The microkernel includes functionality that
enables other components running in separate processes to communicate with each other. It is also responsible for maintaining system wide resources such as files or processes. In addition, it provides
interfaces that enable other components to access its functionality.
Core functionality that cannot be implemented within the micro kernel without unnecessarily increasing its size or complexity should
be separated in internal servers.
External servers implement their own view of the underlying micro kernel. To construct this view, they use the mechanisms available
through the interfaces of the microkernel. Every external server is a
separate process that itself represents an application platform.
Hence, a Microkernel system may be viewed as an application plat form that integrates other application platforms.
Clients communicate with external servers by using the communication facilities provided by the microkernel.
Structure The Microkernel pattern defines five kinds of participating
components:
The microkernel represents the main component of the pattern. It
implements central services such a s communication faculties or
resource handling. Other components build on all or some of these
basic services. They do this indirectly by using one or more interfaces
that comprise the functionality exposed by the microkernel.
Many system-specific dependencies are encapsulated within the
microkernel. For example, most of the hardware-dependent parts are
hidden from other participants. Clients of the microkernel only see
particular views of the underlying application domain and the
platform specifics.
The microkernel is also responsible for maintaining system resources
such as processes or files. It controls and coordinates the access to
these resources.
In summary, a microkernel implements atomic services, which we
refer to a s mechanisms. These mechanisms serve a s a fundamental
base on which more complex functionality, called policies, are
constructed.
In Hydra we want to support UNIX System V and OS/2 Warp,
amongst other operating systems. We face a problem when implementing Hydra's process model. A system call such a s that to create
a new child process is implemented in UNIX by cloning a n existing
process, copying the whole address space. OS/2 Warp handles
process creation totally differently, in that it does not copy the
address space of the parent process. In other words. OS/2 Warp and
UNIX offer different policies for processes. Hydra is therefore designed
to supply basic services such a s mechanisms for creating processes
a s well a s mechanisms for cloning existing process spaces. These are
combined in various ways for implementing both the process model
of UNE System V and the process model of OS/2 Warp.
An internal server-also known as a subsystem-extends the functionality provided by the microkernel. It represents a separate
component that offers additional functionality. The microkernel
invokes the functionality of internal servers via service requests. Internal servers can therefore encapsulate some dependencies on the
underlying hardware or software system. For example, device drivers
that support specific graphics cards are good candidates for internal
servers.
One of the design goals should be to keep the microkernel a s small a s
possible to reduce memory requirements. Another goal is to provide
mechanisms that execute quickly, to reduce service execution time.
Additional and more complex services are therefore implemented by
internal servers that the microkernel activates or loads only when
necessary. You can consider internal servers a s extensions of the
microkernel. Note that internal servers are only accessible by the
microkernel component.
An external server-also known as a personality-is a component that
uses the microkernel for implementing its own view of the underlying
application domain. As already mentioned, a view denotes a layer of
abstraction built on top of the atomic services provided by the
microkernel. Different external servers implement different policies
for specific application domains.
External servers expose their functionality by exporting interfaces in
the same way as the microkernel itself does. Each of these external
servers runs in a separate process. It receives service requests from
client applications using the communication facilities provided by the
microkernel, interprets these requests, executes the appropriate
services and returns results to its clients. The implementation of
services relies on microkernel mechanisms, so external servers need
to access the microkernel's programming interfaces.
In Hydra we want to implement an OS/2 Warp external server
and a UNIX System V external server. Both these servers use the
mechanisms of the underlying microkernel to implement a complete
set of OS/2 Warp and UNIX System V system calls. 
A client is an application that is associated with exactly one external
server. It only accesses the programming interfaces provided by the
external server.
A problem arises if a client needs to access the interfaces of its
external server directly. Each client has to use the available
communication facilities to interoperate with the external servers.
Every communication with an external server must therefore be hard coded into the client code. Such a tight coupling between clients and
servers, however, leads to various disadvantages:
Such a system does not support changeability very well.
If external servers emulate existing application platforms, client
applications developed for these platforms will not run without
modification.
We therefore introduce interfaces between clients and their external
servers to protect clients from direct dependencies. Adapters4so
known as emulators-represent these interfaces between clients and
their external servers, and allow clients to access the services of their
external server in a portable way.They are part of the client's address
space. If the external server implements an existing application plat form, the corresponding adapter mimics the programming interfaces
of that platform. Clients written for the emulated platform can therefore be compiled and run without modification. Adapters also protect
clients from the specific implementation details of the microkernel.
Whenever a client requests a service from an external server, it is the
task of the adapter to forward the call to the appropriate server. For
this purpose the adapter uses the communication services provided
by the microkernel.
Due to encapsulation by an adapter, a Hydra client associated
with the OS/2 Warp external server does not know whether it is
running on a native OS/2 Warp system or on a Microkernel system
that provides an OS/2 Warp external server. It just uses the OS/2
system calls as before. What happens 'behind the scenes' is hidden
by the adapter.
The following OMT diagram shows the static structure of a Micro kernel system. Its central component, the microkernel, collaborates
with external servers, internal servers and adapters. Each client is
associated with an adapter used as a bridge between the client and
its external server. Internal servers are only accessible by the micro kernel component.
Dynamics The dynamic behavior of a Microkernel system depends on the
functionality it provides for inter-process communication. In the
following scenarios we assume the availability of remote procedure
calls. The first scenario also assumes that the external server does
not access the microkernel interfaces-this latter case is illustrated
in the second scenario.
Scenario I demonstrates the behavior when a client calls a service of
its external server:
At a certain point in its control flow the client requests a service
from an external server by calling the adapter.
The adapter constructs a request and asks the microkernel for a
communication link with the external server.
The microkernel determines the physical address of the external
server and returns it to the adapter.
After retrieving this information, the adapter establishes a direct
communication link to the external server.
The adapter sends the request to the external server using a remote
procedure call.
The external server receives the request, unpacks the message and
delegates the task to one of its own methods. After completing the
requested service, the external server sends all results and status
information back to the adapter.
The adapter returns to the client, which in turn continues with its
control flow.
Scenario I1 illustrates the behavior of a Microkernel architecture
when an external server requests a service that is provided by an
internal server. In this scenario we assume that the internal server is
implemented as a separate process. It could alternatively be
implemented as a shared library that is dynamically linked to the
microkernel.
The external server sends a service request to the microkernel.
A procedure of the programming interface of the microkernel is
called to handle the service request. During method execution the
microkernel sends a request to an internal server.
After receiving the request, the internal server executes the
requested service and sends all results back to the microkernel.
The microkernel returns the results back to the external server.
Finally, the external server retrieves the results and continues with
its control flow.
Implementation To implement a Microkernel system, carry out the following steps:
1 Analyze the application domain. If you already know the policies your
external servers need to offer, or if you have a detailed knowledge
about the external servers you are going to implement, continue with
step 2. If not, perform a domain analysis and identify the core
functionality necessary for implementing external servers, then
continue with step 3.
2 Analyze external servers. Analyze the policies external servers are
going to provide. You should then be able to identify the functionality
you require within your application domain.
For Hydra we already know which external servers have to be
implemented: UNIX System V, OS/2 Warp, Microsoft Windows and
NeXTSTEP. We therefore analyze their programming interfaces to
determine the services they provide. This requirement analysis
results in the list of services and service categories necessary for
implementing desktop operating systems. 0
3 Categorize the services.Whenever possible, group all the functionality
into semantically-independent categories.
Build categories of operations that are not directly related with the
application domain, but are necessary to implement the system
infrastructure. Some of these operations may be candidates for
migration to internal servers.
For example, in Hydra the core categories that are predefined by
the domain of operating systems are memory management, process
management, low-level services for 1/0 and communication services.
The following categories are not directly related to the core concepts
of the application domain: page-handler processes, file systems,
hardware and software drivers. We need these categories in our Hydra
implementation, but they may be migrated to internal servers.
4 Partition the categories. Separate the categories into services that
should be part of the microkernel, and those that should be available
as internal servers. You need to establish criteria for this separation.
For example, it is best to implement time-critical, frequently-used or
hardware-dependent operations within the microkernel component.
In Hydra the microkernel provides services such as process
management, memory management, communication and low level I/O. This functionality is time-critical, used by all other
components, and also encapsulates system dependencies. It should
therefore be part of the microkernel. All additional services such as
page fault handlers, drivers or file systems are implemented by internal servers.
5 Find a consistent and complete set of operators and abstractors for
every category you identified in step 1. Remember that the micro kernel provides mechanisms, not policies. Each policy a n external
server provides must be implemented through use of the services the
microkernel offers through its interfaces.
r Operations such as the creation of processes or threads are
handled differently by operating systems like UNIX or Microsoft
Windows. In Hydra we need to support both. We therefore provide a
complete set of basic mechanisms for managing processes and
threads. For example, we provide services for:
Creating and terminating processes and threads.
Stopping and restarting them.
Reading from or writing to process address spaces.
Catching and handling exceptions.
Managing relationships between processes or threads.
Synchronizing and coordinating threads.
6 Determine strategies for request transmission and retrieval. Specify
the facilities the microkernel should provide for communication
between components. You can choose among several alternatives, for
example, asynchronous communication versus synchronous
communication. The relationship between communicating components may be a one-to-one, a many-to-one or a many-to-many
relationship. The communication strategies you integrate depend on
the requirements of the application domain. In many cases low-level
communication facilities such as message-passing or shared memory
are available, and you can build more complex communication
mechanisms on top of them. Compare design patterns such as
Forwarder-Receiver (307) and Client-Dispatcher-Server (323) for
more information on the implementation of communication
mechanisms.
Hydra provides two basic communication facilities:
Synchronous Remote Procedure Calls (RPCs).RPCs enable a client
to invoke the services of a remote server a s if they were
implemented by local procedure calls. The mechanisms necessary
for supporting RPCs, for example the packing and unpacking of
requests or the transmission of messages across process
boundaries, are hidden from the caller and the sewer called.
@ Asynchronous Mailboxes. A mailbox is a type of message buffer. A
set of components is allowed to read messages from the mailbox,
another set of components has permission to write messages to it.
A component may be allowed to perform both activities.
7 Structure the microkernel component. If possible, design the
microkernel using the Layers pattern (31)to separate system-specific
parts from system-independent parts of the microkernel. Place the
services that the microkernel exposes to other components in the
uppermost layer, and use the lower layers to hide system
dependencies from higher layers.
In our Hydra project we decide to use object-oriented techniques
to implement the microkernel:
@ The lowermost layer consists of low-level objects that hide hardware-specific details such as the bus architecture from other parts
of the microkernel.
 In the intermediate layers the primary services are provided by system objects, such as objects responsible for memory management
and objects used for managing processes.
@ The uppermost layer comprises all the functionality that the
microkernel exposes publicly, and represents the gateway to the
microkernel services for any process.
8 To special the programming interfaces of the microkernel, you need to
decide how these interfaces should be accessible externally. You must
obviously take into account whether the microkernel is implemented
as a separate process or as a module that is physically shared by
other components. In the latter case, you can use conventional
method calls to invoke the methods of the microkernel.
If you implement the microkernel as a separate process, existing
communication facilities are required for transmitting requests from
components to the microkernel. In this case you need to be aware
that the kernel represents an exclusive resource, and can therefore
be a bottleneck. To increase overall performance you could provide
multiple threads within the microkernel that wait for incoming
requests, and use the same, or other, threads to execute the
appropriate services. If you design such a multi-threaded system,
make sure that the consistency of internal data is guaranteed.
4 Since Hydra represents an operating system, its microkernel
component is part of each user process. Services are therefore
accessible by conventional system calls. These functions are logically
grouped into APIs (Application Programming Interfaces) that support
functionality such as file system operations or process management.
Invoking a Hydra system call results in a system trap. Software
exceptions are handled by a special trap handler routine in the
microkernel. The trap handler analyzes the type of interrupt that led
to the system trap, and delegates the work to one of its internal
service objects. After the service is completed, a scheduling object
decides which available thread should be executed next and assigns
The microkernel is responsible for managing a ZZ system resources
such as memory blocks, devices or devoice contexts-a handle to an
output area in a graphical user interface implementation. The
microkernel maintains information about resources and allows
access to them in a coordinated and systematic way. If components
want to access a resource, they use a unique identifier (handle)rather
than accessing the resource directly. The microkernel has the task of
creating these handles and providing a mapping between handles and
resources. This mapping can be implemented using hash tables. As
resource management involves more than just providing a mapping,
the microkernel must also implement strategies for the sharing,
locking, allocation and deallocation of resources.
4 Within Hydra, handles refer to objects that are instances of a
resource class. Each of these objects offers a uniform interface to
control access to a specific resource.
Our resource objects expose the following interface, which follows the
Windows NT approach:
Design and implement the internal ser-users as separate processes or
shared libraries. Perform this step in parallel with steps 7-9, because
some of the microkernel services need to access internal servers. I t is
helpful to distinguish between active and passive servers:
Active servers are implemented as processes
Passive servers as shared libraries
While passive servers are always invoked by directly calling their
interface methods, active servers need different treatment. The active
server process waits in an event loop for incoming requests. If it
receives a request via the available communication facilities, it
interprets and executes a service on behalf of the caller. Note that
internal servers are accessed exclusively by the microkernel-no
other component is permitted to invoke the services of internal
servers.
@ In Hydra we provide device drivers, authentication servers and
page fault handlers, among other components, by implementing them
as internal servers.
Graphics card drivers are developed as shared libraries because they
only act on behalf of clients. In contrast, page fault handlers are
separate processes. They always have to remain in main memory and
Implement the external ser-users. All the policies the external servers
include are based on the services available in the programming
interfaces of the microkernel. An external server receives requests,
analyzes them, executes the appropriate services and sends the
results back to the caller. When executing services, the external
server may call operations in the microkernel.
Each external server is therefore implemented as a separate process
that provides its own service interface. The internal architecture of an
external server depends on the policies it comprises.
Specify how external servers dispatch requests to their internal
procedures. For example, they may integrate a dispatcher component
that executes a main event loop and waits for incoming requests.
When a request arrives, the dispatcher unpacks it, interprets the
request and calls the appropriate procedure via a callback
mechanism. This is particularly useful if you design external servers
cannot be swapped to external storage.
as application frameworks. See the Reactor pattern [Sch94] for a
description of this event-driven approach.
b We want to develop the following external servers for Hydra:
A full implementation of Microsoft¡¯s Win32 and Win16 APIs, to
allow users to run Windows NT, Windows 3.11 and Windows 95
applications.
The complete functionality provided by IBM OS/2 Warp 2.0.
An implementation of Openstep.
All relevant UNIX System V interfaces specified by X/Open. a
Implement the adapters. The primary task of an adapter is to provide
operations to its clients that are forwarded to an external server.
Whenever the client calls a function of the external server, the adapter
packages all relevant information into a request and forwards the
request to the appropriate external server. The adapter then waits for
the server¡¯s response and finally returns control to the client, using
the facilities for inter-component communication.
You can design the adapter either as a conventional library that is
statically linked to the client during compilation, or as a shared
library dynamically linked to the client on demand. You can view an
adapter as a proxy that represents exactly one external server. You
could therefore use the Proxy pattern (263) to implement an adapter.
You could optimize the adapter by allowing it to execute some of the
API operations on its own instead of forwarding requests to the
external server, or by storing several client requests in a cache before
forwarding them. Answers to common requests could also be stored
here. See the Proxy pattern (263)for benefits and pitfalls of caching.
You must decide whether one adapter should be available for all
clients, or if every client is associated with its own adapter. The first
approach results in less memory contention, while the second can
lead to better response times.
If we design Hydra with a MicrosoftWindows external server, all
client applications associated with this server use the Win16 or
Win32 APIs. In a native Windows 3 . 1 1 system all APIs are available
as a set of shared libraries. In Hydra, however, Windows clients and
the Windows server are separate processes. Since we want to be able
to run a Windows-application on Hydra without modification, we
need to supply the same environment. We therefore implement an
adapter to work as a bridge between a Windows client and the
Windows server. When the client calls a Win16 or Win32 API function,
the call is handled by the adapter, which forwards a request to the
Windows external server. Existing Windows clients can therefore be
DeueZop client uppZiculions or use existing ones for the ready-to-run
Microkernel system. When creating a new client for a specific external
server, its architecture is only limited by the constraints imposed by
the external server. That is,clients depend on the policies implemented by their external server.
In Hydra we can develop Microsoft Windows applications by
accessing the services of the Microsoft Windows external server via
compiled and executed on the Hydra system.
Shortly after the development of Hydra has been completed, we are
asked to integrate an external server that emulates the Apple MacOS
operating system. To provide a MacOS emulation on top of Hydra, the
following activities are necessary:
Building an external server on top of the Hydra microkernel that
implements all the programming interfaces provided by MacOS,
including the policies of the Macintosh user interface. In its main loop
the MacOS server waits for incoming requests, which are stored in a
message port specifically assigned to the MacOS server. The server
pulls these requests out of the message port, interprets them and
dispatches them to internal procedures. These procedures emulate
the policies that are typical of the MacOS environment.
Providing an adapter that is designed as a library, dynamically linked
to clients. For every API function available in a native MacOS system,
a syntactically-identical procedure must be provided by the library.
Each of these procedures is responsible for packaging the type of
request, the arguments, and the identifiers of sender and receiver into
a message. It then calls the procedure send message in the microkernel, which in turn stores the message in the message port of the
MacOS server.
Architectural Patterns
implementing the internal sewers required for MacOS. For example,
one internal server provides the network protocol AppleTalk. The
microkernel must be modified to invoke these additional internal
servers on behalf of the MacOS server.
Variants Microkernel System with indirect Client-Server connections. In this
variant, a client that wants to send a request or message to an
external server asks the microkernel for a communication channel.
After the requested communication path has been established, client
and server communicate with each other indirectly using the
microkernel as a message backbone. Using this variant leads to an
architecture in which all requests pass through the microkernel. You
can apply it, for example, when security requirements force the
system to control all communication between participants.
Distributed Microkernel System In this variant a microkernel can also
act as a message backbone responsible for sending messages to
remote machines or receiving messages from them. Every machine in
a distributed system uses its own microkernel implementation. From
the user's viewpoint the whole system appears as a single Microkernel
system-the distribution remains transparent to the user. A
distributed Microkernel system allows you to distribute servers and
clients across a network of machines or microprocessors. To achieve
this the microkernels in a distributed implementation must include
additional services for communicating with each other.
Known Uses The Mach operating system [Tan921 was developed at Carnegie Mellon-University, and its first version was released in 1986. The
Mach microkernel is intended to form a base on which other
operating systems can be emulated. One of the commercially available operating systems that use Mach as its system kernel is
NeXTSTEP.
The operating system Amoeba (Tan921 consists of two basic
elements: the microkernel itself and a collection of servers
(subsystems) that are used to implement the majority of Amoeba's
functionality. The kernel provides four basic services: the
management of processes and threads, the low-level-management of
system memory, communication services, both for point-to-point communication as well as group-communication, and low-level I/O services. Services not provided by the kernel must be implemented by
server processes. This leads to a reduction in kernel size and
increases flexibility.
Chorus [Cho9O]is a commercially-available Microkernel system that
was originally developed by the French research institute I N U
specifically for real-time applications. UNIX System V is available a s
an external server.
NT [Cus93]was developed by Microsoft a s an operating system for high-performance servers. From an architectural point of view
Windows NT is definitelya Microkernel system. It offers three external
servers, an OS/2 1.x server, a POSIX server and a Win32 server.
DE (Microkernel Datenbank Engine) system [Woo961
introduces an architecture for database engines that follows the
Microkernel pattern. In this system the microkernel is responsible for
providing fundamental services such as physical data access, caching
of data and transaction management. Various external servers run on
top of the microkernel and provide different conceptual views of the
underlying microkernel. A conceptual view denotes a data abstraction
according to a given data model, for example the data model of a relational SQL database. Applications such a s accounting systems can
use the external servers to access databases. MKDE implements the
Distributed Microkernel variant to support distributed environments.
consequences The Microkernel pattern offers some important be
Portability. A Microkernel system offers a high degree of portability,
for two reasons:
@ In most cases you do not need to port external servers or client
applications if you port the Microkernel system to a new software
or hardware environment.
@ Migrating the microkernel to a new hardware environment only
requires modifications to the hardware-dependent parts.
Flexibility and Extensibility. One of the biggest strengths of a
Microkernel system is its flexibility and extensibility. If you need to
implement an additional view, all you need to do is add a new external
server. Extending the system with additional capabilities only
requires the addition or extension of internal servers.
Separation of policy and mechanism. The microkernel component
provides all the mechanisms necessary to enable external servers to
implement their policies. This strict separation of policies and
mechanisms increases the maintainability and changeability of the
whole system. It also allows you to add new external servers that
implement their own specialized views. If the microkernel component
were to implement policies, this would unnecessarily limit the views
that could be implemented by external servers.
If we consider the Distributed Microkernel variant of the Microkernel
architecture, further benefits appear:
Scalability. A distributed Microkernel system is applicable to the
development of operating systems or database systems for computer
networks, or multiprocessors with local memory. If your Microkernel
system works on a network of machines, it is easy to scale the
Microkernel system to the new configuration when you add a new
machine to the network.
Reliability. Two issues are important in achieving reliability:
availability and fault tolerance ITan921. A distributed Microkernel
architecture supports availability, because it allows you to run the
same server on more than one machine, increasing availability. If a
server or a machine fails, therefore, the failure does not necessarily
have an impact on an application. Fault tolerance may be easily
supported because distributed systems allow you to hide failures
from a user.
Transparency.In a distributed system components can be distributed
over a network of machines. In such a configuration, the Microkernel
architecture allows each component to access other components
without needing to know their location. All details of inter-process
communication with servers are hidden from clients by the adapters
and the microkernel.
The Microkernel architectural framework also has liabilities:
Performance. If we compare a monolithic software system designed to
offer a specific view with a Microkernel system supporting different
views. the performance of the former will be better in most cases. We
therefore have to pay a price for flexibility and extensibility. If the
communication within the Microkernel system is optimized for
performance, however, this price can be overlooked [Tan92].
Complexity of design and implementation. Developing a Microkernel based system is a non-trivial task. For example, it can sometimes be
very difficult to analyze or predict the basic mechanisms a micro kernel component must provide. In addition, the separation between
mechanisms and policies requires in-depth domain knowledge and
considerable effort during requirements analysis and design.
See also The Broker pattern (99) is suitable for distributed software systems
that consist of interacting and decoupled components. In the Broker
pattern clients access the services provided by servers using remote
procedure calls or message-passing. In contrast to the Microkernel
architectural framework. the Broker pattern focuses on distribution
over a network. A further difference between these patterns is that the
coupling of components within a Broker system I s not normally as
tight as It is within a Microkernel system. You can however combine
both patterns when developing a distributed Microkernel system.
The Reflection pattern (193) provides a two-tiered architecture. A base
level corresponds to a combination of microkernel and internal
servers. A meta level enables the behavior of base-level functionality
to be changed dynamically. for example changing the strategies for
resource management or communication between components. In
addition, the meta level allows integration of customer-specific extensions to the base-level services. This corresponds to the provision of
external servers in a Microkernel architecture. In contrast to the
Microkernel pattern, the adaptation of the meta level is performed
indirectly with help of a specific Interface, the Metaobject Protocol
(MOP).This allows users to specify a change, checks its correctness,
and automatically integrates the change into the meta level. In recent
developments of operating systems, the Reflection pattern and the
Microkernel pattern are often combined IZim961.
The relationship between the Layers (31)and the Microkernel pattern
is twofold. Firstly, a Microkernel system may also be considered as a
variant of the Layers pattern. The microkernel implements a virtual
machine, relying on internal servers to do this. The internal servers
are the lowest layer and also belong to the virtual machine. The
applications executed by the virtual machine include external servers
and adapters, representing the layer on top of the virtual machine.
One external server and one adapter together can be considered as a
second virtual machine on top of the microkernel. Each personality
offered corresponds to a single second-level virtual machine. Client
applications make up the highest layer in this hierarchy, and use
specific personalities.
Secondly, for some application domains both patterns may be applied
alternatively. Consider architectures for business applications
[Fow96].A very common approach is to separate these systems into
three tiers:
The lowest layer includes the database management system.
The middle layer contains the business logic.
@ The highest layer comprises different business applications.
If these applications can be grouped into different categories, you
could instead introduce a microkernel responsible for implementing
the core business logic. This microkernel could additionally
encapsulate the functionality for accessing the DBMS into internal
servers. External servers would provide different views of the
microkernel mechanisms, covering the business logic in different
ways to capture functionality specific to a particular business
category. The actual business applications are then the clients. If,
however, all clients build upon the same view of the underlying
business logic, the Microkernel pattern should not be applied.
##%%&&
The Reflection architectural pattern provides a mechanism for
changing structure and behavior of software systems dynamically. It
supports the modification of fundamental aspects. such as type
structures and function call mechanisms. In this pattern, an
application is split into two parts. A meta level provides information
about selected system properties and makes the software self-aware.
A base level includes the application logic. Its implementation builds
on the meta level. Changes to information kept in the meta level affect
subsequent base-level behavior.
Also Known As Open Implementation, Meta-LevelArchitecture
Example Consider a C++ application that needs to write objects to disk and
read them in again. Since persistence is not a built-in feature of C++.
we must specify how to store and read every type in the application.
Many solutions to this problem, such as implementing type-specific
store and read methods, are expensive and error-prone. For example.
whenever we change the class structure of the application. we must
modify these methods as well.
Other solutions to the lack of persistence raise other problems. For
example. we could provide a special base class for persistent objects
from which application classes are derived, with inherited store and
read methods overridden. Changes to the class structure require us
to modify these methods within existing application classes.
Persistence and application functionality are strongly interwoven.
Instead we want to develop a persistence component that is independent of specific type structures. However, to store and read arbitrary
C++objects, we need dynamic access to their internal structure.
Context Building systems that support their own modification a priori.
Problem Software systems evolve over time. They must be open to modifications in response to changing technology and requirements.
Designing a system that meets a wide range of different requirements
a priori can be an overwhelming task. A better solution is to specify
an architecture that is open to modification and extension. The
resulting system can then be adapted to changing requirements on
demand. In other words, we want to design for change and evolution.
Several forces are associated with this problem:
Changing software is tedious, error prone, and often expensive.
Wide-ranging modifications usually spread over many components
and even local changes within one component can affect other
parts of the system. Every change must be implemented and tested
carefully. Software which actively supports and controls its own
modification can be changed more effectively and more safely.
Adaptable software systems usually have a complex inner
structure. Aspects that are subject to change are encapsulated
within separate components. The implementation of application
services is spread over many small components with different
interrelationships [GHJV95].To keep such systems maintainable,
we prefer to hide this complexity from maintainers of the system.
The more techniques that are necessary for keeping a system
changeable, such as parameterization, subclassing, mix-ins, or
even copy and paste, the more awkward and complex its
modification becomes. A uniform mechanism that applies to all
kinds of changes is easier to use and understand.
Changes can be of any scale, from providing shortcuts for
commonly-used commands to adapting an application framework
for a specific customer.
Even fundamental aspects of software systems can change, for
example the communication mechanisms between components.
Solution Make the software self-aware, and make selected aspects of its
structure and behavior accessible for adaptation and change. This
leads to an architecture that is split into two major parts: a meta level
and a base level.
The meta level provides a self-representation of the software to give it
knowledge of its own structure and behavior, and consists of so-called
rnetaobjects. Metaobjects encapsulate and represent information
about the software. Examples include type structures, algorithms, or
even function call mechanisms.
The base level defines the application logic. Its implementation uses
the metaobjects to remain independent of those aspects that are
likely to change. For example, base-level components may only
communicate with each other via a metaobject that implements a
specific user-defined function call mechanism. Changing this
metaobject changes the way in which base-level components
communicate, but without modifying the base-level code.
An interface is specified for manipulating the metaobjects. I t is called
the metaobject protocol (MOP),and allows clients to specify particular
changes, such a s modification of the function call mechanism
metaobject mentioned above. The metaobject protocol itself is
responsible for checking the correctness of the change specification,
and for performing the change. Every manipulation of metaobjects
through the metaobject protocol affects subsequent base-level
behavior, a s in the function call mechanism example.
r For the persistence component, located a t the base level of our
example application, we specify metaobjects that provide run-time
type information. For example, to store a n object, we must know its
internal structure and also the layout of all its data members. With
this information available we can recursively iterate over any given
object structure to break it down into a sequence of built-in types.
The persistence component 'knows' how to store these. If we change
the run-time type information we also modify the behavior of the store
method. For example, objects of classes that are no longer persistent
are no longer stored. Following similar strategies for every method, we
can construct a persistence component that is able to read and store
arbitrary data structures.
Structure The meta level consists of a set of metaobjects. Each metaobject
encapsulates selected information about a single aspect of the
structure, behavior, or state of the base level. There are three sources
for such information:
It can be provided by the run-time environment of the system, such
as C++ type identification objects (DWP95j.
It can be user-defined, such as the function call mechanism in the
previous section.
It can be retrieved from the base level at run-time, for example
information about the current state of computation.
All metaobjects together provide a self-representation of an application. Metaobjects make information, which is otherwise only
implicitly available, explicitly accessible and modifiable. Almost every
system internal can be described in this way. For example, in a distributed system there may be metaobjects that provide information
about the physical location of base-level components. Other base level components can use these metaobjects to determine whether
their communication partners are local or remote. They can select the
most efficient function call mechanism to communicate with them.
The function call mechanisms themselves may be provided by other
metaobjects. Further examples include type structures, real-time
constraints, inter-process communication mechanisms and trans action protocols.
However, what you represent with metaobjects depends on what
should be adaptable. Only system details that are likely to change or
which vary from customer to customer should be encapsulated by
metaobjects. System aspects that are expected to stay stable over the
lifetime of an application should not be.
The interface of a metaobject allows the base level to access the information it maintains or the service it offers. For example, a metaobject
that provides location information about a distributed component will
provide functions to access the name and identifier of the component,
information about the process in which it is located, and information
about the host on which the process runs. A metaobject that implements a function call mechanism will offer a method of activating a
specific function of a specific addressee, including input and output
parameter passing. A metaobject does not allow the base level to
modify its internal state. Manipulation is possible only through the
metaobject protocol or by its own computation.
The base level models and implements the application logic of the
software. Its components represent the various services the system
offers as well as their underlying data model. The base level also
specifies the fundamental collaboration and structural relationships
between the components it includes. If the software includes a user
interface, this is also part of the base level.
The base level uses the information and services provided by the
metaobjects, such as location information about components and
function call mechanisms. This allows the base level to remain
flexible-its code is independent of aspects that may be subject to
change and adaptation. Using the metaobject's services, base-level
components do not need to hard-code information about the concrete
locations of communication partners-they consult appropriate
metaobjects for this information.
Base-level components are either directly connected to the
metaobjects on which they depend, or submit requests to them
through special retrieval functions. These functions are also part of
the meta level. The first type of connection is preferred if the
relationship between the base level and the metaobject is relatively
static. The base-level component always consults the same
metaobject, for example if an object needs type information about
itself. The second type of connection is used if the metaobjects used
by the base level vary dynamically, as in the case of the store
procedure of our persistence component.
The metaobject protocol (MOP] serves as an external interface to the
meta level, and makes the implementation of a reflective system
accessible in a defined way. Clients of the metaobject protocol, which
may be base-level components, other applications, or privileged
human users, can specify modifications to metaobjects or their
relationships using the base level. The metaobject protocol itself is
responsible for performing these changes. This provides a reflective
application with explicit control over its own modification.
To continue our example above, a user may specify a new function
call mechanism to be used for communication between base-level
components. As a first step, the user provides the metaobject protocol
with the code of this new function call mechanism. The metaobject
protocol then performs the change. I t may do this, for example, by
generating an appropriate metaobject that includes the user-defined
code for the new mechanism, compiling the generated metaobject,
dynamically linking it with the application, and updating all
references of the 'old' metaobject to the 'new' one.
The metaobject protocol is usually designed as a separate component.
This supports the implementation of functions that operate on several
metaobjects. For example, modifying metaobjects that encapsulate
location information about distributed components eventually
requires an update of the corresponding function call mechanism
metaobjects. If we delegate the responsibility for such changes to the
metaobjects themselves, consistency between them is hard to
maintain. The metaobject protocol has a better control over every
modification that is performed, because it is implemented separately
from the metaobjectk
To perform changes, the metaobject protocol needs access to the internal of metaobjects. If it is further entitled to change connections
between base-level objects and metaobjects, it also needs access to
base-level components. One way of providing this access is to allow
the metaobject protocol to directly operate on their internal states.
Another safer but more inefficient, way of providing it is for metaobjects and base-level components to provide a special interface for
their manipulation, only accessible by the metaobject protocol.
Since the base-level implementation explicitly builds upon
information and services provided by metaobjects, changing them
has an immediate effect on the subsequent behavior of the base level.
In our example, we changed the way base-level components
communicate. however, in contrast to a conventional modification,
the system was changed without modifying base-level code.
The general structure of a reflective architecture is very much like a
Layered system (31).The meta level and base level are two layers,
each of which provides its own interface.The base-level layer specifies
the user interface for exploiting application functionality. The meta level layer defines the metaobject protocol to modify the metaobjects.
However, in contrast to a layered architecture, there are mutual
dependencies between both layers. The base level builds on the meta
level, and vice-versa. A n example of the latter occurs when
metaobjects implement behavior that is executed in case of an
exception. The kind of exception procedure that must be executed
often depends on the current state of computation. The meta level
retrieves this information from the base level, often from different
components to those providing the interrupted service. In a pure
layered architecture, these bidirectional dependencies between layers
are not allowed. Every layer only builds upon the layers below.
r For our persistence component example we specify metaobjects
that provide introspective access to the type structure of our application-that is, they can access information about the application's
structure or behavior, but cannot modify it. We can obtain information about the name, size, data members and superclasses of a given
type or object. An additional metaobject specifies a function that al lows a client to instantiate objects of arbitrary types. We use this
function, for example, when restoring an object structure from a data
file. The metaobject protocol includes functions for adding new, and
modifying existing, run-time type information.
The body of the persistence component is independent of the concrete
type structure of our application. For example, the store procedure
only implements the general algorithm for recursively breaking down
a given object structure into a sequence of built-in types. If it needs
information about the inner structure of user-defined types, it
consults the meta level. Data members with built-in types are directly
stored. All other data members are further decomposed. 
Dynamics It is almost impossible to describe the dynamic behavior of reflective
systems in general. We therefore present two scenarios based on the
persistence component example. See the Implementation section for
details of the metaobject protocol and metaobjects involved.
Scenario I illustrates the collaboration between base level and meta
level when reading objects stored in a disk file. All data is stored in an
appropriate order, and a type identifier proceeds every object. The
scenario further abstracts from special cases, such as reading
strings, static members, and restoring cycles in the object structure.
The scenario is divided into six phases:
The user wants to read stored objects. The request is forwarded to
the read ( 1 procedure of the persistence component, together with
the name of the data file in which the objects are stored.
Procedure read 0 opens the data file and calls an internal
read object ( 1 procedure which reads the first type identifier.
Procedure readObj e c t ( calls the metaobject that is responsible
for the creation of objects. The 'object creator' metaobject
instantiates an 'empty' object of the previously-determined type. It
returns a handle to this object and a handle to the corresponding
run-time type information (RTTI)metaobject.
Reflection read from its corresponding metaobject. The
procedure iterates over the data members of the object.
Procedure readobject ( ) reads the type identifier for the next data
member. If the type identifier denotes a built-in type--a case we do
not illustrate--the readobject ( ) procedure directly assigns the
next data item from the file to the data member, based on the data
member's size and offset within the object. Otherwise
readobject 0 is called recursively. This recursion starts with the
creation of an 'empty' object if the data member is a pointer. If not,
the recursively called read object ( ) operates on the existing layout of the object that contains the data member.
After reading the data, the read ( ) procedure closes the data file
and returns the new objects to the client that requested them.
Scenario I1 illustrates the use of the metaobject protocol when
adding type information to the meta level. Consider a class library
used by the application that changes to a new version with new types.
To store and read these types, we must extend the meta level with
new metaobjects. Adding this information can be performed by the
user, or automatically, using a tool. For reasons of simplicity we unify
the classes type-info and extTypeInfo as specified in the
Implementation section. The scenario is divided into six phases which
are performed for every new type:
A client invokes the metaobject protocol to specify run-time type
information for a new type in the application. The name of the type
is passed as an argument.
The metaobject protocol creates a metaobject of class type-inf o
for this type. This metaobject also serves as a type identifier.
The client calls the metaobject protocol to add extended type
information. This includes setting the size of the type, whether or
not it is a pointer, and its inheritance relationships to other types.
To handle the inheritance relationship, the metaobject protocol
creates metaobjects of class baseInfo.These maintain a handle to
the type-info object for a particular base class and its offset
within the new type.
In the next step, the client specifies the inner structure for the new
type. The metaobject protocol is provided with the name and type
of every data member. For every data member the metaobject
protocol creates an object of class dataInfo.I t maintains a handle
to the type-info object for the type of the member, its name, and
whether or not it is a static data member. The dataInfo object also
maintains the absolute address of the data member if it is static,
otherwise its offset within the new type.
The client invokes the metaobject protocol to modify existing types
that include the new type as a data member. Appropriate data
member information is added for every type.Since this step is very
similar to the previous one, we do not illustrate it in the object
message sequence chart that follows.
Finally, the client calls the metaobject protocol to adapt the 'object
creator' metaobject. The persistence component must be able to
instantiate an object of the new type when reading persistent data.
The metaobject protocol automatically generates code for creating
objects of the new type, based on the previously-added type
information. It further integrates the new code with the existing
implementation of the 'object creator' metaobject, compiles the
modified implementation, and links it with the application.
Implementation The following guidelines help with implementing a Reflection
architecture. Iterate through any subsequence if necessary.
Define a model of the application. Analyze the problem domain and
decompose it into an appropriate software structure. Answer the
following questions:
Which services should the software provide?
Which components can fulfill these services?
What are the relationships between the components?
How do the components cooperate or collaborate?
What data do the components operate on?
How will the user interact with the software?
Follow an appropriate analysis method when specifying the model.
b The persistence component in our C++ disk-storage example is
part of a warehouse management application ICoad951. We identify
components that represent physical storage, such as warehouses,
aisles and bins. We also identify components for orders and items. It
is a requirement that we can resume computation with a valid state
after system crashes. Both the physical structure of the warehouse
and its current population of items must therefore be made
persistent. We need two components to achieve this. A persistence
component provides the functionality for storing and reading objects.
A file handler is responsible for locking, opening, closing. unlocking
and deleting files, as well a s for writing and reading data. 
2 Identify varying behavior.Analyze the model developed in the previous
step and determine which of the application services may vary and
which remain stable. There are no general rules for specifying what
can alter in a system. Whether a certain aspect varies depends on
many factors such a s the application domain, the environment of the
application and its customers and users. An aspect that is likely to
vary in one system may stay stable in others. The following are
examples of system aspects that often vary:
Real-time constraints [HT92], such as deadlines, time-fence
protocols and algorithms for detecting deadline misses.
Transaction protocols ISW951, for example optimistic and
pessimistic transaction control in accounting systems.
Inter-process communication mechanisms [CM93], such as
remote procedure calls and shared memory.
Behavior in case of exceptions [EKM+94],IHT921, for example the
handling of deadline misses in real-time systems.
Algorithms for application services [EKM+94],such as country specific VAT calculation.
The Open Implementation Analysis and Design Method [mLM95]
helps with this step.
To keep the persistence component example simple, we do not
consider an adaptation of application behavior. 13
3 Identify structural aspects of the system, which, when changed,
should not affect the implementation of the base level. Examples
include the type structure of an application [BKSP92],its underlying
object model [McA951,or the distribution of components IMcA951in a
heterogeneous network.
Our implementation of the persistence component must be
independent of application-specific types. This requires access to
run-time type information, such as the name, size, inheritance
relationships and internal layout of each type, as well a s the types,
order and names of their data members. Ll
4 Identify system services that support both the variation of application
services identified in step 2 and the independence of structural
details identified in step 3. For example, implementing reusable
exceptions in C++ requires explicit access to the exception handling
mechanism of the language. Other examples of basic system services
are:
5 Define the metaobjects. For every aspect identified in the three previous steps, define appropriate metaobjects. Encapsulating behavior is
supported by several domain-independent design patterns, such as
Objectifler IZim941. Strategy. Bridge, Visitor, and Abstract Factory
IGHJV951. For example, metaobjects for function call mechanisms
can be implemented as strategy objects, and multiple implementations of components can be implemented with the Bridge pattern.
Visitor allows you to integrate new functionality without modifying
existing structures. Sometimes you may find appropriate domain specific patterns that support this step, for example the Acceptor and
Connector patterns for developing distributed systems ISch951.
Another example is the Detachable Inspector pattern ISC95al. which
supports the addition of run-time facilities such as debuggers and
inspectors. Detachable inspector builds on the Visitor pattern.
Encapsulating structural and state information is supported by
design patterns like Objectifler [Zim94]and State IGHJV951.
The metaobjects that provide the run-time type information for
our persistence component are organized as follows:
The C++ standard library class type-info is used for identifying
types [DWP951.Its interface offers functions for accessing the name
of a type, for comparing two types. and for determining their system
internal order. Every type in the application is represented by an
instance of class type-info.
None of the other classes of the run-time type information system are
part of the C++ standard.
A class extTypeInEo provides access to Information about the size,
superclasses, and data members of a class. Clients can also
determine whether the type is built-in or a pointer.
The method bases ( ) returns an object of class base1ter,which is
an iterator over either all base classes of a given type or just its direct
base classes. If the type is built-in, the method returns a NULL
iterator. Analogously, the method data ( ) returns an object of class
dataIter.It iterates either over all data members of a given type,
including inherited ones, or just the data members declared
specificallyfor this type. If the type is built-in, the method returns a
NULL iterator.
A class BaseInfo offers functions for accessing type information
about a base class of a class, as well as to determine its offset in the
class layout.
A class DataInfo includes functions that return the name of a data
member, its offset and its associated type-info object.
6 Define the metaobject protocol. Support a defined and controlled
modification and extension of the meta level, and also a modification
of relationships between base-level components and metaobjects.
There are two options for implementing the metaobject protocol:
Integrate it with the metaobjects. Every metaobject provides those
functions of the metaobject protocol that operate on it.
Implement the metaobject protocol a s a separate component.
An advantage of the latter approach is that the control of every
modification of the reflective application is localized a t a central point.
Functions that operate on several metaobjects are easier to
implement. In addition, a separate component can shield metaobjects
from unauthorized access and modification, if its implementation
follows patterns such a s Facade [GHJV95] or Whole-Part (225).The
Singleton idiom [GHJV95]helps ensure that the metaobject protocol
can only be instantiated once.
If implemented a s a separate component, the metaobject protocol
usually does not serve a s a base class for classes that define metaobjects-it just operates on them. I t only makes sense to specify the
metaobject protocol a s a base class from which concrete metaobject
classes are derived if it applies to every metaobject.
b We provide a class MOP which defines the metaobject protocol for
the meta level of our persistence component example. It is
implemented a s a singleton and operates directly on the internal
structure of all classes declared in the previous step.
m e information is accessible by two functions.
The first function allows clients to access the standard type
information about an object. The second function accesses the
extended type information that we defined specifically for our run time type information system. We need this function because objects
of the standard class type-info do not provide access to user defied information. All other type information-such a s that about
base classes--is accessible through the extTypeInf o object.
New type information metaobjects can be initialized with two
functions, one for instantiating type-info objects and one for
creating extTypeInfo objects.
The newTypeInfo ( ) function also calculates and sets the size of a
type. The function delete~ n f o ( ) deletes all available information
about a type, but only if no other class of the system contains a
reference to an object of that type.
We define four functions for adding new or modifying existing type in formation. The functions addBase ( ) and deleteBase ( ) respectively
add and remove base class information, while the functions
addData ( ) and deleteData 0 respectively add and delete data
member information.
Before executing changes, all functions perform consistency checks.
For example, to set base class information, corresponding type-info
and extype i n f o objects must be available.
Two functions support modification of the 'object creator' metaobject.
Internally, the metaobject protocol needs functions for calculating
type sizes and offsets of base classes and data members. These
functions are compiler-dependent and must therefore be changed
when using a different compiler. One way to support changing these
functions is provided by the Strategy pattern [GHJV95].To maintain
type-info and extTypeInfo objects, the metaobject protocol
maintains two maps, tMap and eMap.These maps offer functions to
add, remove and find elements.
Most functions of the metaobject protocol can be implemented
straightforwardly. Calculating offset and sizes and manipulating the
'object creator' metaobject requires higher implementation effort. The
following code defines the addBase ( ) function.
Robustness is a major concern when implementing the metaobject
protocol. Errors in change specifications should be detected wherever
possible. Changes should also be reliable. The metaobject protocol
described above, for example, checks the availability of appropriate
type information metaobjects when adding new base class and data
member information. Before deleting its type information, it also
checks whether a type is used as a base class or data member.
Robustness also means maintaining consistency. For example, if we
add a data member to a specific type, we must recalculate the size of
all types that include the changed type as a base class or a data
member. In addition, any modification should only affect those parts
of the system that are subject to change. Finally, clients of the
metaobject protocol should not take responsibility for integrating
changes into the meta level. Ideally, a client only specifies a change,
and the metaobject protocol is responsible for its integration. This
avoids direct manipulation of source code.
Implement the functional core and user
interface of the system according to the analysis model developed in
step 1.
Use metaobjects to keep the base level extensible and adaptable.
Connect every base-level component with metaobjects that provide
system information on which they depend, such as type information,
or which offer services they need. such as object creation in our
persistence component. To handle system services, use design
patterns such as Strategy. Visitor, Abstract Factory and Bridge
[GHJV951, or idioms like Envelope-Letter [Cope92].For example, the
context class component of the Strategy pattern represents the base level component, and the strategy class hierarchy the metaobjects.
When applying the Visitor pattern, the metaobjects are the visitors.
and the object structure represents the base-level components.
Provide base-level components with functions for maintaining the
relationships with their associated metaobjects. The metaobject
protocol must be able modify every relationship between the base
level and the meta level. For example, when replacing a metaobject
with a new one, the metaobjectprotocol must update all references to
the replaced metaobject. The metaobject protocol operates either
directly on internal data structures of base-level components, or uses
a special interface the base-level components provide.
If the metaobjects to be used are not known a priori, provide the meta
level or the metaobject protocol with appropriate retrieval functions,
such as the getInfo ( ) and getExtInfo ( ) functions in the persistence component example.
Metaobjects often need information about the current state of
computation. For example, the 'object creator' in our persistence
component example must know what type it should instantiate. This
information can either be passed as a parameter to the metaobjects.
the metaobjects can retrieve it from other metaobjects, or the
metaobjects can retrieve it from appropriate base-level components.
Changes to metaobjects affect the subsequent behavior of base-level
components to which they are connected. Changing a relationship
between the base level and the meta level affects only a specific base level component, the one that maintains the modified relationship.
The implementation of the read0 method of our persistence
component follows the first scenario depicted in the Dynamics
section. The method implements a general recursive algorithm for
reading objects from a data file. The method consults the meta level
to get information about how to read user-defined types. Reading
built-in types or strings is hard-coded within its implementation. To
obtain information about types, read ( ) consults the getInfo ( ) and
getExtInfo ( ) functions of the metaobject protocol. For creating
objects of arbitrary types, read ( 1 is directly connected with the
'object creator' metaobject.
The structure of the store ( ) method is similar to that of the read ( )
method. It first opens the data file to be read, then calls an internal
storeObj ect ( ) method that stores the object structure. Finally,
store ( ) closes the data file.
The most challenging part of implementing store ( ) is the detection
of cycles in the object structure to be stored-it is essential to avoid
storing duplicates and running into infinite recursion. To achieve
this, the method marks the structure with a unique identifier which
is also stored, before storing the object. If we return to an object that
is so marked, we then just store its identifier.
The following simplified code illustrates the structure of the
store object ( ) method. It abstracts from several details, such as the
storage of static data members.
Example In the previous sections we explained the Reflection architecture of
Resolved our persistence component example. How we provide run-time type
information is still an open issue.
Unlike languages like CLOS or Smalltalk. C++ does not support
reflection very well-only the standard class type-info provides
reflective capabilities: we can identify and compare types. One
solution for providing extended type information is to include a
special step in the compilation process. In this, we collect type
information from the source files of the application, generate code for
instantiating the'metaobjects, and link this codewith the application.
Similarly, the 'object creator' metaobject is generated. Users specify
code for instantiating an 'empty' object of every type, and the toolkit
generates the code for the metaobject. Some parts of the system are
compiler-dependent, such as offset and size calculation.
As illustrated in the code examples, we use pointer and address arithmetic, offsets, and sizes of types and data members to read and store
objects. Since these features are considered harmful, for example by
incurring the danger of overwriting object code, the persistence component must be implemented and tested very carefully.
Variants ReJection with several meta levels. Sometimes metaobjects depend on
each other. For example, consider the persistence component.
Changes to the run-time type information of a particular type
requires that you update the 'object creator' metaobject. To
coordinate such changes you may introduce separate metaobjects,
and-conceptually-a meta level for the meta level, or in other words,
a meta meta level. In theory this leads to an infinite tower of reflection.
Such a software system has an infinite number of meta levels in
which each meta level is controlled by a higher one, and where each
meta level has its own metaobject protocol. In practice, most existing
reflective software comprises only one or two meta levels.
An example of a programming language with several meta levels is
RbCl [IMY92]. RbCl is an interpreted language. RbCl base-level
objects are represented by several meta-level objects. These are
interpreted by an interpreter that resides at the meta metal level of
RbC1. The metaobject protocol of RbCl allows users to mod@ the
metaobjects that represent RbCl base-level objects, the metaobject
protocol of the meta meta level the behavior of the RbCl metaobject
interpreter.
Known Uses CLOS. This is the classic example of a reflective programming
language [Kee89].In CLOS, operations defined for objects are called
generic functions, and their processing is referred to as generic
function invocation. Generic function invocation is divided into three
phases:
The system first determines the methods that are applicable to a
given invocation.
It then sorts the applicable methods in decreasing order of
precedence.
The system finally sequences the execution of the list of applicable
methods. Note that in CLOS more than one method can be
executed in response to a given invocation.
The process of generic function invocation is defined in the
metaobject protocol of CLOS [KRB91].Basically, it executes a certain
sequence of meta-level generic functions. Through the CLOS
metaobject protocol users can vary the behavior of an application by
modifying these generic functions or the generic functions of the
metaobjects they call.
P [BKSP92] is a run-time type information system for C++. It is
mainly used for introspective access to the type system of an
application. Every type of a C++ software system is represented by a
set of metaobjects that provide general information about that type,
its relationships to other types, and its inner structure. All
information is accessible at run-time. The functionality of MIP is
separated into four layers:
The first layer includes information and functionality that allows
software to identify and compare types. This layer corresponds to
the standard run-time type identification facilities for C++[SL92].
The second layer provides more detailed information about the type
system of an application. For example, clients can obtain
information about inheritance relationships for classes, or about
their data and function members. This information can be used to
browse type structures.
The third layer provides information about relative addresses of
data members, and offers functions for creating 'empty' objects of
user-defined types. In combination with the second layer, this layer
supports object I/O.
The fourth layer provides full type information, such as that about
friends of a class, protection of data members, or argument and
return types of function members. This layer supports the
development of flexible inter-process communication mechanisms,
or of tools such as inspectors, that need very detailed information
about the type structure of an application.
The metaobject protocol of MIP allows you to specify and modify the
metaobjects that provide run-time type information. It offers appropriate functions for every layer of the MIP functionality.
MIP is implemented as a set of library classes. It also includes a
toolkit for collecting type information about an application, and to
generate code for instantiating the corresponding metaobjects. This
code is linked to the application that uses MIP and is executed at the
beginning of the main program. The toolkit can be integrated with the
'standard' compilation process for C++ applications. A special
interface allows users to scale the available type information for every
individual class or type.
PGen [THP941 is a persistence component for C++ that is based on
MIP. It allows an application to store and read arbitrary C++ object
structures.
The example used to explain the Reflection pattern is based mainly
on MIP and PGen. Although simplified, the description of the
persistence component, the class declarations for the metaobjects
and the metaobject protocol widely reflect the original structure of
MIP and PGen.
NEDIS. The car-dealer system NEDIS ISte95) uses reflection to
support its adaptation to customer- and country-specific
requirements. NEDIS includes a meta level called run-time data
dictionary. It provides the following services and system information:
Properties for certain attributes of classes, such as their allowed
value ranges.
Functions for checking attribute values against their required
properties. NEDIS uses these functions to evaluate user input, for
example to validate a date.
Default values for attributes of classes, used to initialize new
objects.
Functions specifying the behavior of the system in the event of
errors, such as invalid input or unexpected 'null' values of
attributes.
Country-specific functionality, for example for tax calculation.
Information about the 'look and feel' of the software, such as the
layout of input masks or the language to be used in the user
interface.
The run-time data dictionary is implemented as a persistent
database. A special interface allows users to modify any information
or service it provides. Whenever the run-time data dictionary
changes, special tools check and eventually restore its consistency.
The run-time data dictionary is loaded when starting the software.
For reasons of safety it cannot be modified while NEDIS is running.
OLE 2.0 [Bro94] provides functionality for exposing and accessing
type information about OLE objects and their interfaces. The
information can be used to dynamically access structural information
about OLE objects, and to create invocations of OLE interfaces. For
example, the run-time environment of Visual Basic [Mic95]checks
the correctness of method calls to an object before dynamically
invoking it. A similar concept is specified for Corba [OMG92].
Further examples of languages and systems that use a Reflection
architecture include Open C++ [CM93], RbCl [IMY92], AL-l/D
IOIT921, R2 IHT921, Apertos Kok92j and CodA [McA95].Even more
examples can be found in IIMSA921, but note that although all
examples provide reflective facilities, not all of them really implement
a Reflection architecture as described by this pattern.
Consequences A Reflection architecture provides the following benefits:
No explicit modification of source code. You do not need to touch
existing code when modifying a reflective system. Instead, you specify
a change by calling a function of the metaobject protocol. When
extending the software, you pass the new code to the meta level as a
parameter of the metaobject protocol. The metaobject protocol itself
is responsible for integrating your change requests: it performs
modifications and extensions to meta-level code, and if necessary re compiles the changed parts and links them to the application while it
is executing.
Changing a software system is easy.The metaobject protocol provides
a safe and uniform mechanism for changing software. I t hides all
specific techniques such as the use of visitors, factories and strategies
from the user. It also hides the inner complexity of a changeable
application. The user is not confronted with the many metaobjects
that encapsulate particular system aspects. The metaobject protocol
also takes control over every modification. A well-designed and robust
metaobject protocol helps prevent undesired changes of the
fundamental semantics of an application [Kic92].
Support for many kinds of change.Metaobjects can encapsulate every
aspect of system behavior, state and structure. An architecture based
on the Reflection pattern thus potentially supports changes of almost
any kind or scale. Even fundamental system aspects can be changed,
such as function call mechanisms or type structures. With the help
of reflective techniques it is also possible to adapt software to meet
specific needs of the environment or to integrate customer-specific
requirements.
However, a Reflection architecture has some significant Abilities:
Modifications a t the meta level may cause damage. Even the safest
metaobject protocol does not prevent users from specifying incorrect
modifications. Such modifications may cause serious damage to the
software or its environment. Examples of dangerous modifications
include changing a database schema without suspending the execu tion of the objects in the application that use it, or passing code to the
metaobject protocol that includes semantic errors. Similarly, bugs in
pointer arithmetic can cause object code to be overwritten.
The robustness of a metaobject protocol is therefore of great
importance [Kic92]. Potential errors within change specifications
should be detected before the change is performed. Each change
should only have a limited effect on other parts of the software.
Increased number of components. It may happen that a reflective
software system includes more metaobjects than base-level
components. The greater the number of aspects that are
encapsulated at the meta level, the more metaobjects there are.
Lower efficiency. Reflective software systems are usually slower than
non-reflective systems. This is caused by the complex relationship
between the base level and the meta level. Whenever the base level is
unable to decide how to continue with computation, it consults the
meta level for assistance. This reflective capability requires extra processing: information retrieval, changing metaobjects. consistency
checking, and the communication between the two levels decrease
the overall performance of the system. You can partly reduce this
performance penalty by optimization techniques, such as injecting
meta-level code directly into the base level when compiling the
system.
Not a potential changes to the software are supported. Although a
Reflection architecture helps with the development of changeable
software,only changes that can be performed through the metaobject
protocol are supported. As a result, it is not possible to integrate easily all unforeseen changes to an application, for example changes or
extensions to base-level code.
Not all languages support refection. A Reflection architecture is hard
to implement in some languages, such as C++,which offers little or
no support for reflection. C++ only provides type identification.
Reflective applications in C++ often build on language constructs
such as pointer arithmetic to handle arbitrary objects, and need tool
support for dynamically modifying meta-level code. This is, however,
tedious and error-prone. In such languages it is also impossible to
exploit the full power of reflection, such as adding new methods to a
class dynamically. However, even in languages that do not provide
reflective capabilities, it is possible to build reflective systems that are
changeable and extensible, such as the C++ systems NEDIS
[EKM+94],MIP [BKSP92]and Open C++[CM93].
See Also The Microkernel architectural pattern (171)supports adaptation and
change by providing a mechanism for extending the software with additional or customer-specific functionality. The central component of
this architecture-the microkernel-serves as a socket for plugging in
such extensions and for coordinating their collaboration. Modifications can be made by exchanging these 'pluggable' parts.
An earlier version of this pattern appeared in [PLoP95].
##%%&&
The Whole-Part design pattern helps with the aggregation of
components that together form a semantic unit. An aggregate
component, the hole'. encapsulates Its constituent components,
the Parts, organizes their collaboration, and provides a common
interface to its functionality. Direct access to the Parts I s not possible.
Example A computer-aided design (CAD) system for 2-Dand 3-D modeling
allows engineers to design graphical objects interactively. In such
systems most graphical objects are modeled as compositions of other
objects. For example. a car object aggregates several smaller objects
such a s wheels and windows, which themselves may be composed of
even smaller objects such as circles and polygons. It I s the
responsibility of the car object to implement functionality that
operates on the car as a whole, such a s rotating or drawing.
Context Implementing aggregate objects.
Problem In almost every software system objects that are composed of other
objects exist. For example, consider a molecule object in a chemical
simulation system-it can be implemented as a graph of separate
atom objects. Such aggregate objects do not represent loosely-coupled sets of components. Instead, they form units that are more than
just a mere collection of their parts. In this example, a molecule object
would have .attributes such as its chemical properties, and methods,
such as rotation. These attributes and methods refer to the molecule
as a semantic unit, and not to the individual atoms of which it is com posed. The molecules example illustrates the typical case in which
aggregates reveal behavior that is not obvious or visible from their individual parts-the combination of parts makes new behavior
emerge. Such behavior is called emergent behavior. Consider. for ex ample, the chemical reactions in which a molecule can participate these cannot be determined by only analyzing its individual atoms.
We need to balance the following forces when modeling such
structures:
A complex object should either be decomposed into smaller objects,
or composed of existing objects, to support reusability,
changeability and the recombination of the constituent objects in
other types of aggregate.
Clients should see the aggregate object as an atomic object that
does not allow any direct access to its constituent parts.
Solution Introduce a component that encapsulates smaller objects, and
prevents clients from accessing these constituent parts directly.
Define an interface for the aggregate that is the only means of access
to the functionality of the encapsulated objects. allowing the
aggregate to appear as a semantic unit.
The general principle of the Whole-Part pattern is applicable to the
organization of three types of relationship:
An assembly-parts relationship, which differentiates between a
product and its parts or subassemblies-such as the relationship
of a molecule to its atoms in our previous example. All parts are
tightly integrated according to the internal structure of the
Whole-Part
assembly. The amount and type of subassemblies is predefined
and does not vary.
 A container-contents relationship, in which the aggregated object
represents a container. For example, a postal package can include
different contents such a s a book, a bottle of wine, and a birthday
card. These contents are less tightly coupled than the parts in an
assembly-parts relationship. The contents may even be dynamically added or removed.
The collection-members relationship, which helps to group similar
objects-such as an organization and its members. The collection
provides functionality, such a s iterating over its members and
performing operations on each of them. There is no distinction
between individual members of a collection-all of them are treated
equally.
These relationships mimic relationships between objects in the real
world. When modeling them with software entities, it is not always
obvious which kind of relationship is appropriate. A molecule may be
considered as a n assembly composed of different atoms, but also as
a container with atoms as its contents. Which relationship is most
appropriate depends on the desired use of the aggregate.
It is important to note that these categorizations define relationships
between objects, and not between data types.
Structure The Whole-Part pattern introduces two types of participant:
A Whole object represents an aggregation of smaller objects, which we
call Parts. It forms a semantic grouping of its Parts in that it coordinates and organizes their collaboration. For this purpose, the Whole
asks the functionality of Part objects for implementing services.
Some methods of the Whole may be just placeholders for specific Part
services. When such a method is invoked the Whole only calls the
relevant Part service, and returns the result to the client.
Each graphical object in a CAD system may contain a Part that
provides version information to the user. When a client invokes the
method get version ( ) , the request is forwarded to the appropriate
method of the Part. 0
Other services of the Whole implement complex strategies that build
on several smaller services offered by Parts.
Consider zooming a group of 2-D objects. To achieve this, the
smallest surrounding rectangles of all group members are determined. Calculating the union of these rectangles leads to the smallest
surrounding rectangle of the group itself. Its center represents the
center of the zoom operation. To complete the execution of the zoom
method, the group object invokes the zoom operations of all its Parts,
passing the center and the percentage zoom as arguments.
The Whole may additionally provide functionality that does not invoke
any Part service at all.
Consider the implementation of collections such a s sets. Set
objects offer functions like get size ( 1 for returning the current
number of contained elements. For performance reasons, get size ( )
can be implemented by introducing caching strategies. An additional
data member size stores the current sizes of elements within the set.
Whenever elements are removed or added, the value of size is
updated accordingly. If a client invokes get size ( 1 , the function
returns the value of size without needing to access any elements of
the set. D
Only the services of the Whole are visible to external clients. The
Whole also acts as a wrapper around its constituent Parts and
protects them from unauthorized access.
Each Part object is embedded in exactly one Whole. Two or more
Wholes cannot share the same Part. Each Part is created and
destroyed within the life-span of its Whole.
The static relationships between a Whole and its Parts are illustrated
in the OMT diagram below:
Client
Dynamics The following scenario illustrates the behavior of a Whole-Part
structure. We use the two-dimensional rotation of a line within a CAD
system as an example. The line acts as a Whole object that contains
two points p and q as Parts. A client asks the line object to rotate
around the point c and passes the rotation angle as an argument.
Since the rotation of a line can be based on the rotation of single
points, it is sufficient for the line object to call the rotate methods of
its endpoints. After rotation, the line redraws itself on the screen. For
brevity, the scenario does not demonstrate how the old line is deleted
from the screen, nor how the drawLine method retrieves the
coordinates of the new endpoints.
Whole
The rotation of a point p around a center c with an angle a can be
calculated using the following formula:
The scenario consists of four phases:
A client invokes the rotate method of the line L and passes the
angle a and the rotation center c as arguments.
The line L calls the rotate method of the point p.
The line L calls the rotate method of the point q.
The line L redraws itself using the new positions of p I and q I as
endpoints.
Implementation To implement a Whole-Part structure, apply the following steps:
1 Design the public interface of the Whole. Analyze the functionality the
Whole must offer to its clients. Only consider the client's viewpoint in
this step. Think of the Whole as an atomic component that is not
structured into Parts, and compile a list of methods that together
comprise the public interface of the Whole.
2 Separate the Whole into Parts, or synthesize it from existing ones.
There are two approaches to assembling the Parts you need--either
assemble a Whole 'bottom-up' from existing Parts, or decompose it
'top-down' into smaller Parts:
The bottom-up approach allows you to compose Wholes from
loosely-coupled Parts that you can later reuse when implementing
other types of Whole. A liability of this approach is the difficulty of
covering all aspects of the required functionality of the Whole using
existing Parts. As a result, you often have to implement 'glue' to
bridge the gap between the composition of Parts and the interface
provided by the Whole.
@ The top-down approach makes it is possible to cover all of the
Whole's functionality. Partitioning into Parts is driven by the
services the Whole provides to its clients, freeing you from the
requirement to implement glue code. However, strictly applying the
top-down approach often leads to Parts that are tightly coupled
and not reusable in other contexts as a result.
A mixture of both approaches is often applied. For example, you may
follow the top-down approach until the resulting structure allows you
to reuse existing Parts.
3 If you follow a bottom-up approach, use existing Parts from component
libraries or class libraries and specify their collaboration. If you
cannot cover all the Whole's functionality with existing Parts, specify
additional ones and their integration with the remaining Parts. You
may need to use the top-down approach to implement such missing
Parts.
4 If you follow a top-down approach, partition the Mirhole's services into
smaller collaborating services and map these collaborating services to
separate Parts. For example, in the Forwarder-Receiver design
pattern (307)a forwarder component is responsible for marshaling an
IPC message and delivering it to the receiver. You can therefore
decompose a forwarder into two Parts, one responsible for marshaling
and another responsible for message delivery.
Note that there are often several ways to decompose a Whole into
Parts. For example, a triangle can be specified by three points that are
not co-linear, or by three lines, or by a line and a point. As a rule of
thumb, select the decomposition strategy that provides the easiest
way of implementing the services of the Whole. If, for example,
hidden-line algorithms are going to be applied to triangles, you
should implement them as compositions of lines.
5 Specify the services of the Whole in terms of services of the Parts. In
the structure you found in the previous two steps, the Whole is represented as a set of collaborating Parts with separate responsibilities.
You need to specify which Part functionality the Whole uses for servicing client requests, and which requests it executes on its own.
Two are two possible ways to call a Part service:
@ If a client request is forwarded to a Part service, the Part does not
use any knowledge about the execution context of the Whole,
relying on its own environment instead. Such forwarding leads to
a loose coupling between the Whole and its Parts-they may even
be implemented as active objects running in different processes.
@ A delegation approach requires the Whole to pass its own context
information to the Part. Delegation is useful when the Part should
be tightly embedded in the Whole's environment. For example,
delegation is required if implementation inheritance between a Part
and the Whole must be simulated.
Decide whether all Part services are called only by their Whole, or if
Parts may also call each other. Usually Parts are activated by their
Whole. Sometimes, however, it is necessary for Parts to interact. For
example, consider a simulation object such as a Whole that
represents a set of astronomical galaxies. If you need to determine the
movements of such galaxies, it is not sufficient to just consider the
effects of the 'Big Bang'-you also have to take the gravitation
Variants
attraction between galaxies into account. The solution to this problem
requires numerical methods in which Parts interact with each other.
Another example is provided by linked lists in which elements contain
references to their neighbors.
You can find further discussion about interaction between Parts in
the Mediator design pattern IGHJV951.
Implement the Parts. If the Parts are Whole-Part structures themselves, design them recursively starting with step 1. If not, reuse
existing Parts from a library, or just implement them if their implementation is straightforward and further decomposition is not
necessary.
Implement the Whole. Implement the Whole's services based on the
structure you developed in the preceding steps. Implement services
that depend on Part objects by invoking their services from the Whole.
You also need to implement those services that do not depend on a
Part object in this step.
When implementing the Whole, you need to take any given
constraints into account, such as cardinality properties. For example,
a water molecule consists of exactly two hydrogen atoms and one
oxygen atom. Constraints may also exist between parts. Consider a
postal package object and its contents-the size of the contents
cannot exceed the size of the package.
You also need to manage the life cycle of Parts. Since a Part lives and
must therefore die with its Whole, the Whole must be responsible for
creating and deleting the Part.
The Example Resolved section presents a concrete example of an
implementation of the Whole-Part pattern.
Shared Parts. This variant relaxes the restriction that each Part must
be associated with exactly one Whole by allowing several Wholes to
share the same Part. The life-span of a shared Part is then decoupled
from that of its Whole. For example, consider an electronic mail
message that consists of a header and several attachments. The
receiver of such a message could extract the attachments and
package them into a new message. Even if the original message is
deleted, its Parts-the attachments-may still exist. In such cases the
Part itself. or a central administration component. is responsible for
managing the Part's life cycle. In programming languages such as
C++you can use reference-counting strategies for this purpose-this
is explained in the Counted Pointer idiom (353).
The next three variants describe the implementation of the Whole-to Parts relationships we introduced in the Solution section:
Assembly-Parts. In this variant the Whole may be an object that
represents an assembly of smaller objects. For example, a CAD
representation of a car might be assembled from wheels, windows,
body panels and so on. Constituent Parts could follow the assembly parts relationship recursively-a wheel may itself be a Whole
consisting of Parts such as circles. Recursively applying whole-part
relationships leads to trees, and may also lead to directed acyclic
graphs if shared Parts are allowed. Assembly-Parts structures are
fixed, in that they do not support the addition or removal of Parts at
run-time. They only allow you to exchange Parts with other Parts of
the same type.
Container-Contents. In this variant a container is responsible for
maintaining differing contents. For example, an electronic mail
message may contain a header, the message body, and optional
attachments. In contrast to the Assembly-Parts variant, a container
component allows you to add or remove its contents dynamically.
The Collection-Members variant is a specialization of Container Contents, in that the Part objects all have the same type. Parts are
usually not coupled to or dependent on each other. You can apply this
variant when implementing collections such as sets, lists, maps, and
arrays. In addition, this pattern supports the inclusion of
functionality for iterating over all members, and for executing
operations on some or all members.
The Composite pattern was introduced in [GHJV95]. It is applicable
to Whole-Part hierarchies in which the Wholes and their Parts can be
treated uniformly-that is, in which both implement the same
abstract interface.
Example In our CAD system we decide to define a Java package that provides
Resolved the basic functionality for graphical objects. The class library consists
of atomic objects such as circles or lines that the user can combine
to form more complex entities. We implement these classes directly
instead of using the standard Java package awt (AbstractWindowing
Toolkit) because awt does not offer all the functionally we need.
Objects use virtual coordinates instead of physical screen coordinates
to hide system dependencies such as screen resolution. The abstract
base class GraphicsObject defines common methods such as draw.
rotate and dump.All other classesare either derived from Graphics -
Object or one of its subclasses. The Implementation of classes that
provide a particular type of graphical objects such as Triangle uses
the Assembly-Parts variant.
The class Triangle is an example of a subclass of GraphicsObj ect.
Each triangle is assembled from exactly three cartesian points that
are not co-linear. A triangle object therefore acts as a Whole that
contains three points as Part objects. The implementation of the class
Triangle. therefore. is based on the implementation of the class
Point. For example. the rotation of a triangle can be performed by
rotating its corners. The rotate method is therefore an example of a
service of the Whole that uses operations provided by the Parts.The
Assembly-Parts relationship between a Mangle and its comers is
illustrated in the following diagram:
When the method rotate is invoked for a point, the center of rotation
is passed as an argument. If the center and the point are the same.
the method does nothing, otherwise it rotates the point around the
center using the specified angle:
An example of constraint checking of the triangle as a Whole is that
the constructor of Triangle must check whether the three points
passed as arguments are collinear. Three points p, q, and r are not
collinear if and only if they define a two-dimensional vector space:
We implement groups of different graphics objects using the
Collection-Member variant. We can use this variant because a group
does not need to know the concrete subtypes of its members-it can
handle each of its members as an instance of class Graphics obj ect
instead. The class GroupObject comprises functionality such as the
addition of graphical objects, and the iteration through all group
members. Note that the class GroupObject does not comply exactly
with the Composite variant (GHJV951.The reason for this is that Part
objects have a type different from the Whole. Whereas the Whole is an
instance of GroupObject,the Parts are not-we have introduced the
class GroupObj ect for this purpose. The alternative would have been
to extend Graphics object with functionality for adding elements,
regardless whether derived classes implement group objects or not.
If a method such as rotate is invoked for such a group, the group
recursively invokes the method on all its members.
Imagine that a user creates different graphics objects, selects them
with the mouse, inserts them into a group, and tells the object editor
to rotate the group around ( 0,0 ) with an angle of n/4. The editor will
execute a code sequence similar to that listed below:
The classes we have already introduced support simple shapes such
as circles or triangles, as well as the grouping of such graphics
objects. To create more complex shapes, instances of the class
TreeObject support the composition of graphics objects using
operators. For example, a circle with a rectangular hole may be
represented by a binary tree. The left child specifies the circle, the
right child the rectangle, and the node consists of the operator SUB as
well as additional data. SUB is defined as subtraction of one figure
from another. In this example, the rectangle is geometrically
subtracted from the circle. resulting in a circle with a hole.
Tree objects implement the Container-Contents variant of Part Whole. The Whole is given by the complex shape that is calculated
from simpler shapes using a geometrical formula. The graphics
objects and the operator in this formula represent the Parts. When an
operation such as move is invoked on the TreeObj ect instance, the
Whole forwards the request to all the sub-shapes of which it is
composed.
Known Uses The key abstractions of many object-oriented applications follow
the Whole-Part pattern. For example, some graphical editors support
the combination of different types of data to form multimedia
documents. These are often implemented according to the Composite
design pattern lGHJV951. In CAD or animation systems, items under
construction are represented by Assembly-Part structures. Almost all
aspects of an application that can be hierarchically structured and
can represent semantic units may be a subject for the application of
the Whole-Part pattern in one of its variants.
Most object-oriented class libraries provide collection classes such
as lists. sets. and maps. These classes implement the Collection Member and Container-Contents variants. See BNI941 and [Lea961
for examples.
Graphical user interface toolkits such as Fresco or ET++IGam911
use the Composite variant of the Whole-Part pattern.
Consequences The Whole-Part pattern offers several benefits:
Whole-Part
Changeability of Parts. The Whole encapsulates the Parts and thus
conceals them from its clients. This makes it possible to modify the
internal structure of the Whole without any impact on clients. Part
implementations may even be completely exchanged without any
need to modify other Parts or clients.
Separation of concerns. A Whole-Part structure supports the
separation of concerns. Each concern is implemented by a separate
Part. It therefore becomes easier to implement complex strategies by
composing them from simpler services than to implement them as
monolithic units.
Reusability. The Whole-Part pattern supports two aspects of
reusability. Firstly, Parts of a Whole can be reused in other aggregate
objects. Secondly,the encapsulation of Parts within a Whole prevents
a client from 'scattering' the use of Part objects all over its source
code-this supports the reusability of Wholes.
The Whole-Part pattern suffers from the following liabilities:
Lower efficiency through indirection.Since the Whole builds a wrapper
around its Parts, it introduces an additional level of indirection
between a client request and the Part that fulfils it. This may cause
additional run-time overhead compared with monolithic structures,
especially when Parts are themselves implemented as Whole-Part
structures.
Complexity of decomposition into Parts. An appropriate composition of
a Whole from different Parts is often hard to find, especially when a
bottom-up approach is applied. This is because an optimal partitioning into Parts depends on many issues, such as the given
application domain, the structure to be modeled and the functionality
to be provided by the Whole.
See also According to [GHJV95] the Composite design pattern is applicable
when:
You want to represent whole-part hierarchies of objects.
You want clients to be able to ignore the difference between
compositions of objects and individual objects. Clients will treat
all objects in the composite structure uniformly.
Composite is a variant of the Whole-Part design pattern that you
should consider when facing these two requirements.
The Facade design pattern [GHJV95]helps to provide a simple inter face to a complex subsystem. A client uses this interface instead of
accessing different Parts of the subsystem directly. However, a
Facade structure does not enforce the encapsulation of Parts--clients
may also access them directly. Another difference from Whole-Part
structures is that facades do not compose complex services from simpler Part services-they only perform necessary interface translations
and forward client requests to the appropriate Parts.
##%%&&
The Master-Slave design pattern supports fault tolerance, parallel
computation and computational accuracy. A master component
distributes work to identical slave components and computes a Anal
result from the results these slaves return.
Example The traveling-salesman problem is well-known in graph theory. The
task is to find an optimal round trip between a given set of locations.
such as the shortest trip that visits each location exactly once.
The solution to this problem is of high computational complexity there are approximately6.0828* different trips that connect the
state capitals of the United States! Generally, the solution to the
traveling-salesman problem with n locations is the best of (n-l)!
possible routes. Since the traveling-salesman problem is NP-complete
lGJ791,there is no way to circumvent this high complexity if the
optimal solution must be found.
Most existing implementations of the traveling-salesman problem
therefore approximate the optimal solution by only comparing a fixed
number of routes. One of the simplest approaches is to select routes
to compare at random, and hope that the best route found
approximates the optimal route sufficiently. We should make sure
however that the routes to be investigated are chosen in a random
and independent fashion, and that the number of selected routes is
sufficiently large.
Context Partitioning work into semantically-identical sub-tasks.
Problem 'Divide and conquer' is a common principle for solving many h d s of
problems. Work is partitioned into several equal sub-tasks that are
processed independently. The result of the whole calculation is computed from the results provided by each partial process. Several
forces arise when implementing such a structure:
Clients should not be aware that the calculation is based on the
'divide and conquer' principle.
Neither clients nor the processing of sub-tasks should depend on
the algorithms for partitioning work and assembling the final
result.
It can be helpful to use different but semantically-identical implementations for processing sub-tasks, for example to increase
computational accuracy.
Processing of sub-tasks sometimes needs coordination, for example in simulation applications using the finite element method.
Solution Introduce a coordination instance between clients of the service and
the processing of individual sub-tasks.
A master component divides work into equal sub-tasks, delegates
these sub-tasks to several independent but semantically-identical
slave components, and computes a final result from the partial
results the slaves return.
This general principle is found in three application areas:
Fault tolerance. The execution of a service is delegated to several
replicated implementations. Failure of service executions can be
detected and handled.
Parallel computing. A complex task is divided into a fixed number
of identical sub-tasks that are executed in parallel. The final result
is built with the help of the results obtained from processing these
sub-tasks.
Computational accuracy. The execution of a service is delegated to
several different implementations. inaccurate results can be
detected and handled.
Provide all slaves with a common interface. Let clients of the overall
service communicate only with the master.
r We decide to approximate the solution to the traveling-salesman
problem by comparing a fixed number of trips. Our strategy for
selecting trips is simple-we just pick them randomly. This simple minded implementation uses an early version of the object-oriented
parallel programming language pSather (MFL931. The program is
tuned for a CM-5computer from Thinking Machines Corporation with
sixty-four processors.
To take advantage of the CM-5 multi-processor architecture, the
lengths of different trips are calculated in parallel. We therefore
implement the trip length calculation as a slave. Each slave takes a
number of trips to be compared as input, randomly selects these trips
and returns the shortest trip found. A master determines a priori the
number of slaves that are to be instantiated, specifies how many trips
each slave instance should compare, launches the slave instances,
and selects the shortest trip from all trips returned. In other words,
the slaves provide local optima that the master resolves to a global
optimum. 
Structure The master component provides a service that can be solved by applying the 'divide and conquer' principle. It offers an interface that allows
clients to access this service. Internally,the master implements functions for partitioning work into several equal sub-tasks, starting and
controlling their processing, and computing a final result from all the
results obtained.The master also maintains references to all slave in stances to which it delegates the processing of sub-tasks.
The slave component provides a sub-service that can process the
sub-tasks defined by the master. Within a Master-Slave structure,
there are at least two instances of the slave component connected to
the master.
Dynamics In the following scenario we assume, for simplicity, that slaves are
called one after the other. However, the Master-Slave pattern
unleashes its full power when slaves are called concurrently, for example by assigning them to several separate threads of control. The
scenario comprises six phases:
A client requests a service from the master.
The master partitions the task into several equal sub-tasks.
The master delegates the execution of these sub-tasks to several
slave instances, starts their execution and waits for the results
they return.
The slaves perform the processing of the sub-tasks and return the
results of their computation back to the master.
The master computes a final result for the whole task from the
partial results received from the slaves.
Implementation The implementation of the Master-Slave pattern follows five steps.
Note that these steps abstract from specific issues that need to be
considered when supporting the application of the pattern to the special cases of fault tolerance. parallel computation, and computational
accuracy, or when distributing slaves to several processes or threads.
These aspects are addressed in the Variants section.
1 Divide work.Specify how the computation of the task can be split into
a set of equal sub-tasks. Identify the sub-services that are necessary
to process a sub-task.
For our parallel traveling-salesman program we could partition
the problem so that a slave is provided with one round trip at time
and computes its cost. However, for a machine like the CM5 with
SPARC node processors, such a partitioning might be too fine grained. The costs for monitoring these parallel executions and for
passing parameters to them decreases the overall performance of the
algorithm instead of speeding it up.
A more efficient solution is to define sub-tasks that identify the
shortest trip of a particular subset of all trips. This solution also takes
account of the fact that there are only sixty-four processors available
on our CM5. The number of available processors limits the number of
sub-tasks that can be processed in parallel. To find the number of
trips to be compared by each sub-task, we divide the number of all
trips to be compared by the number of available processors. cl
2 Combine sub-task results. Specify how the final result of the whole
service can be computed with the help of the results obtained from
processing individual sub-tasks.
@ Each sub-task returns only the shortest trip of a subset of all
trips to be compared.We must still identify the shortest trip of these.cl
3 Specify the cooperation between master and slaves. Define an inter face for the sub-service identified in step 1.It will be implemented by
the slave and used by the master to delegate the processing of individual sub-tasks.
One option for passing sub-tasks from the master to the slaves is to
include them as a parameter when invoking the sub-service. Another
option is to define a repository where the master puts sub-tasks and
the slaves fetch them. When processing a sub-task, individual slaves
can work on separate data structures, or all slaves can share a single
data structure. Slaves may return the result of their processing
explicitly as a return parameter, or they may write it to a separate
repository from which the master retrieves it.
Which of these options are best depends on many factors; for
example, the costs of passing sub-tasks to slaves, of duplicating data
structures, and of operating on a shared data structure with several
slaves.The original problem also influences the decisions to be made.
When slaves modify the data on which they operate, you need to
provide each slave with its own copy of the original data structure. If
they do not modify data, all slaves can work on a shared data
structure, for example when implementing matrix multiplication.
@ For the traveling-salesman program we let each slave operate on
its own copy of the graph that represents all cities and their
connections. We will create these copies when instantiating the
slaves. The alternative-having the slaves read from one shared
graph representation-was not chosen since such a communication
load on the CM5 internal network would reduce the performance of
our application considerably.
The interface of the slave to the master is defined by a function that
takes the number of random routes to be evaluated as an input
parameter. The function returns the optimal route found, which is
represented by an instance of class TOUR.
The term perms in random germs ( ) stands for permutations, since
we represent round trips as permutations of the n nodes that stand
for the n cities to be visited. 0
4 Implement the slave components according to the specifications
developed in the previous step.
b The class TSP is the design center of our small applications. It
includes a constructor, functions to create a random trip and to
update the shortest Mp found so far, and the random perms 0
function specified in the previous step. The class COMPLETE-GRAPH
represents the graph structure on which instances of TSP operate.
The class RANDOM represents a random number generator. The code
is not complete,but is an excerpt from a working application.
Note that the assignment in update-optimum assumes either deep copy semantics, or that current-tour will refer to a new TOUR object
after the assignment. Otherwise, construct-random-tour ( ) Corrupts best-tour when modifying current-tour. The original program solved the problem by swapping the two TOUR objects to which
best-tour and current-tour referred.
5 Implement the master according to the specifications developed in
step 1to 3.
There are two options for dividing a task into sub-tasks. The first is
to split work into a fixed number of sub-tasks.This is most applicable
if the master delegates the execution of the complete task to the
slaves. This might typically occur when the Master-Slave pattern is
used to support fault tolerance or computational accuracy applications, or if the amount of parallel work is always fixed and known a
priori. The second option is to define as many sub-tasks as necessary,
or possible. For example, the master component in our traveling salesman program could define as many sub-tasks as there are processors available.
The exchange of algorithms for subdividing a task can be supported
by applying the Strategy pattern [GHJV95].We discuss further issues
you should consider in the Variants section.
The code for launching the slaves, controlling their execution and collecting their results depends on many factors. Are the slaves executed
sequentially, or do they run concurrently in different processes or
threads? Are slaves independent of each other, or do they need coordination? We give more details about this in the Variants section.
The master computes a final result with help of the results collected
from the slaves. This algorithm may follow different strategies, as
described in the Variants section. To support its dynamic exchange
and variation, you can again apply the Strategy pattern [GHJV951.
You also must deal with possible errors, such as failure of slave
execution or failure to launch a thread. Details are discussed in the
Variants section.
There is only one master component within a Master-Slave structure.
You can apply the Singleton pattern [GHJV95] to ensure this
property.
In the traveling salesman program we represent the master with
an object of class CM5-TSP. It offers a function best-tour ( ) to its
clients which returns the best round trip visited by the whole Master Slave structure. The best-tour ( ) function takes the number of
routes to be generated and the number of processors to use as
parameters.
The function distribute ( ) copies the graph and some additional
data structures to all processors. The implementation we show works
sequentially. 'sj' means 'do this operation on processor j'. The
function distribute ( ) creates as many new slaves as there are
processors available. The function random_perms ( ) launches the
slaves. The function update-optimum ( ) selects the optimal route
from the local optima returned by the slaves.
Our strategy for coordinating the slaves is to start them asynchronously and to synchronize them later, in particular when we want to
select the best trip found. To implement this behavior we use the
'future' principle. A future is a variable that defines a value that is
computed asynchronously in a different process or thread of control.
Synchronization is achieved when the variable is accessed later. Since
pSather supports futures, we use an array of futures for slaves to
coordinate their parallel execution. For reasons of brevity we do not
illustrate object creation. For more details on the pSather version we
use in our example, see [Lim93].
There are three application areas for the Master-Slave pattern:
Master Slave for fault tolerance. In this variant the master just
delegates the execution of a service to a fixed number of replicated
implementations, each represented by a slave. As soon as the first
slave terminates, the result produced is returned to the client of the
master. Fault tolerance is supported by the fact that as long as at
least one slave does not fail, the client can be provided with a valid
result. The master can handle the situation in which all slaves fail, for
example by raising an exception or by returning a special 'Exceptional
Value' [Cun94]with which the client can operate. The master may use
time-outs to detect slave failure. However, this variant does not help
with the situation in which the master itself fails-it is the critical
component that must 'stay alive' to make this structure work.
Master-Slave for parallel computation The most common use of the
Master-Slave pattern is for the support of parallel computation. In
this variant the master divides a complex task into a number of
identical sub-tasks, each of which is executed in parallel by a
separate slave. The master builds the final result from the results
obtained from the slaves. The master contains the strategies for
dividing the overall task and for computing the Anal result.
The algorithm for sub-dividing the task and for coordinating the
slaves is strongly dependent on the hardware architecture of the
machine on which the program runs. On distributed memory
machines with general-purpose processors. for example, the granularity is usually larger than on SIMD (single instruction multiple data)
machines. Other aspects that govern the algorithm are the machine's
topology and the speed of its processor interconnections. The cooperation between the master and the slaves also depends on aspects
such as the existence of shared or distributed memory for machines.
The division of work is further influenced by issues listed in the Slave
as Threads variant (see below), and the cooperation between master
and slaves by issues listed in step 3of the Implementation section.
Before the master can compute the final result it must wait for all
slaves to finish executing their sub-tasks. To free the master from the
task of synchronizing each slave individually, [KSS96]introduces the
concept of a barrier. A barrier is initialized with the slaves on whose
termination the master waits. It then suspends the execution of the
master until all the slaves it controls have terminated. Our pSather
Design Patterns
example, in contrast, works in an incremental fashion-whenever a
slave terminates the random perms ( ) method takes its result.
Master-Slave for computational accuracy.In this variant the execution
of a service is delegated to at least three different implementations,
each of which is a separate slave. The master waits for all slaves to
complete, and votes on their results to detect and handle inaccuracies. This voting may follow different strategies. Examples include
that in which the master selects the result that is returned by the
greatest number of slaves, the average of all results, or the use of an
Exceptional Value [Cun941in the case in which all slaves produce different results.
To provide different slave implementations, we can extend the
structure of the Master-Slave pattern with an additional abstract
class. This defines an interface common to all slave implementations.
Different slave implementations are then derived from this abstract
base.
Further variants exist for implementing slaves:
Slaves as Processes. To handle slaves located in separate processes,
you can extend the original Master-Slave structure with two additional components IBro961.The master includes a top component that
keeps track of all slaves working for the master. To keep the master
and the top component independent of the physical location of distributed slaves, remote proxies (263) represent each slave in the
master process. You can apply the Forwarder-Receiver (307) or
Client-Dispatcher-Server pattern (323) to implement the inter process communication.
Slaves as Threads.In this variant, every slave is implemented within
its own thread of control (KSS961. In this variant the master creates
the threads, launches the slaves, and waits for all threads to complete
before continuing with its own computation. The Active Object
pattern [Sch95]helps in implementing such a structure.
In this variant the master must deal with two problems: what
happens if a thread cannot be created, and how many threads should
be created? A solution to the first problem is to call the slave's services
directly, without launching them in a separate thread. Performance
will suffer, but the result will be correct. The optimal number of
threads depends on the number of processors available and on the
amount of work required from each thread. Too many threads incur
overheads in their creation and destruction, a s well as in memory
consumption. [KSS961 suggests experimenting with different
strategies, starting with 'a few more threads than the number of
processors'.
MasterSlave with slave coordination.The computation of a slave may
depend on the state of computation of other slaves, for example when
performing simulation with finite elements. In this case the
computation of all slaves must be regularly suspended for each slave
to coordinate itself with the slaves on which it depends, after which
the slaves resume their individual computation.
There are two ways of implementing such a behavior. Firstly, you can
include the control logic for slave coordination within the slaves
themselves. This frees the master from the task of implementing this
coordination, but may decrease the performance of the overall
structure. Slaves will stop their execution independently and may idle
until the slaves on which they depend are ready for coordination.
The second option is to let the master maintain dependencies
between slaves and to control slave coordination. At regular time
intervals the master suspends all slaves, retrieves the current state of
their computation, forwards this data to all slaves that depend on this
data, and resumes the execution of all slaves.
n Uses [KSS96] lists three concrete examples of the application of the
Master-Slave pattern for parallel computation:
@ Matrix multiplication. Each row in the product matrix can be
computed by a separate slave.
@ Transform-coding an image, for example in computing the discrete
cosine transform (DCT)of every 8 x 8 pixel block in an image. Each
block can be computed by a separate slave.
@ Computing the cross-correlation of two signals. This is done by
iterating over all samples in the signal, computing the mean square distance between the sample and its correlate, and
summing the distances. We can partition the iteration over the
samples into several parts and compute the square distance and
its sums separately for each partition. The final sum is computed
by summing all sums from these partitions. Each partial summing
can be performed by a separate slave. A master component defines
the partitions, launches the slaves, and computes the final sum.
el described in [KR96]applies the Master-Slave
pattern to implement process control for parallel computing, based
on the principles of Linda [Ge185]. A programmer can assign a
number of so-called workers to a work pool. Each worker offers the
same services and is implemented in a separate process or thread.
Clients send requests to the work pool, which handles these requests
with help of its associated workers. The request itself is a function
whose execution should be parallelized with help of the workers, such
as matrix multiplication. This function corresponds to the master
component in the Master-Slave pattern.
The concept of Gages [BI93] builds upon the principles of the
Master-Slave pattern to handle 'plurality' in an object-oriented
software system. A gaggle represents a set of replicated service
objects. When receiving a service request from a client, the gaggle
forwards this request to one of the service objects it includes. Each of
these service objects can be atomic, which means it executes the
service and delivers a result, or another gaggle which itself represents
a set of replicated service objects.
[Bro96]lists several applications of the Master-Slave design pattern,
all of which focus on distributed slaves. These include the distributed
design rule checking system CalibreTMDRC-MP and the CheckMate
IC verification tool, both from Mentor Graphics.
Factoring large numbers into prime factors can also be done in a
'divide and conquer' fashion. As this problem is central to cryptography, of great interest to governments, and requires vast computing resources, it has been carried out over the Internet. One site
did the subdivision and sent sub-tasks to people willing to provide
computing time and the use of their machines.
Consequences The Master-Slave design pattern provides several benefits:
Exchangeability and extensibility. By providing an abstract slave
class, it is possible to exchange existing slave implementations or add
new ones without major changes to the master. Clients are not
affected by such changes. If they are implemented with the Strategy
pattern [GHJV95],the same holds true when changing the algorithms
for allocating sub-tasks to slaves and for computing the final result.
Separation of concerns. The introduction of the master separates
slave and client code from the code for partitioning work, delegating
work to slaves, collecting the results from the slaves, computing the
final result and handling slave failure or inaccurate slave results.
Efficiency. The Master-Slave pattern for parallel computation enables
you to speed up the performance of computing a particular service
when implemented carefully. However,you must always consider the
costs of parallel computation (see below).
The Master-Slave pattern suffers from three liabilities:
Feasibility. A Master-Slave architecture is not always feasible. You
must partition work, copy data, launch slaves, control their
execution, wait for the slave's results and compute the final result. All
these activities consume processing time and storage space.
Machine dependency. The Master-Slave pattern for parallel
computation strongly depends on the architecture of the machine on
which the program runs-see the Variants section for details. This
may decrease the changeability and portability of a Master-Slave
structure.
Hard to implement.Implementing Master-Slave is not easy, especially
for parallel computation. Many different aspects must be considered
and carefully implemented, such as how work is subdivided, how
master and slaves should collaborate, and how the final result should
be computed. You also must deal with errors such as the failure of
slave execution, failure of communication between the master and
slaves, or failure to launch a parallel slave. Implementing the Master Slave pattern for parallel computation usually requires sound
knowledge about the architecture of the target machine for the
system under development.
Portability. Because of the potential dependency on underlying
hardware architectures, Master-Slave structures are difficult or
impossible to transfer to other machines. This is especially true for
the Master-Slave pattern for parallel computation, and similarly for
our simple traveling-salesman program tuned for the CM5 computer.
See also An earlier version of this pattern appeared in [PLoP94].
The Master-Slave Pattern for Parallel Compute Services (Bro961
provides additional insights for implementing a Master-Slave
structure. It differs from the structure described here, as it
concentrates on describing the Slaves as Processes variant.
The book Programming with Threads [KSS96]describes the Slaves as
Threads variant in detail.
Object Group [Maf96] is a pattern for group communication and
support of fault tolerance in distributed applications. It corresponds
to the Master-Slave for fault tolerance variant and provides additional
details for its implementation. The Object Group pattern provides a
local surrogate for a group of replicated objects distributed across
networked machines. A request is broadcast to all objects of the
group. The request will succeed as long as one group member
terminates successfully.
##%%&&
The Proxy design pattern makes the clients of a component
communicate with a representative rather than to the component
Itself. Introducing such a placeholder can serve many purposes,
including enhanced efficiency, easier access and protection from
unauthorized access.
Example Company engineering staff regularly consult databases for information about material providers, available parts. blueprints, and so on.
Every remote access may be costly, while many accesses are similar
or identical and are repeated often. This situation clearly offers scope
for optimization of access time and cost. However, we do not want to
burden the engineer's application code with such optimization. The
presence of optimization and the type used should be largely transparent to the application user and programmer.
Context A client needs access to the services of another component2. Direct
access is technically possible, but may not be the best approach.
Problem I t is often inappropriate to access a component directly. We do not
want to hard-code its physical location into clients, and direct and
unrestricted access to the component may be inefficient or even insecure. Additional control mechanisms are needed. A solution to such
a design problem has to balance some or all of the following forces:
Accessing the component should be run-time-efficient, cost effective, and safe for both the client and the component.
Access to the component should be transparent and simple for the
client. The client should particularly not have to change its calling
behavior and syntax from that used to call any other direct-access
component.
The client should be well aware of possible performance or
financial penalties for accessing remote clients. Full transparency
can obscure cost differences between services.
Solution Let the client communicate with a representative rather than the
component itself. This representative-called a proxy-offers the
interface of the component but performs additional pre- and post processing such as access-control checking or making read-only
copies of the original-see below.
Structure The original implements a particular service. Such a service may
range from simple actions like returning or displaying data to
complex data-retrieval functions or computations involving further
components.
The client is responsible for a specific task. To do its job, it invokes the
functionality of the original in an indirect way by accessing the proxy.
The client does not have to change its calling behavior and syntax
from that which it uses to call local components.
2. 'Component' is used very vaguely here intentionally. It can mean anything to
which you do not want to give direct access for the above reasons. Some examples of
such components are ordinary local objects, an external database. an HTML page on
the Web or an Image embedded In a text document.
Therefore, the proxy offers the same interface as the original, and
ensures correct access to the original. To achieve this the proxy
maintains a reference to the original it represents. Usually there is a
one-to-one relationship between the proxy and the original, though
there are exceptions to this rule for Remote and Firewall proxies, two
variants of this general pattern. See the Variants section for more
information.
The abstract original provides the interface implemented by the proxy
and the original. In a language like C++, with no notable difference
between sub typing and inheritance, both the proxy and the original
inherit from the abstract original. Clients code against this interface
when accessing the original.
The following OMT diagram shows the relationships between the
classes graphically:
Dynamics The following diagram shows a typical dynamic scenario of a Proxy
structure. Note that the actions performed within the proxy differ
depending on its actual specialization-see the Variants section for
more information:
While working on its task the client asks the proxy to carry out a
service.
The proxy receives the incoming service request and pre-processes
it. This pre-processing involves actions such as looking up the
address of the original, or checking a local cache to see if the
requested information is already available.
If the proxy has to consult the original to fulfill the request, it forwards the request to the original using the proper communication
protocols and security measures.
The original accepts the request and fulfills it. It sends the
response back to the proxy.
The proxy receives the response. Before or after transferring it to
the client it may carry out additional post-processing actions such
as caching the result, calling the destructor of the original or
releasing a lock on a resource.
Implementation To implement the Proxy pattern, carry out the following steps:
1 Identity all responsibilities for dealing with access control to a
component. Attach these responsibilities to a separate component,
the proxy. The details of this step are described in the Variants
section.
2 If possible introduce an abstract base class that specifies the
common parts of the interfaces of both the proxy and the original.
Derive the proxy and the original from this abstract base. If
identical interfaces for the proxy and the original are not feasible
you can use a n adapter [GHJV95] for interface adaptation.
Adapting the proxy to the original's interface retains the client with
the illusion of identical interfaces, and a common base class for the
adapter and the original may be possible again.
3 Implement the proxy's functions. To this end check the roles
specified in the first step.
4 Free the original and its clients from responsibilities that have
migrated into the proxy.
5 Associate the proxy and the original by giving the proxy a handle to
the original. This handle may be a pointer, a reference, an address,
an identifier, a socket, a port and so on.
6 Remove all direct relationships between the original and its clients.
Replace them by analogous relationships to the proxy.
Variants We describe seven variants of the generic Proxy pattern below. We
start by summarizing the situations to which the individual variants
are best suited:
Remote Proxy. Clients of remote components should be shielded
from network addresses and inter-process communication
protocols.
Protection Proxy. Components must be protected from unauthorized access.
Cache Proxy. Multiple local clients can share results from remote
components.
Synchronization Proxy. Multiple simultaneous accesses to a component must be synchronized.
Counting Proxy. Accidental deletion of components must be
prevented or usage statistics collected.
Virtual Proxy. Processing or loading a component is costly, while
partial information about the component may be sufficient.
Firewall Proxy. Local clients should be protected from the outside
world.
The paragraphs that follow detail the characteristics and implementation details of each variant.
A Remote Proxy encapsulates and maintains the physical location of
the original. It also implements the IPC (inter-process communication) routines that perform the actual communication with the
original. For every original, one proxy is instantiated per address
space in which the services of the original are needed. For complex
IPC mechanisms, you can refine the proxy by shifting responsibility
for communication with the original to a forwarder component, a s
described in the Forwarder-Receiver pattern (307).Analogously,
introduce a receiver component into the original.
For reasons of efficiency,we discern remote proxies into three cases:
Client and original live in the same process.
Client and original live in different processes on the same machine.
Client and original live in different processes that run also on
different machines.
The first case is simple: we do not need a proxy for talking to the
original. For the second and third cases we put fields for a remote
address into the proxy, usually consisting of machine ID, port or
process number and an object ID. The second case obviously does not
need the machine ID. If you want to save the few bytes that a machine
ID occupies, bear in mind that the differentiation between the second
and third cases complicates the code of the proxy. The effort of
developing such differentiation logic is usually not justified, except in
cases where the means of inter-process communication are different
in both cases, enforcing such differentiation logic. Even then you can
add a thin layer on top concealing the differences between the three
cases, thus simplifying the code that uses the addressing scheme.
The presence of an abstract original makes it completely transparent
to the client which of the three cases is employed.
In high-performance applications, you often want to determine
whether or not communication is expensive at the application level
before committing to an off-board request. In such cases, a remote
proxy reveals this information.
A Protection Proxy protects the original from unauthorized access. To
achieve this the proxy checks the access rights of every client. You
can most easily achieve this by using the access-control mechanisms
your platform offers. If appropriate and possible, try to give every
client its own set of permissions to other components. Access control
lists are a widespread implementation of this concept.
To implement a Cache Proxy,extend the proxy with a data area to
temporarily hold results. Develop a strategy to maintain and refresh
the cache. When the cache is full and you need to free up space for
new entries, there are several strategies you can use. For example,
you can delete the least-frequently used cache entries, or implement
a 'move-to-front' strategy-this is usually easier to implement and
efficient enough. In this strategy, whenever a client accesses a cache
entry, it is moved to the front of, say, a doubly-linked list. When new
entries have to be added to the cache, entries can be deleted from the
back of the list.
You must also take care of the 'cache invalidation' problem-when
data in the original changes, copies of this data cached elsewhere
become invalid. If it is crucial that your application always has up-to-date data, you can declare the whole cache invalid whenever the
original copy of any of its entries is changed. Alternatively, you can
use a 'write-through strategy, well-known from microprocessor cache
design, for finer-grained control. Whenever the original is modified all
its copies are modified as well. Note that this becomes complicated
when there is more than one copy, or when the copies are remote, in
contrast to a microprocessor cache where the situation is simpler. If
your clients can accept slightly outdated information, you can label
individual cache entries with expiration dates. Examples of this
strategy include World Wide Web browsers.
A Synchronization Proxy controls multiple simultaneous client
accesses. If it is important that only one client-r a specified number
of clients-can access the original at a time, the proxy can implement
mutual exclusion via semaphores [Dij65]. Alternatively, it can use
whatever means of synchronization your operating system offers. You
may also differentiate between read or write access. In the former
case, you can adopt more liberal policies, for example by allowing an
arbitrary number of reads when no write is active or pending. The
operating system literature is a good source for studying these
mechanisms.
A Counting Proxy can be used for collecting usage statistics, or to
implement a well-known technique for automatically deleting obsolete objects-reference counting. To achieve this the counting proxy
maintains the number of references that exist to the original, and
deletes the original when this number becomes zero. You need to
ensure that there is exactly one counting proxy for every original, and
that every access to an original goes through a defined interface of the
respective proxy. Also keep in mind that reference counting alone
does not help with the problem of finding cycles of otherwise isolated
components that refer to each other's proxies.
The Counted Pointier idiom (353) illustrates a different way to
implement a counting proxy in C++. There, the reference counter is
inside the original or its own object, not in a handle or proxy. The
idiom also discusses why some C++ implementations employ another
level of indirection to refer to the reference counter, and to update the
reference counter whenever a handle object is created or deleted.
A Virtual Proxy, also known as lazy construction, assumes that an
application references secondary storage, such as the hard disk. This
proxy does not disclose whether the original is fully loaded or whether
only skeletal information about it is available. Loading missing parts
of the original is performed on demand.
When a service request arrives and the information present in the
proxy is not sufficient to handle the request, load the required data
from disk and forward the request to the freshly-created or expanded
original. If the original is already fully loaded, just forward the
request. This forwarding should be done transparently such that
clients always use the same interface independent of whether the
original is in main memory or not. It is the responsibility of the client
or an associated module to notify the proxy when the original, or parts
of it, are no longer needed. The proxy then frees the space allocated.
When several clients reference the same original, it may be
appropriate to add the capabilities of the Synchronization and Cache
Proxy variants.
all Proxy subsumes the networking and protection code
to communicate with a potentially hostile environment.
Usually the firewall proxy is implemented as a daemon process on a
firewall machine, which can also be referred to as a 'proxy server'. All
clients who pass requests to the outside world reference this proxy.
The proxy works behind the scenes by checking outgoing requests
and incoming answers for compliance with internal security and
access policies. It denies access when a request does not comply with
such policies, or when its resources are exhausted. Clients are
provided with an almost complete illusion of unhindered a
outside, and do not need to go to the inconvenience of 1
the firewall machine. Similarly, security is maintained, as user
accounts are protected from attack from outside. Servers on the
Internet are given the illusion that the proxy is the client. This allows
the internal structure of the network behind the firewall to be hidden.
A notable characteristic of firewall proxies is that the user needs
'proxied' versions of client software. For example, the standard f t p
software must be replaced by another version that contacts the proxy
instead of directly accessing the destination machine. A consequence
of this can be that new services can only be used when equivalent
proxied versions of these services are available.
Because all communication flows through the firewall proxy, it
constitutes a potential bottleneck and provides an ideal place for
optimizations such as caching. It also provides an ideal location for
additional tasks like logging and accounting. For more information on
firewall design, see [CZ95].
Example You may often need to use more than one of the above Proxy variants
Resolved -you may want the proxy to play several of the above roles and fulfill
the corresponding responsibilities. Make your choice by first picking
the desired roles, for instance virtual and cache, then thinking about
combining these roles into one proxy.
If combining them bloats the resulting proxy too much, split it into
smaller objects. One example of this is factoring out complicated
networking code into a forwarder-receiver structure-see the
Forwarder-Receiver pattern (307). In this case the proxy is left only
with the location information of the original and the local-versus remote decision.
You can solve remote data access problems by using proxies with the
properties of both Remote and Cache Proxy variants. Implementing
such a mixed-mode proxy can be accomplished by using the Whole Part pattern (225).
One part is the cache. It contains a storage area and strategies for
updating and querying the cache. By using the 'least frequently used'
strategy and tuning the cache size, you can cut down the cost of
external accesses. How you solve the cache invalidation problem
depends on whether you have control over the database or not. If you
have, you can arrange for individual cache entries to be invalidated
when the corresponding original database entries are modified. If not,
each access to the cache of the combined proxy has to check whether
an entry found is still valid.
The other part of the combined proxy maintains the name and
address of the original and performs the actual IPC. If the original is,
say, a relational data base, it translates the client request into SQL
queries and translates results into the required format. If it is another
type of component, use the Forwarder-Receiver pattern (307).
Known Uses The Proxy pattern is often used in combination with the Forwarder Receiver pattern (307)to implement the 'stub' concept [LPW94].
NeXTSTEP. The Proxy pattern is used in the NeXTSTEP operating
system to provide local stubs for remote objects. Proxies are created
by a special server on the first access to the remote object. The
responsibilities of a proxy object within the NeXTSTEP operating
system are to encode incoming requests and their arguments, and
forward them to their corresponding remote original.
OMG-CORBA [OMG92]uses the Proxy pattern for two purposes. So called 'client-stubs', or IDL-stubs, guard clients against the concrete
implementation of their servers and the Object Request Broker. IDL skeletons are used by the Object Request Broker itself to forward
requests to concrete remote server components.
Orbix [Iona95], a concrete OMG-CORBA implementation, uses
remote proxies. A client can bind to an original by specifying its
unique identifier. In the example of C++ language support, the
bind ( ) call returns a C++ pointer that the client can use to invoke
the remote object using normal C++ function invocation syntax.
World Wide Web Proxy [LA941describes aspects of the CERN H T P
server that typically runs on a firewall machine. It gives people inside
the firewall concurrent access to the outside world. Efficiency is
increased by caching recently transferred files.
OLE. In Microsoft OLE [Bro94] servers may be implemented as
libraries dynamically linked to the address space of the client, or as
separate processes. Proxies are used to hide whether a particular
server is local or remote from a client. When the client calls a server
located in its own address space, it directly invokes that server's
Implementation. If the server is not located in the client's address
space, a proxy takes the arguments, packages them, and generates a
remote procedure call to the remote server. In the server process
another proxy-referred to as a 'stub' in OLE terminology-receives
the request, unpacks the arguments, pushes them on the stack and
invokes the appropriate server method. If the method invocation
returns a result, this result is packaged and transmitted back to the
client proxy. The client proxy unpacks the result and returns it to the
client, which remains ignorant of whether the server was local or
remote.
Consequences One problem with the Proxy pattern as it is described here is that not
all forces are equally well resolved. The traditional focus is on easy
handling and achieving a certain degree of efficiency, as stated in the
first and second forces. But what happens when the user or program mer needs to retain explicit control for fine-tuning, as requested by
force three? One possibility is to mirror this at the level of the source
code by doing away with the abstract superclass. The programmer is
then always aware of whether the object at hand is 'the real thing' [U2]
or just a surrogate. However, this violates forces one and two.
The Proxy pattern provides the following benefits:
Enhanced efficiency and lower cost.The Virtual Proxy variant helps to
implement a 'load-on-demand' strategy. This allows you to avoid un necessary loads from disk and usually speeds up your application. A
similar argument holds for the Cache Proxy variant. Be aware, however, that the additional overhead of going through a proxy may have
the inverse effect, depending on the application-see liabilities below.
Decoupling clients from the location of server components. By putting
all location information and addressing functionality into a Remote
Proxy variant, clients are not affected by migration of servers or
changes in the networking infrastructure. This allows client code to
become more stable and reusable. Note however that a straight forward implementation of a remote proxy still has the location of the
original hard-wired into its code. The advantage of this is that it usually provides better run-time performance. If this loss of flexibility is
important, you can think about introducing a dynamic lookup
scheme in addition to the proxies, as described in the Client Dispatcher-Server pattern (323).
Separation of housekeeping code from functionality. In more general
terms, this benefit applies to all Proxy variants. A proxy relieves the
client of burdens that do not inherently belong to the task the client
is to perform.
Two liabilities of the Proxy pattern can be identified:
Less efficiency due to indirection.All proxies introduce an additional
layer of indirection. This loss of efficiency is usually negligible compared with the cleaner structure of clients and the gain of efficiency
through caching or lazy construction that is achieved by using
proxies. You should however check such impacts on efficiency thoroughly for every application of the Proxy pattern.
Overkill via sophisticated strategies. Be careful with intricate
strategies for caching or loading on demand-they do not always pay.
An example of this occurs when originals are highly dynamic, for
example in an airline reservation or other ticket booking system. Here
complex caching with invalidating may introduce overhead that
defeats the intended purpose due to the rate at which the original's
data changes. Usually, only coarse-grained entities justify the
resultant cache maintenance effort.
See Also The Decorator pattern [GHJV95]is very similar in structure to Proxy.
Concrete component-the original in the Proxy pattern-implements
some behavior that is invoked via a decorator-the proxy in the Proxy
pattern. Both classes inherit from a common base. The major difference between the Decorator and Proxy patterns is one of intent. The
decorator adds functionality or, more generally, gives options for
dynamically choosing functionality in addition to the core functionality of Concrete component. The proxy frees the original from very
specific housekeeping code.
##%%&&
The Command Processor design pattern separates the request for a
service from its execution. A command processor component
manages requests as separate objects,schedules their execution,and
provides additional services such as the storing of request objects for
later undo.
Example A text editor usually provides a way to deal with mistakes made by
the user. A simple example is undoing the most recent change. A
more attractive solution is to enable the undoing of multiple changes.
We want to develop such an editor. For the purpose of this discussion
let us call it TEDDI.
The design of TEDDI includes a multi-level undo mechanism and
allows for future enhancements, such as the addition of new features
or a batch mode of operation.
The user interface of TEDDI offers several means of interaction, such
as keyboard input or pop-up menus. The program has to define one
or several callback procedures that are automatically called for every
human-computer interaction.
Context Applications that need flexible and extensible user interfaces, or
applications that provide services related to the execution of user
functions, such as scheduling or undo.
Problem An application that includes a large set of features benefits from a
well-structured solution for mapping its interface to its internal
functionality. This allows you to support different modes of user
interaction, such a s pop-up menus for novices, keyboard shortcuts
for more experienced users, or external control of the application via
a scripting language.
You often need to implement services that go beyond the core
functionality of the system for the execution of user requests.
Examples are undo, redo, macros for grouping requests, logging of
activity, or request scheduling and suspension.
The following forces shape the solution:
Different users like to work with an application in different ways.
Enhancements of the application should not break existing code.
Additional services such a s undo should be implemented
consistently for all requests.
Solution The Command Processor pattern builds on the Command design
pattern in [GHJV95].Both patterns follow the idea of encapsulating
requests into objects. Whenever a user calls a specific function of the
application, the request is turned into a command object. The
Command Processor pattern illustrates more specifically how
command objects are managed. The See Also section discusses
further differences between the Command pattern and the Command
Processor pattern.
A central component of our pattern description, the command processor, takes care of all command objects. The command processor
schedules the execution of commands, may store them for later undo,
and may provide other services such as logging the sequence of commands for testing purposes. Each command object delegates the
execution of its task to supplier components within the functional
core of the application.
Structure The abstract command component defines the interface of all
command objects. As a minimum this interface consists of a
procedure to execute a command. The additional services
implemented by the command processor require further interface
procedures for all command objects. The abstract command class of
TEDDI, for example, defines an additional undo method.
For each user function we derive a command component from the
abstract command. A command component implements the interface
of the abstract command by using zero or more supplier components.
The commands of TEDDI save the state of associated supplier
components prior to execution, and restore it in case of undo. For
example, the delete command is responsible for storing the text
deleted and its position in the document.
The controlUser represents the interface of the application. It accepts
requests, such as 'paste text,' and creates the corresponding
command objects. The command objects are then delivered to the
command processor for execution. The controller of TEDDI maintains
the event loop and maps incoming events to command objects.
The command processor manages command objects, schedules them
and starts their execution. It is the key component that implements
additional services related to the execution of commands. The
command processor remains independent of specific commands
because it only uses the abstract command interface. In the case of
our TEDDI word processor, the command processor also stores
already-performed commands for later undo.
The supplier components provide most of the functionality required to
execute concrete commands (that is. those related to the concrete
command class, as opposed to the abstract command class). Related
commands often share supplier components. When an undo
mechanism is required, a supplier usually provides a means to save
and restore its internal state. The component implementing the
internal text representation is the main supplier in TEDDI.
The following diagram shows the principal relationships between the
components of the pattern. It demonstrates undo as an example of an
additional service provided by the command processor.
Dynamics The following diagram shows a typical scenario of the Command
Processor pattern implementing an undo mechanism. A request to
capitalize a selected word arrives, is performed and then undone. The
following steps occur:
The controller accepts the request from the user within its event
loop and creates a 'capitalize' command object.
The controller transfers the new command object to the command
processor for execution and further handling.
The command processor activates the execution of the command
and stores it for later undo.
The capitalize command retrieves the currently-selected text from
its supplier, stores the text and its position in the document, and
asks the supplier to actually capitalize the selection.
After accepting an undo request, the controller transfers this
request to the command processor. The command processor
invokes the undo procedure of the most recent command.
The capitalize command resets the supplier to the previous state,
by replacing the saved text in its original position
If no further activity is required or possible of the command, the
command processor deletes the command object.
Implementation To implement this pattern. carry out the following steps:
1 Define the interface of the abstract command.The abstract command
class hides the details of all specific commands. This class always
specifies the abstract method required to execute a command. It also
defines the methods necessary to implement the additional services
offered by the command processor. An example is a method
'getNameAndParameters' for logging commands.
For the undo mechanism inTEDDI we distinguish three types of
commands. They are modeled as an enumeration, because the
command type may change dynamically, as shown in step 3:
No change.A command that requires no undo. Cursor movement falls
into this category.
Normal. A command that can be undone. Substitution of a word in
text is an example of a normal command.
No undo.A command that cannot be undone, and which prevents the
undo of previously performed normal commands.
If we want our text to become 'politically correct' and replace all
occurrences of 'he' by 'he/she', TEDDI would need to store all
corresponding locations in the document to enable later undo. The
potentially high storage requirement of global replacements is the
main reason why commands belong to the category 'no undo'.
The method getName ( ) is used to display the most recent command
to the user when he selects 'undo'.
2 Design the command components for each type of request that the
application supports. There are several options for binding a
command to its suppliers. The supplier component can be hard coded within the command. or the controller can provide the supplier
to the command constructor as a parameter. An example of the
second situation is a multi-document editor in which a command is
connected to a specific document object.
The'delete' command of TEDDI takes the object representing the
text as its first parameter. The range of characters to delete is
specified by two additional parameters:
The implementation of the method do it ( ) calls the method
deleteText ( ) of the TEDDI-Text supplier object. Q
A command object may ask the user for further parameters. The
TEDDI 'load text file' command, for example, activates a dialog to
request the name of the file to be loaded. In this situation the event handling system must deliver user input to the command, rather
than to the controller. Commands that require user interaction
during their creation or execution therefore call for additional care.
The design of the event-handling system-which is outside the scope
of this pattern-must be able to handle such situations.
Undoable commands can use the Memento pattern (GHJV951to store
the state of their supplier for later undo without violating
encapsulation.
Increase flexibility b y providing macro commands that combine
several successive commands. Apply the Composite pattern
[GHJV95]to implement such a macro command component.
The command type of a MacroCmd depends on the commands that are
added to the macro. An appended command of type no-undo will
prevent the undo of the complete macro command. The undo function
otherwise iterates through cmdlist in reverse order undoing all
normal commands and skipping all commands of type no-change. D
4 Implement the controller component. Command objects are created by
the controller, for example with the help of the 'creational' patterns
Abstract Factory and Prototype [GHJV95]. However, since the
controller is already decoupled from the supplier components, this
additional decoupling of controller and commands is optional. A
generic menu controller provides an example of the application of the
Prototype pattern. Such a controller contains a command prototype
object for each menu entry, and passes a copy of this object to the
command processor whenever the user selects the menu entry. If
such a menu controller can be dynamically configured with macro
command objects, we can easily implement user-defined menu
extensions.
implement access to the additional services of the command processor:
A user-accessible additional service is normally implemented by a
specific command class. The command processor supplies the
functionality for the 'do' method. Directly calling the interface of the
command processor is also an option. Other intrinsic services such
as logging of commands are performed automatically by the
command processor.
Implement the command processor component. The command
processor receives command objects from the controller and takes
responsibility for them. For each command object, the command
processor starts the execution by calling the do method. A command
processor implemented in C++,for example, is responsible for
deleting command objects that are no longer useful.
Apply the Singleton design pattern [GHJV95]to ensure that only one
command processor exists.
Spread controller functionality. In this variant the role of the controller
can be distributed over several components. For example, each user
interface element such as a menu button could create a command
object when activated. However, the role of the controller is not
restricted to components of the graphical user interface.
Combination with Interpreter pattern. In this variant a scripting language provides a programmable interface to an application. The
parser component of the script interpreter takes the role of the controller. Apply the Interpreter pattern (GHJV951and build the abstract
syntax tree from command objects. The command processor is the
client in the Interpreter pattern. It carries out interpretation by activating the commands.
Known Uses El'++ [WGM88]provides a framework of command processors that
support unlimited, bounded, and single undo and redo. The abstract
class Command implements a state machine to track the execution
state of each command. This state machine is used to check if a
command is performed or undone. The controller role is distributed
over the event-handler object hierarchy of an ET++application.
MacApp (App891 uses the Command Processor design pattern to
provide undoable operations.
Interviews [LCITV!32]includes an action class that is an abstract
base class providing the functionality of a command component.
ATM-P (ATM931 implements a simplified version of the Command
Processor pattern. It uses a hierarchy of command classes to pass
command objects around, sometimes across process boundaries. The
receiver of a command object decides how and when to execute it.
Each process implements its own command processor.
SICAT [SICAT951 implements the Command Processor pattern to
provide a well-defined undo facility in the control program and the
graphical SDL editors.
Consequences The Command-Processor pattern provides the following benefits:
Flexibility in the way requests are activated. Different user interface
elements for requesting a function can generate the same kind of
command object. It is thus easy to remap user input to application
functionality. This helps to create a n application interface that can be
adapted to user preferences. An example is a text editor that provides
different control modes such as a WordStar or an emacs keyboard.
FZexibility in the number and functionality of requests. The controller
and command processor are implemented independently of the
functionality of individual commands. Changing the implementation
of a command or introducing new command classes does not affect
the command processor or other unrelated parts of the application.
For example, it is possible to build more complex commands from
existing ones. In addition to a macro mechanism, such compound
commands can be pre-programmed, and thus extend the application
without modifying the functional core.
Programming execution-related services. The central command
processor easily allows the addition of services related to command
execution. An advanced command processor can log or store
commands to a file for later examination or replay. A command
processor can queue commands and schedule them at a later time.
This is useful if commands should execute at a specified time, if they
are handled according to priority, or if they will execute in a separate
thread of control. An additional example is a single command
processor shared by several concurrent applications that provides a
transaction control mechanism with logging and rollback of
commands.
Testability at application level. The command processor is a n ideal
entry point for application testing. If combined with the Interpreter
pattern [GHJV95]a s in the second variant above, regression tests can
be written in the scripting language and applied after changes to the
functional core. Furthermore, logging of command objects executed
by the command processor allows you to analyze error situations. If
the sequence of executed commands is stored persistently, it can be
re-applied after error correction, or reused for regression testing.
Concurrency. The Command Processor design pattern allows commands to be executed in separate threads of control. Responsiveness
improves, because the controller does not wait for the execution of a
command to finish. However, this calls for synchronization when the
global variables of the application, for example in a supplier component, are accessed by several commands executing in parallel.
The Command Processor pattern imposes some liabilities:
Efficiency loss. As with all patterns that decouple components, the
additional indirection costs storage and time. A controller that
performs a service request directly does not impose an efficiency
penalty. However, extending such a direct controller with new
requests, changing the implementation of a service, or implementing
an undo mechanism all require more effort.
Potential for an excessive number of command classes.An application
with rich functionality may lead to many command classes. You can
handle the complexity of this situation in a number of ways:
By grouping commands around abstractions.
By unifying very simple command classes by passing the supplier
object as a parameter.
By pre-programmed macro-command objects that rely on the
combination of few low-level commands.
Complexity in acquiring command parameters. Some command
objects retrieve additional parameters from the user prior to or during
their execution. This situation complicates the event-handling
mechanism, which needs to deliver events to different destinations,
such as the controller and some activated command object.
See also The Command Processor pattern builds on the Command design
pattern in [GHJV95]. Both patterns depict the idea of encapsulating
service requests into command objects. Command Processor contributes more details of the handling of command objects. The controller
of the Command Processor pattern takes the role of the client in the
Command pattern. The controller decides which command to use and
creates a new command object for each user request.
In the Command pattern, however, the client configures an invoker
with a command object that can be executed for several user
requests. The command processor receives command objects from
the controller and takes the role of the invoker, executing command
objects. The controller from the Command Processor pattern takes
the role of the client, The suppliers of the Command Processor
pattern correspond to receivers, but we do not require exactly one
supplier for a command.
##%%&&
The View Handler design pattern helps to manage all views that a
software system provides. A view handler component allows clients to
open, manipulate and dispose of views. It also coordinates
dependencies between views and organizes their update.
Example Multi-document editors allow several documents to be worked on
simultaneously. Each document is displayed in its own window.
To use such editors effectively,users need support for handling windows. For example, they might want to clone a window to work with
several independent views of the same document. Users also often do
not close open windows before quitting the editor. It is the task of the
system to keep track of all open documents and to close them carefully. Changes in one window may affect other windows as well. We
therefore need an efficient update mechanism for propagating
changes between windows.
Context A software system that provides multiple views of application-specific
data. or that supports working with multiple documents.
Problem Software systems supporting multiple views often need additional
functionality for managing them. Users want to be able conveniently
to open, manipulate, and dispose of views, such as windows and their
contents. Views must be coordinated, so that an update to one of
them is propagated automatic to related views.
drive the solution to this problem:
Managing multiple views should be easy from the user's
d also for client components thin the system.
Implementations of individual views should not depend on each
other or be mixed with the code used to m
@ View implementations can vary, types of views may
be added during the lifetime of the system.
separate the management of views from the code required to present
or control specific views.
A view handler component m ages all views that the software
It offers the necessary functionality for opening,
closing specific views, and also for handling views for example, a command to 'tile' all views, that is, arrange them in an
orderly pattern.
specific views, functionality for their presentation and
control, are encapsulate separate view components--one for
each kind of view. Suppliers provide views with the data they must
present.
The View Handler pattern adapts the idea of separate
functional core, as proposed the Model-View-Controller
. It does not provide an overall structure for a software
system by itself-it only removes the responsibility of managing
entirety of views and their mutual dependencies from the model and
view components. The pattern gives this responsibility to the view
handler. For example, a view does not need to manage subviews.
The View Handler pattern, therefore, is of finer granulate than the
Model-View-Controller pattern-it ne the relationship
between the model and its associated views.
You can consider the view handler component as an Abstract Factory
and as a Mediator an abstract factory
clients are independent views are created. It
mediator because clients independent of how views
In the example of a document editor, we provide one view
component for each type of document window. The system provides
windows to edit documents, to preview printed output, and to see
'thumbnails' of document pages. A view handler manages these
views. Besides creation and deletion of windows, the view handler
offers functions to bring a specific window to the foreground, to clone
the foreground window, and to tile all open windows so that they do
not overlap. The suppliers of the windows are the documents to be
displayed. There can be multiple simultaneous views of a document,
and multiple documents can be displayed. 
Structure The view handler is the central component of this pattern. It is responsible for opening new views, and clients can specify the view they
want. The view handler instantiates the corresponding view component, takes care of its correct initialization, and asks the new view to
display itself. If the requested view is open already, the view handler
brings this open view to the foreground. If the requested view is open
but iconized, the view handler tells the view to display itself full size.
The view handler also offers functions for closing views, both
individual ones and all currently-open views, a s is needed when
quitting the application.
The main responsibility of the view handler, however, is to offer view
management services. Examples include functions to quickly bring a
specific view into the foreground, to tile all views, to split individual
views into several parts, to refresh all views, and to clone views to
provide several views of the same document. Such management
functionality would be hard to organize if its implementation were
spread over many different view components.
An additional responsibility of the view handler is coordination. There
may be dependencies between views such as occurs, for example, if
several views display different parts of a compound document, such
as VObjectText objects in ET++ [WGM88]. Such views should be
placed next to each other when tiling them. If a user modifies one view
of the document, it may be necessary to update the others in a
predefined order. For example, views that show the most global
information should be updated first.
An abstract view component defines an interface that is common to
all views. The view handler uses this interface for creating, coordinating, and closing views. The platform underlying the system uses the
interface to execute user events, for example the resizing of a window.
The interface of the abstract view must offer a corresponding function
for all possible operations that can be performed on a view.
Specific view components are derived from the abstract view and
implement its interface. In addition, each view implements its own
display function. This retrieves data from the view's suppliers,
prepares this data for display, and presents them to the user. The
display function is called when opening or updating a view.
Supplier components provide the data that is displayed by view components. Suppliers offer a n interface that allows clients--such a s
views-to retrieve and change data. They notify dependent components about changes to their internal state. Such dependent
components are individual views or, in the case where the view
handler organizes updates, the view handler itself.
Dynamics We select two scenarios to illustrate the behavior of the View Handler
pattern: view creation and tiling. Both scenarios assume that each
view is displayed in its own window.
Scenario I shows how the view handler creates a new view. The
scenario comprises four phases:
A client-which may be the user or another component of the
system-calls the view handler to open a particular view.
The view handler instantiates and initializes the desired view. The
view registers with the change-propagation mechanism of its
supplier, as specified by the Publisher-Subscriber pattern (339).
The view handler adds the new view to its internal list of open
views.
The view handler calls the view to display itself. The view opens a
new window, retrieves data from its supplier, prepares this data for
display, and presents it to the user.
Scenario II illustrates how the view handler organizes the tiling of
views. For simplicity, we assume that only two views are open. The
scenario is divided into three phases:
The user invokes the command to tile all open windows. The
request is sent to the view handler.
For every open view, the view handler calculates a new size and
position, and calls its resize and move procedures.
Each view changes its position and size, sets the corresponding
clipping area, and refreshes the image it displays to the user. We
assume that views cache the image they display. If this is not the
case, views must retrieve data from their associated suppliers
before redisplaying themselves.
The implementation of a View Handler structure can be divided into
four steps. We assume that the suppliers already exist, and include a
suitable change-propagation mechanism.
Identify the views.Specify the types of views to be provided and how
the user controls each individual view.
Specify a common interface for all views.This should include functions to open, close, display, update, and manipulate a view. The
interface may also offer a function to initialize a view. This can be
used, for example, to configure a view with data from a particular
supplier. Encapsulate the interface in an abstract class. For some
functions, for example view update, it is often possible to provide a
default implementation.
For our document editor example we specify the class
Abstract view.The protected interface of the Abstract view class
includes methods to display and delete a window, and to display the
window's contents. The public interface includes methods to open,
close, move, size, drag, and update a view, as well as an initialization
method.
Implement the views.Derive a separate class from the Abstract view
class for each specific type of view identified in step 1.Implement the
view-specific parts of the interface, such as the displayData 0
method in our example. Override those methods whose default
implementation does not meet the requirements of the specific view.
If the view handler implements specific coordination and update
policies, views must notify it about all events that may affect other
views. For example, previously-hidden parts of other views can
become visible when resizing a view. If the view handler coordinates
the update of these views, it must be notified about the resizing. The
Publisher-Subscriber pattern (339)helps with implementing such a
change notification.
In our example we implement three view classes: Edit view,
Layout view, and Thumbnail view, as specified in the solution
section. We do not need to override the default implementations
inherited from the Abstract view class for their implementation. 
4 Define the view handler; Implement functions for creating views as
Factory Methods [GHJV95].Clients can specify the view they want,
but they do not control how it is created. The view handler is
responsible for instantiating and initializing the correct view
component.
The view handler maintains references to all open views internally.
The Iterator pattern [GHJV95] can help you to implement this
functionality. The view handler may also maintain additional
information about views, such as the current position and size of a
window on the screen. The view handler's management functionality,
such as operations for cloning windows, uses this information.
Your view handler may need to implement application-specific view
coordination policies. For example, one view may present information
about another view, for example logging information about a n
animated simulation. Tiling should place these two dependent views
next to each other or, if both views are iconized, opening one view
should open the other as well.
Update strategies are another example of view coordination. It may be
necessary, for example, to give a higher priority to the update of
particular views. For instance, a view that displays alarms may need
to be updated before other open views. In such a case, the suppliers
notify the view handler about changes, rather than dependent views.
The view handler forwards these requests to affected views using its
update strategy. View handlers that coordinate the update of views
usually offer an update function in their public interface.
To allow coordination strategies to be exchangeable they can be
implemented with the Strategy pattern [GHJV95]. The Mediator
design pattern (GHJV95)helps with implementing view coordination,
for example by broadcasting a refresh request to all open views. Use
the Singleton pattern [GHJV!351to ensure that the view handler class
can only be instantiated once.
The view handler in our example document editor provides
functions to open and close views, as well as to tile them, bring them
to the foreground, and clone them. Internally the view handler
maintains references to all open views, including information about
their position and size, and whether they are iconized.
View Handler with Command objects. This variant uses command
objects [GHJV95] to keep the view handler independent of specific
view interfaces. Instead of calling view functionality directly, the view
handler creates an appropriate command and executes it. The
command itself knows how to operate on the view. For example, we
can specify a tile command that, when executed, first calls the size
and then the move function of a view. Another option is to create
commands and pass them to a command processor (277)which takes
care of their correct execution, but also allows for additional
functionality such as undoing an executed command.
Known Uses Macintosh Window Manager [App85]. The Window Manager is the
part of the Macintosh toolbox that can be compared to a view handler
component. Its interface offers functions for window allocation,
window display, mouse location, window movement and sizing, and
update region maintenance. It also provides a data structure that
underlies every Macintosh window. Parts of the interface for Pascal
are as follows:
The Macintosh Window Manager does not offer functions that operate
on several or all windows, it only provides support for handling
individual windows. The Macintosh Window Manager can therefore
be viewed as a low-level view handler component.
[Mic93b]. The Microsoft Word word-processing
system offers functions for cloning, splitting, and tiling windows, and
also for bringing an open window into the foreground. Quitting Word
closes all open windows; dialogs are displayed requesting the desired
action if a window contains data that has been changed but not
saved. This provides an example of how a View Handler system can
appear to the user, and the functionality it can provide.
The View Handler pattern provides the following
UniJorrn handling of views. All views share a common interface. The
view handler and all other components of the system can therefore
handle and manipulate all views uniformly, independent of what they
display and how they are implemented.
Extensibility and changeable of views. The organization of view
components in an inheritance hierarchy with an abstract base
supports the integration of new views without changes to existing
views and the view handler. Since individual views are encapsulated
within separate components, changes to their implementation do not
affect other components of the system.
Application-specify view coordination. Since views are managed by a
central instance, it is possible to implement specific view coordination
strategies.
The View Handler pattern also suffers from the following 
Restricted applicability. Using the View Handler pattern is only
worthwhile if the system must support many different views, views
with logical dependencies between each other, or views which can be
configured with different suppliers or output devices. It is also useful
if the system must implement specific view coordination strategies. If
none of these apply, the View Handler pattern just introduces
additional implementation effort and increases the internal
complexity of the system.
Efficiency. The view handler component introduces a level of
indirection between clients that want to create views, and also within
the chain of propagation of change notifications, if the view handler
is responsible to organizing view updating. This results in a loss of
performance. In most cases these losses are negligible, however.
See also The Model-View-Controller architectural pattern (125) provides an
infrastructure for separating functionality from both input and
output behavior. From the perspective of MVC the View Handler
pattern is a refinement of the relationship between the model and its
associated views.
The Presentation-Abstraction-Control architectural pattern (145)
implements the coordination of multiple views according to the
principles of the View Handler pattern. An intermediate level PAC
agent that creates and coordinates views corresponds to the view
handler. Bottom-level view PAC agents that present data to the user
represent the view components.
##%%&&
The Forwarder Receiver design pattern provides transparent inter process communication for software systems with a peer-to-peer
interaction model. It introduces forwarders and receivers to decouple
peers from the underlying communication mechanisms.
Example The company DwarfWare offers applications for the management of
computer networks. In a new project a development team has defined
an infrastructure for network management. Among other components, the system consists of agent processes written in Java that run
on each available network node. These agents are responsible for
observing and monitoring events and resources. In addition, they
allow network administrators to change and control the behavior of
the network. for example by modifying routing tables. To enable the
exchange of information, as well as fast propagation of administration
commands, each agent is connected to remote agents in a peer-to peer fashion, acting as client or server as required. As the infrastructure needs to support a wide variety of different hardware and
software systems. the communication between.peers must not depend on a particular mechanism for inter-process communication.
308 Design Patterns
Context Peer-to-peer communication.
Problem A common way to build distributed applications is to make use of
available low-level mechanisms for inter-process communication
(IPC)such as TCP/IP, sockets or message queues. These are provided
by almost all operating systems, and are very efficient when compared to higher-level mechanisms such as remote procedure calls.
These low-level mechanisms, however, often introduce dependencies
on the underlying operating system and network protocols. By using
a specific IPC mechanism, the resulting solution restricts portability,
constrains the system's capability to support heterogeneous environments, and makes it hard to change the IPC mechanism later.
The Forwarder-Receiver pattern is useful when you need to balance
the following forces:
The system should allow the exchangeability of the communication
mechanisms.
The cooperation of components follows a peer-to-peer model, in
which a sender only needs to know the names of its receivers.
The communication between peers should not have a major impact
on performance.
Solution Distributed peers collaborate to solve a particular problem. A peer
may act as a client, requesting services, as a server, providing services, or both. The details of the underlying IPC mechanism for sending
or receiving messages are hidden from the peers by encapsulating all
system-specific functionality into separate components. Examples of
such functionality are the mapping of names to physical locations.
the establishment of communication channels, or the marshaling and
unmarshaling of messages.
Structure The Forwarder-Receiver design pattern consists of three kinds of
components.forwarders, receivers,and peers:
Peer components are responsible for application tasks. To carry out
their tasks peers need to communicate with other peers. These may
be located in a different process, or even on a different machine. Each
peer knows the names of the remote peers with which it needs to
communicate. It uses a forwarder to send messages to other peers
and a receiver to receive messages from other peers. Such messages
are either requests that a peer sends to remote peers, or responses
that a peer transmits to the originators of requests.
r The peers in our Dwarfware example are the agents running on
the network nodes. They continuously monitor network events and
resources, and listen for incoming messages from remote agents.
Each agent may connect to any other agent to exchange information
and requests. The network management infrastructure connects the
network administrator's console with all other agents. The administrator's task is to control network activities and events. For this purpose, administrators may send requests to network agents or retrieve
messages from them by using available network
tools.,
Forwarder components send messages across process boundaries. A
forwarder provides a general interface that is an abstraction of a
particular IPC mechanism, and includes functionality for marshaling
and delivery of messages. It also contains a mapping from names to
physical addresses. When a forwarder sends a message to a remote
peer, it determines the physical location of the recipient by using its
name-to-address mapping. In the transmitted message the forwarder
specifies the name of its own peer, so that the remote peer is able to
send a response to the message originator.
In our example different kinds of messages exist:
Command messages instruct the recipient to perform some
activities such as changing the routing tables of its host machine.
Information messages contain data on network resources and
network events.
Response messages allow agents to acknowledge the arrival of a
message.
Forwarder components are responsible for forwarding all these messages to remote network agents without introducing any dependencies on the underlying IPC mechanisms. 0
Receiver components are responsible for receiving messages. A receiver offers a general interface that is an abstraction of a particular IPC
mechanism. It includes functionality for receiving and unmarshaling
messages.
The receivers in our example wait for incoming messages on
behalf of their agent process. As soon as a message arrives, they
convert the received data stream into a general message format and
forward the message to their agent process.
The static relationships in the Forwarder-Receiver design pattern are
shown in the diagram below.
To send a message to a remote peer, the peer invokes the method
sendMsg of its forwarder, passing the message as a n argument. The
method sendMsg must convert messages to a format that the
underlying IPC mechanism understands. For this purpose, it calls
marshal. sendMsg uses deliver to transmit the IPC message data to
a remote receiver.
When the peer wants to receive a message from a remote peer, it
invokes the receiveMsg method of its receiver, and the message is
returned. receiveMsg invokes receive,which uses the functionality
of the underlying IPC mechanism to receive IPC messages. After
message reception receiveMsg calls unmarshal to convert IPC
messages to a format that the peer understands.
Dynamics The following scenario illustrates a typical example of the use of a
Forwarder-Receiver structure. Two peers pl and p2 communicate
with each other. For this purpose, PI uses a forwarder ~orwland a
receiver Recvl. ~2 handles all message transfers with a forwarder
Forw2 and a receiver Recv2:
PI requests a service from a remote peer ~ 2 . For this purpose, it
sends the request to its forwarder ~orwland specifies the name of
the recipient.
~orwl determines the physical location of the remote peer and
marshals the message.
Forwl delivers the message to the remote receiver Recv2.
At some earlier time P2 has requested its receiver Recv2 to wait for
an incoming request. Now, Recv2 receives the message arriving
from Forwl.
Recv2 unmarshals the message and forwards it to its peer ~ 2 .
To implement a Forwarder-Receiver design pattern, iterate through
the following steps:
1 Specify a name-to-address mapping.Since peers reference other peers
by name, you need to introduce an appropriate name space. A name
space defines rules and constraints to which names must conform in
a given context. For example you could specify that all names consist
of exactly fifteen characters and have to start with a capital letter,
such as 'Peer video server'. You may alternatively structure names as
path names in an UNIX-like fashion, such as '/Server/VideoServer/
AVIServer'.
A name does not necessarily refer to a single address--it may refer to
a group of addresses. When a peer sends a message with a destination name that represents a group of remote peers, the message is
sent to each member of the group. You may even introduce hierarchical structures that allow a group to be a member of another group.
2 Specify the message protocols to be used between peers and
forwarders. This protocol defines the detailed structure of message
data a forwarder receives from its peer. Perform the same task for the
message protocol to be used between receivers and their peers.
Our Dwarfware example is over-simplified in that it does not
cover the handling of errors, or further details of communication such
as the partitioning of data into multiple packets. Peers use objects of
the class Message when they invoke their forwarder. A receiver
returns a Message object to its peer when it receives a message. In
the example messages only contain the sender and the message data,
both represented as Unicode strings. Messages do not contain the
name of the recipient, because the sender passes this name as an
extra argument to its forwarder. This allows us to send the same
message to more than one recipient.
We also need a protocol for communication between the forwarders
and the receivers of remote peers. A message sent from a forwarder to
a remote receiver also includes the name of the sender.
Each message is transmitted as a sequence of bytes, in which the first
four bytes specify the total length of the message. The succeeding
bytes contain the sender of the message as well as the message data
itself. 0
You often need to enable a system to cope with time-outs. For
example, peers could specify time-out values to forwarders and
receivers in order to prevent the whole system from becoming blocked
should a receiver fail to respond to a message. Alternatively, time outs could be specified by the user at run-time, or forwarders and
receivers could free the user from having to specify such values by
implementing internal time-outs.
You also need to consider what forwarders and receivers are expected
to do in the case of communication failures. They may try to send or
receive messages more than once, or they may immediately report an
exception when the first communication attempt fails. All these
aspects depend on the underlying IPC mechanism as well as the
requirements of your application domain.
3 Choose a communication mechanism. This decision is driven mainly
by the communication mechanisms available in the operating system
you use. When specifying an IPC facility you need to consider the
following aspects:
If efficiency is important, a low-level mechanism such as TCP/IP
[Stego] may be the first choice. Such mechanisms are very
efficient, and flexible in the communication protocols that may be
built using them (see step 21.
Low-level mechanisms such as TCP/IP require substantial
programming effort, and are dependent on the platform you use,
restricting portability. If your system must be portable between
platforms, it is better to use IPC mechanisms such as sockets
instead. Sockets are available on most operating systems and are
efficient enough for almost all applications.
For our DwarfWare application we decide to use sockets as the
underlying communication mechanism. 
4 Implement the forwarder., Encapsulate all functionality for sending
messages across process boundaries in the forwarder. The forwarder
provides its functionality through a public interface and encapsulates
the details of a particular IPC mechanism.
Define a repository that maps names to physical addresses. The
forwarder accesses this repository to retrieve the physical addresses
of recipients before establishing a communication link to the remote
peer. This repository may either be statically predefined or may be
changeable at run-time. In the latter case the system is able to add,
move or delete peers dynamically. Decide whether each forwarder
should have its own private repository, or whether all forwarders
should use a common repository that is local to their process. The
first approach allows you to map the same name to different physical
locations. For example, one peer could associate the name 'Printer'
with a different physical location to that used by another peer. The
structure of physical addresses is determined by the IPC mechanism
you use. For example, if you implement communication using
sockets, the physical address consists of the Internet address of the
receiver, as well as its socket port. You could implement the
repository using a hash table, for example.
In our example forwarders make use of a repository class
Registry for mapping names to addresses. The repository uses a
hash table to manage all the address mappings. The implementation
of the hash table is taken from the standard Java class libraries. A
physical address of a remote peer denotes the combination of a target
machine name and a socket port number. The class Entry thus has
two data members: destinationID for specifying the target machine,
and PortNr for specifying the socket port number of the remote peer.
The repository implementation maps strings to instances of the class
Entry:
We now introduce the Forwarder class. The constructor of the class
Forwarder expects a string argument theName that specifies the
logical name of the peer. When a peer calls the sendMsg method. the
following happens:
The method sendMsg invokes marshal to convert the message
theMsg to a sequence of bytes.
deliver is called. Thls method looks up the physical location that
is associated with the remotc peer theDest in a local repository.
For this purpose. the global class fr contalns a data member f r.reg
that is an instance of Repository. deliver opens a socket port.
connects with the remote peer. transmits the message, and closes all
sockets.
It is useful to separate the forwarder's responsibilities from each
other, such as marshaling, message delivery and the repository. All
this functionality can be decomposed to the concrete IPC mechanism.
Use the Whole-Part design pattern (225) to encapsulate each
responsibility in a separate part component of the forwarder.
5 Implement the receiver.Encapsulate all functionality for receiving IPC
messages in the receiver. Provide the receiver with a general interface
that abstracts from details of a particular IPC mechanism. The
receiver needs to include functionality for receiving and
unmarshaling IPC messages. With the Whole-Part design pattern
(225)each of these responsibilities may be encapsulated in a separate
part component of the receiver (see step 4).
Two other aspects need special consideration when you design the
receivers. Since all peers run asynchronously, you need to decide
whether the receivers should block until a message arrives:
If so, the receiver waits for an incoming message. I t only returns
control back to its peer when it receives a message. In other words,
the peer cannot continue until message reception is successful.
This behavior is appropriate if the peer depends on the incoming
message to continue its work.
In all other cases, you should implement non-blocking receivers
that allow peers to specify time-out values (see also step 2). If no
message has arrived in the specified time period, the receiver
returns an exception to its peer.
If the underlying IPC mechanism does not support non-blocking I/O,
you could use a separate thread within the peer to handle
communication.
The use of more than one communication channel within receivers is
another important design issue. Such receivers are capable of de multiplexing communication channels-they wait until a message
arrives on one of the channels and return the message to their peer.
If it is possible for more than one message to arrive at the same time,
the receivers may provide an internal message queue for buffering
messages. Whether demultiplexing is possible depends on the underlying IPC mechanism. For example, the UNIX system call select
allows a process to wait for events on a set of file and socket descriptors. If the IPC mechanism does not support demultiplexing, you can
provide multiple threads within the receiver, where each thread is
responsible for a particular communicaUon channel. For more details
about demuluplexing events, see the Reactor pattern [Sch94].
In our example the class Receiver provides the receiver's
components. If a peer instantiates a receiver, it calls the constructor
and passes its own name as argument. The receiver uses this name
to determine which socket port to use for message reception. When a
peer wants to retrieve a message, it calls the receiveMsg method of
its Receiver object, which in turn invokes receive.The method
receive does two things:
After retrieving the socket port number from the global repository,
it opens a server socket and waits for connection attempts from
remote peers.
As soon as a connection is established with a second socket, the
incoming message and its size are read fi-om the communication
channel. receiveretums the data to receiveMsg.
Finally, receiveMsg invokes unmarshal to convert the sequence of
bytes into a Message object and returns this object to the peer.
Implement the peers of your application. Partition the peers into two
sets, clients and servers. The intersection of these sets does not need
to be empty. If a peer acts as a client. it sends a message to a remote
peer and waits for the response. After receiving the response. it
continues with its task. Peers acting as servers continuously wait for
incoming messages. When such a message arrives, they execute a
service that depends on the message they received, and send a
response back to the originator of the request. Note that servers may
also be clients of other servers. It is even possible for servers and
clients to change their roles dynamically.
The communication between two peers may not always be two-way.
Sometimes it is sufficient for a peer to send a message to another peer
without requiring a response-way communication. Here the
peer sends a message and continues with its work. The recipient of
the message retrieves the message from its receiver, but does not send
a response to the message originator. You can use one-way
communication to enable asynchronous communication between
senders and recipients.
Implement a start-up configuration. When your system starts up,
forwarders and receivers must be initialized with a valid name-to address mapping. Introduce a separate start-up routine that creates
a repository and enters all name/address pairs. Such a configuration
routine could read these pairs from an external file, removing the
need to touch the source code when changing the mapping.
If your software system allows different peers to have different name to-address mappings. the start-up configuration must be capable of
initializing the repositories according to this requirement (see step 4).
If you need the configuration to be able to change dynamically,
implement additional functionality for modifying the repositories at
run-time.
In the DwarfWare example we introduce the following
configuration class, allowing us to register a server and a client with
the central repository:
In our infrastructure for network management a common protocol
resolved determines the format of requests, information messages and
responses. If an agent wants to retrieve information from a remote
agent, such as current resource contention, for example, it sends a
message to the recipient. The recipient retrieves the message from its
receiver, packages the requested information into a response and
sends the response back to the message originator. When an agent
transmits a command message, the recipient receives the message,
interprets it and performs the appropriate command. It then tells the
sender whether or not it could successfully perform the command.All
relevant information is displayed on the console of the network
administrator using a graphical interface. To increase availability,
every machine in the network is able to host the network
administration console.
Variants Forwarder-Receiver without name-to-address mapping. Sometimes
performance issues are more important than being able to
encapsulate all details of the underlying IPC mechanism. To achieve
this, you can remove the mapping from names to physical locations
within forwarders and receivers, for example. In such a configuration,
peers need to tell their forwarder the physical location of the recipient.
This variant, however, might significantly decrease the ability to
change the IPC mechanism.
KnownUses The software development toolkit TASC ITASC911 supports the
implementation of Forwarder-Receiver structures within distributed
applications for factory automation systems.
The material flow control software for flexible manufacturing that was
developed as part of the REBOOT project [Kar95] uses Fonvarder Receiver structures to facilitate an efficient IPC.
The ATM-P switching system [ATM93]uses the Forwarder-Receiver
design pattern to implement the IPC between statically-distributed
components, for example between process-management and communication agents.
The Forwarder-Receiver design pattern is used to implement inter process communication within the distributed Smalltalk
environment BrouHaIia Istee911.
Consequences The Forwarder-Receiver design pattern offers two benefits:
Efficient inter-process communication. The pattern enables you to
provide very efficient inter-process communication. I t structures
communication between its components in a peer-to-peer fashion, in
which every forwarder of an IPC message knows the physical
locations of its potential receivers. A forwarder does not therefore
need to locate remote components. However, the separation of IPC
functionality from peers introduces an additional level of indirection.
Compared to the time consumption of the actual IPC, however, this
overhead should be negligible in most cases.
Encapsulation of IPC facilities. All dependencies on concrete IPC
facilities are encapsulated within the forwarders and receivers. A
change of the underlying IPC mechanism does not affect other
components of the application, specifically the peers that
communicate with each other through forwarders and receivers.
However, the Forwarder-Receiver design pattern has one significant
liability:
No support for flexible re-configuration of components. Forwarder Receiver systems are hard to adapt if the distribution of peers may
change at run-time. Such a change potentially affects all peers
collaborating with the 'migrated' peer. This problem can be solved by
adding a central dispatcher component to the Forwarder-Receiver
structure, as is described in the Client-Dispatcher-Server design
pattern (323).
See also The Client-Dispatcher-Seroer design pattern (323) provides
transparent inter-process communication for software systems in
which the distribution of components is not known at compile-time.
or may vary dynamically at run-time. You can apply this pattern in
combination with the Forwarder-Receiver design pattern as described
below.
The Client-Dispatcher-Server design pattern may be instantiated in
such a way that the forwarder acts as the client and the receiver acts
as the server. When a peer asks its forwarder to send a message, the
forwarder causes the dispatcher to map the recipient's name to its
physical location and to establish a communication channel with the
remote receiver. Such an arrangement allows peers to migrate to
other locations at run-time by unregistering and then re-registering
with the dispatcher.
##%%&&
The Client-Dispatcher-Server design pattern introduces an
intermediate layer between clients and servers, the dispatcher
component. It provides location transparency by means of a name
service, and hides the details of the establishment of the
communication connection between clients and servers.
Example Imagine we are developing a software system ACHILLES for the
retrieval of new scientific information.The information providers are
both on our local network and distributed over the world. To access
an individual information provider, it is necessary to specify its
location and the service to be executed.When an information provider
receives a request from a client application, it runs the appropriate
service and returns the requested information to the client.
Context A software system integrating a set of distributed servers, with the
servers running locally or distributed over a network.
Problem When a software system uses servers distributed over a network it
must provide a means for communication between them. In many
cases a connection between components may have to be established
before the communication can take place, depending on the available
communication facilities. However, the core functionality of the
components should be separate from the details of communication
mechanisms. Clients should not need to know where servers are
located. This allows you to change the location of servers dynamically,
and provides resilience to network or server failures.
We have to balance the following forces:
A component should be able to use a service independent of the
location of the service provider.
The code implementing the functional core of a service consumer
should be separate from the code used to establish a connection
with service providers.
Solution Provide a dispatcher component to act a s an intermediate layer
between clients and servers. The dispatcher implements a name
service that allows clients to refer to servers by names instead of
physical locations, thus providing location transparency. In addition,
the dispatcher is responsible for establishing the communication
channel between a client and a server.
Add servers to the application that provides services to other
components. Each server is uniquely identified by its name, and is
connected to clients by the dispatcher.
Clients rely on the dispatcher to locate a particular server and to
establish a communication link with the server. In contrast to
traditional Client-Server computing, the roles of clients and servers
can change dynamically.
Structure The task of a client is to perform domain-specific tasks. The client
accesses operations offered by servers in order to carry out its
processing tasks. Before sending a request to a server, the client asks
the dispatcher for a communication channel. The client uses this
channel to communicate with the server.
A server provides a set of operations to clients. It either registers itself
or is registered with the dispatcher by its name and address. A server
component may be located on the same computer as a client, or may
be reachable via a network.
The dispatcher offers functionality for establishing communication
channels between clients and servers. To do this, it takes the name of
a server component and maps this name to the physical location of
the server component. The dispatcher establishes a communication
link to the server using the available communication mechanism and
returns a communication handle to the client. If the dispatcher
cannot initiate a communication link with the requested server, it
informs the client about the error it encountered.
To provide its name service,the dispatcher implements functions for
registering and locating servers.
The static relationships between clients, servers and the dispatcher
are as follows:
Dynamics A typical scenario for the Client-Dispatcher-Server design pattern
includes the following phases:
A server registers itself with the dispatcher component.
At a later time, a client asks the dispatcher for a communication
channel to a specified server.
The dispatcher looks up the server that is associated with the name
specified by the client in its registry.
The dispatcher establishes a communication link to the server. If it
is able to initiate the connection successfully, it returns the
communication channel to the client. If not, it sends the client an
error message.
The client uses the communication channel to send a request
directly to the server.
After recognizing the incoming request, the server executes the
appropriate service.
When the service execution is completed, the server sends the
results back to the client.
Implementation To implement a Client-Dispatcher-Server structure, apply the
following steps. You do not necessarily need to follow the steps in the
order given, because some of them are interrelated.
1 Separate the application into servers and clients. Define which
components should be implemented as servers, and identify the
clients that will access these servers. This partitioning may be
predefined, because the application under construction may have to
integrate existing servers. In such cases the separation into clients
and servers may already be determined to some extent. Since clients
may also act as servers, and vice-versa-their roles are not predefined
and may change at run-time.
2 Decide which communication facilities are required. Select communication facilities for the interaction between clients and the dispatcher,
between servers and the dispatcher and between clients and servers.
You can use a different communication mechanism for each connection, or you can use the same mechanism for all three. Using a single
communication facility decreases the complexity of the implementation. Sometimes, however, this approach is not possible or feasible.
This may be because of performance issues. For example, if the dispatcher and the clients accessing it are on the same machine, shared
memory is the fastest method of inter-process communication. In this
example, clients may communicate with the dispatcher using shared
memory, but the servers and the dispatcher, as well as clients and
servers, could communicate using sockets. The servers may be distributed across different machines, making sockets a good choice for
the communication between clients and servers.
Where existing servers have to be integrated into the application, the
choice of an appropriate communication facility may be driven by the
mechanisms already used by these servers.
If all components are located within the same address space, the
interaction between components can rely on conventional procedure
call interfaces.
3 Specify the interaction protocols between components. Consider the
following diagram:
A protocol specifies an ordered sequence of activities for initializing
and maintaining a communication channel between two components,
as well as the structure of messages or data being transmitted. The
Client-Dispatcher-Server pattern implies three different kinds of
protocol.
We need an interaction protocol DSprotocoZ between a server and the
dispatcher. This addresses two topics: it specifies how servers register
with the dispatcher, and it determines the activities that are
necessary to establish the communication channel to the server.
Between the client and the dispatcher CDprotocoZ defines the inter action that occurs when a client asks the dispatcher to establish a
connection with a particular server. If communication establishment
falls due to network or server problems, the dispatcher informs the
client about the failure. The dispatcher may try to establish a
communication link several times before it reports an error.
CSprotocoZ specifies how clients and servers talk to each other. This
interaction could comprise the following steps:
@ The client sends a message to the server using the communication
channel previously established between them. To make this work,
clients and servers need to share common knowledge about the
syntax and semantics of messages they send and receive.
@ The server receives the message, interprets it and invokes one of its
services. After the service is completed, the server sends a return
message to the client.
@ The client extracts the service result from the message and
continues with its task.
4 Decide b w to name servers. The four-byte Internet IP address
scheme is not applicable, because it does not provide location
transparency. If IP addresses were used, a client would depend on the
concrete location of the server. You need to introduce names that
uniquely identify servers but do not carry any location information.
For example, use strings such as 'ServerX' or predefined constants
such as ID-SERVER-X. These location-independent names are
mapped to physical locations by the dispatcher (see step 5).
5 Design and implement the dispatcher, Determine how the protocols
you introduced in step 3 should be implemented using available
communication facilities. If, for example, the dispatcher is located
within the address space of the client, local procedure calls should be
used for CDprotocol. For all other cases and protocols, you need to use
facilities such as TCP ports or shared memory.
With some communication mechanisms the available communication
channels may be a limited resource. For example, the number of
socket descriptors is constrained by the size of descriptor tables in
the operating system. There are several ways round this. For
example, each server may allocate its own socket, limiting the
number of possible servers. en a client request arrives, the
dispatcher returns the server's socket descriptor to the client.
Alternatively, the dispatcher could temporarily store client requests
in an internal message queue. It would then provide a socket port
where servers can ask whether new requests have arrived. When a
service request arrives, the server opens a new socket and passes the
new socket descriptor to the dispatcher. The dispatcher then forwards
the information to the client. After the interaction between client and
server is completed, the server closes its socket descriptor.
Define the detailed structure of requests, responses, and error
messages based on your chosen communication mechanisms and the
identification scheme you use for servers.
A dispatcher includes a repository for mapping server names to their
physical locations. The representation of server locations depends on
the underlying mechanism you use for Client-Server communication.
For example, physical locations may be described in terms of socket
ports, TCP ports, shared memory handles or some other suitable
scheme.
You need to consider performance issues. en many clients access
many servers using one dispatcher, the dispatcher obviously
constitutes a bottleneck. Use multi-threading if possible to improve
response and execution ti .For example you can provide a pool of
threads in the dispatcher. en a request arrives, one of the threads
is then associated with the request, allowing you to handle many
requests in parallel.
Implement the client and server components according to your desired
solution and the decisions you make about the dispatcher interface.
Configure the system and either register the servers with the
dispatcher or let the servers dynamically register and unregister
themselves. Follow the same strategies for optimizing performance
that are described in step 5.
In our scientific information example ACHILLES, a TCP port number
and the Internet address of the host machine are combined to
uniquely identify servers. Clients connect to the dispatcher and ask
for server locations by using identifiers such as 'NASA/
HUBBLE-TELESCOPE'. The system predefines the structure of all
messages: a message header with a fixed size is followed by a random
amount of raw data. All the information necessary to interpret the
raw data, such as its size or format, is provided in the message
header. Each header also contains the sender and the receiver of the
message. Messages are tagged with sequence numbers to enable the
receiver of a message to recombine the incoming packets into their
correct order. When a server receives a request, it extracts
information from the message such as the service to invoke. For
example, a client may include the following information in its
message: 'HUBBLE-DOC-RECEIVE, ANDR0MEDA.jpg'. The server
determines whether the requested file is available and sends a
message containing the picture to the client.
Distributed Dispatchers. Instead of using a single dispatcher
component in a network environment, distributed dispatchers may
be introduced. In this variant, when a dispatcher receives a client
request for a server on a remote machine, it establishes a connection
with the dispatcher on the target node. The remote dispatcher
initiates a connection with the requested server and sends the
communication channel back to the first dispatcher. The channel is
then returned to the client. Another possibility is to allow clients to
communicate directly with the dispatcher on the remote machine.
This constrains location transparency, however, since clients must
know the network node of each server they want to access. Before
using the Distributed Dispatchers variant, consider the use of the
Broker architectural at tern 1991.
Client-DispatcherSeruer with communication managed by clients. In
this variant, instead of establishing a communication channel to
servers, a dispatcher may only return the physical server location to
the client. It is then the responsibility of the client to manage all communication activities with the server. You can use this variant to increase overall performance, or because the available communication
facilities do not require you to establish an explicit communication
link.
Client-Dispatcher-Server with heterogeneous communication. It is not
always possible to implement the communication between clients and
servers using only one communication mechanism. Some servers
may use sockets, while others use named pipes. This leads to a variant of the Client-Dispatcher-Server pattern in which the dispatcher
is capable of supporting more than one communication mechanism.
In this variant, each server register itself with the dispatcher and
specifies the communication mechanism it supports. When a client
requests a communication channel to a particular server, the dispatcher establishes the communication using to the communication
facility the server specified.
Client-Dispatcher Service,In this variant, clients address services and
not servers. When the dispatcher receives a request, it looks up which
servers provide the specified service in its repository, and establishes
a connection to one of these service providers. If it falls to establish
the connection, it may try to access another server providing the same
service instead, if one is available.
The following sample Java code demonstrates the Client Dispatcher-Service variant. All clients, servers and the dispatcher
exist in the same address space.
The class Dispatcher uses a hash table of vectors as a name service
repository. An entry in the hash table is available for each service
name. Each entry consists of the vector of all servers providing the
same kind of service. A server registers with the dispatcher by
specifying a service name and the new server instance. When a client
asks the dispatcher for a specific service, the dispatcher looks up all
available servers in its repository. It randomly selects one of them and
returns the server reference to the client.
Concrete server classes are derived from the abstract class Service.
They therefore have to implement the abstract method service.
Instances of these concrete classes must call the base class
constructor in their own constructors so that they are automatically
registered.
When the user starts the application, the static method main of the
class cDs is invoked. Two services sl and s 2 register with the
dispatcher disp under the same name. The client is then created and
started by calling client .doTask0. The client asks the dispatcher
to locate the service 'PrlntSvc' twice, and once to locate the service
'DrawSvc'. The dispatcher returns the service objects registered with
a particular name by using a random number generator. The first
service invocations of the client therefore refer to different service
objects in the sample output. Since the service 'DrawSvc' is not
available, an error occurs when the client asks the dispatcher to
locate an appropriate server. 0
Known Uses Sun's implementation of Remote Procedure Calls (RPC) ISun9Oj is
based upon the principles of the Client-Dispatcher-Server design
pattern. It implements a combination of the variants Distributed
Dispatchers and Client-Dispatcher-Server with communication
managed by client. The port mapper process takes the role of the
dispatcher. A process initiaUng an RPC then becomes the client and
the receiving process the server. When a client process invokes a
remote procedure, it connects to the port mapper process on the
target machine. This is possible because all port mappers use the
same TCP/UDP port for receiving requests. The port mapper returns
the TCP/UDP port of the requested service to the client, which then
establishes a direct communication channel with the remote server.
The OMQ Corba (Common Object Request Broker Architecture)
specification IOMG92j uses the principles of the Client-Dispatcher Server design pattern for relining and instantiating the Broker
architectural pattern (99).
Consequences The Client-Dispatcher-Server design pattern has several benefits:
Exchangeability of servers. In the Client-Dispatcher-Server design
pattern a software developer can change servers or add new ones
without modifications to the dispatcher component or the clients
becoming necessary. If a new implementation of a server is available,
the server first unregisters itself. It then registers itself again with the
new implementation.
Location and migration transparency. Clients do not need to know
where servers are located-they do not depend on any location
information. As a consequence, servers may be dynamically migrated
to other machines. This does not work, of course, in the event of the
server being migrated while it is connected to a client.
Re-configuration The developer can defer decisions about which
network nodes servers should run until the start-up time of the
system, or even to run-time. The Client-Dispatcher-Server design
pattern therefore allows you to prepare a software system for later
conversion to a distributed system.
Fault tolerance. When network or server failures occur, new servers
can be activated at a different network node without any impact to
clients. This makes the system more robust and fault-tolerant.
The Client-Dispatcher-Server design pattern imposes some
liabilities:
Lower efficiency through indirection and explicit connection establishment. The performance of a Client-Dispatcher-Server pattern
depends on the overhead introduced by the dispatcher, due to its
activities in locating and registering servers and explicitly establishing the connection. The alternative to this approach is to get rid of the
dispatcher by hard-coding server locations into the clients. This leads
to several disadvantages, however. For example, the clients would
then depend directly on the server locations, thus loosing the exchangeability of servers.
Sensitivity to change in the interfaces of the dispatcher component.
Because the dispatcher plays the central role, the software system is
sensitive to changes in the interface of the dispatcher.
See also The Forwarder-Receiver design pattern (307)can be combined with
the Client-Dispatcher-Server pattern to hide the details of inter-process communication. While the Client-Dispatcher-Server pattern
allows you to decouple clients and servers by supporting location
transparency, it does not encapsulate the details of the underlying
communication facilities. To achieve this, you could introduce
forwarders and receivers between clients and servers, clients and the
dispatcher, and between servers and the dispatcher.
The Acceptor and Connector patterns [Sch96bl demonstrate a
different way to decouple connection set-up from connection
processing. Schmidt's patterns are more decentralized than our
approach, which uses a centralized dispatcher. Every site that
passively accepts connections in Schmidt's patterns can provide a
family of Acceptor factories. These acceptors are responsible for
constructing service handlers, which are entry points to the
application-defined services.
Various Acceptors can be defined, to distinguish between different
connection policies such as synchronous versus asynchronous, and
to use different service policies, such as running concurrently in
separate processes or threads or being demultiplexed reactively in a
single process. The Connector pattern is the 'dual' of the Acceptor
pattern-it is used by sites that actively initiate connection setup.
Our Client-Dispatcher-Server pattern resembles a mini-Broker (99)
that is equipped with a name service that also enables dynamic
relocation of servers.
##%%&&
The Publisher-Subscriber design pattern helps to keep the state of
cooperating components synchronized. To achieve this it enables
one-way propagation of changes: one publisher notifies any number
of subscribers about changes to its state.
In this section we give an abbreviated pattern description based on
the Observer pattern from [GHJV95],to allow us to present additional
viewpoints and variants.
A situation often arises in which data changes in one place, but many
other components depend on this data. The classical example is user -
interface elements: when some internal data element changes all
views that depend on this data have to be updated. We could solve the
problem by introducing direct calling dependencies along which to
propagate the changes, but this solution is inflexible and not
reusable. We are looking for a more general change-propagation
mechanism that is applicable in many contexts.
The solution should balance the following forces:
One or more components must be notified about state changes in
a particular component.
The number and identities of dependent components is not known
a priori, or may even change over time.
Explicit polling by dependents for new information is not feasible.
The information publisher and its dependents should not be tightly
coupled when introducing a change-propagation mechanism.
One dedicated component takes the role of the publisher (called
subject in [GHJV95]).All components dependent on changes in the
publisher are its subscribers (called observers in [GHJV95]).
The publisher maintains a registry of currently-subscribed
components. Whenever a component wants to become a subscriber.
it uses the subscribe interface offered by the publisher. Analogously,
it can unsubscribe.
Whenever the publisher changes state, it sends a notification to all its
subscribers. The subscribers in turn retrieve the changed data at
their discretion.
The pattern offers the following degrees of freedom in its
implementation:
You can introduce abstract base classes to let different classes be
publishers or subscribers, as described in [GHJV95].
The publisher can decide which internal state changes it will notify
its observers about. It may also queue several changes before
calling notify ( ) .
An object can be a subscriber to many publishers.
An object can take both roles, that of a publisher as well as sub scriber.
Subscription and the ensuing notification can be differentiated
according to event type. This allows subscribers to get messages
only about events in which they are interested.
The publisher can send selected details of the data change when it
notifies its subscribers, or can just send a notification and give the
subscribers the responsibility to find out what changed.
In more general terms we differentiate between the push and the pull
model. In the push model, the publisher sends all changed data when
it notifies the subscribers. The subscribers have no choice about if
and when they want to retrieve the data-they just get it. In the pull
model, the publisher only sends minimal information when sending a
change notification-the subscribers are responsible for retrieving
the data they need. Many variations are possible in the middle ground
between these two extremes.
The push model has a very rigid dynamic behavior, whereas the pull
model offers more flexibility, at the expense of a higher number of
messages between publisher and subscribers.
For complex data changes, the push model can be a poor choice,
especially when the publisher sends a large package to a subscriber
that is not interested in it. Even pushing a package that just describes
the nature of the data change can be too great an overhead. In such
cases, use the pull model and make the subscribers find out what
kind of data change occurred. The process of finding out successively
great detail about data changes can be organized as a decision-tree.
Generally, the push model Is a better choice when the subscribers
need the published information most of the time. The pull model is
used when only the individual subscribers can decide if and when
they need a specific piece of information.
Variants Gatekeeper.The Publisher-Subscriber pattern can be also applied to
distributed systems. In this variant a publisher instance in one
process notifies remote subscribers. The publisher may alternatively
be spread over two processes. In one process a component sends out
messages, while in the receiving process a singleton 'gatekeeper'
demultiplexes them by surveying the entry points to the process. The
gatekeeper notifies event-handling subscribers when events for which
they registered occur. The Reactor pattern ISch94) describes this
scheme in detail.
The Event Channel variant was proposed by the OMG in its Event
Service Specification [OMG95]and is targeted at distributed systems.
This pattern strongly decouples publishers and subscribers. For
example, there can be more than one publisher, and the subscribers
only wish to be notified about the occurrence of changes, and not
about the identity of the publisher-subscribers do not care which
component's data has changed. Similarly, publishers are not
interested in which components are subscribing.
In this variant, an event channel is created and placed between the
publisher and the subscribers. To publishers the event channel
appears as a subscriber, while to subscribers it appears as a
publisher. A subscriber registers with the went channel, as
illustrated in the figure below. It asks an administration instance to
create a 'proxy publisher', and connects it over a process boundary
with a local 'proxy subscriber'. Analogously, a 'proxy subscriber' is
created between a publisher and an event channel and, on the event
channel side, a 'proxy publisher'.
In this way publisher, event channel and subscriber can all exist in
different processes. Providing the event channel with a buffer
decouples publishers and subscribers even further. When messages
from a publisher arrive, the event channel does not have to notify the
subscribers immediately, but can implement its own notification
policies.
You can even chain several event channels. The reason for doing this
is that event channels can provide additional capabilities, such as
filtering events, or storing an event internally for a fixed period and
sending it to all components that subscribe during that period. This
is often referred to as 'quality-of-service'. A chain can then assemble
all the capabilities necessary for a system-the chain sums the
capabilities of the individual event channels of which it is composed,
analogously to UNIX pipes.
The Event Channel variant is powerful enough to allow multiple
publishers and typed events.
Another variant of the generic Publisher-Subscriber pattern uses the
Producer-Consumer style of cooperation. In this a producer supplies
information, while a consumer accepts this information for further
processing. Producer and consumer are strongly decoupled, often by
placing a buffer; between them. The producer writes to the buffer
without any regard for the consumer. The consumer reads data from
the buffer at its own discretion. The only synchronization carried out
is checking for buffer overflow and underflow. The producer is
suspended when the buffer is full, while the consumer waits if it
cannot read data because the buffer is empty. Another difference
between the Publisher-Subscriber pattern and the Producer Consumer variant is that in the latter producers and consumers are
usually in a 1:1relationship.
Only more complex patterns such as Event-Channel can simulate a
Producer-Consumer relationship with more than one producer or
consumer. Several producers can provide data by only allowing them
to write to the buffer in series, either directly or indirectly. The case
of more than one consumer is slightly more complicated. When one
consumer reads data from the buffer, the event channel does not
delete that data from the buffer, but only marks it as read by the
consumer. The consumer is given the illusion that the data is
consumed, and hence deleted, while other consumers will be given
the illusion that the data is still present and unread. Iterators are a
good way to implement this behavior. Each consumer has its own
iterator on the buffer. The position of an iterator on the buffer reflects
how far the corresponding consumer has read the buffer. The data in
the buffer can be purged behind the lagging iterator, as all reads on
it have been completed.
##%%&&
The Counted Pointer idiom [Cope921makes memory management of
dynamically-allocated shared objects in C++ easier. It introduces a
reference counter to a body class that is updated by handle objects.
Clients access body class objects only through handles via the
overloaded operator->( ) .
Example When using C++for object-oriented development, memory management is an important issue. Whenever an object is shared by clients.
each of which holds a reference to it, two situations exist that are
likely to cause problems: a client may delete the object while another
client still holds a reference to it, or all clients may 'forget' their references without the object being deleted.
Context Memory management of dynamically allocated instances of a class.
Problem In every object-oriented C++ program you have to pass objects a s
parameters of functions. It is typical to use pointers or references to
objects as parameters. This allows you to exploit polymorphism.
However, passing object references around freely can lead to the
situations shown in the diagram above-you do not know if
references are still valid, or even still needed.
One approach to the problems arising from the use of pointers and
references is to avoid them completely and pass objects by value, as
is normally done with integers. C++ allows you to create programs
that do this, and the compiler will automatically destroy value objects
that go out of scope.
This solution does not work well for all kinds of program, however, for
three reasons. Firstly, if the objects you pass by value are large,
copying them each time they are used is expensive in run-time and
memory consumption. Secondly, you might want to create dynamic
structures of objects, such a s trees or directed graphs, which is
almost impossible to do in C++using value objects alone. Lastly, you
may want to share an object deliberately, for example by storing it in
several collections.
If you have to deal with references or pointers to dynamically
allocated objects of a class, you may need to address the following
forces:
Passing objects by value is inappropriate for a class.
Several clients may need to share the same object.
You want to avoid 'dangling' references-references to an object
that has been deleted.
If a shared object is no longer needed, it should be destroyed to
conserve memory and release other resources it has acquired.
Your solution should not require too much additional code within
each client.
Solution The Counted Pointer idiom eases memory management of shared
objects by introducing reference counting. The class of the shared
objects, called Body, is extended with a reference counter. To keep
track of references used, a second class Handle is the only class
allowed to hold references to Body objects. All Handle objects are
passed by value throughout the program, and therefore are allocated
and destroyed automatically. The Handle class takes care of the Body
object's reference counter. By overloading operator - > ( ) in the
Handle class, its objects can be used syntactically a s if they were
pointers to Body objects.
See the Variants section for a variation of this solution that applies
when Body objects are only shared for performance reasons.
Make the constructors and destructor of the Body,class private (or
protected) to prohibit its uncontrolled instantiation and deletion.
Make the Handle class a friend to the Body class, and thus provide
the Handle class with access to Body's internals.
Extend the Body class with a reference counter.
Add a single data member to the Handle class that points to the Body
object.
Implement the Handle class' copy constructor and its assignment
operator by copying the Body object pointer and incrementing the
reference counter of the shared Body object. implement the
destructor of the Handle class to decrement the reference counter and
to delete the Body object when the counter reaches zero.
implement the arrow operator of the Handle class a s follows:
7 Extend the Handle class with one or several constructors that create
the initial Body instance to which it refers. Each of these constructors
initializes the reference counter to one.
A common application of reference counting, similar to Counted
Pointer, is used for performance improvement with large Body
objects. [Cope921names this variant the Reference Counting Idiom or
Counted Body in [Cope94a].In this variant a client has the illusion of
using its own Body object, even if it is shared with other clients.
Whenever an operation is likely to change the shared Body object, the
Handle creates a new Body instance and uses this copy for all further
processing. To achieve this functionality it is not sufficient to just
overload operator- > ( ).Instead, the interface of the Body class is
duplicated by the Handle class. Each method in the Handle class
delegates execution to the Body instance to which it refers. Methods
that would change the Body object create a new copy of it if other
clients share this Body object.
See Also Bjarne Stroustrup (Str911 discusses several ways of extending the
Handle class. The Handle can be implemented as a template if the
Body class, passed as a template parameter, cooperates with the
Handle template class--for example, if the Body class provides the
Handle class access to the reference counter.
The solution provided by the Counted Pointer idiom has the drawback
that you need to change the Body class to introduce the reference
counter. Coplien and Koenig give two ways to avoid this change.
James Coplien [Cope921 presents the Counted Pointer idiom and
several variations. In cases where the Body class is not intended to
have derived classes, it is possible to embed it in the Handle class.
Another variation, shown in the diagram that follows, is to wrap
existing classes with a reference counter class. This wrapper class
then forms the Body class of the Counted Pointer idiom. This solution
requires a n additional level of indirection when clients access the
Body object.
Andrew Koenig gives a further variation of the theme that allows you
to add reference counting to classes without changing them IKoe951.
He defines a separate abstraction for use counts. Then the Handle
holds two pointers: one to the body object, the other to the use-count
object. The use-count class can be used to implement handles for a
variety of body classes. The Handle objects of this solution require
twice the space of the other Counted Pointer variants, but the access
is as direct as with a change to the Body class.
##%%&&
The Wrapper Facade design pattern encapsulates the functions and data provided by
existing non-object-oriented APIs within more concise, robust, portable, maintainable, and
cohesive object-oriented class interfaces.
Consider a server for a distributed logging service that handles multiple clients concurrently
using the connection-oriented TCP protocol [Ste98]. To log data a client must send a
connection request to a server transport address, which consists of a TCP port number and
IP address. In the logging server, a passive-mode socket handle factory listens on this
address for connection requests. The socket handle factory accepts the connection request
and creates a data-mode socket handle that identifies this client's transport address. This
49
handle is passed to the server, which spawns a logging handler thread that processes client
logging requests.
After a client is connected it sends logging requests to the server. The logging handler
thread receives these requests via its connected socket handle. It then processes the
requests in the logging handler thread and writes the requests to a log file.
A common way to develop this logging server is to use low-level C language APIs, such as
Solaris threads [EKBF+92] and Sockets [Ste98], to program the server's threading,
synchronization, and network communication functionality. If the logging server runs on
multiple platforms, however, there will be differences between functions and data in the low level APIs, as well as different operating system and compiler features and defects.
Developers commonly handle these differences by inserting conditional compilation
directives, such as C/C++ #ifdefs, throughout their code. For instance, the following code
illustrates a logging server that has been implemented using #ifdefs to run on Solaris and
Windows NT:
The design shown above may work for short-lived, 'throw-away' prototypes [FoYo99]. It is
inadequate, however, for software that must be maintained and enhanced over time. The
use of conditional compilation directives and direct programming of low-level APIs makes the
code unnecessarily hard to understand, debug, port, maintain, and evolve.
Certain problems can be alleviated by moving platform-specific declarations, such as the
mutex and socket types, into separate configuration header files. This solution is incomplete,
however, because the #ifdefs that separate the use of platform-specific APIs, such as
thread creation calls, will still pollute application code. Supporting new platforms will also
require modifications to platform-specific declarations, irrespective of whether they are
included directly into application code or separated into configuration files.
Several well-known patterns address similar problems, but unfortunately do not help to
resolve the problems outlined above. For example, Facade [GoF95] encapsulates object oriented subsystems rather than lower-level non-object-oriented APIs. Decorator [GoF95]
extends an object dynamically by attaching additional responsibilities transparently, which
incurs unnecessary performance overhead. Bridge and Adapter [GoF95] also introduce an
additional layer of indirection that can incur overhead. In general, therefore, these patterns
are not well suited to encapsulate existing lower-level non-object oriented APIs, where it may
be more important that the solution be efficient than be dynamically extensible.
Maintainable and evolvable applications that access mechanisms or services provided by
existing non-object-oriented APIs.
Applications are often written using non-object-oriented operating system APIs or system
libraries. These APIs access network and thread programming mechanisms, as well as user
interface or database programming libraries. Although this design is common, it causes
problems for application developers by not resolving the following forces:
??Concise code is often more robust than verbose code because it is easier to develop
and maintain. Using object-oriented languages that support higher-level features, such
as constructors, destructors, exceptions, and garbage collection, reduces the likelihood
of common programming errors. However, developers who program using lower-level
function-based APIs directly tend to rewrite a great deal of verbose and error-prone
software repeatedly.
?The code for creating and initializing an acceptor socket in the main() function of our
logging server example is error-prone. Moreover, these errors are subtle, such as
failing to initialize the sock_addr to zero or not using the htons() macro to
convert the LOGGING_PORT number into network byte order [Sch92]. The lack of
constructors and destructors in C also makes it hard to ensure that resources are
allocated and released properly. For example, note how the lock that serializes
access to request_count will not be released correctly if the write_record()
function returns -1.
??Software that is portable or can be ported easily to different operating systems,
compilers, and hardware platforms helps increase product market-share. Although
reusing existing lower-level APIs may reduce some of the software development effort,
applications programmed directed with lower-level APIs are often non-portable.
Programming using lower-level APIs across different versions of the same operating
system or compiler also may be non-portable due to the lack of source-level or binary level compatibility across software releases [Box97].
?Our logging server example has hard-coded dependencies on several non-portable
operating system threading and network programming C APIs. For example, the
Solaris thr_create(), mutex_lock(), and mutex_unlock() functions are
not portable to Win32 platforms. Although the code is quasi-portable¡ªit also
compiles and runs on Win32 platforms¡ªthere are various subtle portability
problems. In particular, there will be resource leaks on Win32 platforms because
there is no equivalent to the Solaris THR_DETACHED feature, which spawns a
'detached' thread whose exit status is not retained by the threading library [Lew95].
??Improving software maintainability helps reduce lifecycle costs. Programs written
directly to low-level non-object-oriented APIs are often hard to maintain, however. For
example, C and C++ developers often address portability issues by embedding
conditional compilation directives into their application source. Unfortunately,
addressing platform-specific variations via conditional compilation at all points of use
increases the software's physical design complexity [Lak95]. For instance, platform specific details become scattered throughout the application source files.
?Maintenance of our logging server is impeded by the #ifdefs that handle Win32 and
Solaris portability, for example the differences in the type of a socket on Win32 and
Solaris. In general, developers who program to low-level C APIs like these must
have intimate knowledge of many operating system idiosyncrasies to maintain and
evolve their code.
??Cohesive components are easier to learn, maintain, and enhance. However, low-level
APIs are rarely grouped into cohesive components because languages like C lack
features such as classes, namespaces, or packages. It is hard, therefore, to recognize
the extent of low-level APIs. Programming with non-cohesive stand-alone function APIs
also scatters common code throughout an application, making it hard to 'plug in' new
components that support different policies and mechanisms.
?The Socket API is particularly hard to learn because the several dozen C functions in
the Socket library lack a uniform naming convention. For example, it is not obvious
that socket(), bind(), listen(), connect(), and accept() are related.
Other low-level network programming APIs, such as TLI, address this problem by
prepending a common function prefix, such as the t_ prefixed before each function
in the TLI API. However, the use of a common prefix does not by itself make the TLI
API more 'pluggable' than Sockets. It remains a low-level function-based API rather
than a more cohesive object-oriented class interface.
In general, developing applications by programming to non-object-oriented APIs directly is a
poor design choice for software that must be maintained and evolved over time.
Avoid accessing non-object-oriented APIs directly. For each set of related functions and data
in a non-object-oriented API, create one or more wrapper facade classes that encapsulate
these functions and data within the more concise, robust, portable, and maintainable
methods provided by the object-oriented wrapper facade(s).
There are two participants in the Wrapper Facade pattern:
Functions are the building blocks of existing non-object-oriented APIs. They provide a standalone service or mechanism and manipulate data passed as parameters or accessed
through global variables.
A wrapper facade is a set of one or more object-oriented classes that encapsulate existing
functions and their associated data. These class(es) export a cohesive abstraction that
provides a specific type of functionality. Each class represents a specific role in this
abstraction.
The methods in the wrapper facade class(es) generally forward application invocations to
one or more of the functions, passing the data as parameters. The data is often hidden
within the private portion of the wrapper facade and is not accessible to client applications.
Compilers can then enforce type safety because primitive data types, such as pointers or
integers, are encapsulated within strongly-typed wrapper facades.
The following class diagram illustrates the structure of Wrapper Facade:
Collaborations in the Wrapper Facade pattern are often straightforward:
??The application code invokes a method on an instance of the wrapper facade.
??The wrapper facade method forwards the request and its parameters to one or more of
the lower-level API functions that it encapsulates, passing along any internal data
needed by the underlying function(s).
This section describes the activities involved in implementing the Wrapper Facade pattern.
Certain activities may require multiple iterations to identify and implement suitable wrapper
facade abstractions. To reduce repetition elsewhere in the book, we present an in-depth
discussion of concrete wrapper facades for mutexes, condition variables, Sockets, and
threads in this section. Although this lengthens the current section somewhat, the
implementation examples of other patterns in this book, including Acceptor-Connector (285),
Strategized Locking (333), Thread-Specific Storage (475), and Monitor Object (399), are
simplified by using these wrapper facades.
1. Identify the cohesive abstractions and relationships among existing low-level APIs.
Mature low-level APIs contain functions and data structures that define many
cohesive abstractions and map cleanly onto object-oriented classes and methods.
Common examples include the C APIs for Win32 synchronization and threading,
POSIX network programming, and X Windows GUI event dispatching. Due to the lack
of data abstraction in languages like C, however, it may not be clear how functions in
these existing APIs relate to each other. The first activity in implementing the
Wrapper Facade pattern is therefore to identify the cohesive abstractions and
relations among existing APIs.
2. The original implementation of our logging server carefully uses many low-level
functions that provide several cohesive operating system mechanisms, such as
synchronization and network communication. The Solaris mutex_lock() and
mutex_unlock() functions, for example, are associated with a mutex
synchronization abstraction. Similarly, the socket(), bind(), listen(),
and accept() functions play various roles in a network programming
abstraction.
4. If existing functions and data structures have been developed as throw-away code or
via piecemeal growth [FoYo99], they may exhibit little or no cohesive abstractions. In
this case the code should be refactored [Opd92] [FBBOR99], if possible, before
proceeding with the implementation of the Wrapper Facade pattern.
5. Cluster cohesive groups of functions into wrapper facade classes and methods. This
activity defines one or more class abstractions that shield applications from low-level
data representations, arbitrary variations in function syntax, and other implementation
details. It can be decomposed into five sub-activities:
1. Create cohesive classes. We start by defining one or more wrapper facade
classes for each group of existing non-object-oriented APIs that are related to
a particular abstraction. Common criteria used to create cohesive classes
include the following:
??Coalesce functions and data with high cohesion into individual
classes, while minimizing unnecessary coupling between classes.
Examples of cohesive functions are those that manipulate common data
structures, such as a Socket, a file, or a signal set [Ste98].
??Identify the common and variable aspects in the underlying functions
and data [Cope98]. Common aspects include mechanisms for
synchronization, threading, memory management, addressing, and
operating system platform APIs. Variable aspects often include the
implementations of these mechanisms. Whenever possible variation in
functions and data should be factored into classes that isolate variation
behind uniform interfaces to enhance extensibility.
In general, if the original API contains a wide range of related functions, it may
be necessary to create several wrapper facade classes to separate concerns
properly.
2. Coalesce multiple individual functions into a single method. In addition to
grouping existing functions into classes, it may be useful to coalesce multiple
individual functions into a smaller number of methods in each wrapper facade
class. Coalescing can be used to ensure that a group of lower-level functions
are called in the appropriate order, as with the Template Method pattern
[GoF95].
3. Automate creation and destruction operations, if possible. Lower-level APIs
often require programmers to call functions explicitly to create and destroy
data structures that implement instances of the API. This procedure is error prone, however, because developers may forget to call these functions in one
or more paths through their code. A more robust approach therefore is to
leverage the implicit creation and destruction operation capabilities provided
by object-oriented languages, such as C++ and Java. In fact, the ability to
create and destroy objects automatically often justifies the use of the Wrapper
Facade pattern, even if the wrapper facade methods do nothing but forward
control to the lower-level function calls.
4. Select the level of indirection. Most wrapper facade classes simply forward
their method calls to the underlying low-level functions, as mentioned above. If
wrapper facade methods can be inlined implicitly or explicitly, there need be
no run-time indirection overhead when compared to invoking the low-level
functions directly. It is also possible to add another level of indirection by
dispatching wrapper facade implementations using dynamically bound
methods or some other form of polymorphism. In this case the wrapper facade
classes play the role of the abstraction class in the Bridge pattern [GoF95].
5. Determine where to encapsulate any platform-specific variation. A common
use of the Wrapper Facade pattern is to minimize platform-specific variation in
application code. Although wrapper facade method implementations may differ
across different operating system platforms, they should provide uniform,
platform-independent interfaces. Where platform-specific variation exists it can
be encapsulated via conditional compilation or separate directories:
??Conditional compilation can be used to select among different wrapper
facade class method implementations. The use of conditional compilation
is inelegant and tedious when #ifdefs are scattered throughout
application code. Conditional compilation may be acceptable, however, if
it is localized in a few platform-specific wrapper facade classes or files
that are not accessed directly by application developers. When
conditional compilation is used in conjunction with auto-configuration
tools, such as GNU autoconf, platform-independent wrapper facades
can be created within a single source file. As long as the number of
variations supported in this file is not unwieldy therefore, conditional
compilation can help localize variation and simplify maintenance.
??Separate directories can be used to factor out different wrapper facade
implementations, thereby minimizing conditional compilation or avoiding it
altogether. For example, each operating system platform can have its
own directory containing implementations of platform-specific wrapper
facades. Language processing tools can be used to include the
appropriate wrapper facade class from the relevant directory at
compilation. To obtain a different implementation, a different include path
could be provided to the compiler. This strategy avoids the problems with
conditional compilation described above because it physically decouples
the various alternative implementations into separate directories.
Choosing a strategy depends on how often wrapper facade interfaces and
implementations change. If changes occur frequently it may be time consuming to update the conditional compilation sections for each platform.
Similarly, all files that depend on the affected files will be recompiled even if
the change is only necessary for one platform. Therefore the use of condition
compilation becomes increasingly complex as a larger number of different
platforms are supported. Regardless of which strategy is selected, however,
the burden of maintaining wrapper facade implementations should be the
responsibility of wrapper facade developers rather than application
developers.
To simplify our logging server implementation, we define wrapper facades
that encapsulate existing low-level C APIs for mutexes, Sockets, and
threads. Each wrapper facade illustrates how various design issues
outlined above can be addressed systematically. We focus on defining
wrapper facades for C functions because C is used to define popular
operating system APIs, such as POSIX or Win32. However, the same
design principles and techniques can be applied to other non-object oriented languages, such as FORTRAN, Ada 83, Scheme, or Pascal, as
well as to non-operating system APIs, such as X Windows or ODBC
database toolkits [San98].
Mutex wrapper facades. We first define a Thread_Mutex abstraction that
encapsulates the Solaris mutex functions with a uniform and portable
class interface:
Note how we define the copy constructor and assignment operator as
private methods in the Thread_Mutex class. This C++ idiom ensures that
application programmers cannot copy or assign one Thread_Mutex to
another accidentally [Mey98] [Str97]. Copying mutexes is a semantically invalid operation that is erroneously permitted by the less strongly-typed C
programming API. Our Thread_Mutex wrapper facade therefore provides
a mutex interface that is less error-prone than programming directly to the
lower-level Solaris synchronization functions.
By defining a Thread_Mutex class interface and then writing applications
to use it, rather than lower-level native operating system C APIs, we can
port our wrapper facade to other platforms more easily. For example, the
identical Thread_Mutex interface can be implemented to run on Win32:
Naturally, a complete implementation of Thread_Mutex would map the
platform-specific error handling return values from the various mutex_t
and CRITICAL_SECTION functions to portable C++ exceptions.
As described earlier, we can support multiple operating systems
simultaneously by using conditional compilation and #ifdef'ing the
Thread_Mutex method implementations. If conditional compilation is
unwieldy due to the number of supported platforms, it is possible to factor
out the different Thread_Mutex implementations into separate
directories. In this case, language processing tools such as compilers and
preprocessors can be instructed to include the appropriate platform specific variant into the application during compilation.
Condition variable wrapper facade. A condition variable is a
synchronization mechanism used by collaborating threads to suspend
themselves temporarily until condition expressions involving data shared
between the threads attain desired states [IEEE96]. We describe the
wrapper facade for condition variables at this point because they are often
used in conjunction with the Thread_Mutex wrapper facade described
above. Although our logging server example does not use condition
variables, they are used by other patterns throughout the book, such as
Strategized Locking (333), Leader/Followers (447), and Monitor Object
(399).
As mentioned above, a condition variable is always used in conjunction
with a mutex that the client thread must acquire before evaluating the
condition expression. If the condition expression is false the client
suspends itself on the condition variable and releases the mutex
atomically, so that other threads can change the shared data. When a
cooperating thread changes this data it can notify the condition variable,
which resumes a thread atomically that had suspended itself previously on
the condition variable. The thread then re-acquires the mutex associated
with the condition variable.
After re-acquiring its mutex a newly-resumed thread next re-evaluates its
condition expression. If the shared data has attained the desired state, the
thread continues. Otherwise it suspends itself on the condition variable
again until it is resumed. This process can repeat until the condition
expression becomes true.
In general, when complex condition expressions or scheduling behaviors
are required, combining a mutex with a condition variable is more
appropriate than just using a mutex. For example, condition variables can
be used to implement synchronized message queues, as shown in the
Monitor Object pattern example (399). In this situation a pair of condition
variables are employed to block supplier threads cooperatively when a
message queue is full, and to block consumer threads when the queue is
empty.
The constructor initializes the condition variable and associates it with the
Thread_Mutex passed as a parameter. The destructor destroys the
condition variable, which releases allocated resources. Note that the
mutex is not owned by the Thread_Condition so it is not destroyed in
the destructor.
When called by a client thread the wait() method performs the following
two steps atomically:
??It releases the associated mutex and
??It suspends itself atomically for up to a timeout amount of time,
waiting for the Thread_Condition object to be notified by another
thread.
The notify() method resumes one thread waiting on a
Thread_Condition. Similarly the notify_all() method notifies all
threads that are currently waiting on a Thread_Condition. The mutex_
lock is reacquired by the wait() method before it returns to its client
thread, either because the condition variable was notified or because its
timeout expired.
Socket wrapper facades. Our next wrapper facade encapsulates the
Socket API. This API is much larger and more expressive than the Solaris
mutex API [Sch92]. We must therefore define a group of related wrapper
facade classes to encapsulate Sockets. We start by defining a typedef
and a macro that hide some of the UNIX/POSIX and Win32 portability
differences:
Both SOCKET and INVALID_HANDLE_VALUE are defined in the Win32
API already. Therefore, we could either integrate them using #ifdefs or
using separate platform-specific directories, as discussed earlier in
implementation activity 2.5 (55).
Note how the INET_Addr constructor eliminates several common Socket
programming errors. For example, it initializes the sockaddr_in field to
zero, and ensures the TCP port number and IP address are converted into
network byte order by applying the ntons() and ntonl() macros
automatically [Ste98].
As discussed in implementation activity 2.3 (55), this class leverages the
semantics of C++ destructors to ensure that a socket handle is closed
automatically when a SOCK_Stream object goes out of scope. In addition,
the send_n() and recv_n() methods can handle networking
idiosyncrasies, for example 'short' send and receive operations.
SOCK_Stream objects can be created via a connection factory, called
SOCK_Acceptor, which encapsulates passive establishment of Socket
connections. The SOCK_Acceptor constructor initializes the passive mode acceptor socket to listen at the sock_addr address. The
SOCK_Acceptor's accept() method is a factory that initializes the
SOCK_Stream parameter with a socket handle to a new connection:
Note how the constructor for the SOCK_Acceptor applies the strategy
discussed in implementation activity 2.2 (55) to ensure that the low-level
socket(), bind(), and listen() functions are always called
together and in the correct order.
A complete set of Socket [Sch97] wrapper facades would also include a
SOCK_Connector that encapsulates the logic for establishing
connections actively. The SOCK_Acceptor and SOCK_Connector
classes are concrete IPC mechanisms that can be used to instantiate the
generic acceptor and connector classes described in the Acceptor Connector pattern (285) to perform connection establishment.
Thread wrapper facade. Our final wrapper facade encapsulates operating
system threading APIs that are available on different operating system
platforms, including Solaris threads, POSIX Pthreads, and Win32 threads.
These APIs exhibit subtle syntactic and semantic differences. For
example, Solaris and POSIX threads can be spawned in 'detached' mode,
whereas Win32 threads cannot. It is possible, however, to provide a
Thread_Manager wrapper facade that encapsulates these differences in
a uniform manner. The Thread_Manager wrapper facade below, which is
a Singleton [GoF95], illustrates the spawn method implemented for Solaris
threads:
The Thread_Manager class also provides methods for joining and
canceling threads that can be ported to other operating systems.
6. Consider allowing applications controlled access to implementation details. One
benefit of defining a wrapper facade is to make it hard to write incorrect or non portable applications. For example, wrapper facades can shield applications from
error-prone or platform-specific implementation details, such as whether a socket is
represented as a pointer or an integer. Cases may arise, however, where the extra
abstraction and type safety actually prevent programmers from using a wrapper
facade in useful ways not anticipated by its designer. This experience can be
frustrating and may discourage programmers from leveraging other benefits of
wrapper facades.
A common solution to the problem of 'too much' abstraction is to provide an 'escape
hatch' mechanism or open implementation technique, such as AOP [KLM+97]. This
design allows applications to access implementation details in a controlled manner.
The SOCK_Stream class defines a pair of methods that set and get the
underlying SOCKET handle:
These methods can be used to set and get certain Socket options, such as
support for 'out-of-band' data [Ste98], that were not defined by the original Socket
wrapper facades.
Escape-hatch mechanisms should be used sparingly of course, because they
decrease portability and increase the potential for errors, thereby nullifying key
benefits of the Wrapper Facade pattern. If applications use certain escape hatches
repeatedly in similar situations, it may indicate that explicit methods should be added
to the public interface of the wrapper facade. The Extension Interface pattern (141)
defines techniques for adding these new methods without disrupting existing clients.
7. Develop an error-handling mechanism. Low-level C operating system function APIs
often use return values and integer codes, such as errno, to return errors to their
calling code. This technique can be error-prone, however, if callers do not check the
return status of their function calls.
A more elegant way of reporting errors is to use exception handling. Many
programming languages, such as C++ and Java, support exception handling as a
fundamental error-reporting mechanism. It is also used in some operating systems,
for example Win32 [Sol98]. There are several benefits of using exception handling as
the error-handling mechanism for wrapper facade classes:
??It is extensible, for example by defining hierarchies of exception classes in C++
and Java.
??It cleanly decouples error handling from normal processing. Error handling
information is neither passed to an operation explicitly, nor can an application
accidentally ignore an exception by failing to check function return values.
??It can be type-safe. In languages like C++ and Java, exceptions can be thrown
and caught in a strongly-typed manner.
We can define the following exception class to keep track of which operating
system error or condition has occurred:
Platform-independent errors and conditions could be defined via macros or
constants that map onto unique values across all operating systems. For
instance, the Solaris implementation of the Thread_Mutex::acquire()
method shown on page 57 could be written as follows:
Unfortunately, there are several drawbacks to the use of exception handling for
wrapper facade classes:
??Not all languages or implementations provide exception handling. For
example, C does not define an exception model and some C++ compilers do not
implement exceptions.
??Languages implement exceptions in different ways. It can thus be hard to
integrate components written in different languages when they throw exceptions.
Using proprietary exception handling mechanisms, such as Windows NT's
structured exception handling [Sol98], can also reduce the portability of
applications that use these mechanisms.
??Resource management can be complicated if there are multiple exit paths from
a block of C++ or Java code [Mue96]. If garbage collection is not supported by
the language or programming environment, care must be taken to ensure that
dynamically-allocated objects are deleted when an exception is thrown.
??Poor exception handling implementations incur time or space over-heads even
when exceptions are not thrown [Mue96]. This overhead is problematic for
embedded systems that must be efficient and have small memory footprints
[GS98].
The drawbacks of exception handling are particularly problematic for wrapper facades
that encapsulate kernel-level device drivers or low-level operating system APIs that
must run on many platforms [Sch92], such as the mutex, Socket and thread wrapper
facades described above. A common error handling mechanism for system-level
wrapper facades [Sch97] is based on the Thread-Specific Storage pattern (475) in
conjunction with errno. This solution is efficient, portable, and thread-safe, though
more obtrusive and potentially error-prone than using C++ exceptions.
8. Define related helper classes (optional). After lower-level APIs are encapsulated within
wrapper facade classes it often becomes possible to create other helper classes that
further simplify application development. The benefits of these helper classes are
often apparent only after the Wrapper Facade pattern has been applied to cluster
lower-level functions and their associated data into classes.
9. In our logging example we can leverage the Guard template class defined in the
Strategized Locking pattern (333) [Str97]. This class ensures that a
Thread_Mutex is acquired and released properly within a scope regardless of
how the method's flow of control leaves the scope. The Guard class constructor
acquires the mutex and the destructor releases it within a scope automatically:
We can easily substitute a different type of locking mechanism while still using
the Guard's automatic locking and unlocking protocol because we used a class
as the Thread_Mutex wrapper facade. For example, we can replace the
Thread_Mutex class with a Process_Mutex class:
It is much harder to achieve this degree of 'pluggability' using lower-level C
functions and data structures instead of C++ classes. The main problem is that
the functions and data lack language support for cohesion, whereas the C++
classes provide this support naturally.
The code below illustrates the logging_handler() function of our logging server after it
has been rewritten to use the wrapper facades for mutexes, Sockets, and threads described
in the Implementation section. To ease comparison with the original code, we present it in a
two-column table with the original code from the example section in the left-hand column and
the new code in the right-hand column
The code in the right-hand column addresses the problems with the code shown in the left hand column. For example, the destructors of SOCK_Stream and Guard will close the
socket handle and release the Thread_Mutex, respectively, regardless of how the code
blocks are exited. This code is also easier to understand, maintain, and port because it is
more concise and uses no platform-specific APIs.
Analogously to the logging_handler() function, we present a two-column table below
that compares the original code for the main() function with the new code using wrapper
facades:
Note how literally dozens of lines of low-level, conditionally compiled code disappear in the
right-hand column version that uses the Wrapper Facade pattern.
Microsoft Foundation Classes (MFC). MFC [Pro99] provides a set of wrapper facades that
encapsulate many lower-level C Win32 APIs. It focuses largely on providing GUI
components that implement the Microsoft Document-View architecture, which is a variant of
the Document-View architecture described in [POSA1].
ACE. The Socket, thread, and mutex wrapper facades described in the Implementation
section are abstractions of ACE framework [Sch97] components, such as the ACE_SOCK*,
ACE_Thread_Manager and ACE_ Thread_Mutex classes, respectively.
Rogue Wave. Rogue Wave's Net.h++ and Threads.h++ class libraries implement
wrapper facades for Sockets, threads, and synchronization mechanisms on a number of
operating system platforms.
ObjectSpace. The ObjectSpace System<Toolkit> also implements platform-independent
wrapper facades for Sockets, threads, and synchronization mechanisms.
Java Virtual Machine and Java class libraries. The Java Virtual Machine (JVM) and
various Java class libraries, such as AWT and Swing [RBV99], provide a set of wrapper
facades that encapsulate many low-level native operating system calls and GUI APIs.
Siemens REFORM. The REFORM framework [BGHS98] for hot rolling mill process
automation uses the Wrapper Facade pattern to shield the object-oriented parts of the
system, such as material tracking and setpoint transmission, from a neural network for the
actual process control. This neural network is programmed in C due to its algorithmic nature
and contains mathematical models that characterize the physics of the automation process.
The wrapper facades defined in the REFORM framework differ from wrapper facades for
operating system mechanisms because the process-control APIs they encapsulate are at a
higher level of abstraction. In fact the neural network is part of the REFORM system itself.
However, its function-based C APIs are lower-level compared to the complex object-oriented
high-level structure and logic of the hot rolling mill framework. The REFORM wrapper
facades therefore have similar goals and properties as the lower-level operating system
wrapper facades:
??They provide the views and abstractions that the object-oriented parts of the
framework need of the process control neural network. There is a separate wrapper
facade for every component using the neural network.
??They hide API variations. For different customer-specific instances of the framework
there may be (slightly) different implementations of the neural network. As a result,
semantically identical functions in these neural network implementations may have
different signatures. These differences do not affect the framework implementation,
however.
??They ensure lower-level C functions are invoked in the right order.
Books consisting of edited collections of papers. A real-life example of the Wrapper
Facade pattern are books consisting of edited collections of papers that are organized into
one or more 'themes'. For example, the PLoPD series [PLoPD1] [PLoPD2] [PLoPD3]
[PLoPD4] consist of individual papers that are organized into cohesive sections, such as
event handling, fault tolerance, application framework design, or concurrency. Thus, readers
who are interested in a particular topic area or domain can focus their attention on these
sections, rather than having to locate each paper individually.
The Wrapper Facade pattern provides the following benefits:
Concise, cohesive and robust higher-level object-oriented programming interfaces. The
Wrapper Facade pattern can be used to encapsulate lower-level APIs within a more concise
and cohesive set of higher-level object-oriented classes. These abstractions reduce the
tedium of developing applications, thereby decreasing the potential for certain types of
programming error. In addition, the use of encapsulation eliminates programming errors that
occur when using untyped data structures incorrectly, such as socket or file handles.
Application code can therefore use wrapper facades to access lower-level APIs correctly and
uniformly.
Portability and maintainability. Wrapper facades can be implemented to shield application
developers from non-portable aspects of lower-level APIs. The Wrapper Facade pattern also
improves software structure by replacing an application configuration strategy based on
physical design entities, such as files and #ifdefs, with logical design entities, such as
base classes, subclasses, and their relationships. It is often much easier to understand,
maintain, and enhance applications in terms of their logical design rather than their physical
design [Lak95].
Modularity, reusability and configurability. The Wrapper Facade pattern creates cohesive
and reusable class components that can be 'plugged' into other components in a wholesale
fashion, using object-oriented language features like inheritance and parameterized types. In
contrast, it is harder to replace groups of functions without resorting to coarse-grained
operating system tools such as linkers or file systems.
The Wrapper Facade pattern incurs several liabilities:
Loss of functionality. Whenever an abstraction is layered on top of an existing abstraction it
is possible to lose functionality. In particular, situations can occur in which the new
abstraction prevents developers from accessing certain capabilities of the underlying
abstraction. It is hard to define a suitable high-level abstraction that covers all these use
cases without becoming bloated. One useful heuristic to follow is to design wrapper facades
so that they are easy to use correctly, hard to use incorrectly, but not impossible to use in
ways that the original designers did not anticipate. An 'escape-hatch' mechanism or open
implementation [KLM+97] technique can often help reconcile these design forces cleanly.
Performance degradation. The Wrapper Facade pattern can degrade performance. For
example, if wrapper facade classes are implemented with the Bridge pattern [GoF95], or if
they make several forwarding function calls per method, the additional indirection may be
more costly than programming to the lower-level APIs directly. However, languages that
support inlining, such as C++ or certain C compilers, can implement the Wrapper Facade
pattern with no significant overhead, because compilers can inline the method calls used to
implement the wrapper facades. The overhead is therefore the same as calling lower-level
functions directly.
Programming language and compiler limitations. Defining C++ wrapper facades for well designed C APIs is relatively straightforward, because the C++ language and C++ compilers
define features that facilitate cross-language integration. It may be hard to define wrapper
facades for other languages, however, due to a lack of language support or limitations with
compilers. For example, there is no universally accepted standard for integrating C functions
into languages like Ada, Smalltalk, and Java. Programmers may therefore need to use to
non-portable mechanisms to develop wrapper facades.
The Wrapper Facade pattern is related to several of the structural patterns in [GoF95],
including Facade, Bridge, Adapter, and Decorator.
Facade. The intent of Facade is to provide a unified interface that simplifies client access to
subsystem interfaces. The intent of Wrapper Facade is more specific: it provides concise,
robust, portable, maintainable, and cohesive class interfaces that encapsulate lower-level
APIs such as operating system mutex, Socket, thread, and GUI C APIs. In general, facades
hide complex class relationships behind a simpler API, whereas wrapper facades hide
complex function and data structure API relationships behind richer object-oriented classes.
Wrapper facades also provide building-block components that can be 'plugged' into higher level objects or components.
Bridge. The intent of Bridge is to decouple an abstraction from its implementation, so the two
can vary independently and dynamically via polymorphism. Wrapper Facade has a similar
intent: minimizing the overhead of indirection and polymorphism. Wrapper Facade
implementations rarely vary dynamically, however, due to the nature of the systems
programming mechanisms that they encapsulate.
Adapter. The intent of Adapter is to convert the interface of a class into another interface that
is expected by a client. A common application of Wrapper Facade is to create a set of
classes that 'adapt' low-level operating system APIs to create a portable set of wrapper
facades that appear the same for all applications. Although the structure of this solution is
not identical to either the object or class form of Adapter in [GoF95], the wrapper facades
play a similar role as an adapter by exporting an object-oriented interface that is common
across platforms.
Decorator. The intent of Decorator is to extend an object dynamically by attaching
responsibilities transparently. In contrast, Wrapper Facade statically encapsulates lower level functions and data with object-oriented class interfaces.
In general, Wrapper Facade should be applied in lieu of these other patterns when there are
existing lower-level, non-object-oriented APIs to encapsulate, and when it is more important
that the solution be efficient than be dynamically extensible.
The Layers pattern [POSA1] helps organize multiple wrapper facades into a separate
component layer. This layer resides directly on top of the operating system and shields
applications from all the low-level APIs they use.
##%%&&
The Component Configurator design pattern allows an application to link and unlink its
component implementations at run-time without having to modify, recompile, or statically
relink the application. Component Configurator further supports the reconfiguration of
components into different application processes without having to shut down and re-start
running processes.
A distributed time service [Mil88] [OMG97c] provides accurate clock synchronization for
computers that collaborate in local-area or wide-area networks. Its architecture contains
three types of components:
??Time server components answer queries about the current time.
??Clerk components query one or more time servers to sample their notion of the current
time, calculate the 'approximate' correct time, and update their own local system time
accordingly.
??Client application components use the globally-consistent time information maintained
by their clerks to synchronize their behavior with clients on other hosts.
The conventional way to implement this distributed time service is to configure the
functionality of the time server, clerk, and client components statically at compile-time into
separate processes running on hosts in the network:
In such a configuration, one or more hosts run processes containing time service
components that handle requests for time updates. A clerk component runs in a process on
each host on which applications require global time synchronization. Client components in
application processes perform computations using the synchronized time reported by their
local clerk component.
Although a distributed time service design can be implemented in this way, two general
types of problems arise:
??The choice of component implementation can depend on the environment in which
applications run. For example, if a WWV receiver is available,[2] the Cristian time service
algorithm [Cris89] is most appropriate. Otherwise the Berkeley algorithm [GZ89] is the
better choice.
Changing the environment in which applications run may therefore also require a
change to the implementation of time service components. A design in which the
implementation of a particular component is fixed statically within a process at compile time, however, makes it hard to exchange this component's implementation. In addition,
as each component is coupled statically with a process, existing applications must be
modified, recompiled, and statically relinked when changes occur.
??Components may also need to be reconfigured to enhance key quality-of-service
(QoS) properties, such as latency and throughput. For example, we can reconfigure the
clerk and the time server components in our distributed time service so they are
collocated [WSV99] on the same host. In this case, communication overhead can be
minimized by allowing the clerk to access the time server's notion of time via shared
memory, rather than exchanging data through a pipe or 'loopback' Socket connection.
However, if components are configured statically into processes, making the changes
outlined above requires terminating, reconfiguring, and restarting running time service
processes. These activities are not only inefficient, they are potentially infeasible for
systems with high availability requirements.
Unfortunately patterns such as Bridge and Strategy [GoF95] are not sufficient by
themselves to solve these types of problems. For example, Bridge and Strategy are
often used to alleviate unnecessary coupling between components. When these
patterns are applied to our example application in isolation, however, all possible
implementations of time service components must be configured at compile-time in
order to support the selection of different strategies at run-time. This constraint may be
excessively inflexible or costly for certain applications.
For example, if a time service runs on a personal computing device with stringent
memory and power limitations, components that are not currently in use should be
unlinked to minimize resource consumption. This 'dynamic reconfiguration' aspect is not
addressed directly by patterns such as Bridge and Strategy.
An application or system in which components must be initiated, suspended, resumed, and
terminated as flexibly and transparently as possible.
Applications that are composed of components must provide a mechanism to configure
these components into one or more processes. The solution to this problem is influenced by
three forces:
??Changes to component functionality or implementation details are common in many
systems and applications. For example, better algorithms or architectures may be
discovered as an application matures. It should be possible therefore to modify
component implementations at any point during an application's development and
deployment lifecycle.
Modifications to one component should have minimal impact on the implementation of
other components that use it. Similarly, it should be possible to initiate, suspend,
resume, terminate, or exchange a component dynamically within an application at
runtime. These activities should have minimal impact on other components that are
configured into the application.
??Developers often may not know the most effective way to collocate or distribute
multiple component components into processes and hosts at the time an application is
developed. If developers commit prematurely to a particular configuration of
components it may impede flexibility, reduce overall system performance and
functionality, and unnecessarily increase resource utilization.
In addition, initial component configuration decisions may prove to be sub-optimal over
time. For example, platform upgrades or increased workloads may require the
redistribution of certain components to other processes and hosts. In such cases, it may
be helpful to make these component configuration decisions as late as possible in an
application's development or deployment cycle, without having to modify or shut down
an application obtrusively.
??Performing common administrative tasks such as configuring, initializing, and
controlling components should be straightforward and component-independent. These
tasks can often be managed most effectively by a central administrator rather than
being distributed throughout an application or system. They should be automated
whenever possible, for example by using some type of scripting mechanism [MGG00].
Decouple component interfaces from their implementations and make applications
independent of the point(s) in time at which component implementations are configured into
application processes.
In detail: a component defines a uniform interface for configuring and controlling a particular
type of application service or functionality that it provides. Concrete components implement
this interface in an application-specific manner. Applications or administrators can use
component interfaces to initiate, suspend, resume, and terminate their concrete components
dynamically, as well as to obtain run-time information about each configured concrete
component. Concrete components are packaged into a suitable unit of configuration, such
as a dynamically linked library (DLL). This DLL can be dynamically linked and unlinked into
and out of an application under the control of a component configurator, which uses a
component repository to manage all concrete components configured into an application.
The Component Configurator pattern includes four participants:
A component defines a uniform interface that can be used to configure and control the type
of application service or functionality provided by a component implementation. Common
control operations include initializing, suspending, resuming, and terminating a component.
Concrete components implement the component control interface to provide a specific type
of component. A concrete component also implements methods to provide application specific functionality, such as processing data exchanged with other connected peer
components. Concrete components are packaged in a form that can be dynamically linked
and unlinked into or out of an application at run-time, such as a DLL.
Two types of concrete components are used in our distributed time service: time server
and clerk. Each of these concrete components provides specific functionality to the
distributed time service. The time server component receives and processes requests
for time updates from clerks. The clerk component queries one or more time servers to
determine the 'approximate' correct time and uses this value to update its own local
system time. Two time server implementations are available in our example, one for the
Cristian algorithm and one for the Berkeley algorithm.
A component repository manages all concrete components that are configured currently into
an application. This repository allows system management applications or administrators to
control the behavior of configured concrete components via a central administrative
mechanism.
A component configurator uses the component repository to coordinate the (re)configuration
of concrete components. It implements a mechanism that interprets and executes a script
specifying which of the available concrete components to (re)configure into the application
via dynamic linking and unlinking from DLLs.
The class diagram for the Component Configurator pattern is as follows:
The behavior of the Component Configurator pattern can be characterized by three phases:
??Component initialization. The component configurator dynamically links a component
into an application and initializes it.[3] After a component has been initialized
successfully the component configurator adds it to its component repository. This
repository manages all configured components at run-time.
??Component processing. After being configured into an application, a component
performs its processing tasks, such as exchanging messages with peer components
and performing service requests. The component configurator can suspend and resume
existing components temporarily, for example when (re)configuring other components.
??Component termination. The component configurator shuts down components after
they are no longer needed, allowing them the opportunity to clean up their resources
before terminating. When terminating a component, the component configurator
removes it from the component repository and unlinks it from the application's address
space.
The following state chart diagram illustrates how a component configurator controls the
lifecycle of a single concrete component:
This diagram illustrates the event-driven 'inversion of control' [Vlis98a] behavior of a
component configurator. For example, in response to the occurrence of events like
CONFIGURE and TERMINATE, the component configurator invokes the component's
corresponding method, in this case init() and fini(), respectively.
The participants in the Component Configurator pattern can be decomposed into two layers:
??Configuration management layer components. This layer performs general-purpose,
application-independent strategies that install, initialize, control, and terminate
components.
??Application layer components. This layer implements the concrete components that
perform application-specific processing.
The implementation activities in this section start at the 'bottom' with the configuration
management layer and work upwards to components in the application layer.
1. Define the component configuration and control interface. Components should support
the following operations so that they can be configured and controlled by a
component configurator:
??Component initialization. Initialize or re-initialize a component.
??Component finalization. Shut down a component and clean up its resources.
??Component suspension. Suspend component execution temporarily.
??Component resumption. Resume execution of a suspended component.
??Component information. Report information describing the static or dynamic
directives of a component.
The interface used to configure and control a component can be based on either an
inheritance or a message passing strategy:
??Inheritance-based interface. In this strategy, each component inherits from a
common base class that contains pure virtual hook methods [Pree95] for each
component configuration and control operation.
The component execution mechanism for our time service example is based
on a reactive event handling model within a single thread of control, as
described by the Reactor pattern (179). By inheriting from the Reactor
pattern's Event_Handler participant, a Component implementation can
register itself with a reactor, which then demultiplexes and dispatches events
to the component.
??Message-based interface. Another strategy for configuring and controlling
components is to program them to respond to a set of messages, such as INIT,
SUSPEND, RESUME, and FINI, sent to the component from the component
configurator. Component developers must write code to process these
messages, in this case to initialize, suspend, resume, and terminate a
component, respectively. Using messages rather than inheritance makes it
possible to implement the Component Configurator pattern in non-object oriented programming languages that lack inheritance, such as C or Ada 83.
2. Implement a component repository. All concrete component implementations that are
linked into an application via DLLs are managed by a component repository. A
component configurator uses this repository to control a component when it is
configured into or out of an application. Each component's current status, such as
whether it is active or suspended, can be maintained in the repository.
A component repository can be a reusable container, for example a Java
java.util.Hashtable [Sun00a] or a C++ standard template library map [Aus98].
Conversely it can be implemented as a container in accordance with the Manager
pattern [Som97]. This container can be stored in main memory, a file system, or
shared memory. Depending on where it resides, a component repository can be
managed within the application or by a separate process.
Implement the component (re)configuration mechanism. A component must be
configured into an application's address space before it can be executed. The
component configurator defines a mechanism to control the static and/or dynamic
(re)configuration of components into application processes. The implementation of a
component configurator involves five sub-activities:
1. Define the component configurator interface. The component configurator is
often implemented as a singleton facade [GoF95]. This can mediate access to
other Component Configurator pattern components, such as the component
repository described in implementation activity 2 (84) and the mechanism for
interpreting the component configuration directives described in
implementation activity 3.3 (88).
Define a language for specifying component configuration directives. These
directives supply the component configurator with the information it needs to
locate and initialize a component's implementation at run-time, as well as to
suspend, resume, re-initialize, and/or terminate a component after it has been
initialized. Component configuration directives can be specified in various
ways, such as via the command line, environment variables, a graphical user
interface, or a configuration script.
30. To simplify installation and administration, the component configurator in
our distributed time server example uses a component scripting
mechanism similar to the one provided by ACE [SchSu94]. A script file,
which we call comp.conf, consolidates component configuration
directives into a single location that can be managed centrally by
applications, developers, or administrators. Every component to be
(re)configured into an application is specified by a directive in the
comp.conf script.
The directive in this comp.conf script contains a dynamic command,
which instructs the interpreter to perform two actions:
??Dynamically link the cristian.dll DLL into the application's
address space and
??Invoke the make_Time_Server() factory function automatically. This
function allocates a new time server instance dynamically:
The string parameter "-p $TIME_SERVER_PORT" at the end of the
directive contains an environment variable that specifies the port number
on which the time server component listens to receive connections from
clerks. The component configurator converts this string into an
'argc/argv'-style array and passes it to the init() hook method of the
time server component. If the init() method initializes the component
successfully, a pointer to the component is stored in the component
repository under the name 'Time_Server'. This name identifies the
newly-configured component so that it can be controlled dynamically by
the component configurator on behalf of an application or an
administrator.
The directives in a comp.conf script are processed by the component
configurator's directive interpreter, as described in implementation activity
3.3(88). Each directive begins with a command that instructs the
interpreter how to configure, reconfigure, or control a component:
Implement a mechanism for parsing and processing component configuration
directives. This mechanism is often implemented as a directive interpreter that
decouples the configuration-related aspects of a component from its run-time
aspects. A directive interpreter can be implemented using the Interpreter
pattern [GoF95], or standard parser-generator tools, such as lex and yacc
[SchSu94].
38. The Component_Configurator facade class defines two methods that
allow applications to invoke a component configurator's directive
interpreter. The process_directives() method can process a
sequence of (re)configuration and control directives that are stored in a
designated script file. This method allows multiple directives to be stored
persistently and processed iteratively. Conversely, the
process_directive() method can process a single directive passed
as a string parameter. This method allows directives to be created
dynamically and/or processed interactively.
40. A simple directive interpreter executes each component configuration directive
in the order in which they are specified. In this case, application developers
are responsible for ensuring this execution sequence satisfies any ordering
dependencies among components being configured. A more complex
interpreter and scripting language could of course be devised to allow the
directive interpreter to handle ordering dependencies automatically, for
example by using topological sorting.
41. Implement the dynamic configuration mechanism. A component configurator
uses this mechanism to link and unlink components into and out of an
application process dynamically. Modern operating systems, such as System
V Release 4 (SVR4) UNIX and Win32, support this feature via explicit dynamic
linking mechanisms [WHO91].
SVR4 UNIX, for example, defines the dlopen(), dlsym(), and
dlclose() API to link a designated DLL dynamically into an application
process explicitly, extract a designated factory function from the DLL, and
unlink the DLL, respectively. Microsoft's Win32 operating systems support the
LoadLibrary(), GetProcAddr(), and CloseHandle() APIs to perform
the same functionality. As the component configurator's directive interpreter
parses and processes directives, it uses these APIs to link and unlink DLLs
dynamically into the application's address space.
To illustrate how a component configurator can use this API, consider the
directive used to configure a Time_Server component shown in
implementation activity 3.2 (86). In this example the component configurator
performs seven steps:
1. It creates a DLL object and passes the 'cristian.dll' string to its
constructor.
2. The cristian.dll DLL is then linked into the application's address
space dynamically via the OS::dlopen() method called in the DLL
class constructor.
3. The component configurator next passes the string
'make_Time_Server()' to the symbol() method of the DLL object.
4. This method uses the OS::dlsym() method to locate the
make_Time_Server entry in the symbol table of the cristian.dll
DLL and returns a pointer to this factory function.
5. Assuming the first four steps succeed, the component configurator
invokes the factory function, which returns a pointer to a Time_Server
component.
6. The component configurator then calls the init() method of this
component, passing the string '-p $TIME_SERVER_PORT' as an
'argc/argv'-style array. The init() method is a hook that the
Time_Server component uses to initialize itself.
7. Finally, the component configurator stores the initialized Time_Server
component into its component repository.
42. Implement the dynamic reconfiguration mechanism. This mechanism builds on
the dynamic configuration mechanism described above to trigger dynamic
reconfiguration of component implementations. Component reconfiguration
should have minimal impact on the execution of other components in an
application process. The following two aspects should therefore be addressed
when implementing a dynamic reconfiguration mechanism:
Define the reconfiguration triggering strategy. There are two strategies for
triggering component reconfiguration, in-band and out-of-band:
??An in-band strategy initiates reconfigurations synchronously by using
an IPC mechanism, such as a Socket connection or a CORBA operation.
The application and/or component configurator is responsible for checking
for such a reconfiguration event at designated 'reconfiguration points'.
??An out-of-band strategy generates an asynchronous event, such as a
UNIX SIGHUP signal, that can interrupt a running application process or
thread to initiate reconfiguration. In either case, on receiving a
reconfiguration event the component configurator will interpret a new set
of component configuration directives.
An in-band strategy for triggering reconfiguration is generally easier to
implement, because there is less potential for race conditions. In-band
triggering may, however, be less responsive, because reconfiguration can only
occur at designated reconfiguration points. In contrast, out-of-band
reconfiguration triggering is more responsive. However, it is harder to use out of-band reconfiguration to implement robust protocols for determining when
configuration can occur.
Define protocols for ensuring robust reconfiguration. Another important aspect
to consider when implementing a reconfiguration mechanism is robustness.
For example, if other components in an application are using a component that
is being reconfigured, a component configurator may not be able to execute
requests to remove or suspend this component immediately. Instead, certain
components must be allowed to finish their computation before reconfiguration
can be performed.
If a new component is configured into an application, other components may
want to be notified, so that they can interact with the new component.
Similarly, when a suspended component is resumed, other components may
want to be notified so that they can resume their computations.
The Component Configurator pattern focuses on (re)configuration
mechanisms, such as how to interpret a script containing component
configuration directives to link and unlink components dynamically. It is
therefore beyond the scope of Component Configurator to ensure robust
dynamic component reconfiguration unilaterally. Supporting robust
reconfiguration requires collaboration between a component configurator and
component/configuration-specific protocols. These protocols determine when
to trigger a reconfiguration and which components to link and interact with to
configure particular application processes.
One way to implement a robust reconfiguration mechanism is to apply the
Observer pattern [GoF95]. Client components that want to access a particular
component are observers. These observers register with the component
configurator, which contains a notifier that plays the role of the Observer
pattern's subject participant.
When a component is scheduled for termination, the component configurator
implements a two-phase protocol. The first phase notifies its registered client
component 'observers' to finish their computations. In the second phase, the
component configurator removes the component after all client components
acknowledge this notification. When a new component is initialized, the
component configurator re-notifies its registered client components to indicate
that they can connect to the new component.
Similarly, client components can register with the component configurator and
be notified when a particular component's execution is suspended and
resumed.
Implement the concrete components. Concrete component classes can be derived
from a common base class such as the Component class specified in implementation
activity 1 (82). They can also be implemented via a message-passing mechanism
that allows them to receive and process component control messages. Components
often implement other methods, such as establishing connections with remote peer
components and processing service requests received from clients. Component
implementations typically reside in DLLs, though they can also be linked statically
with the application.
Implementing concrete components involves three sub-activities:
0. Implement the concrete component concurrency model. An important aspect
of implementing a concrete component involves selecting the component's
concurrency strategy. For example, a component configured into an
application by a component configurator can be executed using event
demultiplexing patterns such as Reactor (179) or Proactor (215), or
concurrency patterns, such as Active Object (369), Monitor Object (399), Half Sync/Half-Async (423), or Leader/Followers (447):
??Reactive/proactive execution. Using these strategies, one thread of
control can be used to process all components reactively or proactively.
Components implemented using the Reactor pattern are relatively
straightforward to (re)configure and control, because race conditions are
minimized or eliminated. However, reactive components may not scale as
well as other strategies because they are single-threaded.
Conversely, components using the Proactor pattern may be more efficient
than reactive implementations on platforms that support asynchronous I/O
efficiently. However, it may be more complicated to reconfigure and
control proactive components, due to the subtleties of canceling
asynchronous operations. See the Proactor pattern's liability discussion
on page 258 for more details.
??Multi-threaded or multi-process concurrent execution. Using these
strategies, the configured components execute in their own threads or
processes after being initialized by a component configurator. For
instance, components can run concurrently using the Active Object
pattern (369), or execute within a pre-spawned pool of threads or
processes in accordance with the Leader/Followers (447) or Half Sync/Half-Async (423) patterns.
In general, executing components in one or more threads within the same
process as the component configurator may be more efficient than
running the components in separate processes. Conversely, configuring
components into separate processes may be more robust and secure,
because each component can be isolated from accidental corruption via
operating system and hardware protection mechanisms [Sch94].
1. Implement a mechanism for inter-component communication. Some
components run in complete isolation, whereas other components must
communicate with one another. In the latter case, component developers must
select a mechanism for inter-component communication.
The choice of mechanism is often guided by whether the communicating
components will be collocated or distributed:
??When components are collocated, the choice is typically between
hard-coding pointer relationships between components, which is inflexible
and can defeat the benefits of dynamic component configuration, versus
accessing components 'by name' using a component repository.
When components are distributed, the typical choice is between low level IPC mechanisms, such as TCP/IP connections programmed using
Sockets [Ste98] or TLI [Rago93], and higher-level mechanisms, such as
CORBA [OMG98a]. One of the benefits of using CORBA is that the ORB
can transparently optimize for the fastest IPC mechanism, by determining
automatically whether the component is collocated or distributed
[WSV99].
2. Implement a mechanism to re-establish component relationships. As outlined
in implementation activity 4.2 (93), components can use other components, or
even other objects in an application, to perform the services they offer.
Replacing one component implementation with another at run-time therefore
requires the component configurator to reconnect the new component
automatically with components used by the removed component.
One strategy for implementing this mechanism is to checkpoint a component's
references to its related components and store it in a Memento [GoF95]. This
memento can be passed to the component configurator before shutting down
the component. Similarly, the memento may contain additional state
information passed from the old to the new component. After the new
component is installed, the component configurator can pass the memento to
the new component. This component then re-installs the connections and
state information that were saved in the memento.
Implementing a mechanism to save and re-establish component
relationships would require three changes to our
Component_Configurator and Component classes:
??Define a Memento hierarchy. For every concrete component type,
define a memento that saves the references that the component type
can maintain to other components. A reference can be denoted either
by a component's name or by a pointer, as outlined in implementation
activity 4.2 (93). All mementos derive from an abstract memento. This
allows the Component_Configurator to handle arbitrary
mementos using polymorphism.
??Implement a mechanism for maintaining mementos in the component
configurator. During a component's reconfiguration, the memento
containing references to other components is stored in the
Component_Configurator. The corresponding infrastructure for
handling this memento within the Component_Configurator can
contain a reference to the memento, as well as the component type
whose references the memento stores.
??Change the component interface and implementation. To pass a
memento from a component to the Component_Configurator and
vice versa, we must change the Component interface. For example,
the memento can be passed to a Component as a parameter to its
init() method, and back to the Component_Configurator via a
parameter in the Component's fini() method. Within the init()
and fini() method implementations of concrete components, the
memento is then used to retrieve and save the component's
relationships to other components and objects.
In addition to component references, the memento could maintain other
state information that is passed to the new component. For example,
Clerk components could pass the frequency at which they poll time
servers, so that new Clerk components can update their local system
time at the same frequency.
In the remainder of this section, we show how the implementation activity 4
(92) and its sub-activities can be applied to guide the implementation of
concrete component participants in our distributed time service example.
There are two types of concrete components in a distributed time service:
Time_Server and Clerk. The Time_Server component receives and
processes requests for time updates from Clerks. Both Time_Server
and Clerk components are designed using the Acceptor-Connector
pattern (285). As outlined in implementation activity 1 (82), the component
execution mechanism for the Time_Server and Clerk is based on a
reactive event-handling model within a single thread of control, in
accordance with the Reactor pattern (179).
By inheriting from Component, Time_Server objects can be linked and
unlinked by the Component_Configurator dynamically. This design
decouples the implementation of the Time_Server from the time or
context when it is configured, allowing developers to switch readily
between different Time_Server algorithms.
Before storing the Time_Server component in its component repository,
the application's component configurator singleton invokes the
component's init() hook method. This allows the Time_Server
component to initialize itself.
Internally, the Time_Server contains a Time_Server_Acceptor that
listens for connection requests to arrive from Clerks. It also contains a
C++ standard template library [Aus98] list of Time_Server_Handlers
that process time update requests. The Time_Server_Acceptor is
created and registered with a reactor when the Time_Server's init()
method is called.
When a new connection request arrives from a Clerk, the acceptor
creates a new Time_Server_Handler, which processes subsequent
time update requests from the Clerk. When its init() method is
invoked by the Time_Server_Acceptor, each handler registers itself
with the singleton reactor, which subsequently dispatches the handler's
handle_event() method when time update requests arrive.
When a component configurator terminates a Time_Server, it calls the
Time_Server's fini() method. This method unregisters the
Time_Server_Acceptor and all of its associated
Time_Server_Handlers from the reactor and destroys them.
We provide two Time_Server component implementations:
??The first component implements Cristian's algorithm [Cris89]. In this
algorithm each Time_Server is a passive entity that responds to
queries made by Clerks. In particular, a Time_Server does not
query other machines actively to determine its own notion of time.
??The second component implements the Berkeley algorithm [GZ89]. In
this algorithm, the Time_Server is an active component that polls
every machine in the network periodically to determine its local time.
Based on the responses it receives, the Time_Server computes an
aggregate notion of the correct time.
By inheriting from Component, the Clerk can be linked and unlinked
dynamically by a component configurator. Similarly, a component
configurator can configure, control and reconfigure the Clerk it manages
by calling its init(), suspend(), resume(), and fini() hook
methods.
Our Clerk component establishes and maintains connections with
Time_Servers and queries them to calculate the current time. The
Clerk's init() method dynamically allocates Clerk_Handlers that
send time update requests to Time_Servers connected via a
Clerk_Connector. It also registers the Clerk with a reactor to receive
timeout events periodically, such as every five minutes.
When the timeout period elapses, the reactor notifies the Clerk's
handle_event() hook method. This method instructs the Clerk's
Clerk_Handlers to request the current time at the time servers to which
they are connected. The Clerk receives and processes these server
replies, then updates its local system time accordingly. When Clients
ask the Clerk component for the current time, they receive a locally cached time value that has been synchronized with the global notion of
time. The Clerk's fini() method shuts down and cleans up its
connector and handlers.
The two alternative implementations of the time services are provided
within two DLLs. The cristian.dll contains a factory that creates
components that run the Cristian algorithm. Likewise, the berkeley.dll
contains a factory that creates components that run the Berkeley
algorithm.
In this section, we show how our example distributed time server implementation applies the
Component Configurator pattern using a configuration mechanism based on explicit dynamic
linking [SchSu94] and a comp.conf configuration script. The example is presented as
follows:
??We first show how the configuration mechanism supports the dynamic configuration of
Clerk and Time_Server components into application processes via scripting.
??We then show how these features allow Clerk components to change the algorithms
used to compute local system time. In particular, after a new algorithm has been
selected, a singleton Component_Configurator can reconfigure the Clerk
component dynamically without affecting the execution of other types of components
controlled by the component configurator.
There are two general strategies for configuring a distributed time component application:
collocated and distributed. We outline each strategy to illustrate how a component
configurator-enabled application can be dynamically (re)configured and run.
Collocated configuration. This configuration uses a comp.conf script to collocate the
Time_Server and the Clerk within the same process.
The directives in comp.conf specify to the Component_Configurator how to configure
a collocated Time_Server and Clerk dynamically in the same application process using
the Cristian algorithm. The Component_Configurator links the cristian.dll DLL into
the application's address space dynamically and invokes the appropriate factory function to
create new component instances. In our example, these factory functions are called
make_Time_Server() and make_Clerk(), which are defined as follows:
After each factory function returns its new allocated component, the designated initialization
parameters in the comp.conf script are passed to the respective init() hook methods.
These perform the corresponding component-specific initialization, as illustrated in
implementation activity 4 (92).
Distributed configuration. To reduce the memory footprint of an application, we may want to
collocate the Time_Server and the Clerk in different processes. Due to the flexibility of
the Component Configurator pattern, all that is required to distribute these components is to
split the comp.conf script into two parts and run them in separate processes or hosts. One
process contains the Time_ Server component and the other process contains the Clerk
component.
The figure below shows what the configuration looks like with the Time_Server and Clerk
collocated in the same process, as well as the new configuration after the reconfiguration
split. Note that the components themselves need not change, because the Component
Configurator pattern decouples their processing behavior from the point in time when they
are configured.
Reconfiguring an application's components. Now consider what happens if we decide to
change the algorithms that implement components in the distributed time service. For
example, we may need to switch from Cristian's algorithm to the Berkeley algorithm to take
advantage of new features in the environment. For example, if the machine on which the
Time_Server resides has a WWV receiver, the Time_Server can act as a passive entity
and the Cristian algorithm may be appropriate. Conversely, if the machine on which the
Time_Server resides does not have a WWV receiver, an implementation of the Berkeley
algorithm may be more appropriate.
Ideally, we should be able to change Time_Server algorithm implementations without
affecting the execution of other components of the distributed time service. Accomplishing
this using the Component Configurator pattern simply requires minor modifications to our
distributed time service configuration activities:
This directive instructs the Component_Configurator to shut down the
Time_Server component, remove it from the Component_Repository, and unlink
the cristian.dll if there are no more references to it.
4. Notify the component configurator to reinterpret the comp.conf script. Next we must
instruct the Component_Configurator to process the updated comp.conf script.
This can be triggered either in-band, such as via a Socket connection or a CORBA
operation, or out-of-band, such as via a UNIX SIGHUP signal. Regardless of which
triggering strategy is used, after the Component_Configurator receives a
reconfiguration event, it consults its comp.conf script again and shuts down the
Time_Server component by calling its fini() method. During this step the
execution of other components should be unaffected.
5. Initiate reconfiguration. We can now repeat steps 1 and 2 to reconfigure the Berkeley
Time_Server component implementation into an application. The comp.conf script
must be modified with a new directive to specify that the Berkeley Time_Server
component be linked dynamically from the berkeley.dll DLL:
Finally, an event is generated to trigger the Component_Configurator in the
process to reread its comp.conf script and add the updated Time_Server
component to the Component_Repository. This component starts executing
immediately after its init() method is invoked successfully.
The ease with which new component implementations can be replaced dynamically
exemplifies the flexibility and extensibility provided by the Component Configurator pattern.
In particular, no other configured components in an application should be affected when the
Component_Configurator removes or reconfigures the Time_Server component.
The Windows NT Service Control Manager (SCM). The SCM allows a master SCM process
to initiate and control administrator-installed service components automatically using the
message-based strategy described in the Implementation section. The master SCM process
initiates and manages system service components by passing them various control
messages, such as PAUSE, RESUME, and TERMINATE, that must be handled by each
service component. SCM-based service components run as separate threads within either a
single-service or a multi-service server process. Each installed service component is
responsible for configuring itself and monitoring any communication endpoints, which can be
more general than socket ports. For instance, the SCM can control named pipes and shared
memory.
Modern operating system device drivers. Most modern operating systems, such as Solaris,
Linux, and Windows NT, provide support for dynamically-configured kernel-level device
drivers. These drivers can be linked into and unlinked out of the system dynamically via
hooks, such as the init(), fini(), and info() functions defined in SVR4 UNIX
[Rago93]. These operating systems apply the Component Configurator pattern to allow
administrators to reconfigure the operating system kernel without having to shut it down,
recompile, and statically relink new drivers and restart it.
Java applets. The applet mechanism in Java supports dynamic downloading, initializing,
starting, stopping, and terminating of Java applets. Web browsers implement the
infrastructure software to actually download applets and prepare them for execution. The
class java.applet.Applet provides empty methods init(), start(), stop(), and
destroy(), to be overridden in application-specific subclasses. Java therefore uses the
inheritance-based strategy described in the Implementation section. The four life-cycle hook
methods mentioned above are called by the browser at the correct time. They give the
applet a chance to provide custom behavior that will be called at appropriate times.
For example, the init() hook will be called by the browser once the applet is loaded. The
start() hook will be called once set-up is complete and the applet should start its
application logic. The stop() hook will be called when the user leaves the Web site. Note
that start() and stop() can be called repeatedly, for example when the user visits and
leaves a Web site multiple times. The destroy() hook is called once the applet is
reclaimed and should free all resources. Finer-grained life-cycle behavior inside an applet
can be achieved by creating multiple threads inside the applet and having them scheduled
as in ordinary Java applications. Additional examples of how the Component Configurator
pattern is used for Java applets are presented in [JS97b].
The dynamicTAO reflective ORB [KRL+00] implements a collection of component
configurators that allow the transfer of components across a distributed system, loading and
unloading modules into the ORB run-time system, and inspecting and modifying the ORB
configuration state. Each component configurator is responsible for handling the
(re)configuration of a particular aspect of dynamicTAO. For example, its TAOConfigurator
component configurator contains hooks to which implementations of concurrency and
scheduling strategies, as well as security and monitoring interceptors (109), can be attached.
In addition, a DomainConfigurator provides common services for loading and unloading
components into dynamicTAO. It is the base class from which all other component
configurators derive, such as TAOConfigurator.
ACE [Sch97]. The ADAPTIVE Communication Environment (ACE) framework provides a set
of C++ mechanisms for configuring and controlling components dynamically using the
inheritance-based strategy described in the Implementation section. The ACE Service
Configurator framework [SchSu94] extends the mechanisms provided by Inetd, Listen,
and SCM to support automatic dynamic linking and unlinking of communication service
components.
The Service Configurator framework provided by ACE was influenced by the mechanisms
and patterns used to configure and control device drivers in modern operating systems.
Rather than targeting kernel-level device drivers, however, ACE focuses on dynamic
configuration and control of application-level components. These ACE components are often
used in conjunction with the Reactor (179), Acceptor-Connector (285), and Active Object
(369) patterns to implement communication services.
In football, which Americans call soccer, each team's coach can substitute a limited number
of players during a match. The coach is the component configurator who decides which
players to substitute, and the players embody the role of components. All players obey the
same protocol with respect to substitution, which occurs dynamically, that is, the game does
not stop during the substitutions. When players see a sign waved with their numbers, they
leave the field and new players join the game immediately. The coach's list of the current 11
players corresponds to the Component Repository. Just as the reconfiguration script is not
always written by the coach: some home crowds are renowned for asking and shouting for
specific players to be put into the game¡ªand for firing the coach.
The Component Configurator pattern offers the following benefits:
Uniformity. The Component Configurator pattern imposes a uniform configuration and
control interface for managing components. This uniformity allows components to be treated
as building blocks that can be integrated as components into a larger application. Enforcing
a common interface across all components makes them 'look and feel' the same with
respect to their configuration activities, which simplifies application development by
promoting the 'principle of least surprise'.
Centralized administration. The Component Configurator pattern groups one or more
components into a single administrative unit. This consolidation simplifies development by
enabling common component initialization and termination activities, such as
opening/closing files and acquiring/releasing locks, to be performed automatically. In
addition, the pattern centralizes the administration of components by ensuring that each
component supports the same configuration management operations, such as init(),
suspend(), resume(), and fini().
Modularity, testability, and reusability. The Component Configurator pattern improves
application modularity and reusability by decoupling the implementation of components from
the manner in which the components are configured into processes. Because all
components have a uniform configuration and control interface, monolithic applications can
be decomposed more easily into reusable components that can be developed and tested
independently. This separation of concerns encourages greater reuse and simplifies
development of subsequent components.
Configuration dynamism and control. The Component Configurator pattern enables a
component to be dynamically reconfigured without modifying, recompiling, or statically
relinking existing code. In addition, (re)configuration of a component can often be performed
without restarting the component or other active components with which it is collocated.
These features help create an infrastructure for application-defined component configuration
frameworks.
Tuning and optimization. The Component Configurator pattern increases the range of
component configuration alternatives available to developers by decoupling component
functionality from component execution mechanisms. For instance, developers can tune
server concurrency strategies adaptively to match client demands and available operating
system processing resources. Common execution alternatives include spawning a thread or
process upon the arrival of a client request or pre-spawning a thread or process at
component creation time.
The Component Configurator pattern has several liabilities:
Lack of determinism and ordering dependencies. The Component Configurator pattern
makes it hard to determine or analyze the behavior of an application until its components are
configured at runtime. This can be problematic for certain types of system, particularly real time systems, because a dynamically-configured component may not behave predictably
when run with certain other components. For example, a newly configured component may
consume excessive CPU cycles, thereby starving other components of processing time and
causing them to miss deadlines.
Reduced security or reliability. An application that uses the Component Configurator pattern
may be less secure or reliable than an equivalent statically-configured application. It may be
less secure because impostors can masquerade as components in DLLs. It may be less
reliable because a particular component configuration may adversely affect component
execution. A faulty component may crash, for example, corrupting state information it shares
with other components configured into the same process.
Increased run-time overhead and infrastructure complexity. The Component Configurator
pattern adds levels of abstraction and indirection when executing components. For example,
the component configurator first initializes components and then links them into the
component repository, which may incur excessive overhead in time-critical applications. In
addition, when dynamic linking is used to implement components many compilers add extra
levels of indirection to invoke methods and access global variables [GLDW87].
Overly narrow common interfaces. The initialization or termination of a component may be
too complicated or too tightly coupled with its context to be performed in a uniform manner
via common component control interfaces, such as init() and fini().
The intent of the Component Configurator pattern is similar to the Configuration pattern
[CMP95]. The Configuration pattern decouples structural issues related to configuring
protocols and services in distributed applications from the execution of the protocols and
services themselves. The Configuration pattern has been used in frameworks that support
the construction of distributed systems out of building-block components.
In a similar way, the Component Configurator pattern decouples component initialization
from component processing. The primary difference is that the Configuration pattern focuses
on the active composition of chains of related protocols and services. In contrast, the
Component Configurator pattern focuses on the dynamic initialization of components that
process requests exchanged between transport endpoints.
##%%&&
The Interceptor architectural pattern allows services to be added transparently to a
framework and triggered automatically when certain events occur.
MiddleSoft Inc. is developing an object request broker (ORB) middleware framework called
MiddleORB, which is an implementation of the Broker pattern [POSA1]. MiddleORB provides
communication services that simplify the development of distributed applications. In addition
to core communication services, such as connection management and transport protocols,
applications using MiddleORB may require other services, such as transactions and security,
load balancing and fault tolerance, auditing, and logging, non-standard communication
mechanisms like shared memory, and monitoring and debugging tools.
To satisfy a wide-range of application demands, the MiddleORB architecture must support
the integration of these extended services. One strategy for coping [Cope98] with this
requirement is to integrate as many services as possible into the default MiddleORB
configuration. This strategy is often infeasible, however, because not all ORB services can
be anticipated at its development time. As distributed applications evolved, the ORB
framework would inevitably expand to include new features. Such piecemeal growth can
complicate ORB design and maintenance, as well as increase its memory footprint, even
though many of these features are not used by all applications all the time.
An alternative strategy is to keep the MiddleORB framework as simple and concise as
possible. In this model, if application developers require services not available in the
framework, they would implement them along with their own client and server code.
However, this strategy would require developers to implement much code that was unrelated
to their application logic.
In addition, certain services cannot be implemented solely at the application client and object
level, because they must interact intimately with core ORB features. For example, a security
service should be integrated with the ORB infrastructure. Otherwise, applications can
masquerade as privileged users and gain unauthorized access to protected system
resources.
Clearly, neither strategy outlined above is entirely satisfactory. With the first strategy
MiddleORB will be too large and inflexible, whereas with the second, applications will
become overly complex and potentially insecure or error-prone. We must therefore devise a
better strategy for integrating application-specific services into MiddleORB.
Developing frameworks that can be extended transparently.
Frameworks, such as ORBs, application servers, and domain-specific software architectures
[SG96], cannot anticipate all the services they must offer to users. It may also not be feasible
to extend certain types of frameworks, particularly black-box frameworks [HJE95], with new
services that they were not originally designed to support. Similarly, it is often undesirable to
rely upon applications to implement all the necessary services themselves, because this
defeats many benefits of reuse. Framework developers must therefore address the following
three forces:
??A framework should allow integration of additional services without requiring
modifications to its core architecture.
?For example, it should be possible to extend MiddleORB to support security services,
such as Kerberos or SSL [OSSL00], without modifying the structure of its internal
design [OMG98d].
??The integration of application-specific services into a framework should not affect
existing framework components, nor should it require changes to the design or
implementation of existing applications that use the framework.
?For instance, adding load balancing to MiddleORB should be unobtrusive to existing
MiddleORB client and server applications.
??Applications using a framework may need to monitor and control its behavior.
?For example, some applications may want to control MiddleORB's fault tolerance
strategies [OMG99g] via the Reflection pattern [POSA1] to direct its responses to
failure conditions.
Allow applications to extend a framework transparently by registering 'out-of-band' services
with the framework via predefined interfaces, then let the framework trigger these services
automatically when certain events occur.[4] In addition, open the framework's implementation
[Kic92] so that the out-of-band services can access and control certain aspects of the
framework's behavior.
In detail: for a designated set of events processed by a framework, specify and expose an
interceptor callback interface. Applications can derive concrete interceptors from this
interface to implement out-of-band services that process occurrences of these events in an
application-specific manner. Provide a dispatcher for each interceptor that allows
applications to register their concrete interceptors with the framework. When the designated
events occur, the framework notifies the appropriate dispatchers to invoke the callbacks of
the registered concrete interceptors.
Define context objects to allow a concrete interceptor to introspect and control certain
aspects of the framework's internal state and behavior in response to events. Context
objects provide methods to access and modify a framework's internal state, thus opening its
implementation. Context objects can be passed to concrete interceptors when they are
dispatched by the framework.
A concrete framework instantiates a generic and extensible architecture to define the
services provided by a particular system, such as an ORB, a Web server, or an application
server.
Two types of concrete frameworks are available in MiddleORB, one for the client and
one for the server:[5]
?Client applications use the client concrete ORB framework's programming interface to
access remote objects. This concrete framework provides common services, such
as binding to a remote object, sending requests to the object, waiting for replies,
and returning them to the client.
?The server concrete ORB framework provides complementary services, including
registering and managing object implementations, listening on transport endpoints,
receiving requests, dispatching these requests to object implementations, and
returning replies to clients.
Interceptors are associated with a particular event or set of events exposed by a concrete
framework. An interceptor defines the signatures of hook methods [Pree95] [GHJV95] that
the concrete framework will invoke automatically via a designated dispatching mechanism
when the corresponding events occur. Concrete interceptors specialize interceptor interfaces
and implement their hook methods to handle these events in an application-specific manner.
In our MiddleORB example, we specify an interceptor interface containing several hook
methods that the client and server concrete ORB frameworks dispatch automatically
when a client application invokes a remote operation and the corresponding server
receives the new request, respectively.
To allow interceptors to handle the occurrence of particular events, a concrete framework
defines dispatchers for configuring and triggering concrete interceptors. Typically there is a
dispatcher for each interceptor. A dispatcher defines registration and removal methods that
applications use to subscribe and un-subscribe concrete interceptors with the concrete
framework.
A dispatcher also defines another interface that the concrete framework calls when specific
events occur for which concrete interceptors have registered. When the concrete framework
notifies a dispatcher that such an event has occurred, the dispatcher invokes all the concrete
interceptor callbacks that have registered for it. A dispatcher maintains all its registered
interceptors in a container.
In our MiddleORB example, the client concrete ORB framework implements a dispatcher
that allows client applications to intercept certain events, such as outgoing requests to
remote objects and incoming object replies. Servers use a corresponding dispatcher in
the server concrete ORB framework to intercept related events, such as incoming client
requests and outgoing object replies. Other dispatchers can be defined at different
layers in the ORB to intercept other types of events such as connection and message
transport events.
Concrete interceptors can use context objects to access and control certain aspects of a
concrete framework. Context objects can provide accessor methods to obtain information
from the concrete framework and mutator methods to control the behavior of the concrete
framework. A context object can be instantiated by a concrete framework and passed to a
concrete interceptor with each callback invocation. In this case the context object can
contain information related to the event that triggered its creation.
Conversely, a context object can be passed to an interceptor when it registers with a
dispatcher. This design provides less information but also incurs less overhead.
In our MiddleORB example, the interceptor interface defines methods that the client
concrete ORB framework dispatches automatically when it processes an outgoing
request. These methods are passed a context object parameter containing information
about the current request. Each context object defines accessor and mutator methods
that allow a concrete interceptor to query and change ORB state and behavior,
respectively.
For example, an accessor method in a context object can return the arguments for a
remote operation. Using the context object's mutator methods, a client application's
concrete interceptor can redirect an operation to a different object. This feature can be
used to implement custom load balancing and fault tolerance services [ZBS97].
An application runs on top of a concrete framework and reuses the services it provides. An
application can also implement concrete interceptors and register them with the concrete
framework to handle certain events. When these events occur, they trigger the concrete
framework and its dispatchers to invoke concrete interceptor callbacks that perform
application-specific event processing.
The class diagram below illustrates the structure of participants in the Interceptor pattern.
A typical scenario for the Interceptor pattern illustrates how an application implements a
concrete interceptor and registers it with the corresponding dispatcher. The dispatcher then
invokes the interceptor callback when the concrete framework notifies it that an event of
interest has occurred:
??An application instantiates a concrete interceptor that implements a specific interceptor
interface. The application registers this concrete interceptor with the appropriate
dispatcher.
??The concrete framework subsequently receives an event that is subject to interception.
In this scenario a special context object is available for each kind of event. The concrete
framework therefore instantiates an event-specific context object that contains
information related to the event, as well as functionality to access and potentially control
the concrete framework.
??The concrete framework notifies the appropriate dispatcher about the occurrence of
the event, passing the context object as a parameter.
??The dispatcher iterates through its container of registered concrete interceptors and
invokes their callback hook methods, passing the context object as an argument.
??Each concrete interceptor can use its context object to retrieve information about the
event or the concrete framework. After processing this information, a concrete
interceptor can optionally call method(s) on the context object to control the behavior of
the concrete framework and its subsequent event processing.
??After all concrete interceptor callback methods have returned, the concrete framework
continues with its normal operation.
Seven implementation activities describe a common approach for implementing the
Interceptor pattern.
1. Model the internal behavior of the concrete framework using a state machine or an
equivalent notation, if such a model is not available already. This modeling need not
capture all abstractions of the concrete framework, but should document the aspects
that are related to interception. To minimize the complexity of any given state
machine, the modeled parts of the concrete framework can be composed from
smaller state machines that together form a composite state machine.[6]
Each smaller state machine represents a particular aspect of the concrete framework.
Once the dynamic aspects of the concrete framework are modeled as a state
machine, use this model to determine where and when certain events can be
intercepted.
In ORB middleware and many other component-based systems at least two types of
concrete frameworks exist, one for the role of client and one for the role of server. In
this case the concrete frameworks should be modeled as separate state machines. In
general, state machine modeling helps identify where to place interceptors and how
to define their behavior in a concrete framework.
Consider the client concrete ORB framework defined by MiddleORB. During ORB
start-up this framework is initialized to continue processing client requests until it
is shut down. The client concrete ORB framework provides two types of service
to clients:
??When a client binds to a new remote object, the concrete framework creates a
proxy that connects to the object.
??If the bind operation is successful the client can send requests to the remote
object. Each request is marshaled and delivered to the remote object using a
pre-established connection. After successful delivery, the concrete
framework waits for the object's response message, demarshals it upon
arrival, returns the result to the client, and transitions to the idle state.
Additional error states denote situations in which problems are encountered, such
as communication errors or marshaling errors, are shown in the following figure.
Note that this figure illustrates only a portion of the client concrete ORB
framework's internal composite state machine.
2. Identify and model interception points. This implementation activity can be divided into
four sub-activities:
1. Identify concrete framework state transitions that may not be visible to external
applications, but are subject to interception. For example, a client may want to
intercept outgoing requests so it can add functionality, such as logging or
changing certain request parameters, dynamically. We call these state
transitions 'interception points'.
2. Partition interception points into reader and writer sets. The reader set includes
all state transitions in which applications only access information from the
concrete framework. Conversely the writer set includes all state transitions in
which applications can modify the behavior of the concrete framework.
3. Integrate interception points into the state machine model. Interception points
can be modeled in the state machine by introducing intermediary states. If a
state transition is subject to interception, place a new interception state
between the source state and the sink state of the original transition. This
interception state triggers the corresponding interceptors. For interception
points that belong to the writer set, introduce additional state transitions in
which the following properties apply:
??The interception state is the start node and
??The target nodes are states that represent the subsequent behavior of
the concrete framework after the interception.
Many component-based distributed systems define peer concrete frameworks,
such as client and server ORBs, that are organized in accordance with the
Layers pattern [POSA1]. When identifying interception points in one of these
concrete frameworks, introduce a related interception point in the other peer
concrete framework at the same logical layer. For example, if a client ORB
intercepts outgoing requests, it is likely that the server ORB should also
intercept incoming requests. When integrating layered services, such as
adding security tokens on the client-side and encrypting outgoing request
data, a corresponding interceptor is therefore required on the server to extract
the security token and decrypt the incoming data.
By applying the state machine model of the client concrete ORB
framework shown above, we can identify the potential interception points
shown in the following table:
Additional interception points may be required if a client intercepts
exceptions, such as failed connection events. The server concrete ORB
framework can also define peer interception points.
4. Partition interception points into disjoint interception groups. To process
events, concrete frameworks often perform a series of related activities, each
of which may be associated with an interception point. To emphasize the
relationship between each activity, it may be useful to coalesce a series of
semantically-related interception points into an interception group.
For example, all interception points associated with sending a request can
form one interception group, whereas all interception points associated with
receiving a request can form another group. These interception groups help to
minimize the number of necessary interceptors and dispatchers as shown in
implementation activity 4 (123).
To identify interception groups, analyze the state machine for interception
points that are located in the same area of the state machine and participate in
the same activity. For example, interception points that are triggered by
transitions originating from a particular state, ending in a particular state, or
ending in a particular set of neighbor states may be candidates for
consideration as part of the same interception group.
In MiddleORB, both the PreMarshalOutRequest and
PostMarshalOutRequest interception points participate in sending a
request. These interception points can therefore constitute the
OutRequest interception group. This interception group coalesces all
events related to the activities of sending a request in order to differentiate
these events from other interception groups, such as InRequest,
OutReply, or InReply.
3. Specify the context objects. Context objects allow interceptors to access and control
aspects of the framework's internal state and behavior in response to certain events.
Three sub-activities can be applied to specify context objects:
0. Determine the context object semantics. Context objects provide information
about an interception point and may also define services to control the
framework's subsequent behavior. Concrete interceptors use the information
and services to handle interception points in an application-specific manner.
The accessor and mutator methods defined for context objects can be based
on information that a concrete framework provides to interceptors, as well as
the degree to which a framework is 'open':
??If an interception point belongs to the reader set, determine what
information the concrete framework should provide the interceptor for
each event it handles. For example, if a context object provides
information about a particular remote operation invocation, it may contain
the reference of the target object being called as well as the operation's
name and parameter values.
??If the interception point belongs to the writer set, determine how to
'open' the concrete framework's implementation so that concrete
interceptors can control selected aspects of its behavior [Kic92]. For
example, if a context object provides information about a particular
remote operation invocation, it may contain methods that can modify the
operation's parameter values. The design force to balance here, of
course, is 'open extensibility' versus 'errorprone interception code'.
Although concrete frameworks with open implementations can ha ve
powerful interceptors, they are also more vulnerable to interceptors that
maliciously or accidentally corrupt the concrete framework's robustness
and security. Some interception designs therefore disallow mutator
functionality within context objects.
1. Determine the number of context object types. Here are two strategies for
selecting the number and types of context objects:
??Multiple interfaces. If the interception points in a concrete framework
cover a diverse set of requirements, different types of context objects can
be defined for different interception points. This strategy is flexible,
because it allows fine-grained control of particular interception points.
However it increases the number of interfaces that developers of concrete
interceptors must understand.
??Single interface. It is possible to specify a generic context object with a
single interface. Using a single interface reduces the number of context
object interfaces, but may yield a bloated and complex context object
interface.
In general, multiple interfaces are useful when client applications intercept a
wide variety of different framework events. In other cases, however, the single
interface strategy may be preferable due to its simplicity.
When the MiddleORB client ORB framework intercepts outgoing client
requests, applications may want to access and/or control the following
aspects:
??Reading and changing the target object reference to implement fault
tolerance or load balancing.
??Reading and modifying parameter values to encrypt data, validate
selected arguments, or change behavior reflectively [POSA1].
??Adding new data to the request to send out-of-band information, such
as security tokens or transaction contexts.
??Integrating custom parameter marshalers and demarshalers.
Define how to pass context objects to concrete interceptors. Context objects
are instantiated by the concrete framework. They are passed to a concrete
interceptor using one of the following two strategies:
??Per-registration. In this strategy a context object is passed to an
interceptor once when it registers with a dispatcher.
??Per-event. In this strategy a context object is passed to a concrete
interceptor with every callback invocation.
The per-event strategy allows a concrete framework to provide finegrained
information about the occurrence of a particular event. In contrast, the per registration strategy only provides general information common to all
occurrences of a particular event type. The per-event strategy may incur
higher overhead, however, due to repeated creation and deletion of context
objects.
4. Specify the interceptors. An interceptor defines a generic interface that a concrete
framework uses to invoke concrete interceptors, via dispatchers, when interception
points are triggered. An interceptor is defined for each interception group identified in
implementation activity 2.4 (120). Consequently each concrete interceptor that
derives from a particular interceptor is responsible for handling all the interception
points of a specific interception group.
For each interception point in an interception group, an interceptor defines a
designated callback hook method. There is thus a one-to-one relationship between
an interception point and an interceptor hook method. In general the interceptor
corresponds to the observer participant in the Observer pattern [GoF95], where its
callback hook methods play the role of event-specific update methods. If the
'perevent' context object strategy described in implementation activity 3 (121) is
applied, context objects can be passed as parameters to the concrete interceptor
callback hook methods. These methods can return results or raise exceptions, in
accordance with the policies described in implementation activity 6 (126).
Specify the dispatchers. For each interceptor, define a dispatcher interface that
applications can use to register and remove concrete interceptors with the concrete
framework. In addition, this interface is used by the framework to dispatch concrete
interceptors registered at interception points. Two sub-activities are involved:
0. Specify the interceptor registration interface. A dispatcher corresponds to the
Observer pattern's [GoF95] subject role. It implements a registration interface
for interceptors, which correspond to the observer role. Applications pass a
reference to a concrete interceptor to the registration method, which stores the
reference in a container in accordance with the Manager pattern [Som97].
To implement different callback policies, an application can pass a dispatcher
additional parameters. For example, it can pass a priority value that
determines the invocation order when multiple interceptors are registered for
the same interception point, as described in implementation activity 6 (126).
The dispatcher returns a key to the application that identifies the registered
interceptor uniquely. An application passes this key to the dispatcher when it
removes an interceptor it registered previously.
To automate interceptor registration, and to hide its implementation, a
concrete framework can implement helper classes that provide 'noop'
implementations of interceptor interfaces. The constructors of these classes
register instances automatically with the concrete framework. Applications
derive their concrete interceptor implementations from the appropriate helper
class, override its methods and call the base class constructor to register their
interceptors implicitly.
In general, a specific dispatcher can forward every occurrence of its
corresponding event types from the concrete framework to the concrete
interceptors that registered for these events. Dispatchers are therefore often
implemented using the Singleton pattern [GoF95].
Specify the dispatcher callback interface. When an interception event occurs
the concrete framework notifies its dispatcher. When notified, a dispatcher
invokes the corresponding hook methods of its registered concrete
interceptors. A dispatcher often provides the same interface to the concrete
framework that its associated interceptor provides to the dispatcher.
There are two reasons for this similarity:
??It streamlines performance, by allowing a dispatcher to delegate event
notifications to its registered interceptors efficiently, without transforming
any parameters.
??It localizes and minimizes the modifications required if the public
interface of the dispatcher changes. An example of such a modification
might be the addition of a new interception point to the interception group
associated with the dispatcher callback interface. In this case an
additional hook method would be added to the callback interface.
Implement the callback mechanisms in the concrete framework. When an interception
event occurs the concrete framework notifies the corresponding dispatcher. The
dispatcher then invokes the hook methods of all registered concrete interceptor
callbacks in turn. A mechanism is therefore needed to propagate events from the
concrete framework to its dispatchers and from the dispatchers to the registered
interceptors. This mechanism can be implemented by applying the Observer pattern
[GoF95] twice.
The first application of the Observer pattern occurs whenever the concrete framework
reaches an interception point. At this point it creates the appropriate context object
and notifies the dispatcher about the occurrence of the event. In terms of the
Observer pattern, the concrete framework is a subject that is observed by a
dispatcher.
When the concrete framework notifies the dispatcher, it can either pass the context
object as a parameter, or it can use a pre-allocated singleton context object that acts
as an interface to the concrete framework. In the first strategy, all event-related
information is encapsulated in the context object, while the second strategy requires
the concrete framework to store all of the necessary information. The choice of
strategy depends on the design of the concrete framework, as described in
implementation activity 3.3 (123).
The second application of the Observer pattern occurs after the dispatcher is notified.
At this point it iterates over all interceptors that have registered at this interception
point and invokes the appropriate callback method in their interface, passing the
context object as a parameter. The dispatcher is thus also a subject that is observed
by concrete interceptors.
The dispatcher's internal callback mechanism can be implemented with the Iterator
pattern [GoF95]. Similarly, a dispatcher can apply the Strategy pattern [GoF95] to
allow applications to select from among several interceptor callback orderings:
??Simple invocation strategies include 'first-in first-out' (FIFO) or 'last-in first-out'
(LIFO) ordering strategies, where interceptors are invoked in the order they were
registered or vice-versa. When using the Interceptor pattern to implement a
particular 'interceptor stack', a combined FIFO/LIFO approach can be used to
process messages traversing the stack. On the client a FIFO strategy can be
used to pass messages down the stack. On the server a LIFO strategy can be
used to pass messages up the stack.
??A more sophisticated ordering callback strategy dispatches concrete
interceptors in priority order. In this strategy an application passes a priority
parameter when registering a concrete interceptor with a dispatcher. When
propagating an event, the dispatcher invokes interceptors with higher priorities
first.
??Another sophisticated callback strategy is based on the Chain of Responsibility
pattern [GoF95]. If a concrete interceptor can handle the event that its dispatcher
delivers, it returns the corresponding result. Otherwise it can return a special
value or raise an exception to indicate it is not interested in intercepting the
event. In this case the callback dispatching mechanism asks the next interceptor
in the chain to handle the event. This progression stops after one of the
interceptors handles the event.
If an interceptor encounters error conditions that prevent it from completing its work
successfully, it can invoke exceptions or return failure values to propagate these
errors to handlers. In this case the concrete framework must be prepared to handle
these errors.
Implement the concrete interceptors. Concrete interceptors can derive from and
implement the corresponding interceptor interface in application-specific ways. A
concrete interceptor can use the context object it receives as a parameter to either:
??Obtain additional information about the event that occurred or
??Control the subsequent behavior of the concrete framework, as described in
implementation activity 3 (121)
The Extension Interface pattern (141) can be applied to minimize the number of
different interceptor types in an application. Each interception interface becomes an
extension interface of a single interceptor object. The same 'physical' object can thus
be used to implement different 'logical' interceptors.
Applications can use the Interceptor pattern to integrate a customized load-balancing
mechanism into MiddleORB. By using interceptors, this mechanism is transparent to the
client application, the server application, and the ORB infrastructure itself. In this example a
pair of concrete interceptors are interposed by the client application:
??Bind interceptor. When a client binds to a remote object, the bind interceptor
determines whether subsequent invocations on the CORBA object should be load
balanced. All such 'load balancing' objects can be replicated [GS97] automatically on
predefined server machines. Information on load balancing, servers, and available
replicated objects can be maintained in the ORB's Implementation Repository [Hen98]
and cached within memory-resident tables. Information on the current system load can
reside in separate tables.
??Client request interceptor. When a client invokes an operation on a remote object, the
client request concrete interceptor is dispatched. This interceptor checks whether the
object is replicated. If it is, the interceptor finds a server machine with a light load and
forwards the request to the appropriate target object. The algorithm for measuring the
current load can be configured using the Strategy pattern [GoF95]. Client developers
can thus substitute their own algorithms transparently without affecting the ORB
infrastructure or the client/server application logic.
The following diagram illustrates the scenario executed by the client request interceptor after
the bind interceptor has replicated an object that is load balanced on multiple servers:
This scenario involves three steps:
??A client invokes an operation on a replicated object (1).
??The client request interceptor intercepts this request (2). It then consults a table
containing the object's replicas to identify a server with a lightest load (3). The bind
interceptor created this table earlier when the object was replicated.
??The client ORB forwards the request to the server with a light load (4). The server's
ORB then delivers it to the object implementation residing on this server (5) and
dispatches its operation (6).
Interceptor Proxy variant (also known as Delegator). This variant is often used on the server side of a distributed system to intercept remote operations. The server concrete framework
automatically instantiates a proxy [POSA1] to a local object implementation residing on the
server. This proxy implements the same interfaces as the object. When the proxy is
instantiated it receives a reference to the actual server object.
When a client issues a request, the server's proxy intercepts the incoming request and
performs certain pre-processing functionality, such as starting a new transaction or validating
a security tokens. The proxy then forwards the request to the local server object, which
performs its process operations in the context established by the proxy:
After the object processing is finished, the proxy performs any post-processing that is
needed and returns the result, if any, to the client. Both the client and the server object are
oblivious to the existence of the interceptor proxy.
Single Interceptor-per-Dispatcher. This variant allows only one interceptor to register with a
specific dispatcher. This restriction can simplify the pattern's implementation when it makes
no sense to have more than one interceptor, in which case there is no need for the concrete
framework to retain a whole collection of interceptors.
In MiddleORB there could be an interceptor interface for changing the concrete
framework's transport protocol dynamically [Naka00]. At most there should be one
interceptor that changes the default behavior of the concrete framework. Thus, there is
no reason to register a chain of different interceptors that are each responsible for
changing the transport protocol.
Interceptor Factory. This variant is applicable when the concrete framework instantiates the
same class multiple times and each instance of the class is subject to interception. Instead
of registering an interceptor for each object with the dispatcher explicitly, applications
register interceptor factories with the concrete framework. Thus, for every object the
concrete framework instantiates, it also instantiates a concrete interceptor using the supplied
factory.
In MiddleORB there could be a different interceptor for each object implementation
created by the server concrete ORB framework. In addition the client concrete ORB
framework could use a factory to instantiate a separate client interceptor for each proxy.
Implicit Interceptor Registration. Rather than registering interceptors via dispatchers
explicitly, a concrete framework can load interceptors dynamically. There are two ways to
implement this strategy:
??The concrete framework searches for interceptor libraries in predefined locations. It
then loads these libraries into the concrete framework and ensures that they support the
required interceptor interfaces before installing and dispatching events to them.
??The concrete framework can link interceptors dynamically using a run-time
configuration mechanism, such as the one defined by the Component Configurator
pattern (75). In this design a component configurator component within the concrete
framework interprets a script that specifies which interceptors to link, where to find the
dynamically linked libraries (DLLs) that contain these interceptors, and how to initialize
them. The component configurator then links the specified DLLs and registers the
interceptors contained within them with the concrete framework.
Component-based application servers for server-side components, such as EJB
[MaHa99], CORBA Components [OMG99a], or COM+ [Box97], implement the Interceptor
Proxy variant. To help developers focus on their application-specific business logic, special
concrete frameworks¡ªoften denoted as 'containers' in this context¡ªare introduced to shield
components from the system-specific run-time environment. Components need not
implement all their infrastructural services, such as transactions, security, or persistence, but
instead declare their requirements using configuration-specific attributes. The diagram below
illustrates this container architecture:
After a new component is instantiated, the concrete framework also instantiates an
interceptor proxy and associates it with that particular component, for example, by providing
the proxy with a component reference during its initialization. After any client request arrives
the proxy checks the configuration-specific attributes of the component and performs the
services it expects, such as initiating new transactions.
Application servers often provide an instantiation of the standard Interceptor pattern to notify
components about lifecycle events, such as connection initiation and termination, component
activation and passivation, or transaction-specific events.
CORBA implementations [OMG98c] such as TAO [SLM98] and Orbix [Bak97] apply the
Interceptor pattern so that application developers can integrate additional services to handle
specific types of events. Interceptors enhance ORB flexibility by separating request
processing from the traditional ORB communication mechanisms required to send and
receive requests and replies.
For example, Orbix defines the concept of filters that are based on the concept of 'flexible
bindings' [Shap93]. By deriving from a predefined base class, developers can intercept
events. Common events include client-initiated transmission and arrival of remote
operations, as well as the object implementation-initiated transmission and arrival of replies.
Developers can choose whether to intercept the request or result before or after marshaling.
Orbix programmers can leverage the same filtering mechanism to build multi-threaded
servers [SV96a] [SV96b] [SV96c]. Other ORBs, such as Visibroker, implement the
Interceptor Factory variant of the Interceptor pattern.
The OMG has introduced a CORBA Portable Interceptor specification [OMG99f] to
standardize the use of interceptors for CORBA-compliant implementations. Portable
Interceptors are intimately tied into the communication between a client and server. They
can thus affect the contents of CORBA requests and replies as they are exchanged, as
outlined in the following two examples:
??A client-side security interceptor can add authorization information to a request
transparently before it leaves the client process. The matching server-side security
interceptor in the receiving server could then verify that the client is authorized to invoke
requests on the target object before the request is dispatched. If authorization fails the
request should be rejected.
??A transaction interceptor is another example of a Portable Interceptor. This interceptor
adds a transaction ID to a request before it leaves the client. The corresponding server side transaction interceptor then ensures the request is dispatched to the target object
within the context of that particular transaction.
Fault-tolerant ORB frameworks. The Interceptor pattern has been applied in a number of
fault-tolerant ORB frameworks, such as the Eternal system [NMM99] [MMN99] and the
CORBA Fault-Tolerance specification [OMG99g]. Eternal intercepts system calls made by
clients through the lower-level I/O subsystem and maps these system calls to a reliable
multicast subsystem. Eternal does not modify the ORB or the CORBA language mapping,
thereby ensuring the transparency of fault tolerance from applications.
The AQuA framework [CRSS+98] also provides a variant of the Interceptor pattern. The
AQuA gateway acts as an intermediary between the CORBA objects and the Ensemble
group communication subsystem, and translates GIOP messages to group communication
primitives. AQuA uses the Quality Objects (QuO) [ZBS97] framework to allow applications to
specify their dependability requirements.
COM [Box97] [HS99a] programmers can use the Interceptor pattern to implement the
standard interface IMarshal in their components. IMarshal provides custom marshaling
functionality rather than standard marshaling, which is useful for several reasons. For
example, custom marshaling can be used to send complex data such as graph structures
across a network efficiently.
When the COM run-time system transfers an interface pointer from a component to a client
residing in another execution environment, it queries the corresponding component for an
implementation of the interceptor interface IMarshal. If the component actually implements
IMarshal, the COM run-time uses the methods of this interceptor interface to ask the
component for specific information to allow it to externalize the data to a stream object.
Web browsers. Web browsers implement the Interceptor pattern to help third-party vendors
and users integrate their own tools and plug-ins. For example, Netscape Communicator and
Internet Explorer allow browsers to register plug-ins for handling specific media types. When
a media stream arrives from a Web server the browser extracts the content type. If the
browser does not support the content type natively, it checks whether a plug-in has
registered for it. The browser then invokes the appropriate plug-in automatically to handle
the data.
The dynamicTAO reflective ORB [KRL+00] supports interceptors for monitoring and
security. Particular interceptor implementations are loaded into dynamicTAO using
component configurators (75). Using component configurators to install interceptors in
dynamicTAO allows applications to exchange monitoring and security strategies at run-time.
Change of address surface mail forwarding. A real-life example of the Interceptor pattern
arises when people move from one house to another. The post office can be instructed to
intercept surface mail addressed to the original house and have it transparently forwarded to
the new house. In this case, the contents of the mail is not modified and only the destination
address is changed.
The Interceptor pattern offers the following benefits:
Extensibility and flexibility. By customizing and configuring Interceptor and dispatcher
interfaces, users of a concrete framework can add, change, and remove services without
changing the concrete framework architecture or implementation.
Separation of concerns. Interceptors can be added transparently without affecting existing
application code because interceptors are decoupled from application behavior. Interceptors
can be viewed as aspects [KLM+97] that are woven into an application, so that programmers
can focus on application logic rather than on infrastructure services. The Interceptor pattern
also helps to decouple programmers who write interceptor code from programmers who are
responsible for developing and deploying application logic.
Support for monitoring and control of frameworks. Interceptors and context objects help to
obtain information from the concrete framework dynamically, as well as to control its
behavior. These capabilities help developers build administration tools, debuggers, and
other advanced services, such as load balancing and fault tolerance.
When a client invokes a remote operation, an interceptor can be notified automatically. By
using the context object the interceptor can change the target object specified in the method
invocation from the original destination to another server that provides the requested service.
The choice of server can depend on various dynamic factors, such as current server load or
availability. If a framework cannot complete a request successfully, another interceptor can
be activated to re-send the request to a replicated server that provides the same service,
thereby enhancing fault tolerance via replication [OMG99a].
Layer symmetry. To implement layered services, developers can introduce symmetrical
interceptors for related events exposed by the concrete framework. For example, in a
CORBA environment developers could write a client-side interceptor that creates security
tokens and automatically adds these tokens to outgoing requests. Similarly, they could write
a symmetrical server-side interceptor that extracts these tokens before the incoming request
is forwarded to the actual object implementation.
Reusability. By separating interceptor code from other application code, interceptors can be
reused across applications. For example, an interceptor used to write information into a log
file may be reused in other applications that require the same type of logging functionality.
The Interceptor pattern also incurs the following liabilities:
Complex design issues. Anticipating the requirements of applications that use a specific
concrete framework is non-trivial, which makes it hard to decide which interceptor
dispatchers to provide. In general, providing insufficient dispatchers reduces the flexibility
and extensibility of the concrete framework. Conversely, providing too many dispatchers can
yield large, inefficient systems that are complex to implement, use and optimize.
A similar problem arises when a concrete framework defines many different interceptor
interfaces and dispatchers. In this case interceptor implementors must address all these
heterogeneous extensibility mechanisms. If there are too many different mechanisms it is
hard to learn and use them. In contrast, providing only one generic interceptor and one
generic dispatcher can lead to bloated interfaces or complex method signatures. In general,
it is hard to find the right balance without knowledge of common application usages.
Malicious or erroneous interceptors. If a concrete framework invokes an interceptor that fails
to return, the entire application may block. To prevent blocking, concrete frameworks can
use configurable time-out values. If the interceptor does not return control after a specified
time, a separate thread can interrupt the execution of the interceptor. This approach can
complicate concrete framework design, however.
For example, complex functionality may be required to help concrete frameworks recover
from time-outs without leaking resources or corrupting important data structures. Interceptors
can also perform unanticipated activities or cause run-time errors. It is hard to prevent these
problems because concrete frameworks and interceptors generally execute in the same
address space.
Potential interception cascades. If an interceptor leverages a context object to change the
behavior of the concrete framework it may trigger new events, thereby initiating state
transitions in the underlying state machine. These state transitions may cause the concrete
framework to invoke a cascade of interceptors that trigger new events, and so on.
Interception cascades can lead to severe performance bottlenecks or deadlocks. The more
interceptor dispatchers that a concrete framework provides, the greater the risk of
interception cascades.
The Template Method pattern [GoF95] specifies a skeleton for an algorithm¡ªcalled the
'template method'¡ªwhere different steps in the algorithm can vary. The execution of these
variants is delegated to hook methods, which can be overridden in subclasses provided by
clients. The template method can therefore be viewed as a lightweight concrete framework,
and the hook methods as lightweight interceptors. The Template Method pattern can be
used to leverage interception locally at a particular level of abstraction, whereas the
Interceptor pattern promotes interception as a fundamental design aspect that cuts across
multiple layers in a framework architecture.
The Chain-of-Responsibility pattern [GoF95] defines different handlers that can be
interposed between the sender and the receiver of a request. As with the Interceptor pattern,
these handlers can be used to integrate additional services between senders and receivers.
In the Chain-of-Responsibility pattern, however, requests are forwarded until one of the
intermediary handlers processes the request. In contrast, a dispatcher in the Interceptor
pattern usually forwards events to all concrete interceptors that have registered for it.
To emulate the Interceptor pattern, each intermediary handler in a chain of responsibility
must therefore both handle and forward the request. Interceptor and Chain of Responsibility
differ in two other aspects, however. Event handlers in a chain of responsibility are chained
together, as the name of the pattern implies. In contrast, concrete interceptors in a
framework need not be chained together, but can instead be associated at various levels of
abstraction in a layered architecture [POSA1]. Event handlers in a chain of responsibility
also cannot control the subsequent behavior of other event handlers or application
components. Conversely, a key aspect of the Interceptor pattern is its ability to control a
concrete framework's subsequent behavior when a specific event occurs.
The Pipes and Filters pattern [POSA1] defines an architecture for processing a stream of
data in which each processing step is encapsulated in a filter component. Data is passed
through pipes between adjacent filters. If a concrete framework is structured as a Pipes and
Filters architecture with clients and objects being the endpoints, each pipe in the Pipes and
Filter chain defines a potential location at which interceptors can be interposed between
adjacent filters. In this case, registration of interceptors consists of reconfiguring the Pipes
and Filters chain.
The context object is the information passed from the source filter to the interceptor. The
interceptor is responsible for sending information to the sink filter in the appropriate format.
However, in the Pipes and Filters pattern, filters are chained via pipes, whereas in the
Interceptor pattern concrete interceptors at different layers are often independent. In
addition, Pipes and Filters defines a fundamental computational model for a complete
application 'pipeline', whereas interceptors are used to implement 'out-of-band' services in
any type of concrete framework.
The Proxy pattern [GoF95] [POSA1] provides a surrogate or placeholder for an object to
control access to itself. Although proxies can be used to integrate additional functionality to a
system, their use is restricted to objects that are already visible in a system. In contrast,
interceptors allow external components to access and control internal and otherwise
'invisible' components. As described in the Variants section, to instantiate the Interceptor
Proxy variant, we can instantiate the Proxy pattern with enhancements such as context
objects.
The Observer [GoF95] and Publisher-Subscriber [POSA1] patterns help synchronize the
state of cooperating components. These patterns perform a one-way propagation of
changes in which a publisher can notify one or more observers/subscribers when the state of
a subject changes. In contrast to the Interceptor pattern, the Observer and Publisher Subscriber patterns do not specify how observers/subscribers should access the
functionality of publishers because they define only one-way communication from the
publishers to the subscribers. These patterns also emphasize event notifications, whereas
the Interceptor pattern focuses on the integration of services into a framework.
These differences are also illustrated by the difference between event objects and context
objects. While event objects often contain values related to the current event, context objects
provide an additional programming interface to access and control concrete frameworks.
The Observer and Publisher-Subscriber patterns can therefore be viewed as variants of the
Interceptor pattern, in which context objects correspond to event types that are transferred
from concrete frameworks playing the subject role to interceptors playing the
observer/subscribe roles.
The Reflection pattern [POSA1] provides a mechanism for changing structure and behavior
of software systems. A layer of base-level objects includes the application logic. An
additional layer, the meta-level, provides information about system properties and allows
developers to control the semantics of the base level. The relationship between the
Reflection pattern and the Interceptor pattern is twofold:
??Interception provides a means to implement reflective mechanisms. For example, to
instantiate the Reflection pattern we can introduce dispatchers that help developers
introduce new behavior by registering interceptors with the meta-level. Interception can
thus be viewed as a lightweight reflective approach that is easier to implement and less
consumptive of CPU and memory. Moreover, interception only exposes certain of the
internals of the underlying system, whereas reflection often covers a broader scope.
??Reflection can define a type of interception mechanism. The main intent of reflection is
to allow applications to observe their own state so that they can change their own
behavior dynamically. In contrast, the main intent of Interceptor is to allow other
applications to extend and control the behavior of a concrete framework.
The Reactor pattern (179) demultiplexes and dispatches service requests that are delivered
concurrently to an application from one or more clients. While the Reactor pattern focuses
on handling system-specific events, the Interceptor pattern helps to intercept application specific events. The Reactor pattern is often instantiated to handle system events occurring
in the lower layers of a communication framework, whereas the Interceptor pattern is used in
multiple layers between the framework and the application.
##%%&&
The Extension Interface design pattern allows multiple interfaces to be exported by a
component, to prevent bloating of interfaces and breaking of client code when developers
extend or modify the functionality of the component.
Consider a telecommunication management network (TMN) [ITUT92] framework that can be
customized to monitor and control remote network elements such as IP routers and ATM
switches. Each type of network element is modeled as a multi-part framework component in
accordance with the Model-View-Controller pattern [POSA1]. A view and a controller are
located on a management application console. The view renders the current state of a
network element on the console and the controller allows network administrators to manage
the network element.
A model resides on the network element and communicates with the view and controller to
receive and process commands, such as commands to send state information about the
network element to the management application console. All components in the TMN
framework are organized in a hierarchy. The UniversalComponent interface shown in the
following figure provides the common functionality needed by every component, such as
displaying key properties of a network element and accessing its neighbors.
In theory, this design might be appropriate if the UniversalComponent interface shown
above is never changed, because it would allow client applications to access a wide range of
network elements via a uniform interface. In practice, however, as the TMN framework
becomes increasingly popular, management application developers will request that new
functionality and new methods, such as dump () and persist (), be added to the
UniversalComponent interface.
Over time the addition of these requests can bloat the interface with functionality not
anticipated in the initial framework design. If new methods are added to the
UniversalComponent interface directly, all client code must be updated and recompiled.
This is tedious and error-prone. A key design challenge is therefore to ensure that
evolutionary extensions to the TMN framework do not bloat its interfaces or break its client
code.
An application environment in which component interfaces may evolve over time.
Coping with changing application requirements often necessitates modifications and
extensions to component functionality. Sometimes all interface changes can be anticipated
before components are released to application developers. In this case it may be possible to
apply the 'Liskov Substitution Principle' [Mar95]. This principle defines stable base interfaces
whose methods can be extended solely via subclassing and polymorphism.
In other cases, however, it is hard to design stable interfaces, because requirements can
change in unanticipated ways after components have been delivered and integrated into
applications. When not handled carefully, these changes can break existing client code that
uses the components. In addition, if the new functionality is used by only few applications, all
other applications must incur unnecessary time and space overhead to support component
services they do not need.
To avoid these problems, it may be necessary to design components to support evolution,
both anticipated and unanticipated. This requires the resolution of four forces:
??When component interfaces do not change, modifications to component
implementations should not break existing client code.
?If implementations of our UniversalComponent interface store their state
persistently in external storage, clients should not be affected if this functionality is
re-implemented differently, as long as the component's interface is unchanged.
??Existing client code should not break when developers extend a component with new
services that are visible externally. Ideally it should not be necessary to re-compile client
code.
?It may be necessary to add a logging service to the UniversalComponent interface
so that management applications and network elements can log information to a
central repository. Existing clients that are aware of the original version of
UniversalComponent should not be affected by this change, whereas new
clients should be able to take advantage of the new logging functionality.
??Changing or extending a component's functionality should be relatively straightforward,
neither bloating existing component interfaces nor destabilizing the internal architecture
of existing components.
?When adding the logging service outlined above, we should minimize changes to
existing implementations of the UniversalComponent interface.
??It should be possible to access components remotely or locally using the same
interface. If components and their clients are distributed across network nodes, the
interfaces and implementations of a component should be decoupled.
?Management applications can benefit from location-transparent access to remote
network elements in our TMN system. It should therefore be possible to separate
the interfaces of network element management components from their physical
implementations. These can be distributed throughout the network.
Program clients to access components via separate interfaces, one for each role a
component plays, rather than programming clients to use a single component that merges all
its roles into a single interface or implementation.
In detail: export component functionality via extension interfaces, one for each semantically related set of operations. A component must implement at least one extension interface. To
add new functionality to a component, or to modify existing component functionality, export
new extension interfaces rather than modify existing ones. Moreover, program clients to
access a component via its extension interfaces instead of its implementation. Hence, clients
only have dependencies on the different roles of a component, each of which is represented
by a separate extension interface.
To enable clients to create component instances and retrieve component extension
interfaces, introduce additional indirection. For example, introduce an associated component
factory for each component type that creates component instances. Ensure that it returns an
initial interface reference that clients can use to retrieve other component extension
interfaces. Similarly, ensure that each interface inherits from a root interface that defines
functionality common to all components, such as the mechanism for retrieving a particular
extension interface. All other extension interfaces derive from the root interface. This
ensures that at minimum they offer the functionality it exports.
The structure of the Extension interface pattern includes four participants:
Components aggregate and implement various types of service-specific functionality. This
functionality can often be partitioned into several independent roles, each of which defines a
set of semantically-related operations.
Components in our TMN framework play various roles, such as storing and retrieving the
state of a network element or managing the persistence of a component's internal state.
Extension interfaces export selected facets of a component's implementation. There is one
extension interface for each role [RG98] that a component implements. In addition, an
extension interface implicitly specifies a contract that describes how clients should use the
component's functionality. This contract defines the protocol for invoking the methods of the
extension interface, such as the acceptable parameter types and the order in which methods
must be called.
The components in the TMN framework can implement the IStateMemory interface,
which allows them to maintain their state in memory. A persistence manager, such as
the CORBA Persistent State Service [OMG99e] can use the IStateMemory interface to
manage component persistence without requiring components to expose their
representational details.
If new network element components are added that also implement IStateMemory, the
persistence manager can manage their persistence without requiring any changes. The
IStateMemory interface contains methods to prepare the component for reading and
writing its state, as well as its read and write operations. The implicit contract between
the interface and its users therefore prescribes that the prepare() method must be
called before either readState() or writeState().
The root interface is a special extension interface that provides three types of functionality:
??Core functionality that all extension interfaces must support, for example functionality
that allows clients to retrieve the interfaces they request. This functionality defines the
basic mechanisms a component must implement to allow clients to retrieve and
navigate among its interfaces.
??Domain-independent functionality, such as methods that manage component life cycles.
??Domain-specific functionality that should be provided by all components within a
particular domain.
Although the root interface must implement core functionality, it need not support domain independent or domain-specific functionality. However, all extension interfaces must support
the functionality defined by the root interface. Each extension interface can thus play the role
of the root interface, which guarantees that every extension interface can return any other
extension interface on behalf of a client request.
A UniversalComponent interface can be defined as the root interface in our TMN
framework. Unlike the multi-faceted¡ªand increasingly bloated¡ª
UniversalComponent interface outlined in the Example section, however, this root
interface only defines the minimum set of methods that are common to all components
in the TMN framework.
Clients access the functionality provided by components only via extension interfaces. After
a client retrieves a reference to an initial extension interface, it can use this reference to
retrieve any other extension interface supported by a component.
A management application console client can use components in the TMN framework to
render the state of network elements and their relations visually on the screen, as well
as to store and retrieve their state using persistent storage.
To retrieve an initial reference, clients interact with a component factory associated with a
particular component type. This component factory separates the creation and initialization
aspects of a component from its processing aspects. When a client creates a new
component instance, it delegates this task to the appropriate component factory.
After a component is created successfully, the component factory returns a reference to an
extension interface to the client. A component factory may allow clients to request a specific
type of initial extension interface. Factories may also provide functionality to locate and
return references to existing component instances.
The class diagram below illustrates the participants in the Extension Interface pattern. This
diagram emphasizes logical rather than physical relationships between components. For
example, extension interfaces could be implemented using multiple inheritance or nested
classes, as described in implementation activity 6.1 (155). Such implementation details are
transparent to clients.
We illustrate the key collaborations in the Extension Interface pattern using two scenarios.
Scenario I depicts how clients create new components and retrieve an initial extension
interface:
??The client requests a component factory to create a new component and return a
reference to a particular extension interface.
??The component factory creates a new component and retrieves a reference to its root
interface.
??The component factory asks the root interface for the requested extension interface,
then returns a reference to the extension interface to the client.
Note that the factory could return any interface to the client, instead of retrieving a specific
extension one. Such a design can incur additional round-trips in a distributed system,
however, which increases the overhead of accessing the required interface.
Scenario II depicts the collaboration between clients and extension interfaces. Note that the
component implementation itself is not visible to the client, because it only deals with
extension interfaces:
??The client invokes a method on extension interface A, which can either be the root
interface or an extension interface.
??The implementation of extension interface A within the component executes the
requested method and returns the results, if any, to the client.
??The client calls the getExtension() method on extension interface A and passes it a
parameter that specifies the extension interface in which the client is interested. The
getExtension() method is defined in the root interface, so it is supported by all
extension interfaces. The implementation of extension interface A within the component
locates the requested extension interface B and returns the client a reference to it.
??The client invokes a method on extension interface B, which is then executed within
the component implementation.
This section describes the activities associated with implementing the Extension Interface
pattern. This pattern should be familiar to anyone who has programmed with Microsoft's
Component Object Model (COM) [Box97], Enterprise JavaBeans (EJB) [MaHa99], or the
CORBA Component Model (CCM) [OMG99a], because it captures and generalizes the core
concepts underlying these component technologies.
1. Determine the stability of the design and the long-term application requirements.
Before applying the Extension Interface pattern, it is important to determine whether it
is really needed. Although this pattern is a powerful solution to a particular set of
forces, it is nontrivial to implement. It can also complicate a software design
significantly if applied unnecessarily.
We therefore recommend that the forces outlined in the Problem section are
considered carefully. You should ensure that these issues are faced in your software
system before applying this pattern. For example, it may turn out that the complete
set of methods an interface requires can be determined during system development,
and that the interface will not change over time as application requirements evolve. In
this case, it may be simpler to use the Liskov Substitution Principle [Mar95] rather
than the Extension Interface pattern.
2. Analyze the domain and specify a domain-specific component model. Assuming that
the Extension Interface pattern is necessary, the next activity involves analyzing
domain-specific application requirements. In particular, this activity focuses on
identifying application-specific entities, such as the network elements in our TMN
example, the roles a particular entity provides to the system, and the functionality that
supports the different roles. The result is a domain model that identifies which
components to implement, as well as the functionality they must provide.
3. For the management application console, every type of entity to be controlled is
implemented as a separate managed object [ITUT92], which is an abstraction
used to represent hardware units, such as routers, computers, bridges, or
switches. Managed objects can also represent software elements, such as
applications, ports, or connections. Management applications use managed
objects to control and monitor the state of network elements, display debugging
information, or visualize system behavior on a management console.
5. After devising a domain model, it is necessary to specify a component model to
implement the identified components:
??If the components are restricted to a single application domain or a small set of
related domains, consider specifying a domainspecific component model that is
tailored for the application or family of applications being developed.
??Conversely, if the components must be applied to a wide range of applications,
or even across multiple domains, consider using an existing component
technology, such as Microsoft COM [Box97], EJB [MaHa99], or the CORBA
Component Model [OMG99a].
In the latter case, the next implementation activity can be skipped, because these
component models define the infrastructure it specifies.
6. Specify the root interface. Determine if each type of functionality identified in
implementation activity 2 above should form part of the root interface, or be separated
into an extension interface.
With this criteria in mind, iterate through three sub-activities:
1. Specify core functionality. Several issues must be addressed when defining
the core functionality of the root interface:
??Extension interface retrieval. At minimum, the root interface must
include a method that returns extension interface references to clients.
The type of information returned from this method depends largely on the
programming language. For example, Java clients expect to retrieve an
object reference, whereas pointers are an appropriate choice for C++.
??Unique naming. Extension interfaces must be named using integer
values or strings. Strings can be read more easily by programmers and
simple management tools, but integer values can be smaller and
processed more efficiently. To prevent name clashes, interface identifiers
can be generated algorithmically. For instance, Microsoft COM uses 128
bit globally unique identifiers (GUIDs) based on the address of the
network interface, the date, and the time.
??Error handling. Component developers must determine what a
component should do when a client requests an extension interface that
is not supported. For example, a component could either return an error
value or raise an exception. The Implementation section of the Wrapper
Facade pattern (47) discusses several strategies for implementing an
error-handling mechanism and evaluates their trade-offs.
2. Specify domain-independent services. In addition to defining a method for
retrieving extension interfaces, the root interface can provide methods for
various domain-independent services. Here are two possibilities:
??Reference counting. In programming languages that do not provide
automatic garbage collection, such as C or C++, clients are responsible
for deleting extension interfaces they no longer need. However, multiple
clients may share the same extension interface. Components can thus
provide a reference counting mechanism to prevent the accidental
deletion of resources used to implement extension interfaces.
Reference counting enables components to track the number of clients
accessing specific extension interfaces. After an extension interface is no
longer referenced by any clients, the resources used by the component's
implementation of the interface can be released automatically. The
Counted Pointer idiom [POSA1] [Cope92] presents several options for
implementing a reference counting mechanism.
??Run-time reflection. Another example of a domain-independent service
is a run-time reflection mechanism. This mechanism allows components
to publish information about the specific roles, extension interfaces, and
methods they support. Using this knowledge, clients can construct and
send method invocations dynamically [GS96]. This enables scripting
languages to integrate components into existing client applications at run time. Reflection mechanisms can be instantiated using the Reflection
architectural pattern [POSA1].
3. Specify domain-specific functionality. The root interface can also export
domain-specific functionality if all components implementing the root interface
provide this functionality.
4. In our management application console, the drawing functionality could be
moved to the root interface.
6. The decision about which domain-specific services to specify in the root
interface should normally be deferred until after implementation activity 4
(153). Ideally, all domain-specific functionality should reside in separate
extension interfaces. If all components end up implementing a particular
extension interface, refactor the current solution [Opd92] [FBBOR99] and
move the methods of that particular extension interface to the root interface.
In our TMN framework example, extension interfaces are identified by
unique integer constants. We use Java as the implementation language
because it provides automatic garbage collection, which simplifies
memory management. The only common functionality therefore required
in the root interface¡ªwhich we call IRoot¡ªis a method that allows client
to retrieve any interface they need.
Another potential candidate for inclusion in the root interface is a persistence
mechanism. However, there are many different strategies and policies for
handling persistence, such as managing component state in databases or flat
files, which makes it hard to anticipate all possible use cases. Therefore,
components can choose to support whatever persistence mechanism they
consider appropriate by implementing specific extension interfaces.
7. Introduce general-purpose ext3ension interfaces. General-purpose extension
interfaces contain functional roles that must be provided by more than one
component and that are not included in the root interface. A separate extension
interface should be defined for each role. For example, extension interfaces can be
defined to handle persistence aspects of components, as discussed at the end of
implementation activity 3 (150).
8. Our management application console helps to control and monitor remote
network entities via managed objects. Managed objects are implemented as
components that send information to the management application console and
receive commands from it.
This example illustrates managed objects that are visualized on a management
console. We therefore introduce two additional extension interfaces, IDump and
IRender, which are implemented by all components that print debug information
on the console or draw themselves.
If a particular general-purpose extension interface must be supported by all
components, it may be feasible to refactor the root interface specified in
implementation activity 3 (150) and integrate this functionality there. Note, however,
that refactoring the root interface may bloat it with functionality or break existing
applications, thereby defeating the benefits of the Extension Interface pattern.
38. Define component-specific extension interfaces. The extension interfaces needed to
export generic component functionality were specified in implementation activities 3
(150) and 4 (153). This implementation activity defines additional interfaces that are
specific to a particular component or that are applicable to a limited range of
components.
Implement the components. The implementation of components involves five sub activities:
0. Specify the component implementation strategy. This activity determines how
extension interface implementations should be linked, in accordance with the
following three strategies:
??Multiple inheritance. In this strategy a component class inherits from
all of its extension interfaces.
??Nested classes. In this strategy extension interfaces can be
implemented as nested classes within the component class. The
component class instantiates a singleton instance [GoF95] of each nested
class. Whenever the client asks for a particular extension interface, the
getExtension() method implementation returns the appropriate nested
class object.
??Separate interface classes. Extension interfaces can use the Bridge or
Adapter patterns [GoF95] to implement separate classes that are
independent of the component itself. This strategy is particularly useful
when applying the Extension Interface pattern to refactor an existing
component that does not implement the pattern.
The 'tie' adapter [SV98a] defined in the CORBA IDL mappings for Java
and C++ is an example of this component implementation strategy. In
CORBA a tie adapter inherits from an automatically-generated servant
class, overrides all its pure virtual methods, and delegates these methods
to another C++ object, the so-called 'tied object'. A server application
developer defines the tied object.
1. Regardless of which component implementation strategy is selected, the client
is unaffected, because it only accesses the component via references to
extension interfaces.
2. Implement the mechanism to retrieve extension interfaces. When
implementing the generic method that retrieves extension interfaces on behalf
of clients, ensure that the method implementation conforms to three
conventions:
??Reflexivity. When clients query extension interface A for the same
extension interface A, they must always receive the same reference A.
??Symmetry. If a client can retrieve extension interface B from extension
interface A, it also must be able to retrieve extension interface A from
extension interface B.
??Transitivity. If a client can retrieve extension interface B from extension
interface A and extension interface C from extension interface B, it must
be possible to retrieve extension interface C directly from extension
interface A.
Following the conventions above ensures that a client can always navigate
from a specific extension interface of a component to any other extension
interface of the same component. In other words, each extension interface can
be connected with every other extension interface via navigation.
3. Implement a reference counting mechanism (optional). If the root interface
requires the reference counting mechanism discussed in implementation
activity 3.2 (151), specify the resources in the component implementation that
must be managed by this mechanism. There are two common options for
implementing a reference counting mechanism:
??If each interface implementation requires or uses separate resources,
or if there are separate implementations for each interface, introduce a
separate reference counter for each extension interface. If a particular
reference counter drops to zero, release all resources used by the
corresponding extension interface. After the last reference counter has
fallen to zero, all resources associated with the component can be
released.
??If all interface implementations share the same resources, introduce a
global reference counter for the entire component. After this global
reference counter reaches zero, release the component's resources.
The first option can optimize resource management more effectively than the
second option, because in the second option all resources must always be
available. In the first option, in contrast, extension interfaces and their required
resources may be activated and deactivated on demand. Only those extension
interfaces and resources actually used by clients are activated. The
disadvantage of maintaining extension interface-specific reference counters,
however, is their complex implementation within the component.
We can apply reference counting to activate and deactivate extension
interface implementations on-demand in our TMN framework. This avoids
the unnecessary commitment of resources such as memory or socket
handles. For example, when a management application client accesses
an extension interface whose reference counter is zero, the component
can activate the interface implementation and its resources transparently.
When no clients access the extension interface, the corresponding
implementation and resources can be deactivated and released
selectively. The COM [Box97] component model implements this strategy.
4. Select a concurrency strategy. In concurrent or networked systems, multiple
clients can access a particular extension interface simultaneously.
Implementations of different extension interfaces may share state and
resources within the component. Critical sections and state within the
component's implementation must be serialized therefore to provide corruption
from concurrent access by clients.
The Active Object (369) and Monitor Object (399) concurrency patterns, as
well as the Scoped Locking (325), Strategized Locking (333), and Thread-Safe
Interface (345) synchronization patterns, define various strategies and
mechanisms for protecting critical sections and state within components.
5. Implement the extension interface functionality using the selected component
implementation strategy. This implementation activity is largely domain- or
application-specific, so there are no general issues to address.
6. In our TMN framework example we implement components using multiple
interface inheritance. Our components do not require explicit reference
counting, because Java provides automatic garbage collection.
7. For simplicity, we do not illustrate the component concurrency strategy. To
identify different extension interfaces uniquely, we define an
InterfaceID class that enumerates all interface identifiers. These are
defined to be integers via the following types:
A more sophisticated implementation could use a repository of interface
identifiers. In this case, unique identifiers could be generated automatically
by tools to prevent name clashes when different component providers
define different interfaces. We could also use a String as the identifier
type rather than an int. This might improve the readability and
debug ability of the component system, but at the expense of larger
memory footprint and slower lookup time.
18. One of the component types in the management application console
represents a connection between two ports. This component supports the
extension interfaces IManagedObject, IRender, IConnection, and
IDump. We implement all extension interfaces using Java interface
inheritance:
Note how the getExtension() interface uses a switch statement to
determine which interface is supported by the component. Had the
identifier type been defined as a String rather than an int, we would
have used a different type of lookup strategy such as linear search,
dynamic hashing, or perfect hashing [Sch98a].
Implement component factories. Every component type must implement a factory that
clients can use to obtain instances of the component type. This involves three sub activities:
0. Define the association between component factories and components. For
every component type, a singleton [GoF95] component factory can be defined
to create instances of this component type. Two strategies can be applied to
implement this association:
??One component factory interface per component type. In this strategy
a separate factory interface is defined for every component type and used
to instantiate the component type. One component type could offer a
component factory interface with a single method create(). Another
component type could offer a selection of different methods for creating
components. The Factory Method pattern [GoF95] can be used to
implement this strategy. It requires clients to handle many different
component factory interfaces, however.
??One component factory interface for all component types. In this
strategy there is only one component factory interface that all concrete
component factories must implement. This design enables clients to
create different components in a uniform manner. For example, when a
client creates a new component, it only must know how to invoke the
generic component factory interface. The Abstract Factory design pattern
[GoF95] can be used to implement this strategy.
1. Decide which functionality the factory will export. Regardless of the strategy
selected in implementation activity 7.1 (160), the following issues must be
addressed when specifying the interface of a particular component factory:
??There could be one or more different methods for creating new
components. These creation methods are similar to constructors in
object-oriented programming languages, such as C++ or Java, in that
they instantiate and initialize component types. Different types of
initialization information might be necessary to construct a new
component instance. For each of these alternatives, a separate create
method is introduced with its own, possibly empty, set of initialization
parameters.
??Methods could be available for finding existing components rather than
creating them for each invocation. If component instances are already
available and if they can be identified uniquely, a factory can be
implemented using the Manager pattern [Som97]. In this case, its find()
methods are passed a set of conditions as arguments, such as the
primary key associated with each component. It then retrieves one or
more components that adhere to the condition arguments.
??Clients can specify component usage policies. For example, one
policy could provide a singleton implementation for a particular
component type. Another policy could determine whether a specific
component is expected to maintain its state persistently.
??Life-cycle management support for components is another candidate
for the component factory interface. For example, methods to release
existing components might be included in the component factory.
Introduce a component factory finder. As the number of component types
increases, the problem of how to find the associated component factories
arises. One way to resolve this is to define a global component factory finder.
This finder could maintain the associations between component types and
their component factories, as specified in implementation activity 7.1 (160).
To obtain the component factory for a particular component type, clients must
indicate to the component factory finder which component type they require.
Component types must therefore be identified uniquely. A common way to
implement this identification mechanism is to introduce a primary key type for
every component type. This key type helps to associate component instances
with instances of the primary key type uniquely.
For example, each component instance might be associated uniquely with an
integer value. This integer value might be passed as an argument to a
particular find() method of the component factory, which uses the primary
key to obtain the associated component instance. For this purpose, the
component factory can apply the Manager pattern [Som97] and map from
primary key values to component instances. To simplify client programming,
the same primary key type can be used to identify both component instances
and extension interfaces, as shown in implementation activity 3.1 (151). In
Microsoft COM, for example, globally-unique identifiers (GUIDs) identify both
extension interfaces and component types.
When clients request a specific component factory from the component factory
finder, the factory finder returns the interface of the component factory. By
using this interface, clients can instantiate the components they need. If there
is only one global component factory finder in the system, use the Singleton
pattern [GoF95] to implement it.
The component factory finder can optionally provide a trading mechanism
[OMG98b]. In this case, clients do not pass a concrete component type to the
component factory finder. Instead, they specify properties that can be used by
the component factory finder to retrieve an appropriate component factory. For
example, a client might specify certain properties of extension interfaces in
which it is interested to a component factory finder. The component factory
finder then locates a component type that implements all the requested
interfaces.
Management application clients in our TMN system need not know all
component factories. We therefore introduce a component factory finder
that is responsible for managing a hash table with component-to-factory
associations. Clients need only know where the single component factory
finder is located. To identify components uniquely, we apply the same
strategy used for interfaces in implementation activity 6.5 (158).
The component factory finder is implemented as a singleton. It contains
two methods that are publicly accessible. The registerFactory()
method must be called¡ªeither by clients or by components¡ªto register
component factories with the component factory finder. The
findFactory() method is used to search for existing component
factories.
Implement the clients. Clients use functionality provided by components. They may
also act as containers[8] for these components. To implement clients apply the
following steps:
??First determine which component functionality they require. For example,
determine if there are existing components that cover some or all of the
functionality the clients are expected to provide.
??Identify which components should be composed together and determine which
components can use other components.
??Determine if there are any subsystems within the client application that might
be used in other applications and separate these into new component types.
After evaluating these issues, integrate the client application using the components
identified via the analysis outlined in the implementation activities above.
The main class of the client application defines the methods dumpAll() and
drawAll(). Both methods are passed an array of components as a parameter.
They then iterate through the array querying each component for the extension
interface IDump and IRender, respectively, calling the methods dump() and
render() if the query succeeds. This example shows that polymorphism can be
supported by using interface inheritance rather than implementation inheritance.
Note that a client could type cast port1Root and port2Root below instead of
calling the getExtension() method, because the components use interface
inheritance to implement the extension interfaces. However, this design would
tightly couple the client implementation and the component implementation. If we
later restructured the components to use Java inner classes rather than multiple
interface inheritance, for example, all the client code would break.
Shortly after delivering the component-based management application console to their
customers, the TMN framework developers receive two change requests. The first request
requires each component in the TMN framework to load and store its state from a persistent
database. The second request requires a new component with a star connection topology.
This topology denotes a set of network elements that are all connected to a central element,
yielding a star-like shape.
Every existing component is then enhanced to implement this interface. In detail, a
component implementor must add all methods defined in the new interface to the
component implementation. The amount of work necessary to extend a component with
a new extension interface directly depends on the particular extension interface added
to the component. The persistence example requires just a few database calls to
implement the new interface.
The InterfaceID class defined in implementation activity 6.5 (158) is extended with
identifiers for the new interfaces.
Extension Object [PLoPD3]. In this variant there is no need for a component factory because
each component is responsible for returning interface references to clients. Extension
objects are well-suited for components that are built using a single object-oriented
programming language, such as C++ or Java, where components derive from all interfaces
they implement. Type-safe downcasting can be used to retrieve component interfaces. In
these language-specific implementations component factories are not needed because
component classes map directly to language classes, which are themselves responsible for
instance creation.
Distributed Extension Interface. This variant features an additional type of participant,
servers, which host the implementations of components. Each server contains the factory as
well as the implementation of all supported extension interfaces. A single server can host
more than one component type. In distributed systems, clients and servers do not share the
same address space. It is the task of the server to register and unregister its components
with a locator service, so that clients or factory finders can retrieve remote components.
In distributed systems there is a physical separation of interfaces and implementations.
Client proxies can be introduced to attach clients to remote extension interfaces
transparently [POSA1] [GoF95]. Client-side proxies implement the same extension
interfaces as the components they represent. They also shield clients from tedious and
error-prone communication mechanisms by forwarding method invocations over the network
to remote components. Proxies can be defined so that clients can leverage the Extension
Object variant outlined above. To enhance performance, client proxies can provide co located [WSV99] local implementations of general-purpose extension interfaces to reduce
network traffic, in accordance with the Half Object plus Protocol pattern [Mes95].
In distributed object computing middleware [OMG98c], proxies can be implemented
automatically via an interface definition language (IDL) compiler. An IDL compiler parses
files containing interface definitions and generates source code that performs various
network programming tasks, such as marshaling, demarshaling, and error-checking [GS98].
The use of interface definition languages simplifies the connection of components and
clients written in different programming languages. To ensure this degree of distribution and
location transparency, the underlying component infrastructure can instantiate the Broker
architectural pattern [POSA1].
Extension Interface with Access Control. In this variant the client must authenticate itself to
the extension interface. Client access to an extension interface can be restricted by this
method. For example, an administrator might be granted access to all interfaces of a
component, whereas another client would be allowed to invoke methods on a subset of
interfaces that provided specific functionality.
Asymmetric Extension Interface. This variant specifies one distinguished interface that is
responsible for providing access to all other interfaces. In contrast to the symmetric case,
clients are not capable of navigating from an extension interface to any other extension
interface. They must instead use the distinguished extension interface to navigate to any
other extension interface. This interface may be provided by the component itself, as defined
by the Extension Object variant.
Microsoft's COM/COM+ technology is based upon extension interfaces [Box97]. Each COM
class implementation must provide a factory interface called IClassFactory that defines
the functionality to instantiate new instances of the class. When the COM run-time activates
the component implementation, it receives a pointer to the associated factory interface.
Using this interface, clients can to create new component instances.
Each COM class implements one or more interfaces that are derived from a common root
interface called IUnknown. The IUnknown interface contains the method
QueryInterface(REFIID, void**), which allows clients to retrieve particular extension
interfaces exported by a component. The first parameter to QueryInterface() is a unique
identifier that determines which extension interface to return to a client. If the component
implements the interface requested by a client, it returns an interface pointer in the second
parameter, otherwise an error is returned.
This activity is called interface negotiation, because clients can interrogate components to
determine whether they support particular extension interfaces. COM/COM+ implements the
Distributed Extension Interface variant and allows clients and components to be developed
in any programming language supported by Microsoft, including Visual Basic, C, C++ and
Java.
CORBA 3 [Vin98] introduces a CORBA Component Model (CCM) [OMG99a] in which each
component may provide more than one interface. Clients first retrieve a distinguished
interface, the component's so-called 'equivalent' interface. They then use specific 'provide'
methods to navigate to one of the extension interfaces, called 'facets' in CCM. Every CCM
interface must implement the method get_component(), which is similar to COM's
QueryInterface() method described above. It is therefore always possible to navigate
from a facet back to the component's equivalent interface.
To obtain a reference to an existing component, or to create a new component, clients
access a so-called 'home' interface, which is associated with a single component type. This
interface represents the component factory interface, as defined by CORBA components
and Enterprise JavaBeans. The factory finder within CCM is implemented by the
ComponentHomeFinder, whereas EJB relies on the Java Naming and Directory Interface
(JNDI) for the same purpose. CORBA components and the Java-centric subset Enterprise
JavaBeans (EJB) [MaHa99] use the Asymmetric Extension Interface variant.
OpenDoc [OHE96] introduces the concept of adding functionality to objects using
extensions. Functionality is provided to retrieve extensions in the root interface, as well as
for reference counting. OpenDoc implements the Extension Object variant of Extension
Interface.
The Extension Interface pattern offers the following benefits:
Extensibility. Extending the functionality of a component should only require adding new
extension interfaces. Existing interfaces remain unchanged, so existing clients should not be
affected adversely. Developers can prevent interface bloating by using multiple extension
interfaces rather than merging all methods into a single base interface.
Separation of concerns. Semantically-related functionality can be grouped together into
separate extension interfaces. A component can play different roles for the same or different
clients by defining a separate extension interface for each role.
Polymorphism is supported without requiring inheritance from a common interface. If two
components implement the same extension interface, a client of that particular extension
interface need not know which component actually provides the functionality. Similarly,
multiple components can implement the same set of interfaces, thereby allowing them to
exchange component implementations transparent.
Decoupling of components and their clients. Clients access extension interfaces rather than
component implementations. There is therefore no (tight) coupling between a component
implementation and its clients. New implementations of extension interfaces can thus be
provided without breaking existing client code. It is even possible to separate the
implementation of a component from its interfaces by using proxies [POSA1] [GoF95].
Support for interface aggregation and delegation. Components can aggregate other
components and offer the aggregated interfaces as their own. The aggregate interfaces
delegate all client requests to the aggregated component that implements the interface. This
allows the aggregate interfaces to assume the identity of every aggregated component and
to reuse their code. However, a pre-condition for this design is that the aggregate interface
component and its constituent aggregated components collaborate via the
getExtension() method.
However, the Extension Interface pattern also can incur the following liabilities:
Increased component design and implementation effort. The effort required to develop and
deploy components can be non-trivial. The component programming effort is particularly
tedious when the Extension Interface pattern is not integrated transparently in a particular
programming language. For example, it is relatively straightforward to instantiate the pattern
using Java or C++. Implementing it in C is extremely complex, however, due to the lack of
key language features such as inheritance or polymorphism.
Increased client programming complexity. The Extension Interface pattern makes clients
responsible for determining which interfaces are suitable for their particular use case. Clients
must therefore perform a multi-step protocol to obtain a reference to an extension interface
before using it. A client must also keep track of a variety of bookkeeping details, such as
interface or instance identifiers and reference counts, that can obscure the client's core
application logic.
Additional indirection and run-time overhead. Clients never access components directly,
which may reduce run-time efficiency slightly. Similarly, run-time reference counting of
initialized components is complex and potentially inefficient in multi-threaded or distributed
environments. In certain cases, however, this additional indirection is negligible, particularly
when accessing components across high latency networks.
Components and clients may not reside in the same address space, be written in the same
programming language or be deployed in binary form, but it still may be necessary to
interconnect them. The Proxy pattern [POSA1] [GoF95] can be applied in this context to
decouple a component's interface from its implementation. For a more sophisticated and
flexible solution, the Broker pattern [POSA1] can be applied. In this pattern components act
as servers and the broker, among its other responsibilities, provides a globally-available
factory finder service.
The Extension Object variant of the Extension Interface pattern is introduced in [PLoPD3].
This variant is applicable whenever the object model of the underlying programming
language can be used to implement a non-distributed component extension mechanism. In
this case,
??Components and component factories map directly to programming language classes
??Component interfaces map to programming language interfaces and
??The retrieval of component interfaces is implemented using type safe down casting
##%%&&
The Reactor architectural pattern allows event-driven applications to demultiplex and
dispatch service requests that are delivered to an application from one or more clients.
Consider an event-driven server for a distributed logging service. Remote client applications
use this logging service to record information about their status within a distributed system.
This status information commonly includes error notifications, debugging traces, and
performance diagnostics. Logging records are sent to a central logging server, which can
write the records to various output devices, such as a console, a printer, a file, or a network
management database.
Clients communicate with the logging server using a connection-oriented protocol, such as
TCP [Ste98]. Clients and the logging service are thus bound to transport endpoints
designated by full associations consisting of the IP addresses and TCP port numbers that
uniquely identify clients and the logging service.
The logging service can be accessed simultaneously by multiple clients, each of which
maintains its own connection with the logging server. A new client connection request is
indicated to the server by a CONNECT event. A request to process logging records within the
logging service is indicated by a READ event, which instructs the logging service to read new
input from one of its client connections. The logging records and connection requests issued
by clients can arrive concurrently at the logging server.
One way to implement a logging server is to use some type of multi-threading model. For
example, the server could use a 'thread-per-connection' model that allocates a dedicated
thread of control for each connection and processes logging records as they arrive from
clients. Using multi-threading can incur the following liabilities, however:
??Threading may be inefficient and non-scalable due to context switching,
synchronization, and data movement among CPUs.
??Threading may require the use of complex concurrency control schemes throughout
server code.
??Threading is not available on all operating systems, nor do all operating systems
provide portable threading semantics.
??A concurrent server may be better optimized by aligning its threading strategy to
available resources, such as the number of CPUs, rather than to the number of clients
is services concurrently.
These drawbacks can make multi-threading an inefficient and overly-complex solution for
developing a logging server. To ensure adequate quality of service for all connected clients,
however, a logging server must handle requests efficiently and fairly. In particular, it should
not service just one client and starve the others.
An event-driven application that receives multiple service requests simultaneously, but
processes them synchronously and serially.
Event-driven applications in a distributed system, particularly servers,[1] must be prepared to
handle multiple service requests simultaneously, even if those requests are ultimately
processed serially within the application. The arrival of each request is identified by a
specific indication event, such as the CONNECT and READ events in our logging example.
Before executing specific services serially, therefore, an event-driven application must
demultiplex and dispatch the concurrently-arriving indication events to the corresponding
service implementations.
Resolving this problem effectively requires the resolution of four forces:
??To improve scalability and latency, an application should not block on any single
source of indication events and exclude other event sources, because blocking on one
event source can degrade the server's responsiveness to clients.
??To maximize throughput, any unnecessary context switching, synchronization, and
data movement among CPUs should be avoided, as outlined in the Example section.
??Integrating new or improved services with existing indication event demultiplexing and
dispatching mechanisms should require minimal effort.
??Application code should largely be shielded from the complexity of multi-threading and
synchronization mechanisms.
Synchronously wait for the arrival of indication events on one or more event sources, such
as connected socket handles. Integrate the mechanisms that demultiplex and dispatch the
events to services that process them. Decouple these event demultiplexing and dispatching
mechanisms from the application-specific processing of indication events within the services.
In detail: for each service an application offers, introduce a separate event handler that
processes certain types of events from certain event sources. Event handlers register with a
reactor, which uses a synchronous event demultiplexer to wait for indication events to occur
on one or more event sources. When indication events occur, the synchronous event
demultiplexer notifies the reactor, which then synchronously dispatches the event handler
associated with the event so that it can perform the requested service.
There are five key participants in the Reactor pattern:
Handles are provided by operating systems to identify event sources, such as network
connections or open files, that can generate and queue indication events. Indication events
can originate from external sources, such as CONNECT events or READ events sent to a
service from clients, or internal sources, such as time-outs. When an indication event occurs
on an event source, the event is queued on its associated handle and the handle is marked
as 'ready'. At this point, an operation, such as an accept() or read(), can be performed
on the handle without blocking the calling thread.
Socket handles are used in the logging server to identify transport endpoints that receive
CONNECT and READ indication events. A passive-mode transport endpoint and its
associated socket handle listen for CONNECT indications events. The logging server then
maintains a separate connection, and thus a separate socket handle, for each
connected client.
A synchronous event demultiplexer is a function called to wait for one or more indication
events to occur on a set of handles¡ªa handle set. This call blocks until indication events on
its handle set inform the synchronous event demultiplexer that one or more handles in the
set have become 'ready', meaning that an operation can be initiated on them without
blocking.
select() is a common synchronous event demultiplexer function for I/O events [Ste98]
supported by many operating systems, including UNIX and Win32 platforms. The
select() call indicates which handles in its handle set have indication events pending.
Operations can be invoked on these handles synchronously without blocking the calling
thread.
An event handler specifies an interface consisting of one or more hook methods [Pree95]
[GoF95]. These methods represent the set of operations available to process application specific indication events that occur on handle(s) associated with an event handler.
Concrete event handlers specialize the event handler and implement a specific service that
the application offers. Each concrete event handler is associated with a handle that identifies
this service within the application. In particular, concrete event handlers implement the hook
method(s) responsible for processing indication events received through their associated
handle. Any results of the service can be returned to its caller by writing output to the handle.
The logging server contains two types of concrete event handlers: logging acceptor and
logging handler. The logging acceptor uses the Acceptor-Connector pattern (285) to
create and connect logging handlers. Each logging handler is responsible for receiving
and processing logging records sent from its connected client.
A reactor defines an interface that allows applications to register or remove event handlers
and their associated handles, and run the application's event loop. A reactor uses its
synchronous event demultiplexer to wait for indication events to occur on its handle set.
When this occurs, the reactor first demultiplexes each indication event from the handle on
which it occurs to its associated event handler, then it dispatches the appropriate hook
method on the handler to process the event.
Note how the structure introduced by the Reactor pattern 'inverts' the flow of control within
an application. It is the responsibility of a reactor, not an application, to wait for indication
events, demultiplex these events to their concrete event handlers, and dispatch the
appropriate hook method on the concrete event handler. In particular, a reactor is not called
by a concrete event handler, but instead a reactor dispatches concrete event handlers,
which react to the occurrence of a specific event. This 'inversion of control' is known as the
Hollywood principle [Vlis98a].
Application developers are thus only responsible for implementing the concrete event
handlers and registering them with the reactor. Applications can simply reuse the reactor's
demultiplexing and dispatching mechanisms.
The structure of the participants in the Reactor pattern is illustrated in the following class
diagram:
The collaborations in the Reactor pattern illustrate how the flow of control oscillates between
the reactor and event handler components:
??An application registers a concrete event handler with the reactor. At this point, the
application also indicates the type of indication event(s) the event handler wants the
reactor to notify it about, when such event(s) occur on the associated handle.
??The reactor instructs each event handler to provide its internal handle, in our example
by invoking their get_handle() method. This handle identifies the source of indication
events to the synchronous event demultiplexer and the operating system.
??After all event handlers are registered, the application starts the reactor's event loop,
which we call handle_events(). At this point the reactor combines the handles from
each registered event handler into a handle set. It then calls the synchronous event
demultiplexer to wait for indication events to occur on the handle set.
??The synchronous event demultiplexer function returns to the reactor when one or more
handles corresponding to event sources becomes 'ready', for example when a Socket
becomes 'ready to read'.
??The reactor then uses the ready handles as 'keys' to locate the appropriate event
handler(s) and dispatch the corresponding hook method(s). The type of indication event
that occurred can be passed as a parameter to the hook method. This method can use
this type information to perform any additional application-specific demultiplexing and
dispatching operations.[2]
??After the appropriate hook method within the event handler is dispatched, it processes
the invoked service. This service can write the results of its processing, if any, to the
handle associated with the event handler so that they can be returned to the client that
originally requested the service.
The participants in the Reactor pattern decompose into two layers:
??Demultiplexing/dispatching infrastructure layer components. This layer performs
generic, application-independent strategies for demultiplexing indication events to event
handlers and then dispatching the associated event handler hook methods.
??Application layer components. This layer defines concrete event handlers that perform
application-specific processing in their hook methods.
The implementation activities in this section start with the generic demultiplexing/dispatching
infrastructure components and then cover the application components. We focus on a
reactor implementation that is designed to demultiplex handle sets and dispatch hook
methods on event handlers within a single thread of control. The Variants section describes
the activities associated with developing concurrent reactor implementations.
1. Define the event handler interface. Event handlers specify an interface consisting of
one or more hook methods [Pree95]. These hook methods represent the set of
services that are available to process indication events received and dispatched by
the reactor. As described in implementation activity 5 (196), concrete event handlers
are created by application developers to perform specific services in response to
particular indication events. Defining an event handler interface consists two sub activities:
1. Determine the type of the dispatching target. Two types of event handlers can
be associated with a handle to serve as the target of a reactor's dispatching
strategy:
??Event handler objects. In object-oriented applications a common way
to associate an event handler with a handle is to create an event handler
object. For example, the Reactor pattern implementation shown in the
Structure section dispatches concrete event handler objects. Using an
object as the dispatching target makes it convenient to subclass event
handlers to reuse and extend existing components. Similarly, objects
make it easy to integrate the state and methods of a service into a single
component.
??Event handler functions. Another strategy for associating an event
handler with a handle is to register a pointer to a function with a reactor
rather than an object. Using a pointer to a function as the dispatching
target makes it convenient to register callbacks without having to define a
new subclass that inherits from an event handler base class.
The Adapter pattern [GoF95] can be employed to support both objects and
pointers to functions simultaneously. For example, an adapter could be
defined using an event handler object that holds a pointer to an event handler
function. When the hook method was invoked on the event handler adapter
object it could automatically forward the call to the event handler function that
it encapsulates.
2. Determine the event handling dispatch interface strategy. We must next define
the type of interface supported by the event handlers for processing events.
Assuming that we use event handler objects rather than pointers to functions,
there are two general strategies:
??Single-method dispatch interface strategy. The class diagram in the
Structure section illustrates an implementation of the Event_Handler
base class interface that contains a single event handling method, which
is used by a reactor to dispatch events. In this case, the type of the event
that has occurred is passed as a parameter to the method.
The single-method dispatch interface strategy makes it possible to
support new types of indication events without changing the class
interface. However, this strategy encourages the use of C++ switch and
if statements in the concrete event handler's handle_event() method
implementation to handle a specific event, which degrades its
extensibility.
??Multi-method dispatch interface strategy. A different strategy for
defining the Event_Handler dispatch interface is to create separate
hook methods for handling each type of event, such as input events,
output events, or time-out events. This strategy can be more extensible
than the single-method dispatch interface because the demultiplexing is
performed by a reactor implementation, rather than by a concrete event
handler's handle_event() method implementation.
The multi-method dispatch interface strategy makes it easy to override
methods in the base class selectively, which avoids additional
demultiplexing via switch or if statements in the hook method
implementation. However, this strategy requires pattern implementors to
anticipate the event handler methods in advance. The various
handle_*() methods in the Event_Handler dispatch interface above
are tailored for I/O and time-out indication events supported by the
select() function. This function does not encompass all the types of
indication events, such as synchronization events that can be handled via
the Win32 WaitForMultipleObjects() function [SchSt95].
3. Both the single-method and multi-method dispatch interface strategies are
implementations of the Hook Method [Pree95] and Template Method [GoF95]
patterns. Their intent is to provide well-defined hooks that can be specialized
by applications and called back by lower-level dispatching code. This allows
application programmers to define concrete event handlers using inheritance
and polymorphism.
2. Define the reactor interface. The reactor's interface is used by applications to register
or remove event handlers and their associated handles, as well as to invoke the
application's event loop. The reactor interface is often accessed via a Singleton
[GoF95] because a single reactor is often sufficient for each application process.
To shield applications from complex and non-portable demultiplexing and dispatching
operating system platform mechanisms, the Reactor pattern can use the Bridge
pattern [GoF95]. The reactor interface corresponds to the abstraction participant in
the Bridge pattern, whereas a platform-specific reactor instance is accessed internally
via a pointer, in accordance with the implementation hierarchy in the Bridge pattern.
A typical reactor interface also defines a pair of overloaded methods, which we call
register_handler(), that allow applications to register handles and event
handlers at run-time with the reactor's internal demultiplexing table described in
implementation activity 3.3 (193). In general, the method for registering event
handlers can be defined using either or both of the following signatures:
??Two parameters. In this design, one parameter identifies the event handler and
another that indicates the type of indication event(s) the event handler has
registered to process. The method's implementation uses 'double-dispatching'
[GoF95] to obtain a handle by calling back to an event handler method
get_handle(). The advantage of this design is that the 'wrong' handle cannot
be associated with an event handler accidentally.
Three parameters. In this design a third parameter is used to pass the handle
explicitly. Although this design can be more error-prone than the two-parameter
signature, it allows an application to register the same event handler for multiple
handles, which may help to conserve memory.
Both types of registration methods store their parameters into the appropriate
demultiplexing table, as indicated by the handle.
The reactor interface also defines two other overloaded methods, which we call
remove_handler(), that can be used to remove an event handler from a reactor.
For example, an application may no longer want to process one or more types of
indication events on a particular handle. These methods remove the event handler
from a reactor's internal demultiplexing table so that it is no longer registered for any
types of indication events. The signatures of the methods that remove an event
handler can be passed either a handle or an event handler in the same way as the
event handler registration methods.
The reactor interface also defines its main entry point method, which we call
handle_events(), that applications can use to run their reactive event loop. This
method calls the synchronous event demultiplexer to wait for indication events to
occur on its handle set. An application can use the timeout parameter to bound the
time it spends waiting for indication events, so that the application will not block
indefinitely if events never arrive.
When one or more indication events occur on the handle set, the synchronous event
demultiplexer function returns. At this point the handle_events() method 'reacts'
by demultiplexing to the event handler associated with each handle that is now ready.
It then dispatches the handler's hook method to process the event.
3. Implement the reactor interface. Four sub-activities help implement the reactor
interface defined in implementation activity 2 (189):
0. Develop a reactor implementation hierarchy. The reactor interface abstraction
illustrated in implementation activity 2 (189) delegates all its demultiplexing
and dispatching processing to a reactor implementation, which plays the role
of the implementation hierarchy in the Bridge pattern [GoF95]. This design
makes it possible to implement and configure multiple types of reactors
transparently. For example, a concrete reactor implementation can be created
using different types of synchronous event demultiplexers, such as select()
[Ste98], poll() [Rago93], or WaitForMultipleObjects() [Sol98], each
of which provides the features and limitations described in implementation
activity 3.2 (192).
1. In our example the base class of the reactor implementation hierarchy is
defined by the class Reactor_Implementation. We omit its declaration
here because this class has essentially the same interface as the
Reactor interface in implementation activity 2 (189). The primary
difference is that its methods are pure virtual, because it forms the base of
a hierarchy of concrete reactor implementations.
3. Choose a synchronous event demultiplexer mechanism. The reactor
implementation calls a synchronous event demultiplexer to wait for one or
more indication events to occur on the reactor's handle set. This call returns
when any handle(s) in the set are 'ready', meaning that operations can be
invoked on the handles without blocking the application process. The
synchronous event demultiplexer, as well as the handles and handle sets, are
often existing operating system mechanisms, so they need not be developed
by reactor implementors.
The select() function examines the three 'file descriptor set' (fd_set)
parameters whose addresses are passed in read_fds, write_fds,
and except_fds to see if any of their handles are 'ready for reading',
'reading for writing', or have an 'exceptional condition', respectively.
Collectively, the handle values in these three file descriptor set parameters
constitute the handle set participant in the Reactor pattern.
9. The select() function can return multiple 'ready' handles to its caller in a
single invocation. It cannot be called concurrently on the same handle set
by multiple threads of control, however, because the operating system will
erroneously notify more than one thread calling select() when I/O
events are pending on the same subset of handles [Ste98]. In addition,
select() does not scale up well when used with a large set of handles
[BaMo98].
11. Two other synchronous event demultiplexers that are available on some
operating systems are the poll() and WaitForMultipleObjects()
functions. These two functions have similar scalability problems as select().
They are also less portable, because they are only available on platforms
compatible with Win32 and System V Release 4 UNIX, respectively. The
Variants section describes a unique feature of
WaitForMultipleObjects() that allows it to be called concurrently on the
same handle set by multiple threads of control.
12. Implement a demultiplexing table. In addition to calling the synchronous event
demultiplexer to wait for indication events to occur on its handle set, a reactor
implementation maintains a demultiplexing table. This table is a manager
[Som97] that contain a set of <handle, event handler, indication event types>
tuples. Each handle serves as a 'key' that the reactor implementation uses to
associate handles with event handlers in its demultiplexing table. This table
also stores the type of indication event(s), such as CONNECT and READ, that
each event handler has registered on its handle.
The demultiplexing table can be implemented using various search strategies,
such as direct indexing, linear search, or dynamic hashing. If handles are
represented as a continuous range of integers, as they are on UNIX platforms,
direct indexing is most efficient, because demultiplexing table tuple entries can
be located in constant O(1) time.
On platforms like Win32 where handles are non-contiguous pointers, direct
indexing is infeasible. Some type of linear search or hashing must therefore be
used to implement a demultiplexing table.
I/O handles in UNIX are contiguous integer values, which allows our
demultiplexing table to be implemented as a fixed-size array of structs.
In this design, the handle values themselves index directly into the
demultiplexing table's array to locate event handlers or event registration
types in constant time. The following class illustrates such an
implementation that maps HANDLEs to Event_Handlers and
Event_Types:
In this simple implementation, the Demux_Table's table_ array is
indexed by UNIX I/O handle values, which are unsigned integers ranging
from 0 to FD_SETSIZE-1. Naturally, a more portable solution should
encapsulate the UNIX-specific implementation details with a wrapper
facade (47).
13. Define the concrete reactor implementation. As shown in implementation
activity 2 (189), the reactor interface holds a pointer to a concrete reactor
implementation and forwards all method calls to it.
14. Our concrete reactor implementation uses select() as its synchronous
event demultiplexer and the Demux_Table class as its demultiplexing
table. It inherits from the Reactor_Implementation class and
overrides its pure virtual methods:
Note that this implementation only works on operating system platforms
where I/O handles are implemented as contiguous unsigned integers,
such as UNIX. Implementing this pattern on platforms where handles are
non-contiguous pointers, such as Win32, therefore requires an additional
data structure to keep track of which handles are in use.
Determine the number of reactors needed in an application. Many applications can be
structured using a single instance of the Reactor pattern. In this case the reactor can
be implemented using the Singleton pattern [GoF95], as shown in implementation
activity 2 (189). This pattern is useful for centralizing event demultiplexing and
dispatching in one reactor instance within an application.
However, some operating systems limit the number of handles that it is possible to
wait for within a single thread of control. Win32, for example, allows
WaitForMultipleObjects() to wait for a maximum of 64 handles in a single
thread. To develop a scalable application in this case, it may be necessary to create
multiple threads, each of which runs its own instance of the Reactor pattern.
Allocating a separate reactor to each of the multiple threads can also be useful for
certain types of real-time applications [SMFG00]. For example, different reactors can
be associated with threads running at different priorities. This design provides
different quality of service levels to process indication events for different types of
synchronous operations.
Note that event handlers are only serialized within an instance of the Reactor pattern.
Multiple event handlers in multiple threads can therefore run in parallel. This
configuration may necessitate the use of additional synchronization mechanisms if
event handlers in different threads access shared state concurrently. The Variants
section describes techniques for adding concurrency control to reactor and event
handler implementations.
5. Implement the concrete event handlers. Concrete event handlers derive from the event
handler interface described in implementation activity 1 (186) to define application specific functionality. Three sub-activities must be addressed when implementing
concrete event handlers.
0. Determine policies for maintaining state in concrete event handlers. An event
handler may need to maintain state information associated with a particular
request. In our example, this could occur when an operating system notifies
the logging server that only part of a logging record was read from a Socket,
due to the occurrence of transport-level flow control. As a result, a concrete
event handler may need to buffer the logging record fragment and return to the
reactor's event loop to await notification that the remainder of the record has
arrived. The concrete event handler must therefore keep track of the number
of bytes read so that it can append subsequent data correctly.
1. Implement a strategy to configure each concrete event handler with a handle.
A concrete event handler performs operations on a handle. The two general
strategies for configuring handles with event handlers are:
??Hard-coded. This strategy hard-codes handles, or wrapper facades
(47) for handles, into the concrete event handler. This strategy is
straightforward to implement, but is less reusable if different types of
handles or IPC mechanisms must be configured into an event handler for
different use cases.
??The Example Resolved section illustrates the SOCK_Acceptor and
SOCK_Stream classes, which are hard-coded into the logging server
components. These two classes are wrapper facades that are defined
in the Implementation section of the Wrapper Facade pattern (47).
They encapsulate the stream Socket semantics of socket handles
within a portable and type-secure object-oriented interface. In the
Internet domain, stream Sockets are implemented using TCP.
??Generic. A more generic strategy is to instantiate wrapper facades
(47) via parameterized types in a templatized event handler class. This
strategy creates more flexible and reusable event handlers, although it
may be unnecessarily general if a single type of handle or IPC
mechanism is always used.
??The Acceptor, Connector, and Service_Handler classes
shown in the Implementation section of the Acceptor-Connector
pattern (285) are templates instantiated with wrapper facades.
2. Implement concrete event handler functionality. Application developers must
decide the processing actions to be performed to implement a service when its
corresponding hook method is invoked by a reactor implementation. To
separate connection-establishment functionality from subsequent service
processing, concrete event handlers can be divided into several categories in
accordance with the Acceptor-Connector pattern (285). In particular, service
handlers implement application-specific services, whereas the reusable
acceptors and connectors establish connections on behalf of these service
handlers passively and actively, respectively.
Our logging server uses a singleton reactor implemented via the select() synchronous
event demultiplexer along with two concrete event handlers¡ªlogging acceptor and logging
handler¡ªthat accept connections and handle logging requests from clients, respectively.
Before we discuss the implementation of the two concrete event handlers, which are based
on the single-method dispatch interface strategy, we first illustrate the general behavior of
the logging server using two scenarios.
The first scenario depicts the sequence of steps performed when a client connects to the
logging server:
??The logging server first registers the logging acceptor with the reactor (1) to handle
indication events corresponding to client connection requests. The logging server next
invokes the event loop method of the reactor singleton (2).
??The reactor singleton invokes the synchronous event demultiplexing select()
operation to wait for connection indication events or logging data indication events to
arrive (3). At this point, all further processing on the server is driven by the reactive
demultiplexing and dispatching of event handlers.
??A client sends a connection request to the logging server (4), which causes the reactor
singleton to dispatch the logging acceptor's handle_event() hook method (5) to
notify it that a new connection indication event has arrived.
??The logging acceptor accepts the new connection (6) and creates a logging handler to
service the new client (7).
??The logging handler registers its socket handle with the reactor singleton (8) and
instructs the reactor to notify it when the reactor receives an indication event signaling
that the Socket is now 'ready for reading'.
After the client is connected, it can send logging records to the server using the socket
handle that was connected in step 6.
The second scenario therefore depicts the sequence of steps performed by the reactive
logging server to service a logging record:
??A client sends a logging record request (1), which causes the server's operating
system to notify the reactor singleton that an indication event is pending on a handle it is
select()'ing on.
??The reactor singleton dispatches the handle_event() method of the logging handler
associated with this handle (2), to notify it that the new indication event is intended for it.
??The logging handler reads the record from the Socket in a non-blocking manner (3).
Steps 2 and 3 are repeated until the logging record has been completely received from
the socket handle.
??The logging handler processes the logging record and writes it to the standard output
of the logging server (4), from which it can be redirected to the appropriate output
device.
??The logging handler returns control back to the reactor's event loop (5), which
continues to wait for subsequent indication events.
The following code implements the concrete event handlers for our logging server example.
A Logging_Acceptor class provides passive connection establishment and a
Logging_Handler class provides application-specific data reception and processing.
The Logging_Acceptor class is an example of the acceptor component in the Acceptor Connector pattern (285). It decouples the task of connection establishment and service
initialization from the tasks performed after a connection is established and a service is
initialized. The pattern enables the application-specific portion of a service, such as the
Logging_Handler, to vary independently of the mechanism used to establish the
connection and initialize the handler.
A Logging_Acceptor object accepts connection requests from client applications
passively and creates client-specific Logging_Handler objects, which receive and process
logging records from clients. Note that Logging_Handler objects maintain sessions with
their connected clients. A new connection is therefore not established for every logging
record.
The Logging_Acceptor class inherits from the 'single-method' dispatch interface variant
of the Event_Handler base class that was defined in implementation activity 1.2 (187).
The Logging_Acceptor constructor registers itself with a reactor for ACCEPT events:
Note that the register_handler() method 'double dispatches' to the
Logging_Acceptor's get_handle() method to obtain its passive-mode socket handle.
From this point, whenever a connection indication arrives the reactor dispatches the
Logging_Acceptor's handle_event() method, which is a factory method [GoF95]:
The handle_event() hook method invokes the accept() method of the
SOCK_Acceptor, which initializes a SOCK_Stream. After the SOCK_Stream is connected
with the new client passively, a Logging_Handler object is allocated dynamically in the
logging server to process the logging requests.
The final method in this class returns the I/O handle of the underlying passive-mode socket:
This method is called by the reactor singleton when the Logging_Acceptor is registered.
The private portion of the Logging_Acceptor class is hard-coded to contain a
SOCK_Acceptor wrapper facade (47):
The SOCK_Acceptor handle factory enables a Logging_Acceptor object to accept
connection indications on a passive-mode socket handle that is listening on a transport
endpoint. When a connection arrives from a client, the SOCK_Acceptor accepts the
connection passively and produces an initialized SOCK_Stream. The SOCK_Stream is then
uses TCP to transfer data reliably between the client and the logging server.
The Logging_Handler class receives and processes logging records sent by a client
application. As with the Logging_Acceptor class shown above, the Logging_Handler
inherits from Event_Handler so that its constructor can register itself with a reactor to be
dispatched when READ events occur:
Subsequently, when a logging record arrives at a connected Socket and the operating
system generates a corresponding READ indication event, the reactor dispatches the
handle_event() method of the associated Logging_Handler automatically:
The handle_event() method receives, processes, and writes the logging record[3] to the
standard output (STDOUT). Similarly, when the client closes down the connection, the reactor
passes the CLOSE event flag, which informs the Logging_Handler to shut down its
SOCK_Stream and delete itself. The final method in this class returns the handle of the
underlying data-mode stream socket:
This method is called by the reactor when the Logging_Handler is registered. The private
portion of the Logging_Handler class is hard-coded to contain a SOCK_Stream wrapper
facade (47):
The logging server contains a single main() function that implements a single-threaded
logging server that waits in the reactor singleton's handle_events() event loop:
As requests arrive from clients and are converted into indication events by the operating
system, the reactor singleton invokes the hook methods on the Logging_Acceptor and
Logging_Handler concrete event handlers to accept connections, and receive and
process logging records, respectively.
The Implementation section described the activities involved in implementing a reactor that
demultiplexes indication events from a set of I/O handles within a single thread of control.
The following are variations of the Reactor pattern that are needed to support concurrency,
re-entrancy, or timer-based events.
Thread-safe Reactor. A reactor that drives the main event loop of a single-threaded
application requires no locks, because it serializes the dispatching of event handler
handle_event() hook methods implicitly within its application process.
However, a reactor also can serve as a single-threaded demultiplexer/dispatcher in multi threaded applications. In this case, although only one thread runs the reactor's
handle_events() event loop method, multiple application threads may register and
remove event handlers from the reactor. In addition, an event handler called by the reactor
may share state with other threads and work on that state concurrently with them. Three
issues must be addressed when designing a thread-safe reactor:
??Preventing race conditions. Critical sections within a reactor must be serialized to
prevent race conditions from occurring when multiple application threads modify the
reactor's internal shared state. A common technique for preventing race conditions is to
use mutual exclusion mechanisms, such as semaphores or mutexes, to protect internal
state shared by multiple threads.
For example, a mutex can be added to the reactor's demultiplexing table, and the
Scoped Locking idiom (325) can be used in the reactor's methods for registering and
removing event handlers to acquire and release this lock automatically. This
enhancement helps ensure that multiple threads cannot corrupt the reactor's
demultiplexing table by registering or removing handles and event handlers
simultaneously.
To ensure the reactor implementation is not penalized when used in single-threaded
applications, the Strategized Locking pattern (333) can be applied to parameterize the
locking mechanism.
??Preventing self-deadlock. In multi-threaded reactors, the reactor implementation
described in implementation activity 3.4 (194) must be serialized, to prevent race
conditions when registering, removing, and demultiplexing event handlers. However, if
this serialization is not added carefully, self-deadlock can occur when the reactor's
handle_events() method calls back on application-specific concrete event handlers
that then subsequently re-enter the reactor via its event handler registration and
removal methods.
To prevent self-deadlock, mutual exclusion mechanisms can use recursive locks
[Sch95], which can be re-acquired by the thread that owns the lock without incurring
self-deadlock on the thread. In the Reactor pattern, recursive locks help prevent
deadlock when locks are held by the same thread across event handler hook methods
dispatched by a reactor.
??Explicitly notify a waiting reactor event loop thread. The thread running a reactor's
event loop often spends much of its time waiting on its synchronous event demultiplexer
for indication events to occur on its handle set. The reactor event loop thread may
therefore need to be notified explicitly when other threads change the contents of its
demultiplexing table by calling its methods for registering and removing event handlers.
It may not otherwise find out about these changes until much later, which may impede
its responsiveness to important events.
An efficient way for an application thread to notify the reactor thread is to pre-establish a
pair of 'writer/reader' IPC handles when a reactor is initialized, such as a UNIX pipe or a
'loopback' TCP Socket connection. The reader handle is registered with the reactor
along with a special 'notification event handler', whose purpose is simply to wake up the
reactor whenever a byte is sent to it via its connected writer handle.
When any application thread calls the reactor's methods for registering and removing
event handlers, they update the demultiplexing table and then send a byte to the writer
handle. This wakes up the reactor's event loop thread and allows it to reconstruct its
updated handle set before waiting on its synchronous event demultiplexer again.
Concurrent Event Handlers. The Implementation section described a single-threaded
reactive dispatching design in which event handlers borrow the thread of control of a reactor.
Event handlers can also run in their own thread of control. This allows a reactor to
demultiplex and dispatch new indication events concurrently with the processing of hook
methods dispatched previously to its event handlers. The Active Object (369),
Leader/Followers (447), and Half-Sync/Half-Async (423) patterns can be used to implement
concurrent concrete event handlers.
Concurrent Synchronous Event Demultiplexer. The synchronous event demultiplexer
described in the Implementation section is called serially by a reactor in a single thread of
control. However, other types of synchronous event demultiplexers, such as the
WaitForMultiple Objects() function, can be called concurrently on the same handle
set by multiple threads.
When it is possible to initiate an operation on one handle without the operation blocking, the
concurrent synchronous event demultiplexer returns a handle to one of its calling threads.
This can then dispatch the appropriate hook method on the associated event handler.
Calling the synchronous event demultiplexer concurrently can improve application
throughput, by allowing multiple threads to simultaneously demultiplex and dispatch events
to their event handlers. However, the reactor implementation can become much more
complex and much less portable.
For example, it may be necessary to perform a reference count of the dispatching of event
handler hook methods. It may also be necessary to queue calls to the reactor's methods for
registering and removing event handlers, by using the Command pattern [GoF95] to defer
changes until no threads are dispatching hook methods on an event handler. Applications
may also become more complex if concrete event handlers must be made thread-safe.
Re-entrant Reactors. In general, concrete event handlers just react when called by a reactor
and do not invoke the reactor's event loop themselves. However, certain situations may
require concrete event handlers to retrieve specific events by invoking a reactor's
handle_events() method to run its event loop. For example, the CORBA asynchronous
method invocation (AMI) feature [ARSK00] requires an ORB Core to support nested
work_pending()/perform_work() ORB event loops. If the ORB Core uses the Reactor
pattern [SC99], therefore, its reactor implementation must be re-entrant.
A common strategy for making a reactor re-entrant is to copy the handle set state
information residing in its demultiplexing table to the run-time stack before calling the
synchronous event demultiplexer. This strategy ensures that any changes to the handle set
will be local to that particular nesting level of the reactor.
Integrated Demultiplexing of Timer and I/O Events. The reactor described in the
Implementation section focuses primarily on demultiplexing and dispatching features
necessary to support our logging server example. It therefore only demultiplexes indication
events on handle sets. A more general reactor implementation can integrate the
demultiplexing of timer events and I/O events.
A reactor's timer mechanism should allow applications to register time-based concrete event
handlers. This mechanism then invokes the handle_timeout() methods of the event
handlers at an application-specified future time. The timer mechanism in a reactor can be
implemented using various strategies, including heaps [BaLee98], delta-lists [CoSte91], or
timing wheels [VaLa97]:
??A heap is a 'partially-ordered, almost-complete binary tree' that ensures the average and worst-case time complexity for inserting or deleting a concrete event handler is
O(log n).
??Delta-lists store time in 'relative' units represented as offsets or 'deltas' from the earliest
timer value at the front of the list.
??Timing wheels use a circular buffer that makes it possible to start, stop, and maintain
timers within the range of the wheel in constant O(1) time.
Several changes are required to the Reactor interface defined in implementation
activity 2 (189) to enable applications to schedule, cancel, and invoke timer-based event
handlers:
An application uses the schedule() method to schedule a concrete event handler to
expire after future_time. An asynchronous completion token (ACT) (261) can be
passed to schedule(). If the timer expires the ACT is passed as the value to the event
handler's handle_timeout() hook method. The schedule() method returns a timer
id value that identifies each event handler's registration in the reactor's timer queue
uniquely. This timer id can be passed to the cancel() method to remove an event
handler before it expires. If a non-NULL act parameter is passed to cancel(), it will be
assigned the ACT passed by the application when the timer was scheduled originally,
which makes it possible to delete dynamically-allocated ACTs to avoid memory leaks.
To complete the integration of timer and I/O event demultiplexing, the reactor
implementation must be enhanced to allow for both the timer queue's scheduled event
handler deadlines and the timeout parameter passed to the handle_events()
method. This method is typically generalized to wait for the closest deadline, which is
either the timeout parameter or the earliest deadline in the timer queue.
InterViews [LC87]. The Reactor pattern is implemented by the InterViews windowing
system, where it is known as the Dispatcher. The InterViews Dispatcher is used to define an
application's main event loop and to manage connections to one or more physical GUI
displays. InterViews therefore illustrates how the Reactor pattern can be used to implement
reactive event handling for graphical user interface systems that play the role of both client
and server.
The Xt toolkit from the X Windows distribution uses the Reactor pattern to implement its
main event loop. Unlike the Reactor pattern implementation described in the Implementation
section, callbacks in the Xt toolkit use C function pointers rather than event handler objects.
The Xt toolkit is another example of how the Reactor pattern can be used to implement
reactive event handling for graphical user interface systems that play the role of both client
and server.
ACE Reactor Framework [Sch97]. The ACE framework uses an object-oriented framework
implementation of the Reactor pattern as its core event demultiplexer and dispatcher. ACE
provides a class, called ACE_Reactor, that defines a common interface to a variety of
reactor implementations, such as the ACE_Select_Reactor and the
ACE_WFMO_Reactor. These two reactor implementations can be created using different
synchronous event demultiplexers, such as WaitForMultipleObjects() and
select(), respectively.
The ORB Core component in many implementations of CORBA [OMG98a], such as TAO
[SC99] and ORBacus, use the Reactor pattern to demultiplex and dispatch client requests to
servants that process the requests.
Call Center Management System. The Reactor pattern has been used to manage events
routed by Event Servers [SchSu94] between PBXs and supervisors in a Call Center
Management system.
Project Spectrum. The high-speed I/O transfer subsystem of Project Spectrum [PHS96]
uses the Reactor pattern to demultiplex and dispatch events in an electronic medical
imaging system.
Receiving phone calls. The Reactor pattern occurs frequently in everyday life, for example
in telephony. Consider yourself as an event handler that registers with a reactor¡ªa
telecommunication network¡ªto 'handle' calls received on a particular phone number¡ªthe
handle. When somebody calls your phone number, the network notifies you that a 'call
request' event is pending by ringing your phone. After you pick up the phone, you react to
this request and 'process' it by carrying out a conversation with the connected party.
The Reactor pattern offers the following benefits:
Separation of concerns. The Reactor pattern decouples application-independent
demultiplexing and dispatching mechanisms from application-specific hook method
functionality. The application-independent mechanisms can be designed as reusable
components that know how to demultiplex indication events and dispatch the appropriate
hook methods defined by event handlers. Conversely, the application-specific functionality in
a hook method knows how to perform a particular type of service.
Modularity, reusability, and configurability. The pattern decouples event-driven application
functionality into several components. For example, connection-oriented services can be
decomposed into two components: one for establishing connections and another for
receiving and processing data.
This decoupling enables the development and configuration of generic event handler
components, such as acceptors, connectors, and service handlers, that are loosely
integrated together through a reactor. This modularity helps promote greater software
component reuse, because modifying or extending the functionality of the service handlers
need not affect the implementation of the acceptor and connector components.
In our logging server, the Logging_Acceptor class can easily be generalized to create
the acceptor component described in the Acceptor-Connector pattern (285). This
generic acceptor can be reused for many different connection-oriented services, such as
file transfer, remote log-in, and video-on-demand. It is thus straightforward to add new
functionality to the Logging_Handler class without affecting the reusable acceptor
component.
Portability. UNIX platforms offer two synchronous event demultiplexing functions, select()
[Ste98] and poll() [Rago93], whereas on Win32 platforms the
WaitForMultipleObjects() [Sol98] or select() functions can be used to demultiplex
events synchronously. Although these demultiplexing calls all detect and report the
occurrence of one or more indication events that may occur simultaneously on multiple event
sources, their APIs are subtly different. By decoupling the reactor's interface from the lower level operating system synchronous event demultiplexing functions used in its
implementation, the Reactor pattern therefore enables applications to be ported more readily
across platforms.
Coarse-grained concurrency control. Reactor pattern implementations serialize the
invocation of event handlers at the level of event demultiplexing and dispatching within an
application process or thread. This coarse-grained concurrency control can eliminate the
need for more complicated synchronization within an application process.
The Reactor pattern can also incur the following liabilities:
Restricted applicability. The Reactor pattern can be applied most efficiently if the operating
system supports synchronous event demultiplexing on handle sets. If the operating system
does not provide this support, however, it is possible to emulate the semantics of the
Reactor pattern using multiple threads within the reactor implementation. This is possible, for
example, by associating one thread to process each handle.
Whenever events are available on a handle, its associated thread reads the event and
places it on a queue that is processed sequentially by the reactor implementation. This
design can be inefficient, however, because it serializes all the event handler threads. Thus,
synchronization and context switching overhead increases without enhancing application level parallelism.
Non-pre-emptive. In a single-threaded application, concrete event handlers that borrow the
thread of their reactor can run to completion and prevent the reactor from dispatching other
event handlers. In general, therefore, an event handler should not perform long duration
operations, such as blocking I/O on an individual handle, because this can block the entire
process and impede the reactor's responsiveness to clients connected to other handles.
To handle long-duration operations, such as transferring multimegabyte images [PHS96], it
may be more effective to process event handlers in separate threads. This design can be
achieved via an Active Object (369) or Half-Sync/Half-Async (423) pattern variant that
performs services concurrently to the reactor's main event loop.
Complexity of debugging and testing. It can be hard to debug applications structured using
the Reactor pattern due to its inverted flow of control. In this pattern control oscillates
between the framework infrastructure and the method call-backs on application-specific
event handlers. The Reactor's inversion of control increases the difficulty of 'single-stepping'
through the run-time behavior of a reactive framework within a debugger, because
application developers may not understand or have access to the framework code.
These challenges are similar to the problems encountered trying to debug a compiler's
lexical analyzer and parser written with lex and yacc. In such applications, debugging is
straightforward when the thread of control is within user-defined semantic action routines.
After the thread of control returns to the generated Deterministic Finite Automata (DFA)
skeleton, however, it is hard to follow the program's logic.
The Reactor pattern is related to the Observer [GoF95] and Publisher-Subscriber [POSA1]
patterns, where all dependents are informed when a single subject changes. In the Reactor
pattern, however, a single handler is informed when an event of interest to the handler
occurs on a source of events. In general, the Reactor pattern is used to demultiplex
indication events from multiple event sources to their associated event handlers. In contrast,
an observer or subscriber is often associated with only a single source of events.
The Reactor pattern is related to the Chain of Responsibility pattern [GoF95], where a
request is delegated to the responsible service handler. The Reactor pattern differs from the
Chain of Responsibility because the Reactor associates a specific event handler with a
particular source of events. In contrast, the Chain of Responsibility pattern searches the
chain to locate the first matching event handler.
The Reactor pattern can be considered a synchronous variant of the asynchronous Proactor
pattern (215). The Proactor supports the demultiplexing and dispatching of multiple event
handlers that are triggered by the completion of asynchronous operations. In contrast, the
Reactor pattern is responsible for demultiplexing and dispatching multiple event handlers
that are triggered when indication events signal that it is possible to initiate an operation
synchronously without blocking.
The Active Object pattern (369) decouples method execution from method invocation to
simplify synchronized access to shared state by methods invoked in different threads. The
Reactor pattern is often used in lieu of the Active Object pattern when threads are
unavailable or the overhead and complexity of threading is undesirable.
The Reactor pattern can be used as the underlying synchronous event demultiplexer for the
Leader/Followers (447) and Half-Sync/Half-Async (423) pattern implementations. Moreover,
if the events processed by a reactor's event handlers are all short-lived, it may be possible to
use the Reactor pattern in lieu of these other two patterns. This simplification can reduce
application programming effort significantly and potentially improve performance, as well.
Java does not offer a synchronous demultiplexer for network events. In particular, it does not
encapsulate select() due to the challenges of supporting synchronous demultiplexing in a
portable way. It is therefore hard to implement the Reactor pattern directly in Java. However,
Java's event handling in AWT, particularly the listener or delegation-based model, resembles
the Reactor pattern in the following way:
??Typically, application developers reuse prefabricated graphical components, such as
different kinds of buttons. Developers typically write event handlers that encode the
application-specific logic to process certain events, such as a mouse-click on a button.
Before receiving button-related events on a button, an event handler must register itself
with this button for all events of this type, which are called ActionEvents.
??When the underlying native code is called by the Java virtual machine (JVM), it notifies
the button's peer, which is the first Java layer on top of the native code. The button peer
is platform-specific and posts a new ActionEvent to be executed in the event handler
thread, which is a specific-purpose thread created by the JVM.
??Events are then entered into a queue and an EventDispatchThread object runs a
loop to 'pump' events further up the AWT widget hierarchy, which ultimately dispatches
the event to all registered listeners stored in a recursive data structure called
AWTEventMulticaster.
All pumping, dispatching, and subsequent event processing runs synchronously in the same
thread, which resembles the synchronous processing of events by a reactor.
##%%&&
The Proactor architectural pattern allows event-driven applications to efficiently demultiplex
and dispatch service requests triggered by the completion of asynchronous operations, to
achieve the performance benefits of concurrency without incurring certain of its liabilities.
Consider a networking application that must perform multiple operations simultaneously,
such as a high-performance Web server that processes HTTP requests sent from multiple
remote Web browsers [HPS99]. When a user wants to download content from a URL four
steps occur:
1. The browser establishes a connection to the Web server designated in the URL and
then sends it an HTTP GET request.
2. The Web server receives the browser's CONNECT indication event, accepts the
connection, reads and then parses the request.
3. The server opens and reads the specified file.
4. Finally, the server sends the contents of the file back to the Web browser and closes
the connection.
One way to implement a Web server is to use a reactive event demultiplexing model in
accordance with the Reactor pattern (179). In this design, whenever a Web browser
connects to a Web server, a new event handler is created to read, parse, and process the
request and transfer the contents of the file back to the browser. This handler is registered
with a reactor that coordinates the synchronous demultiplexing and dispatching of each
indication event to its associated event handler.
Although a reactive Web server design is straightforward to program, it does not scale up to
support many simultaneous users and/or long-duration user requests, because it serializes
all HTTP processing at the event demultiplexing layer. As a result, only one GET request can
be dispatched and processed iteratively at any given time.
A potentially more scalable way to implement a Web server is to use some form of
synchronous multi-threading. In this model a separate server thread processes each
browser's HTTP GET request [HS98]. For example, a new thread can be spawned
dynamically for each request, or a pool of threads can be pre-spawned and managed using
the Leader/Followers (447) or Half-Sync/Half-Async (423) patterns. In either case each
thread performs connection establishment, HTTP request reading, request parsing, and file
transfer operations synchronously¡ªthat is, server processing operations block until they
complete.
Synchronous multi-threading is a common concurrency model. However, problems with
efficiency, scalability, programming complexity, and portability may occur, as discussed in
the Example section of the Reactor pattern (179).
On operating systems that support asynchronous I/O efficiently, our Web server can
therefore invoke operations asynchronously to improve its scalability further. For example,
on Windows NT the Web server can be implemented to invoke asynchronous Win32
operations that process externally-generated indication events, such as TCP CONNECT and
HTTP GET requests, and transmit requested files to Web browsers asynchronously.
When these asynchronous operations complete, the operating system returns the
associated completion events containing their results to the Web server, which processes
these events and performs the appropriate actions before returning to its event loop. Building
software that achieves the potential performance of this asynchronous event processing
model is hard due to the separation in time and space of asynchronous invocations and their
subsequent completion events. Thus, asynchronous programming requires a sophisticated
yet comprehensible event demultiplexing and dispatching mechanism.
An event-driven application that receives and processes multiple service requests
asynchronously.
The performance of event-driven applications, particularly servers, in a distributed system
can often be improved by processing multiple service requests asynchronously. When
asynchronous service processing completes, the application must handle the corresponding
completion events delivered by the operating system to indicate the end of the asynchronous
computations.
For example, an application must demultiplex and dispatch each completion event to an
internal component that processes the results of an asynchronous operation. This
component can reply to external clients, such as a Web browser client, or to internal clients,
such as the Web server component that initiated the asynchronous operation originally. To
support this asynchronous computation model effectively requires the resolution of four
forces:
??To improve scalability and latency, an application should process multiple completion
events simultaneously without allowing long-duration operations to delay other
operation processing unduly.
??To maximize throughput, any unnecessary context switching, synchronization, and
data movement among CPUs should be avoided, as outlined in the Example section.
??Integrating new or improved services with existing completion event demultiplexing
and dispatching mechanisms should require minimal effort.
??Application code should largely be shielded from the complexity of multi-threading and
synchronization mechanisms.
Split application services into two parts: long-duration operations that execute
asynchronously and completion handlers that process the results of these operations when
they finish. Integrate the demultiplexing of completion events, which are delivered when
asynchronous operations finish, with their dispatch to the completion handlers that process
them. Decouple these completion event demultiplexing and dispatching mechanisms from
the application-specific processing of completion events within completion handlers.
In detail: for every service offered by an application, introduce asynchronous operations that
initiate the processing of service requests 'proactively' via a handle, together with completion
handlers that process completion events containing the results of these asynchronous
operations. An asynchronous operation is invoked within an application by an initiator, for
example, to accept incoming connection requests from remote applications. It is executed by
an asynchronous operation processor. When an operation finishes executing, the
asynchronous operation processor inserts a completion event containing that operation's
results into a completion event queue.
This queue is waited on by an asynchronous event demultiplexer called by a proactor. When
the asynchronous event demultiplexer removes a completion event from its queue, the
proactor demultiplexes and dispatches this event to the application-specific completion
handler associated with the asynchronous operation. This completion handler then
processes the results of the asynchronous operation, potentially invoking additional
asynchronous operations that follow the same chain of activities outlined above.
The Proactor pattern includes nine participants:
Handles are provided by operating systems to identify entities, such as network connections
or open files, that can generate completion events. Completion events are generated either
in response to external service requests, such as connection or data requests arriving from
remote applications, or in response to operations an application generates internally, such
as time-outs or asynchronous I/O system calls.
Our Web server creates a separate socket handle for each Web browser connection. In
Win32 each socket handle is created in 'overlapped I/O' mode, which means that
operations invoked on the handles run asynchronously. The Windows NT I/O subsystem
also generates completion events when asynchronously-executed operations complete.
Asynchronous operations represent potentially long-duration operations that are used in the
implementation of services, such as reading and writing data asynchronously via a socket
handle. After an asynchronous operation is invoked, it executes without blocking its caller's
thread of control. Thus, the caller can perform other operations. If an operation must wait for
the occurrence of an event, such as a connection request generated by a remote
application, its execution will be deferred until the event arrives.
Our proactive Web server invokes the Win32 AcceptEx() operation to accept
connections from Web browsers asynchronously. After accepting connections the Web
server invokes the Win32 asynchronous ReadFile() and WriteFile() operations to
communicate with its connected browsers.
A completion handler specifies an interface that consists of one or more hook methods
[Pree95] [GHJV95]. These methods represent the set of operations available for processing
information returned in the application-specific completion events that are generated when
asynchronous operations finish executing.
Concrete completion handlers specialize the completion handler to define a particular
application service by implementing the inherited hook method(s). These hook methods
process the results contained in the completion events they receive when the asynchronous
operations associated with the completion handler finish executing. A concrete completion
handler is associated with a handle that it can use to invoke asynchronous operations itself.
For example, a concrete completion handler can itself receive data from an asynchronous
read operation it invoked on a handle earlier. When this occurs, the concrete completion
handler can process the data it received and then invoke an asynchronous write operation to
return the results to its connected remote peer application.
Our Web server's two concrete completion handlers¡ªHTTP acceptor and HTTP
handler¡ªperform completion processing on the results of asynchronous AcceptEx(),
ReadFile(), and WriteFile() operations. The HTTP acceptor is the completion
handler for the asynchronous AcceptEx() operation¡ªit creates and connects HTTP
handlers in response to connection request events from remote Web browsers. The
HTTP handlers then use asynchronous ReadFile() and WriteFile() operations to
process subsequent requests from remote Web browsers.
Asynchronous operations are invoked on a particular handle and run to completion by an
asynchronous operation processor, which is often implemented by an operating system
kernel. When an asynchronous operation finishes executing the asynchronous operation
processor generates the corresponding completion event. It inserts this event into the
completion event queue associated with the handle upon which the operation was invoked.
This queue buffers completion events while they wait to be demultiplexed to their associated
completion handler.
In our Web server example, the Windows NT operating system is the asynchronous
operation processor. Similarly, the completion event queue is a Win32 completion port
[Sol98], which is a queue of completion events maintained by the Windows NT kernel on
behalf of an application. When an asynchronous operation finishes the Windows NT
kernel queues the completion event on the completion port associated with the handle
on which the asynchronous operation was originally invoked.
An asynchronous event demultiplexer is a function that waits for completion events to be
inserted into a completion event queue when an asynchronous operation has finished
executing. The asynchronous event demultiplexer function then removes one or more
completion event results from the queue and returns to its caller.
One asynchronous event demultiplexer in Windows NT is
GetQueuedCompletionStatus(). This Win32 function allows event-driven proactive
applications to wait up to an application-specified amount of time to retrieve the next
available completion event.
A proactor provides an event loop for an application process or thread. In this event loop, a
proactor calls an asynchronous event demultiplexer to wait for completion events to occur.
When an event arrives the asynchronous event demultiplexer returns. The proactor then
demultiplexes the event to its associated completion handler and dispatches the appropriate
hook method on the handler to process the results of the completion event.
Our Web server application calls the proactor's event loop method. This method calls the
GetQueuedCompletionStatus() Win32 function, which is an asynchronous event
demultiplexer that waits until it can dequeue the next available completion event from
the proactor's completion port. The proactor's event loop method uses information in the
completion event to demultiplex the next event to the appropriate concrete completion
handler and dispatch its hook method.
An initiator is an entity local to an application that invokes asynchronous operations on an
asynchronous operation processor. The initiator often processes the results of the
asynchronous operations it invokes, in which case it also plays the role of a concrete
completion handler.
In our example HTTP acceptors and HTTP handlers play the role of both initiators and
concrete completion handlers within the Web server's internal thread of control. For
example, an HTTP acceptor invokes AcceptEx() operations that accept connection
indication events asynchronously from remote Web browsers. When a connection
indication event occurs, an HTTP acceptor creates an HTTP handler, which then
invokes an asynchronous ReadFile() operation to retrieve and process HTTP GET
requests from a connected Web browser.
Note how in the Proactor pattern the application components, represented by initiators and
concrete completion handlers, are proactive entities. They instigate the control and data flow
within an application by invoking asynchronous operations proactively on an asynchronous
operation processor.
When these asynchronous operations complete, the asynchronous operation processor and
proactor collaborate via a completion event queue. They use this queue to demultiplex the
resulting completion events back to their associated concrete completion handlers and
dispatch these handlers' hook methods. After processing a completion event, a completion
handler may invoke new asynchronous operations proactively.
The structure of the participants in the Proactor pattern is illustrated in the following class
diagram:
The following collaborations occur in the Proactor pattern:
??An application component playing the role of an initiator invokes an asynchronous
operation on an asynchronous operation processor via a particular handle. In addition to
passing data parameters to the asynchronous operation, the initiator also passes
certain completion processing parameters, such as the completion handler or a handle
to the completion event queue. The asynchronous operation processor stores these
parameters internally for later use.
?The HTTP handler in our Web server can instruct the operating system to read a new
HTTP GET request by invoking the ReadFile() operation asynchronously on a
particular socket handle. When initiating this operation on the handle, the HTTP
handler passes itself as the completion handler so that it can process the results of
an asynchronous operation.
??After an initiator invokes an operation on the asynchronous operation processor, the
operation and initiator can run independently. In particular, the initiator can invoke new
asynchronous operations while others continue to execute concurrently.[4] If the
asynchronous operation is intended to receive a service request from a remote
application, the asynchronous operation processor defers the operation until this
request arrives. When the event corresponding to the expected request arrives, the
asynchronous operation will finish executing.
?The Windows NT operating system defers the asynchronous ReadFile() operation
used to read an HTTP GET request until this request arrives from a remote Web
browser.
??When an asynchronous operation finishes executing, the asynchronous operation
processor generates a completion event. This event contains the results of the
asynchronous operation. The asynchronous operation processor then inserts this event
into the completion event queue associated with the handle upon with the asynchronous
operation was originally invoked.
?If an HTTP handler invoked an asynchronous ReadFile() operation to read an HTTP
GET request, the Windows NT operating system will report the completion status in
the completion event, such as its success or failure and the number of bytes read.
??When an application is ready to process the completion events resulting from its
asynchronous operations, it invokes the proactor's event loop entry-point method, which
we call handle_events(). This method calls an asynchronous event demultiplexer[5]
to wait on its completion event queue for completion events to be inserted by the
asynchronous operation processor. After removing a completion event from the queue
the proactor's handle_events() method demultiplexes the event to its associated
completion handler. It then dispatches the appropriate hook method on the completion
handler, passing it the results of the asynchronous operation.
?The proactor in our Web server example uses a Win32 completion port as its
completion event queue. Similarly, it uses the Win32
GetQueuedCompletionStatus() function [Sol98] as its asynchronous event
demultiplexer to remove completion events from a completion port.
??The concrete completion handler then processes the completion results it receives. If
the completion handler returns a result to its caller, two situations are possible. First, the
completion handler that processes the results of the asynchronous operations also can
be the initiator that invoked the operation originally. In this case the completion handler
need not perform additional work to return the result to its caller, because it is the caller.
Second, a remote application or an application internal component may have requested
the asynchronous operation. In this case, the completion handler can invoke an
asynchronous write operation on its transport handle to return results to the remote
application.
In response to an HTTP GET request from a remote Web browser, an HTTP
handler might instruct the Windows NT operating system to transmit a large file
across a network by calling WriteFile() asynchronously. After the operating
system completes the asynchronous operation successfully the resulting completion
event indicates the number of bytes transferred to the HTTP handler. The entire file
may not be transferred in one WriteFile() operation due to transport-layer flow
control. In this case the HTTP handler can invoke another asynchronous
WriteFile() operation at the appropriate file offset.
??After the completion handler finishes its processing it can invoke other asynchronous
operations, in which case the whole cycle outlined in this section begins again.
The participants in the Proactor pattern can be decomposed into two layers:
??Demultiplexing/dispatching infrastructure layer components. This layer performs
generic, application-independent strategies for executing asynchronous operations. It
also demultiplexes and dispatches completion events from these asynchronous
operations to their associated completion handlers.
??Application layer components. This layer defines asynchronous operations and
concrete completion handlers that perform application-specific service processing.
The implementation activities in this section start with the generic demultiplexing/dispatching
infrastructure components and then cover the application components. We focus on a
proactor implementation that is designed to invoke asynchronous operations and dispatch
hook methods on their associated completion handlers using a single thread of control. The
Variants section describes the activities associated with developing multi-threaded proactor
implementations.
1. Separate application services into asynchronous operations and completion handlers.
To implement the Proactor pattern, application services must be designed to separate
the initiation of asynchronous operations via a handle from the processing of these
operations' results. Asynchronous operations are often long-duration and/or
concerned with I/O, such as reading and writing data via a socket handle or
communicating with a database. The results of asynchronous operations are
processed by completion handlers. In addition to processing results, completion
handlers can play the role of initiators, that is, they invoke asynchronous operations
themselves.
The products of this activity are a set of asynchronous operations, a set of completion
handlers, and a set of associations between each asynchronous operation and its
completion handler.
2. Define the completion handler interface. Completion handlers specify an interface
consisting of one or more hook methods [Pree95]. These hook methods represent the
completion handling for application-specific completion events generated when
asynchronous operations finish executing. The implementation of completion
handlers consists of three sub-activities:
1. Define a type to convey the results of asynchronous operation. When an
asynchronous operation completes or is canceled its completion event results
must be conveyed to its completion handler. These results indicate its success
or failure and the number of bytes that were transmitted successfully. The
Adapter pattern [GoF95] is often used to convert information stored in a
completion event into a form used to dispatch to its associated concrete
completion handler.
Deriving Async_Result from the OVERLAPPED struct allows
applications to add custom state and methods to the results of
asynchronous operations. C++ inheritance is used because the Win32 API
does not provide a more direct way to pass a per-operation result object to
the operating system when an asynchronous operation is invoked.
28. Determine the type of the dispatching target. Two types of completion handlers
can be associated with a handle to serve as the target of a proactor's
dispatching mechanism, objects and pointers to functions. Implementations of
the Proactor pattern can choose the type of dispatching target based on the
same criteria described in implementation activity 1.1 of the Reactor (179)
pattern.
29. Define the completion handler dispatch interface strategy. We next define the
type of interface supported by the completion handler to process completion
events. As with the Reactor pattern (179), assuming that we use completion
handler objects rather than pointers to functions, two general strategies exist:
??Single-method dispatch interface strategy. The class diagram in the
Structure section illustrates an implementation of the
Completion_Handler interface that contains a single event handling
method, which we call handle_event(). A proactor uses this method to
dispatch completion events to their associated completion handlers. In
this case the type of completion event that has occurred is passed as a
parameter to the method. The second parameter is the base class for all
asynchronous results, which, depending on the completion event, can be
further downcast to the correct type.
The single-method dispatch interface strategy makes it possible to add
new types of events without changing the class interface. However, to
handle a specific event, this strategy encourages the use of C++ switch
and if statements in the concrete event handler's handle_event()
method implementation, which degrades its internal extensibility.
??Multi-method dispatch interface strategy. A different strategy for
implementing the Completion_Handler interface is to define separate
hook methods for handling each type of event, such as
handle_read(), handle_write(), or handle_accept(). This
strategy can be more extensible than the single-method dispatch interface
because the demultiplexing is performed by a proactor implementation,
rather than by a concrete event handler's handle_event() method
implementation.
The multi-method dispatch interface strategy makes it easy to override
methods in the base class selectively, which avoids further demultiplexing
via switch or if statements in the hook method implementation.
However, this strategy requires pattern implementors to anticipate the
hook methods in advance. The various handle_*() hook methods in the
Completion_Handler interface above are tailored for networking
events. However, these methods do not encompass all the types of
events handled via the Win32 WaitForMultipleObjects()
mechanism, such as synchronization object events [SchSt95].
30. Both the single-method and multiple-method dispatch interface strategies are
implementations of the Hook Method [Pree95] and Template Method [GoF95]
patterns. The intent of these patterns is to provide well-defined hooks that can
be specialized by applications and called back by lower-level dispatching
code.
31. Completion handlers are often designed to act both as a target of a proactor's
completion dispatching and an initiator that invokes asynchronous operations,
as shown by the HTTP_Handler class in the Example Resolved section.
Therefore, the constructor of class Completion_Handler associates a
Completion_Handler object with a pointer to a proactor. This design allows
a Completion_Handler's hook methods to invoke new asynchronous
operations whose completion processing will be dispatched ultimately by the
same proactor.
3. Implement the asynchronous operation processor. An asynchronous operation
processor executes operations asynchronously on behalf of initiators. Its primary
responsibilities therefore include:
??Defining the asynchronous operation interface
??Implementing a mechanism to execute operations asynchronously and
generating and
??Queueing completion events when an operation finishes
4. Define the asynchronous operation interface. Asynchronous operations can be
passed various parameters, such as a handle,[6] data buffers, buffer lengths,
and information used to perform completion processing when the operation
finishes. Two issues must be addressed when designing a programming
interface that initiators use to invoke asynchronous operations on an
asynchronous operation processor:
??Maximizing portability and flexibility. Asynchronous operations can be
used to read and write data on multiple types of I/O devices, such as
networks and files, and on multiple operating systems, such as Windows
NT, VMS, Solaris, and Linux. The Wrapper Facade (47) and Bridge
[GoF95] patterns can be applied to decouple the asynchronous operation
interface from underlying operating system dependencies and ensure the
interface works for multiple types of I/O devices.
??Handling multiple completion handlers, proactors, and completion
event queues efficiently and concisely. More than one completion
handler, proactor, and completion event queue can be used
simultaneously within an application. For example, different proactors can
be associated with threads running at different priorities, to provide
different quality of service levels for processing different completion
handlers. In addition to its data parameters, an asynchronous operation
must then indicate which handle, concrete completion handler, proactor,
and completion event queue to use when processing the completion of
asynchronous operations.
A common strategy to consolidate all this completion processing
information efficiently is to apply the Asynchronous Completion Token
pattern (261). When an initiator invokes an asynchronous operation on a
handle, an asynchronous completion token (ACT) can then be passed to
the asynchronous operation processor, which can store this ACT for later
use. Each ACT contains information that identifies a particular operation
and guides its subsequent completion processing.
When an asynchronous operation finishes executing, the asynchronous
operation processor locates the operation's ACT it stored earlier and
associates it with the completion event it generates. It then inserts this
updated completion event into the appropriate completion event queue.
Ultimately, the proactor that runs the application's event loop will use an
asynchronous event demultiplexer to remove the completion event results
and ACT from its completion event queue. The proactor will then use this
ACT to complete its demultiplexing and dispatching of the completion
event results to the completion handler designated by the ACT.
Although our Web server is implemented using Win32 asynchronous
Socket operations, we apply the Wrapper Facade pattern (47) to
generalize this class and make it platform-independent. It can
therefore be used for other types of I/O devices supported by an
asynchronous operation processor.
This class plays the role of an ACT and an adapter [GoF95]. It inherits
from Async_Result, which itself inherits from the Win32
OVERLAPPED struct, as shown in implementation activity 2.1 (227).
The ACT can be passed as the lpOverlapped parameter to the
ReadFile() asynchronous function. ReadFile() forwards the ACT
to the Windows NT operating system, which stores it for later use.
When the asynchronous ReadFile() operation finishes it generates
a completion event that contains the ACT it received when this
operation was invoked. When the proactor's handle_events()
method removes this event from its completion event queue, it
invokes the complete() method on the
Async_Stream_Read_Result. This adapter method then
dispatches the completion handler's handle_event() hook method
to pass the event, as shown in implementation activity 5.4 (240).
5. Choose the asynchronous operation processing mechanism. When an initiator
invokes an asynchronous operation, an asynchronous operation processor
executes the operation without blocking the initiator's thread of control. An
asynchronous operation processor provides mechanisms for managing ACTs
and executing operations asynchronously. It also generates completion events
when operations finish and queues the events into the appropriate completion
event queue.
Some asynchronous operation processors allow initiators to cancel
asynchronous operations. However, completion events are still generated.
Thus, ACTs and other resources can be reclaimed properly by completion
handlers.
Certain operating environments provide these asynchronous operation
execution and completion event generation mechanisms, such as Real-time
POSIX [POSIX95] and Windows NT [Sol98]. In this case implementing the
asynchronous completion processor participant simply requires mapping
existing operating system APIs onto the asynchronous operation wrapper
facade (47) interfaces described in implementation activity 3.1 (232). The
Variants section describes techniques for emulating an asynchronous
operation processor on operating system platforms that do not support this
feature natively.
4. Define the proactor interface. The proactor's interface is used by applications to invoke
an event loop that removes completion events from a completion event queue,
demultiplexes them to their designated completion handlers, and dispatches their
associated hook method. The proactor interface is often accessed via a singleton
[GoF95] because a single proactor is often sufficient for each application process.
The Proactor pattern can use the Bridge pattern [GoF95] to shield applications from
complex and non-portable completion event demultiplexing and dispatching
mechanisms. The proactor interface corresponds to the abstraction participant in the
Bridge pattern, whereas a platform-specific proactor instance is accessed internally
via a pointer, in accordance with the implementation hierarchy in the Bridge pattern.
A proactor interface also defines a method, which we call register_handle(),
that associates a handle with the proactors completion event queue, as described in
implementation activity 5.5 (240). This association ensures that the completion events
generated when asynchronous operations finish executing will be inserted into a
particular proactor's completion event queue.
The proactor interface also defines the main entry point method, we call it
handle_events(), that applications use to run their proactive event loop.[7] This
method calls the asynchronous event demultiplexer, which waits for completion
events to arrive on its completion event queue, as discussed in implementation
activity 3.1 (232). An application can use the timeout parameter to bound the time it
spends waiting for completion events. Thus, the application need not block
indefinitely if events never arrive.
After the asynchronous operation processor inserts a completion event into the
proactor's completion event queue, the asynchronous event demultiplexer function
returns. At this point the proactor's handle_events() method dequeues the
completion event and uses its associated ACT to demultiplex to the asynchronous
operation's completion handler and dispatch the handler's hook method.
5. Implement the proactor interface. Five sub-activities can be used to implement the
proactor interface:
0. Develop a proactor implementation hierarchy. The proactor interface
abstraction illustrated in implementation activity 4 (235) delegates all its
demultiplexing and dispatching processing to a proactor implementation. This
plays the role of the implementation hierarchy in the Bridge pattern [GoF95].
This design allows multiple types of proactors to be implemented and
configured transparently. For example, a concrete proactor implementation
can be created using different types of asynchronous event demultiplexers,
such as POSIX aio_suspend() [POSIX95], or the Win32
GetQueuedCompletionStatus() or WaitForMultipleObjects()
functions [Sol98].
1. In our example the base class of the proactor implementation hierarchy is
defined by the class Proactor_Implementation. We omit its
declaration here because this class has essentially the same interface as
the Proactor interface in implementation activity 4 (235). The primary
difference is that its methods are purely virtual, because it forms the base
of a hierarchy of concrete proactor implementations.
3. Choose the completion event queue and asynchronous event demultiplexer
mechanisms. The handle_events() method of the proactor implementation
calls an asynchronous event demultiplexer function, which waits on the
completion event queue for the asynchronous operation processor to insert
completion events. This function returns whenever there is a completion event
in the queue. Asynchronous event demultiplexers can be distinguished by the
types of semantics they support, which include one of the following:
??FIFO demultiplexing. This type of asynchronous event demultiplexer
function waits for completion events corresponding to any asynchronous
operations that are associated with its completion event queue. The
events are removed from the queue in the order in which they are
inserted.
??The Win32 GetQueuedCompletionStatus() function allows event driven proactive applications to wait up to an application-specified
amount of time for any completion events to occur on a completion
port. Events are removed in FIFO order [Sol98].
??Selective demultiplexing. This type of asynchronous event
demultiplexer function waits selectively for a particular subset of
completion events that must be passed explicitly when the function is
called.
??The POSIX aio_suspend() function [POSIX95] and the Win32
WaitForMultipleObjects() function [Sol98] are passed an array
parameter designating asynchronous operations explicitly. They
suspend their callers for an application-specified amount of time until
at least one of these asynchronous operations has completed.
4. The completion event queue and asynchronous event demultiplexer are often
existing operating system mechanisms that need not be developed by
Proactor pattern implementors.
5. The primary difference between GetQueuedCompletionStatus(),
aio_suspend(), and WaitForMultipleObjects() is that the latter two
functions can wait selectively for completion events specified via an array
parameter. Conversely, GetQueuedCompletionStatus() just waits for the
next completion event enqueued on its completion port. Moreover, the POSIX
aio_*() functions can only demultiplex asynchronous I/O operations, such
as aio_read() or aio_write(), whereas
GetQueuedCompletionStatus() and WaitForMultipleObjects() can
demultiplex other Win32 asynchronous operations, such as timers and
synchronization objects.
As shown in implementation activity 5.5 (240), our proactor
implementation's handle_events() method uses this function to
dequeue a completion event from the specified CompletionPort. The
number of bytes transferred is returned as an 'out' parameter. The
lpOverlapped parameter points to the ACT passed by the original
asynchronous operation, such as the ReadFile() call in the
Async_Stream::async_read() method shown in implementation
activity 3.1 (232).
14. If there are no completion event results queued on the port, the function
blocks the calling thread, waiting for asynchronous operations associated
with the completion port to finish. The
GetQueuedCompletionStatus() function returns when it is able to
dequeue a completion event result or when the dwMilliseconds
timeout expires.
16. Determine how to demultiplex completion events to completion handlers. An
efficient and concise strategy for demultiplexing completion events to
completion handlers is to use the Asynchronous Completion Token pattern
(261), as described in implementation activity 3.1 (232). In this strategy, when
an asynchronous operation is invoked by an initiator the asynchronous
operation processor is passed information used to guide subsequent
completion processing. For example, a handle can be passed to identify a
particular socket endpoint and completion event queue, and an ACT can be
passed to identify a particular completion handler.
When the asynchronous operation completes, the asynchronous operation
processor generates the corresponding completion event, associates it with its
ACT and inserts the updated completion event into the appropriate completion
event queue. After an asynchronous event demultiplexer removes the
completion event from its completion event queue, the proactor
implementation can use the completion event's ACT to demultiplex to the
designated completion handler in constant O(1) time.
As shown in implementation activity 3.1 (232), when an async_read()
or async_write() method is invoked on an Async_Stream, they
create a new Async_Stream_Read_Result or
Async_Stream_Write_Result ACT, respectively and pass it to the
corresponding Win32 asynchronous operation. When this asynchronous
operation finishes, the Windows NT kernel queues the completion event
on the completion port designated by the handle that was passed during
the original asynchronous operation invocation. The ACT is used by the
proactor to demultiplex the completion event to the completion handler
designated in the original call.
17. Determine how to dispatch the hook method on the designated completion
handler. After the proactor's handle_events() method demultiplexes to the
completion handler it must dispatch the appropriate hook method on the
completion handler. An efficient strategy for performing this dispatching
operation is to combine the Adapter pattern [GoF95] with the Asynchronous
Completion Token pattern (261), as shown at the end of implementation
activity 3.1 (232).
Note how the handle_event() dispatch hook method is passed a
reference to the Async_Stream_Read_Result object that invoked it.
This double-dispatching interaction [GoF95] allows the completion handler
to access the asynchronous operation results, such as the number of
bytes transferred and its success or failure status.
26. Define the concrete proactor implementation. The proactor interface holds a
pointer to a concrete proactor implementation and forwards all method calls to
it, as shown in implementation activity 4 (235).
Determine the number of proactors in an application. Many applications can be
structured using just one instance of the Proactor pattern. In this case the proactor
can be implemented using the Singleton pattern [GoF95], as shown in
implementation activity 4 (235). This design is useful for centralizing event
demultiplexing and dispatching of completion events to a single location in an
application.
It can be useful to run multiple proactors simultaneously within the same application
process, however. For example, different proactors can be associated with threads
running at different priorities. This design provides different quality of service levels to
process completion handlers for asynchronous operations.
Note that completion handlers are only serialized per thread within an instance of the
proactor. Multiple completion handlers in multiple threads can therefore run in
parallel. This configuration may necessitate the use of additional synchronization
mechanisms if completion handlers in different threads access shared state
concurrently. Mutexes and synchronization idioms such as Scoped Locking (325) are
suitable.
7. Implement the concrete completion handlers. Concrete completion handlers specialize
the completion handler interface described in implementation activity 2.3 (228) to
define application-specific functionality. Three sub-activities must be addressed when
implementing concrete completion handlers:
0. Determine policies for maintaining state in concrete completion handlers. A
concrete completion handler may need to maintain state information
associated with a particular request. For example, an operating system may
notify a server that only part of a file was written to a Socket asynchronously,
due to the occurrence of transport-level flow control. A concrete completion
handler must then send the remaining data, until the file is fully transferred or
the connection becomes invalid. It must therefore know which file was
originally specified, how many bytes remain to be sent, and the position of the
file at the start of the previous request.
1. Select a mechanism to configure concrete completion handlers with a handle.
Concrete completion handlers perform operations on handles. The same two
strategies described in implementation activity 6.2 of the Reactor (179)
pattern¡ªhard-coded and generic¡ªcan be applied to configure handles with
event handlers in the Proactor pattern. In both strategies wrapper facades (47)
can encapsulate handles used by completion handler classes.
206
2. Implement completion handler functionality. Application developers must
decide the processing actions that should be performed to implement a
service when its corresponding hook method is invoked by a proactor. To
separate connection establishment functionality from subsequent service
processing, concrete completion handlers can be divided into several
categories in accordance with the Acceptor-Connector pattern (285). In
particular, service handlers implement application-specific services. In
contrast, acceptors and connectors establish connections passively and
actively, respectively, on behalf of these service handlers.
8. Implement the initiators. In many proactive applications, such as our Web server
example, the concrete completion handlers are the initiators. In this case this
implementation activity can be skipped. Initiators that are not completion handlers,
however, are often used to initiate asynchronous service processing during an
application's start-up phase.
Our Web server uses Windows NT features, such as overlapped I/O, completion ports, and
GetQueuedCompletionStatus(), to implement proactive event demultiplexing. It
employs a single-method completion handler dispatch interface strategy that can process
multiple Web browser service requests asynchronously. HTTP acceptors asynchronously
connect and create HTTP handlers using a variant of the Acceptor-Connector pattern (285).
Each HTTP handler is responsible for asynchronously receiving, processing, and replying to
a Web browser GET request delivered to the Web server's proactor via a completion event.
The example shown here uses a single thread to invoke asynchronous operations and
handle completion event processing. It is straightforward to enhance this example to take
advantage of multiple threads, however, as described in the Variants section.
The Web server's main() function starts by performing its initialization activities, such as
creating a proactor singleton, a Windows NT completion port, and an HTTP acceptor. This
acceptor associates its passive-mode acceptor handle with the proactor singleton's
completion port. The Web server next performs the following scenario during its connection
processing:
??The Web server invokes the HTTP acceptor's accept() method (1). This method
creates an ACT containing itself as the concrete completion handler.
??Acting in the role of an initiator, the HTTP acceptor's accept() method then invokes
the Win32 AcceptEx() operation asynchronously. It passes the ACT to AcceptEx(),
together with a HANDLE that identifies both the passive-mode socket endpoint to accept
connections and the completion port that Windows NT[8] should use to queue the
completion event when AcceptEx() finishes accepting a connection.
??The Web server's main() function then invokes the proactor's (3) handle_events()
method. This method runs the proactor's event loop, which calls the
GetQueuedCompletionStatus() asynchronous event demultiplexer. This function
waits on its completion port for the operating system to queue completion events when
asynchronous operations finish executing.
??A remote Web browser subsequently connects to the Web server (4), which causes
the asynchronous AcceptEx() operation to accept the connection and generate an
accept completion event. The operating system then locates this operation's ACT and
associates it with the completion event. At this point it queues the updated completion
event on the appropriate completion port (5).
??The GetQueuedCompletionStatus() function running in the application's event
loop thread then dequeues the completion event from the completion port. The proactor
uses the ACT associated with this completion event to dispatch the handle_event()
hook method on the HTTP acceptor completion handler (6), passing it the
ACCEPT_EVENT event type.
??To process the completion event, the HTTP acceptor creates an HTTP handler (7) that
associates its I/O handle with the proactor's completion port. This HTTP handler then
immediately invokes an asynchronous ReadFile() operation (8) to obtain the GET
request data sent by the Web browser. The HTTP handler passes itself as the
completion handler in the ACT to ReadFile() together with the I/O handle. The
operating system uses the completion port associated with this handle to notify the
proactor's handle_events() method when the asynchronous ReadFile() operation
finishes executing.
??Control of the Web server then returns to the proactor's event loop (9), which calls the
GetQueuedCompletionStatus() function to continue waiting for completion events.
After the connection is established and the HTTP handler is created, the following diagram
illustrates the subsequent scenario used by a proactive Web server to service an HTTP GET
request:
??The Web browser sends an HTTP GET request (1).
??The asynchronous ReadFile() operation invoked in the previous scenario then
finishes executing and the operating system queues the read completion event onto the
completion port (2). This event is then dequeued by
GetQueuedCompletionStatus(), which returns to the proactor's
handle_events() method. This method demultiplexes the completion event's ACT to
the designated HTTP handler and dispatches the handler's handle_event() hook
method, passing the READ_EVENT event type (3).
??The HTTP handler parses the request (4). Steps (2) through (4) then repeat as
necessary until the entire GET request has been received asynchronously.
??After the GET request has been completely received and validated, the HTTP handler
memory-maps the requested file (5) and invokes an asynchronous WriteFile()
operation to transfer the file data via the connection (6). The HTTP handler passes an
ACT that identifies itself as a completion handler to WriteFile(), so that the proactor
can notify it after the asynchronous WriteFile() operation finishes.
??After the asynchronous WriteFile() operation finishes the operating system inserts
a write completion event into the completion port. The proactor uses
GetQueuedCompletionStatus() again to dequeue the completion event (7). It uses
its associated ACT to demultiplex to the HTTP handler, then dispatches its
handle_event() hook method (8) to process the write completion event results.
Steps (6) through (8) continue asynchronously until the entire file has been delivered to
the Web browser.
In open(), the Async_Stream is initialized with the completion handler, handle, and
proactor to use when asynchronous ReadFile() and WriteFile() operations finish. It
then invokes an async_read() operation and returns to the proactor that dispatched it.
When the call stack unwinds the Web server will continue running its handle_events()
event loop method on its proactor singleton.
After the asynchronous ReadFile() operation completes, the proactor singleton
demultiplexes to the HTTP_Handler completion handler and dispatches its subsequent
handle_event() method:
If the entire request has not arrived, another asynchronous ReadFile() operation is
invoked and the Web server returns once again to its event loop. After a complete GET
request has been received from a Web browser, however, the following parse_request()
method maps the requested file into memory and writes the file data to the Web browser
asynchronously:
This sample implementation of parse_request() uses a C++ switch statement for
simplicity and clarity. A more extensible implementation could apply the Command pattern
[GoF95] or Command Processor pattern [POSA1] instead.
When the asynchronous WriteFile() operation completes, the proactor singleton
dispatches the handle_event() hook method of the HTTP_Handler:
After all the data has been received the HTTP handler frees resources that were allocated
dynamically.
The Web server contains a main() function that implements a single-threaded server. This
server first calls an asynchronous accept operation and the waits in the proactor singleton's
handle_events() event loop:
As service requests arrive from Web browsers and are converted into indication events by
the operating system, the proactor singleton invokes the event handling hook methods on
the HTTP_Acceptor and HTTP_Handler concrete event handlers to accept connections
and receive and process logging records asynchronously. The sequence diagram below
illustrates the behavior in the proactive Web server.
The proactive processing model shown in this diagram can scale when multiple HTTP
handlers and HTTP acceptors process requests from remote Web browsers simultaneously.
For example, each handler/acceptor can invoke asynchronous ReadFile(),
WriteFile(), and AcceptEx() operations that run concurrently. If the underlying
asynchronous operation processor supports asynchronous I/O operations efficiently the
overall performance of the Web server will scale accordingly.
Asynchronous Completion Handlers. The Implementation section describes activities used to
implement a proactor that dispatches completion events to completion handlers within a
single proactor event loop thread. When a concrete completion handler is dispatched, it
borrows the proactor's thread to perform its completion processing. However, this design
may restrict the concrete completion handler to perform short-duration synchronous
processing to avoid decreasing the overall responsiveness of the application significantly.
To resolve this restriction, all completion handlers could be required to act as initiators and
invoke long-duration asynchronous operations immediately, rather than performing the
completion processing synchronously. Some operating systems, such as Windows NT,
explicitly support asynchronous procedure calls (APCs). An APC is a function that executes
asynchronously in the context of its calling thread. When an APC is invoked the operating
system queues it within the thread context. The next time the thread is idle, such as when it
blocks on an I/O operation, it can run the queued APCs.
Concurrent Asynchronous Event Demultiplexer. One downside to using APCs is that they
may not use multiple CPUs effectively. This is because each APC runs in a single thread
context. A more scalable strategy therefore may be to create a pool of threads that share an
asynchronous event demultiplexer, so that a proactor can demultiplex and dispatch
completion handlers concurrently. This strategy is particularly scalable on operating system
platforms that implement asynchronous I/O efficiently.
For example, a Windows NT completion port [Sol98] is optimized to run efficiently when
accessed by GetQueuedCompletionStatus() from multiple threads simultaneously
[HPS99]. In particular, the Windows NT kernel schedules threads waiting on a completion
port in 'last-in first-out' (LIFO) order. This LIFO protocol maximizes CPU cache affinity
[Mog95] by ensuring that the thread waiting the shortest time is scheduled first, which is an
example of the Fresh Work Before Stale pattern [Mes96].
Shared Completion Handlers. Iinitiators can invoke multiple asynchronous operations
simultaneously, all of which share the same concrete completion handler [ARSK00]. To
behave correctly, however, each shared handler may need to determine unambiguously
which asynchronous operation has completed. In this case, the initiator and proactor must
collaborate to shepherd operation-specific state information throughout the entire
asynchronous processing life-cycle.
As with implementation activity 3.1 (232), the Asynchronous Completion Token pattern (261)
can be re-applied to disambiguate each asynchronous operation¡ªan initiator can create an
asynchronous completion token (ACT) that identifies each asynchronous operation uniquely.
It then 'piggy-backs' this initiator-ACT onto the ACT passed when an asynchronous
operation is invoked on an asynchronous operation processor. When the operation finishes
executing and is being processed by the proactor, the 'initiator-ACT can be passed
unchanged to the shared concrete completion handler's hook method. This initiator-ACT
allows the concrete completion handler to control its subsequent processing after it receives
an asynchronous operation's completion results.
Asynchronous Operation Processor Emulation. Many operating system platforms, including
the traditional versions of UNIX [MBKQ96] and the Java Virtual Machine (JVM), do not
export asynchronous operations to applications. There are several techniques that can be
used to emulate an asynchronous operation processor on such platforms, however. A
common solution is to employ a concurrency mechanism to execute operations without
blocking initiators, such as the Active Object pattern (369) or some type of threading model.
Three activities must be addressed when implementing a multi-threaded asynchronous
operation processor:
??Operation invocation. When an operation is invoked the asynchronous operation
processor must first store its associated ACT in an internal table. This can be
implemented using the Manager pattern [Som97].
??Asynchronous operation execution. The operation will next be executed in a different
thread of control than the invoking initiator thread. One strategy is to spawn a thread for
each operation. A more scalable strategy is for the asynchronous operation processor
to maintain a pool of threads using the Active Object pattern (369) Thread Pool variant.
This strategy requires the initiator thread to queue the operation request before
continuing with its other computations.
Each operation will subsequently be dequeued and executed in a thread internal to the
asynchronous operation processor. For example, to implement asynchronous read
operations an internal thread can block while reading from socket or file handles.
Operations thus appear to execute asynchronously to initiators that invoke them, even
though the operations block internally within the asynchronous operation processor in
their own thread of control.
??Operation completion handling. When an asynchronous operation completes the
asynchronous operation processor generates a completion event and associates it with
the appropriate ACT it had cached during the original invocation. It then queues the
updated completion event into the appropriate completion event queue.
Other variants. Several variants of the Proactor pattern are similar to variants in the Reactor
pattern (179), such as integrating the demultiplexing of timer and I/O events, and supporting
concurrent concrete completion handlers.
Completion ports in Windows NT. The Windows NT operating system provides the
mechanisms to implement the Proactor pattern efficiently [Sol98]. Various asynchronous
operations are supported by Windows NT, such as time-outs, accepting new network
connections, reading and writing to files and Sockets, and transmitting entire files across a
Socket connection. The operating system itself is thus the asynchronous operation
processor. Results of the operations are queued as completion events on Windows NT
completion ports, which are then dequeued and dispatched by an application-provided
proactor.
The POSIX AIO family of asynchronous I/O operations. On some real-time POSIX
platforms the Proactor pattern is implemented by the aio_*() family of APIs [POSIX95].
These operating system features are similar to those described above for Windows NT. One
difference is that UNIX signals can be used to implement a pre-emptively asynchronous
proactor in which a signal handler can interrupt an application's thread of control. In contrast,
the Windows NT API is not pre-emptively asynchronous, because application threads are
not interrupted. Instead, the asynchronous completion routines are called back at well defined Win32 function points.
ACE Proactor Framework. The ADAPTIVE Communication Environment (ACE) [Sch97]
provides a portable object-oriented Proactor framework that encapsulates the overlapped I/O
and completion port mechanisms on Windows NT and the aio_*() family of asynchronous
I/O APIs on POSIX platforms. ACE provides an abstraction class, ACE_Proactor, that
defines a common interface to a variety of proactor implementations, such as
ACE_Win32_Proactor and ACE_POSIX_Proactor. These proactor implementations can
be created using different asynchronous event demultiplexers, such as
GetQueuedCompletionStatus() and aio_suspend(), respectively.
Operating system device driver interrupt-handling mechanisms. The Proactor pattern is
often used to enhance the structure of operating system kernels that invoke I/O operations
on hardware devices driven by asynchronous interrupts. For example, a packet of data can
be written from an application to a kernel-resident device driver, which then passes it to the
hardware device that transmits the data asynchronously. When the device finishes its
transmission it generates a hardware interrupt that notifies the appropriate handler in the
device driver. The device driver then processes the interrupt to completion, potentially
initiating another asynchronous transfer if more data is available from the application.
Phone call initiation via voice mail. A real-life application of the Proactor pattern is the
scenario in which you telephone a friend, who is currently away from her phone, but who
returns calls reliably when she comes home. You therefore leave a message on her voice
mail to ask her to call you back. In terms of the Proactor pattern, you are a initiator who
invokes an asynchronous operation on an asynchronous operation processor¡ªyour friend's
voice mail¡ªto inform your friend that you called. While waiting for your friend's 'call-back'
you can do other things, such as re-read chapters in POSA2. After your friend has listened
to her voice mail, which corresponds to the completion of the asynchronous operation, she
plays the proactor role and calls you back. While talking with her, you are the completion
handler that 'processes' her 'callback'.
The Proactor pattern offers a variety of benefits:
Separation of concerns. The Proactor pattern decouples application-independent
asynchronous mechanisms from application-specific functionality. The application independent mechanisms become reusable components that know how to demultiplex the
completion events associated with asynchronous operations and dispatch the appropriate
callback methods defined by concrete completion handlers. Similarly, the application-specific
functionality in concrete completion handlers know how to perform particular types of
service, such as HTTP processing.
Portability. The Proactor pattern improves application portability by allowing its interface to
be reused independently of the underlying operating system calls that perform event
demultiplexing. These system calls detect and report the events that may occur
simultaneously on multiple event sources. Event sources may include I/O ports, timers,
synchronization objects, signals, and so on. For example, on real-time POSIX platforms the
asynchronous I/O functions are provided by the aio_*() family of APIs [POSIX95].
Similarly, on Windows NT, completion ports and overlapped I/O are used to implement
asynchronous I/O [MDS96].
Encapsulation of concurrency mechanisms. A benefit of decoupling the proactor from the
asynchronous operation processor is that applications can configure proactors with various
concurrency strategies without affecting other application components and services.
Decoupling of threading from concurrency. The asynchronous operation processor executes
potentially long-duration operations on behalf of initiators. Applications therefore do not need
to spawn many threads to increase concurrency. This allows an application to vary its
concurrency policy independently of its threading policy. For instance, a Web server may
only want to allot one thread per CPU, but may want to service a higher number of clients
simultaneously via asynchronous I/O.
Performance. Multi-threaded operating systems use context switching to cycle through
multiple threads of control. While the time to perform a context switch remains fairly
constant, the total time to cycle through a large number of threads can degrade application
performance significantly if the operating system switches context to an idle thread.[9] For
example, threads may poll the operating system for completion status, which is inefficient.
The Proactor pattern can avoid the cost of context switching by activating only those logical
threads of control that have events to process. If no GET request is pending, for example, a
Web server need not activate an HTTP Handler.
Simplification of application synchronization. As long as concrete completion handlers do not
spawn additional threads of control, application logic can be written with little or no concern
for synchronization issues. Concrete completion handlers can be written as if they existed in
a conventional single-threaded environment. For example, a Web server's HTTP handler
can access the disk through an asynchronous operation, such as the Windows NT
TransmitFile() function [HPS99], hence no additional threads need to be spawned.
The Proactor pattern has the following liabilities:
Restricted applicability. The Proactor pattern can be applied most efficiently if the operating
system supports asynchronous operations natively. If the operating system does not provide
this support, however, it is possible to emulate the semantics of the Proactor pattern using
multiple threads within the proactor implementation. This can be achieved, for example, by
allocating a pool of threads to process asynchronous operations. This design is not as
efficient as native operating system support, however, because it increases synchronization
and context switching overhead without necessarily enhancing application-level parallelism.
Complexity of programming, debugging and testing. It is hard to program applications and
higher-level system services using asynchrony mechanisms, due to the separation in time
and space between operation invocation and completion. Similarly, operations are not
necessarily constrained to run at well-defined points in the processing sequence¡ªthey may
execute in non-deterministic orderings that are hard for many developers to understand.
Applications written with the Proactor pattern can also be hard to debug and test because
the inverted flow of control oscillates between the proactive framework infrastructure and the
method callbacks on application-specific handlers. This increases the difficulty of 'single stepping' through the run-time behavior of a framework within a debugger, because
application developers may not understand or have access to the proactive framework code.
Scheduling, controlling, and canceling asynchronously running operations. Initiators may be
unable to control the scheduling order in which asynchronous operations are executed by an
asynchronous operation processor. If possible, therefore, an asynchronous operation
processor should employ the Strategy pattern [GoF95] to allow initiators to prioritize and
cancel asynchronous operations. Devising a completely reliable and efficient means of
canceling all asynchronous operations is hard, however, because asynchronous operations
may complete before they can be cancelled.
The Proactor pattern is related to the Observer [GoF95] and Publisher-Subscriber [POSA1]
patterns, in which all dependents are informed when a single subject changes. In the
Proactor pattern, however, completion handlers are informed automatically when completion
events from multiple sources occur. In general, the Proactor pattern is used to demultiplex
multiple sources of asynchronously delivered completion events to their associated
completion handlers, whereas an observer or subscriber is usually associated with a single
source of events.
The Proactor pattern can be considered an asynchronous variant of the synchronous
Reactor pattern (179). The Reactor pattern is responsible for demultiplexing and dispatching
multiple event handlers that are triggered when it is possible to invoke an operation
synchronously without blocking. In contrast, the Proactor pattern supports the demultiplexing
and dispatching of multiple completion handlers that are triggered by the completion of
operations that execute asynchronously.
Leader/Followers (447) and Half-Sync/Half-Async (423) are two other patterns that
demultiplex and process various types of events synchronously. On platforms that support
asynchronous I/O efficiently, the Proactor pattern can often be implemented more efficiently
than these patterns. However, the Proactor pattern may be harder to implement because it
has more participants, which require more effort to understand. The Proactor's combination
of 'inversion of control' and asynchrony may also require application developers to have
more experience to use and debug it effectively.
The Active Object pattern (369) decouples method execution from method invocation. The
Proactor pattern is similar, because an asynchronous operation processor performs
operations asynchronously on behalf of initiators. Both patterns can therefore be used to
implement asynchronous operations. The Proactor pattern is often used instead of the Active
Object pattern on operating systems that support asynchronous I/O efficiently.
The Chain of Responsibility [GoF95] pattern decouples event handlers from event sources.
The Proactor pattern is similar in its segregation of initiators and completion handlers. In the
Chain of Responsibility pattern, however, the event source has no prior knowledge of which
handler will be executed, if any. In Proactor, initiators have full control over the target
completion handler. The two patterns can be combined by establishing a completion handler
that is the entry point into a responsibility chain dynamically configured by an external
factory.
Current Java implementations do not support Proactor-like event processing schemes,
because java.io does not support asynchronous I/O. In basic Java implementations
blocking I/O operations can even block the whole Java Virtual Machine (JVM)¡ªthe I/O
operation blocks the current thread and, as multi-threading may be implemented in user
space, the operating system considers the task running the JVM as blocked and schedules
other operating system processes instead of other JVM threads.
More sophisticated Java implementations work around this problem by implementing
asynchronous I/O internally on the native code level¡ªthe thread doing the blocking call is
blocked, but other threads are able to run. The blocked thread is subsequently called back,
or may explicitly wait for the blocking call to return. Applications cannot make use of this
directly, however, because current JDK libraries do not expose asynchronous I/O. This will
change with the next generation of the Java I/O system, which is under development and will
appear as a package called java.nio or something similar [JSR51].
Certain programming languages, such as Scheme, support continuations. Continuations can
be used in single-threaded programs to enable a sequence of function calls to relinquish its
runtime call stack when blocked without losing the execution history of the call stack. In the
context of the Proactor pattern, the indirect transfer of control from an asynchronous
operation invocation to the subsequent processing by its completion handler can be modeled
as a continuation.
##%%&&
The Acceptor-Connector design pattern decouples the connection and initialization of
cooperating peer services in a networked system from the processing performed by the peer
services after they are connected and initialized.
Consider a large-scale distributed system management application consisting that monitors
and controls a satellite constellation [Sch96]. Such a management application typically
consists of a multi-service, application-level gateway that routes data between transport
endpoints connecting remote peer hosts.
Each service in the peer hosts uses the gateway to send and receive several types of data,
including status information, bulk data, and commands, that control the satellites. The peer
hosts can be distributed throughout local area and wide-area networks.
The gateway transmits data between its peer hosts using the connection-oriented TCP/IP
protocol [Ste93]. Each service in the system is bound to a particular transport address, which
is designated by a tuple consisting of an IP host address and a TCP port number. Different
port numbers uniquely identify different types of service.
Unlike the binding of services to specific TCP/IP host/port tuples, which can be selected
early in the distributed system's lifecycle, it may be premature to designate the connection
establishment and service initialization roles a priori. Instead, the services in the gateway
and peer hosts should be able to change their connection roles flexibly to support the
following run-time behavior:
??Services in a gateway may actively initiate connection requests to services located in
remote peer hosts, then route data to them.
??Services in a gateway may passively accept connection requests from services within
the peer hosts, which then route data through the gateway to a service on another peer
host.
??Services residing in peer hosts may be active connection initiators in one situation and
passive connection acceptors in another.
??Hybrid configurations that combine passive and active connection behavior in the
same gateway or peer host may also occur.
In general, the inherent flexibility required to support such a runtime behavior demands
communication software that allows the connection establishment, initialization, and
processing of peer services to evolve gracefully and to vary independently.
A networked system or application in which connection-oriented protocols are used to
communicate between peer services connected via transport endpoints.
Applications in connection-oriented networked systems often contain a significant amount of
configuration code that establishes connections and initializes services. This configuration
code is largely independent of the processing that services perform on data exchanged
between their connected transport endpoints. Tightly coupling the configuration code with
the service processing code is therefore undesirable, because it fails to resolve four forces:
??It should be easy to change connection roles to support different application behavior,
as discussed in the Example section. Connection roles determine whether an
application actively initiates or passively accepts a connection. In contrast,
communication roles determine whether an application plays the role of a client, a
server, or both client and server in a peer-to-peer configuration.
??It should be easy to add new types of services, service implementations, and
communication protocols without affecting existing connection establishment and
service initialization configuration code.
?The gateway from our example may require integration with a directory service that
runs over the TP4 or SPX transport protocols rather than TCP. Ideally, this
integration should have little or no effect on the implementation of the gateway
services themselves.
??In general, connection establishment and service initialization strategies change less
frequently than the communication protocols and services implemented by an
application.
?FTP, TELNET, HTTP and CORBA IIOP services all use different application-level
communication protocols. However, they can all be configured using the same
connection and initialization mechanisms.
??For large-scale networked systems it should be possible to reduce connection
establishment latency by using advanced operating system features, such as
asynchronous connection mechanisms.
?Applications with a large number of peers may need to establish many connections
asynchronously and concurrently. Efficient and scalable connection establishment is
particularly important for applications, such as our gateway example, that
communicate over long-latency wide area networks.
Decouple the connection and initialization of peer services in a networked application from
the processing these peer services perform after they are connected and initialized.
In detail: encapsulate application services within peer service handlers. Each service handler
implements one half of an end-to-end service in a networked application. Connect and
initialize peer service handlers using two factories: acceptor and connector. Both factories
cooperate to create a full association [Ste93] between two peer service handlers and their
two connected transport endpoints, each encapsulated by a transport handle.
The acceptor factory establishes connections passively on behalf of an associated peer
service handler upon the arrival of connection request events[12] issued by remote peer
service handlers. Likewise, the connector factory establishes connections actively to
designated remote peer service handlers on behalf of peer service handlers.
After a connection is established, the acceptor and connector factories initialize their
associated peer service handlers and pass them their respective transport handles. The
peer service handlers then perform application-specific processing, using their transport
handles to exchange data via their connected transport endpoints. In general, service
handlers do not interact with the acceptor and connector factories after they are connected
and initialized.
There are six key participants in the Acceptor-Connector pattern:
A passive-mode transport endpoint is a factory that listens for connection requests to arrive,
accepts those connection requests, and creates transport handles that encapsulate the
newly connected transport endpoints. Data can be exchanged via connected transport
endpoints by reading and writing to their associated transport handles.
In the gateway example, we use socket handles to encapsulate transport endpoints. In
this case, a passive-mode transport endpoint is a passive-mode socket handle [Ste98]
that is bound to a TCP port number and IP address. It creates connected transport
endpoints that are encapsulated by data-mode socket handles. Standard Socket API
operations, such as recv() and send(), can use these connected data-mode socket
handles to read and write data.
A service handler defines one half of an end-to-end service in a networked system. A
concrete service handler often plays either the client role or server role in this end-to-end
service. In peer-to-peer use cases it can even play both roles simultaneously. A service
handler provides an activation hook method that is used to initialize it after it is connected to
its peer service handler. In addition, the service handler contains a transport handle, such as
a data-mode socket handle, that encapsulates a transport endpoint. Once connected, this
transport handle can be used by a service handler to exchange data with its peer service
handler via their connected transport endpoints.
In our example the service handlers are both cooperating components within the
gateway and peer hosts that communicate over TCP/IP via their connected socket
handles. Service handlers are responsible for processing status information, bulk data,
and commands that monitor and control a satellite constellation.
An acceptor is a factory that implements a strategy for passively establishing a connected
transport endpoint, and creating and initializing its associated transport handle and service
handler. An acceptor provides two methods, connection initialization and connection
completion, that perform these steps with the help of a passive-mode transport endpoint.
When its initialization method is called, an acceptor binds its passive-mode transport
endpoint to a particular transport address, such as a TCP port number and IP host address,
that listens passively for the arrival of connection requests.
When a connection request arrives, the acceptor's connection completion method performs
three steps:
??Firstly, it uses its passive-mode transport endpoint to create a connected transport
endpoint and encapsulate the endpoint with a transport handle.
??Secondly, it creates a service handler that will process data requests emanating from
its peer service handler via their connected transport endpoints.
??Thirdly, it stores the transport handle in its associated service handler and invokes the
service handler's activation hook method, which allows the service handler to finish
initializing itself.
A connector[13] is a factory that implements the strategy for actively establishing a connected
transport endpoint and initializing its associated transport handle and service handler. It
provides two methods, connection initiation and connection completion, that perform these
steps.
The connection initiation method is passed an existing service handler and establishes a
connected transport endpoint for it with an acceptor. This acceptor must be listening for
connection requests to arrive on a particular transport address, as described above.
Separating the connector's connection initiation method from its completion method allows a
connector to support both synchronous and asynchronous connection establishment
transparently:
??In the synchronous case, the connector initiating the connection request blocks its
caller until the transport endpoints are connected. At this point, the connector calls the
service handler's activation hook method directly.
??In the asynchronous case, the connection request runs asynchronously and the
connector's initiation method returns immediately. The service handler is activated by
the connection completion method only after the connector is notified that the transport
endpoint has finished connecting asynchronously.
Regardless of whether a transport endpoint is connected synchronously or asynchronously,
both acceptors and connectors initialize a service handler by calling its activation hook
method after a transport endpoint is connected. From this point service handlers generally
do not interact with their acceptor and connector factories.
A dispatcher is responsible for demultiplexing indication events that represent various types
of service requests, such as connection requests and data requests.
??For the acceptor, the dispatcher demultiplexes connection indication events received
on one or more transport handles that encapsulate transport endpoints. Multiple
acceptors can register with a dispatcher, which listens on their behalf for connection
requests to arrive from peer connectors.
??For the connector, the dispatcher demultiplexes completion events that arrive in
response to connections that were initiated asynchronously. To handle this situation, a
connector registers itself with a dispatcher to receive these connection completion
events. The dispatcher then runs its event loop. When a completion event arrives it
notifies the corresponding connector. The connector can then invoke the designated
service handler's activation hook method to allow the service handler to initialize itself. A
single dispatcher and connector can therefore initiate and complete connections
asynchronously on behalf of multiple service handlers.
Note that a dispatcher is not necessary for synchronous connection establishment,
because the thread that initiates the connection will block awaiting the connection
completion event. As a result this thread can activate the service handler directly.
??Service handlers can register their transport handles with a dispatcher, which will notify
the service handlers when indication events occur on those handles.
Networked applications and services can be built by subclassing and instantiating the
generic participants of the Acceptor-Connector pattern described above to create the
following concrete components.
Concrete service handlers define the application-specific portions of end-to-end services.
They are activated by concrete acceptors or concrete connectors. Concrete acceptors
instantiate generic acceptors with concrete service handlers, transport endpoints, and
transport handles used by these service handlers. Similarly, concrete connectors instantiate
generic connectors.
Concrete service handlers, acceptors, and connectors are also instantiated with a specific
type of interprocess communication (IPC) mechanism, such as Sockets [Ste98] or TLI
[Rago93]. These IPC mechanisms are used to create the transport endpoints and transport
handles that connected peer service handlers use to exchange data.
The class diagram of the Acceptor-Connector pattern is shown in the following figure:
To illustrate the collaborations performed by participants in the Acceptor-Connector pattern,
we examine three canonical scenarios:
??Scenario I shows passive connection establishment
??Scenario II shows synchronous active connection establishment
??Scenario III shows asynchronous active connection establishment
Scenario I: This scenario illustrates the collaboration between acceptor and service handler
participants and is divided into three phases:
??Passive-mode transport endpoint initialization phase. An application that plays a
passive connection role first calls the acceptor's connection initialization method. This
method initializes a passive-mode transport endpoint and binds it to a transport
address, such as the local host's IP address and a TCP port number. The acceptor then
listens on this transport address for connection requests initiated by peer connectors.[14]
Next, the acceptor's initialization method registers itself with a dispatcher, which will
notify the acceptor when connection indication events arrive from peer connectors. After
the acceptor's initialization method returns, the application initiates the dispatcher's
event loop. This loop waits for connection requests and other types of indication events
to arrive from peer connectors.
??Service handler initialization phase. When a connection request arrives for a particular
transport address, the dispatcher notifies the associated acceptor. This acceptor uses
its passive-mode transport endpoint to create a new connected transport endpoint and
encapsulate it with a transport handle. It next creates a new service handler, stores the
transport handle into the service handler, and calls the service handler's activation hook
method.
This hook method performs service handler-specific initialization, for example allocating
locks, spawning threads, or establishing a session with a logging service. A service
handler may elect to register itself with a dispatcher, which will notify the handler
automatically when indication events containing data requests arrive for it.
??Service processing phase. After a transport endpoint has been connected and the
associated service handler has been initialized, the service processing phase begins. In
this phase an application-level communication protocol such as TELNET, FTP, HTTP
or CORBA IIOP can be used to exchange data between the peer service handlers via
their connected transport endpoints. When all exchanges are complete, the transport
endpoints, transport handles, and service handlers can be shut down and their
resources released.
A connector can initialize its service handler using two general strategies: synchronous and
asynchronous. Synchronous service initialization is useful:
??If connection establishment latency is very low, for example establishing a connection
with a server on the same host via a 'loopback network' device
??If multiple threads of control are available and it is efficient to use a thread-per connection model [Sch97] to connect each service handler synchronously or
??If the services must be initialized in a fixed order and the client cannot perform useful
work until all connections are established.
Asynchronous service initialization is useful in different situations, such as establishing
connections over high latency links, using single-threaded applications, or initializing a large
number of peers that can be connected in an arbitrary order.
Scenario II: The collaborations among participants in the synchronous connector scenario
can be divided into three phases:
??Connection initiation phase. To establish a connection synchronously between a
service handler and its peer, an application can invoke the connector's connection
initiation method. Using the transport handle associated with the service handler, this
method actively establishes the connection by blocking the application's thread until the
connection completes synchronously.
??Service handler initialization phase. After a connection completes synchronously, the
connection initiation method calls the associated service handler's activation hook
method directly. This hook method performs service handler-specific initializations.
??Service processing phase. After a service handler is initialized, it performs application specific service processing using data exchanged with its connected peer service
handler. This phase is similar to the service processing phase performed by service
handlers that are created and initialized by acceptors.
In the synchronous scenario, the connector combines the connection initiation and service
initialization phases into a single blocking operation. Only one connection is established per
thread for every invocation of a connector's connection initiation method.
Scenario III: The collaborations among participants in the asynchronous connector scenario
are also divided into three phases:
??Connection initiation phase. To connect a service handler and its peer service handler
asynchronously, an application invokes the connector's connection initiation method. As
with the synchronous Scenario II, the connector initiates the connection actively.
Unlike Scenario II, however, the connection initiation request executes asynchronously.
The application thread therefore does not block while waiting for the connection to
complete. To receive a notification when a connection completes, the connector
registers itself and the service handler's transport handle with the dispatcher and returns
control back to the application.
??Service handler initialization phase. After a connection completes asynchronously, the
dispatcher notifies the connector's connection completion method. This method cleans
up any resources allocated to manage the pending connection, then it calls the service
handler's activation hook method to perform service-specific initialization.
??Service processing phase. After a service handler is activated, it performs application specific service processing using data exchanged with its connected peer service
handler. This phase is similar to the service processing phases described in Scenarios I
and II.
Note in the following figure how the connection initiation phase in Scenario III is separated
temporally from the service handler initialization phase. This decoupling enables multiple
connection initiations and completions to proceed concurrently, thereby maximizing the
parallelism inherent in networks and hosts.
The participants in the Acceptor-Connector pattern can be decomposed into three layers:
??Demultiplexing/dispatching infrastructure layer components. This layer performs
generic, application-independent strategies for dispatching events.
??Connection management layer components. This layer performs generic, application independent connection and initialization services.
??Application layer components. This layer then customizes the generic strategies
performed by the other two layers via subclassing, object composition, and/or
parameterized type instantiation, to create concrete components that establish
connections, exchange data, and perform service-specific processing.
Our coverage of the Acceptor-Connector implementation starts with the
demultiplexing/dispatching component layer and progresses upwards through the
connection management and application component layers.
1. Implement the demultiplexing/dispatching infrastructure component layer. This layer
handles events that occur on transport endpoints and consists of transport
mechanisms and dispatching mechanisms. This implementation activity can be
divided into two sub-activities:
1. Select the transport mechanisms. These mechanisms consist of:
??Passive-mode transport endpoint components
??Connected transport endpoint components
??Transport address components
??Transport handle components
A passive-mode transport endpoint is a factory that listens on an advertised
transport address for connection requests to arrive. When such a request
arrives, a passive-mode transport endpoint creates a connected transport
endpoint and encapsulates this new endpoint with a transport handle. An
application can use this transport handle to exchange data with its peer connected transport endpoint. The transport mechanism components are often
provided by the underlying operating system platform and may be accessed
via wrapper facades (47), as shown in implementation activity 3 (311).
For our gateway example we implement the transport mechanism
components using the Sockets API [Ste98]. The passive-mode transport
endpoints and connected transport endpoints are implemented by
passive-mode and data-mode sockets, respectively. Transport handles
are implemented by socket handles. Transport addresses are
implemented using IP host addresses and TCP port numbers.
2. Implement the dispatching mechanisms. These mechanisms consist of
dispatcher and event handler components. A dispatcher is responsible for
associating requests to their corresponding acceptors, connectors, and service
handlers. An event handler defines the event processing interface provided by
a service of an event-driven application.
To implement the dispatching mechanisms, follow the guidelines described in
the Reactor (179) or Proactor (215) event demultiplexing patterns. These
patterns handle synchronous and asynchronous event demultiplexing,
respectively. A dispatcher can also be implemented as a separate thread or
process using the Active Object pattern (369) or Leader/Followers (447)
thread pools.
For our gateway example, we implement the dispatcher and event handler
using components from the Reactor pattern (179). This enables efficient
synchronous demultiplexing of multiple types of events from multiple
sources within a single thread of control. The dispatcher, which we call
'reactor' in accordance with Reactor pattern terminology, uses a reactive
model to demultiplex and dispatch concrete event handlers.
We use a reactor Singleton [GoF95] because only one instance of it is
needed in the entire application process. The event handler class, which
we call Event_Handler in our example, implements methods needed by
the reactor to notify its service handlers, connectors, and acceptors when
events they have registered for occur. To collaborate with the reactor,
therefore, these components must subclass from class Event_Handler,
as shown in implementation activity 2 (299).
2. Implement the connection management component layer. This layer creates service
handlers, passively or actively connects service handlers to their remote peer service
handlers, and activates service handlers after they are connected. All components in
this layer are generic and delegate to concrete IPC mechanisms, concrete service
handlers, concrete acceptors, and concrete connectors. These are instantiated by the
application layer described in implementation activity 3 (311). There are three primary
components in the connection management layer: service handler, acceptor, and
connector.
1. Define a generic service handler interface. A service handler provides a
generic interface for processing services defined by clients, servers, or both
client and server roles in peer-to-peer services. This interface includes
methods for initializing a service handler, executing the service it defines, and
for maintaining the IPC mechanism it uses for communication. To create a
concrete service handler, applications must customize this component using
subclassing, object composition, or parameterized type instantiation.
Applications must also configure a service handler component with a concrete
IPC mechanism that encapsulates a transport handle and its corresponding
transport endpoint. These IPC mechanisms are often implemented as wrapper
facades (47). Concrete service handlers can use these IPC mechanisms to
communicate with their remote peer service handlers.
In our gateway example we define a Service_Handler abstract base
class that inherits from the Event_Handler class defined in
implementation activity 1 of the Reactor pattern (179):
This design allows a Reactor to dispatch the service handler's event
handling method it inherits from class Event_Handler. In addition, the
Service_Handler class defines methods to access its IPC mechanism,
which is configured into the class using parameterized types. Finally this
class includes an activation hook that acceptors and connectors can use
to initialize a Service_Handler object once a connection is established.
This pure virtual method, which we call open(), must be overridden by a
concrete service handler to perform service-specific initialization.
2. Define the generic acceptor interface. The acceptor component implements
the generic strategy for establishing connections passively and creating and
initializing the concrete service handlers that exchange data with peer service
handlers over these connections. An acceptor also defines an initialization
method that an application can call to advertise its passive-mode transport
endpoint address to other applications on the network.
A generic acceptor is customized by the components in the application layer,
as described in implementation activity 3 (311) to establish connections
passively on behalf of a particular service handler using a designated IPC
mechanism. To support this customization an acceptor implementation can
use two general strategies, polymorphism or parameterized types:
??Polymorphism. In this strategy, illustrated in the Structure section,
concrete acceptors are specified by subclassing from a generic acceptor.
The acceptor method, which we call accept(), responsible for accepting
connection requests from remote clients is a template method [GoF95]. It
performs the generic processing that constitutes the acceptor's passive
connection establishment and service initialization logic.
The behavior of the individual steps in this process are delegated to hook
methods [Pree95] that are also declared in the acceptor's interface.
These hook methods can be overridden by concrete acceptors to perform
application-specific strategies, for example to use a particular concrete
IPC mechanism to establish connections passively. The concrete service
handler created by a concrete acceptor can be manufactured via a factory
method [GoF95] called as one of the steps in the acceptor's accept()
template method.
??Parameterized types. This strategy encapsulates IPC mechanisms for
passive connection establishment within wrapper facades (47) and
configures them into an acceptor component via parameterized types. As
with the polymorphic strategy outlined above, the acceptor's accept()
method is a template method [GoF95] whose hook methods delegate to
the particular IPC mechanism configured into the acceptor. The type of
concrete service handler used to instantiate a concrete acceptor can also
be supplied as a template parameter to the generic acceptor.
One advantage of using parameterized types is that they allow the IPC
connection mechanisms and service handlers associated with an acceptor to
be changed easily and efficiently. This flexibility simplifies porting an
acceptor's connection establishment code to platforms with different IPC
mechanisms. It also allows the same connection establishment and service
initialization code to be reused for different types of concrete service handlers.
Inheritance and parameterized types have the following trade-offs:
??Parameterized types may incur additional compile and link-time
overhead, but generally compile into faster code [CarEl95]
??Inheritance may incur additional run-time overhead due to the
indirection of dynamic binding [HLS97], but is generally faster to compile
and link
Different applications and application services have different needs that are
best served by one or the other strategies.
In our gateway example we use parameterized types to configure the
acceptor with its designated service handler and a concrete IPC
mechanism that establishes connections passively. The acceptor inherits
from Event_Handler to receive events from a reactor singleton, which
pays the role of the dispatcher in this example:
The Acceptor template is parameterized by concrete types of
IPC_ACCEPTOR and SERVICE_HANDLER. The IPC_ACCEPTOR is a
placeholder for the concrete IPC mechanism used by an acceptor to
passively establish connections initiated by peer connectors. The
SERVICE_HANDLER is a placeholder for the concrete service handler that
processes data exchanged with its peer connected service handler. Both
of these concrete types are provided by the components in the application
layer.
3. Implement the generic acceptor methods. Applications initialize an acceptor by
calling its initialization method with a parameter specifying a transport address,
such as the local host's IP name and TCP port number. The acceptor uses
this address to listen for connections initiated by peer connectors. It forwards
this address to the concrete IPC connection mechanism configured into the
generic acceptor, either by an acceptor's concrete acceptor subclasses or by a
parameterized type. This IPC connection mechanism then initializes the
acceptor's passive-mode transport endpoint, which advertises its address to
remote applications who are interested in connecting to the acceptor.
The behavior of the acceptor's passive-mode transport endpoint is determined
by the type of concrete IPC mechanism used to customize the generic
acceptor. This IPC mechanism is often accessed via a wrapper facade (47),
such as the ACE wrapper facades [Sch92] for Sockets [Ste98], TLI [Rago93],
STREAM pipes [PR90], or Win32 Named Pipes [Ric97].
The acceptor's initialization method also registers itself with a dispatcher. This
dispatcher then performs a 'double dispatch' [GoF95] back to the acceptor to
obtain a handle to the passive-mode transport endpoint of its underlying
concrete IPC mechanism. This handle allows the dispatcher to notify the
acceptor when connection requests arrive from peer connectors.
When a connection request arrives from a remote peer, the dispatcher
automatically calls back to the acceptor's accept() template method
[GoF95]. This template method implements the acceptor's strategies for
creating a new concrete service handler, accepting a connection into it, and
activating the handler. The details of the acceptor's implementation are
delegated to hook methods. These hook methods represent the set of
operations available to perform customized service handler connection and
initialization strategies.
If polymorphism is used to specify concrete acceptors, the hook methods are
dispatched to their corresponding implementations within the concrete
acceptor subclass. When using parameterized types, the hook methods
invoke corresponding methods on the template parameters used to instantiate
the generic acceptor. In both cases, concrete acceptors can modify the
generic acceptor's strategies transparently without changing its accept()
method's interface. This flexibility makes it possible to design concrete service
handlers whose behavior can be decoupled from their passive connection and
initialization.
The make_service_handler() factory method [GoF95] is a hook used
by the generic acceptor template method to create new concrete service
handlers. Its connection acceptance strategy is defined by the
accept_service_handler() hook method. By default this method
delegates connection establishment to the accept() method of the
IPC_ACCEPTOR, which defines a concrete passive IPC connection
mechanism. The acceptor's service handler activation strategy is defined
by the activate_service_handler() method. This method can be
used by a concrete service handler to initialize itself and to select its
concurrency strategy.
In our gateway example the dispatcher is a reactor that notifies the
acceptor's accept() method indirectly via the handle_event() method
that the acceptor inherits from class Event_Handler. The
handle_event() method is an adapter [GoF95] that transforms the
general-purpose event handling interface of the reactor to notify the
acceptor's accept() method.
When an acceptor terminates, due to errors or due to its application process
shutting down, the dispatcher notifies the acceptor to release any resources it
acquired dynamically.
In our gateway, the reactor calls the acceptor's handle_close() hook
method, which closes its passive-mode socket.
4. Define the generic connector interface. The connector component implements
the generic strategy for actively establishing connections and initializing the
associated service handlers that process request and response events on the
connections.
A connector contains a map of concrete service handlers that manage the
completion of pending asynchronous connections. Service handlers whose
connections are initiated asynchronously are inserted into this map. This
allows their dispatcher and connector to activate the handlers after the
connections complete.
As with the generic acceptors described in implementation activity 2.2 (300),
components in the application layer described in implementation activity 3
(311) customizes generic connectors with particular concrete service handlers
and IPC mechanisms. We must therefore select the strategy¡ªpolymorphism
or parameterized types¡ªfor customizing concrete connectors. Implementation
activity 2.2 (300) discusses both strategies and their trade-offs.
As with the acceptor's accept() method, a connector's connection initiation
and completion methods are template methods [GoF95], which we call
connect() and complete(). These methods implement the generic
strategy for establishing connections actively and initializing service handlers.
Specific steps in these strategies are delegated to hook methods [Pree95].
We define the following interface for the connector used to implement our
gateway example. It uses C++ templates to configure the connector with a
concrete service handler and the concrete IPC connection mechanism. It
inherits from class Event_Handler to receive asynchronous completion
event notifications from the reactor dispatcher:
The Connector template is parameterized by a concrete
IPC_CONNECTOR and SERVICE_HANDLER. The IPC_CONNECTOR is a
concrete IPC mechanism used by a connector to synchronously or
asynchronously establish connections actively to remote acceptors. The
SERVICE_HANDLER template argument defines one-half of a service that
processes data exchanged with its connected peer service handler. Both
concrete types are provided by components in the application layer.
5. Implement the generic connector methods. The connector's connect()
method is used by an application to initiate a connection. This template
method [GoF95] allows concrete connectors to modify the active connection
strategy transparently, without changing the connector's interface or
implementation. Therefore, connect() delegates individual steps of its
connection strategy to hook methods that concrete connectors can over-ride to
perform custom operations.
When connect() establishes a connection asynchronously on behalf of a
service handler, the connector inserts that handler into an internal container¡ª
in our example a C++ standard template library map [Aus98]¡ªthat keeps track
of pending connections. When an asynchronously-initiated connection
completes, its dispatcher notifies the connector. The connector uses the
pending connection map to finish activating the service handler associated
with the connection.
If connect() establishes a connection synchronously, the connector can call
the concrete service handler's activation hook directly, without calling
complete(). This short-cut reduces unnecessary dynamic resource
management and processing for synchronous connection establishment and
service handler initialization.
The code fragment below shows the connect() method of our
Connector. If a SYNC value of the Connection_Mode parameter is
passed to this method, the concrete service handler will be activated after
the connection completes synchronously. Conversely, connections can be
initiated asynchronously by passing the Connection_Mode value ASYNC
to the connect() method:
The template method connect() delegates the initiation of a connection
to its connect_service_handler() hook method. This method
defines a default implementation of the connector's connection strategy.
This strategy uses the concrete IPC mechanism provided by the
IPC_CONNECTOR template parameter to establish connections, either
synchronously or asynchronously.
Note how connect() will activate the concrete service handler directly if
the connection happens to complete synchronously. This may occur, for
example, if the peer acceptor is co-located in the same host or process.
The complete() method activates the concrete service handler after its
initiated connection completes. For connections initiated synchronously,
complete() need not be called if the connect() method activates the
service handler directly. For connections initiated asynchronously, however,
complete() is called by the dispatcher when the connection completes. The
complete() method examines the map of concrete service handlers to find
the service handler whose connection just completed. It removes this service
handler from the map and invokes its activation hook method. In addition,
complete() unregisters the connector from the dispatcher to prevent it from
trying to notify the connector accidentally.
Note that the service handler's open() activation hook method is called
regardless of whether connections are established synchronously or
asynchronously, or even if they are connected actively or passively. This
uniformity makes it possible to define concrete service handlers whose
processing can be completely decoupled from the time and manner in which
they are connected and initialized.
When a connection is initiated synchronously in our gateway, the concrete
service handler associated with it is activated by the Connector's
connect_service_handler() method, rather than by its complete()
method, as described above. For asynchronous connections, conversely,
the reactor notifies the handle_event() method inherited by the
Connector from the Event_Handler class.
We implement this method as an adapter [GoF95] that converts the
reactor's event handling interface to forward the call to the connector's
complete() method. The complete() method then invokes the
activation hook of the concrete service handler whose asynchronously
initiated connection completed successfully most recently.
The complete() method shown below finds and removes the connected
service handler from its internal map of pending connections and transfers
the socket handle to the service handler.
Note how the complete() method initializes the service handler by
invoking its activate_service_handler() method. This method
delegates to the initialization strategy designated in the concrete service
handler's open() activation hook method.
3. Implement the components in the application layer. This layer defines the concrete
service handlers, concrete acceptors, and concrete connectors that Acceptor Connector pattern specifies in the Structure section. Components in the application
layer instantiate the generic service handler, acceptor, and connector components
described in implementation activity 2 (299) to create custom concrete components.
Concrete service handlers define the application's services. When implementing an
end-to-end service in a networked system that consists of multiple peer service
handlers, the Half Object plus Protocol pattern [Mes95] can help structure the
implementation of these service handlers. In particular, Half Object plus Protocol
helps decompose the responsibilities of an end-to-end service into service handler
interfaces and the protocol used to collaborate between them.
Concrete service handlers can also define a service's concurrency strategy. For
example, a service handler may inherit from the event handler and employ the
Reactor pattern (179) to process data from peers in a single thread of control.
Conversely, a service handler may use the Active Object (369) or Monitor Object
(399) patterns to process incoming data in a different thread of control than the one
used by the acceptor that connects it.
In the Example Resolved section, we illustrate how several different concurrency
strategies can be configured flexibly into concrete service handlers for our gateway
example without affecting the structure or behavior of the Acceptor-Connector
pattern.
Concrete connectors and concrete acceptors are factories that create concrete
service handlers. They generally derive from their corresponding generic classes and
implement these in an application-specific manner, potentially overriding the various
hook methods called by the accept() and connect() template methods [GoF95].
Another way to specify a concrete acceptor is to parameterize the generic acceptor
with a concrete service handler and concrete IPC passive connection mechanism, as
discussed in previous implementation activities and as outlined in the Example
Resolved section. Similarly, we can specify concrete connectors by parameterizing
the generic connector with a concrete service handler and concrete IPC active
connection mechanism.
Components in the application layer can also provide custom IPC mechanisms for
configuring concrete service handlers, concrete connectors, and concrete acceptors.
IPC mechanisms can be encapsulated in separate classes according to the Wrapper
Facade pattern (47). These wrapper facades create and use the transport endpoints
and transport handles that exchange data with connected peer service handlers
transparently.
The use of the Wrapper Facade pattern simplifies programming, enhances reuse,
and enables wholesale replacement of concrete IPC mechanisms via generic
programming techniques. For example, the SOCK_Connector, SOCK_Acceptor,
and SOCK_Stream classes used in the Example Resolved section are provided by
the ACE C++ Socket wrapper facade library [Sch92].
Peer host and gateway components in our satellite constellation management example use
the Acceptor-Connector pattern to simplify their connection establishment and service
initialization tasks:
??First we illustrate how to implement the peer host components, which play a passive
role in our example.
??Second we illustrate how to implement the gateway, which plays an active role in
establishing connections with the passive peer hosts.
By using the Acceptor-Connector pattern, we can also reverse or combine these roles with
minimal impact on the service handlers that implement the peer hosts and gateway services.
Implement the peer host application. Each peer host contains Status_Handler,
Bulk_Data_Handler, and Command_Handler components, which are concrete service
handlers that process routing messages exchanged with a gateway.
Each of these concrete service handlers inherit from the Service_Handler class defined
in implementation activity 2.4 (306), enabling them to be initialized passively by an acceptor.
For each type of concrete service handler, there is a corresponding concrete acceptor that
creates, connects, and initializes instances of the concrete service handler.
To demonstrate the flexibility of the Acceptor-Connector pattern, each concrete service
handler's open() hook method in our example implements a different concurrency strategy.
For example, when a Status_Handler is activated it runs in a separate thread, a
Bulk_Data_Handler runs as a separate process, and a Command_Handler runs in the
same thread as the Reactor that demultiplexes connection requests to concrete acceptors.
Note that changing these concurrency strategies does not affect the implementation of the
Acceptor class.
We start by defining a type definition called Peer_Handler:
This type definition instantiates the Service_Handler generic template class with a
SOCK_Stream wrapper facade (47). This wrapper facade[15]defines a concrete IPC
mechanism for transmitting data between connected transport endpoints using TCP. The
PEER_HANDLER type definition is the basis for all the subsequent concrete service handlers
used in our example. For example, the following Status_Handler class inherits from
Peer_Handler and processes status data, such as telemetry streams, exchanged with a
gateway:
The other concrete service handlers in our example, Bulk_Data_Handler and
Command_Handler, also subclass from Peer_Handler. The primary differences between
these classes are localized in the implementation of their open () and handle_event ()
methods, which vary according to their selected concurrency mechanisms.
The Status_Acceptor, Bulk_Data_Acceptor, and Command_Acceptor type
definitions shown below are template instantiations of concrete acceptor factories that
create, connect, and activate the Status_Handlers, Bulk_Data_Handlers, and
Command_Handlers concrete service handlers, respectively.
The type definitions above are defined by instantiating the generic Acceptor template class
defined in implementation activity 2.2 (300) with a SOCK_Acceptor wrapper facade (47),
which is a concrete IPC mechanism that establishes connections passively.[16]
Note how the use of C++ templates and dynamic binding permits specific details of concrete
acceptors and concrete service handlers to change flexibly. In particular, no changes to the
acceptor component are required if the concurrency strategies of Status_Handlers,
Bulk_Data_Handlers, and/or Command_Handlers change. The flexibility of this design
stems from the separation of concerns enforced by the Acceptor-Connector pattern. In
particular, concurrency strategies have been factored out into concrete service handlers
rather than being tightly coupled with the acceptors.
The main peer host application function initializes the concrete acceptors by passing their
constructors the TCP ports used to advertise each service to peer connectors. Each
concrete acceptor registers itself automatically with an instance of the Reactor passed as a
parameter to its constructor, as shown in the implementation activity 2.3(303).
After all three concrete acceptors are initialized, the main peer host application enters an
event loop that uses the reactor singleton to detect connection requests from the gateway.
When such requests arrive, the reactor notifies the appropriate concrete acceptor. The
acceptor then creates the appropriate concrete service handler, accepts the connection into
the handler, and activates the handler so that it can exchange routing messages with the
gateway.
Implement the gateway application. The main () function above illustrates how to define
the concrete acceptors and their concrete service handlers for the peer host application in
our satellite constellation management example. We now illustrate how to implement the
concrete connectors and corresponding concrete service handlers used within a gateway
application.
A gateway contains multiple instances of Status_Router, Bulk_Data_Router, and
Command_Router concrete service handlers, which route data they receive from a peer
host source to one or more peer host destinations. These concrete service handlers inherit
from the Service_Handler class, which enables them to be connected actively and
initialized automatically by a connector.
Each open() hook method in a concrete service handler implements a different
concurrency strategy. This is analogous to the handlers defined for the peer host application.
As before, changes to these concurrency strategies need not affect the Connector's
implementation, which is highly flexible and reusable.
We start by defining a concrete service handler that is specialized for TCP/IP data transfer.
We instantiate it with the appropriate IPC mechanism encapsulated by the SOCK_Stream
wrapper facade (47):
Unlike the Status_Handler defined in the peer host application, which ran in its own
thread, the Bulk_Data_Router runs in its own process. We can define similar subclasses
to form the Status_Router and Command_Router. As before, all these subclasses differ
primarily in their open() and handle_event () method implementations, which vary
according to the concurrency mechanisms they select.
This type definition instantiates the Connector class defined in implementation activity 2.4
(306) with a SOCK_Connector wrapper facade (47). This wrapper facade is a concrete IPC
mechanism that establishes connections actively to remote peer transport endpoints. The
gateway application requires just one concrete connector, because the concrete service
handlers passed to its connect() method are created and initialized externally to the
concrete connector. The gateway's concrete service handlers, such as its
Bulk_Data_Routers or Command_Routers, can therefore be treated uniformly as
Peer_Routers.
In contrast, peer host applications require a concrete acceptor for each type of concrete
service handler, such as a Bulk_Data_Handler or a Command_Handler, because the
concrete type of service handler must be specified a priori in the signature of the concrete
Acceptor's template instantiation.
In the main function of the gateway application, the concrete service handlers
Status_Router, Bulk_Data_Router, and Command_Router are created by the
function get_peer_addrs(). This function, whose implementation we omit, reads a list of
peer addresses from a configuration file or naming service. Each peer address consists of
an IP host address and a port number. Once these concrete service handlers are initialized,
all connections are initiated asynchronously by passing the ASYNC flag to the concrete
connector's connect() method.
All connections are invoked asynchronously and complete concurrently via the
peer_connector's complete () method, which the reactor notifies via a callback within
the context of its event loop. The reactor's event loop also demultiplexes and dispatches
routing events for Command_Router objects. These run in the same thread of control as the
reactor. Conversely, instances of Status_Router and Bulk_Data_Router execute in
separate threads and processes, respectively.
UNIX network superservers. Superserver implementations, for example Inetd [Ste98],
Listen [Rago93], and the Service Configurator [JS97b] from the ACE framework, use a
master acceptor process that listens for connections on a set of communication ports. In
Inetd, for example, each port is associated with a service, such as the standard Internet
services FTP, TELNET, DAYTIME, and ECHO. The acceptor process decouples the
functionality of the Inetd superserver into two separate parts: one for establishing
connections and another for receiving and processing requests from peers. When a service
request arrives on a port monitored by Inetd, it accepts the request and dispatches an
appropriate pre-registered handler to perform the service.
CORBA Object Request Brokers (ORB) [OMG98a]. The ORB Core layer in many
implementations of CORBA uses the Acceptor-Connector pattern to passively and actively
initialize connection handlers when clients request ORB services. [SC99] describes how the
Acceptor-Connector pattern is used to implement the ORB Core portion in The ACE ORB
(TAO), which is a real-time implementation of CORBA.
Web Browsers. The HTML parsing components in Web browsers, such as Netscape and
Internet Explorer, use the asynchronous version of the connector component to establish
connections with servers associated with images embedded in HTML pages. This pattern
allows multiple HTTP connections to be initiated asynchronously. This avoids the possibility
of the browser's main event loop blocking.
Ericsson EOS Call Center Management System. This system uses the Acceptor Connector pattern to allow application-level. Call Center Manager event servers [SchSu94]
to establish connections actively with passive supervisors in a networked center
management system.
Project Spectrum. The high-speed medical image transfer subsystem of project Spectrum
[BBC94] uses the Acceptor-Connector pattern to establish connections passively and
initialize application services for storing large medical images. Once connections are
established, applications send and receive multi-megabyte medical images to and from the
image stores.
ACE [Sch97]. Implementations of the generic Service_Handler, Connector, and
Acceptor components described in the Implementation section are provided as reusable
C++ classes in the ADAPTIVE Communication Environment (ACE) concurrent object oriented network programming framework.
Java ACE [JACE99] is a version of ACE implemented in Java. It provides the
JACE.Connection.SvcHandler, JACE.Connection.Acceptor, and
JACE.Connection.Connector components. These correspond to the service handler,
acceptor, and connector participants of the Acceptor-Connector pattern.
Managers and secretaries. A real-life implementation of the Acceptor-Connector pattern is
often found in organizations that provide secretaries for their managers. A manager wishing
to make a phone call to another manager asks her secretary to establish the call rather than
doing it herself. However, the call is not received by the called manager directly, but by his
secretary. Once the connection is established it is then passed to the managers. In terms of
the Acceptor-Connector pattern, the secretary that initiates the phone call is the connector,
the secretary that receives the call is the acceptor and the two managers are peer service
handlers.
The Acceptor-Connector pattern provides three benefits:
Reusability, portability, and extensibility. The Acceptor-Connector pattern decouples
mechanisms for connecting and initializing service handlers from the service processing
performed after service handlers are connected and initialized. Application-independent
mechanisms in acceptors and connectors are reusable components that know how to
establish connections and initialize the associated service handler when the connection is
established. Similarly, service handlers know how to perform application-specific service
processing.
This strict separation of concerns is achieved by decoupling the connection and initialization
strategies from the service handling strategy. Each strategy can therefore evolve
independently. The strategies for connection and initialization can be written once, placed in
a class library or framework and reused via inheritance, object composition or template
instantiation. The same connection and initialization code therefore need not be rewritten for
each application.
Service handlers, in contrast, may vary according to different application requirements. By
parameterizing the acceptor and connector components with a particular type of service
handler, the impact of this variation is localized to a small number of components in the
software.
Robustness. The Acceptor-Connector pattern strongly decouples the service handler from
the acceptor. This decoupling ensures that a passive-mode transport endpoint¡ªthe
PEER_ACCEPTOR in our gateway example¡ªcannot be used to read or write data
accidentally. This added degree of type-safety eliminates a class of errors that often arise
when programming with weakly-typed network programming interfaces, such as Sockets or
TLI [SHS95].
Efficiency. The Acceptor-Connector pattern can establish connections actively with a large
number of hosts asynchronously and efficiently over long-latency wide area networks.
Asynchrony is important in this situation because a large networked system may have
hundreds or thousands of host that must be connected.
One way to connect all these peers to the gateway is to use the synchronous mechanisms
described in the Implementation section. However, the round-trip delay for a 3-way TCP
connection handshake over a long-latency wide area network, such as a geosynchronous
satellite or trans-atlantic fiber cable, may be several seconds per handshake. In this case
asynchronous connection mechanisms may perform better, because they can utilize the
inherent parallelism of the network and hosts in the wide area network.
The Acceptor-Connector pattern has the following liabilities:
Additional indirection. The Acceptor-Connector pattern can incur additional indirection
compared to using the underlying network programming interfaces directly. However,
languages that support parameterized types, such as C++, Ada, or Eiffel, can implement
these patterns with no significant overhead when compilers inline the method calls used to
implement the patterns.
Additional complexity. The Acceptor-Connector pattern may add unnecessary complexity for
simple client applications that connect with only one server and perform one service using a
single network programming interface. However, the use of generic acceptor and connector
wrapper facades may simplify even these applications by shielding developers from tedious,
error-prone, and non-portable low-level network programming mechanisms.
The intent of the Acceptor-Connector pattern is similar to that of the Client-Dispatcher-Server
pattern [POSA1] in that both are concerned with the separation of active connection
establishment from subsequent service processing. The primary difference is that the
Acceptor-Connector pattern addresses passive and active connection establishment and
initialization of both synchronous and asynchronous connections. In contrast, the Client Dispatcher-Server pattern focuses on synchronous connection establishment.
##%%&&
The Scoped Locking C++ idiom ensures that a lock is acquired when control enters a scope
and released automatically when control leaves the scope, regardless of the return path from
the scope.
Commercial Web servers often maintain a 'hit count' that records how many times each URL
is accessed by clients over a period of time. To reduce latency a Web server process
maintains the hit counts in a memory-resident component rather than in a disk file.
Web server processes are often multi-threaded [HS98] to increase throughput. Public
methods in the hit-count component must therefore be serialized to prevent threads from
corrupting its internal state when hit counts are updated concurrently.
One way to serialize access to a hit-count component is to acquire and release a lock
explicitly in each public method. The following C++ example uses the Thread_Mutex
defined in the Wrapper Facade pattern (47) to serialize access to critical sections:
Although this code works, the Hit_Counter implementation is unnecessarily hard to
develop and maintain. For instance, maintenance programmers may forget to release the
lock_ on some return paths out of the increment() method, such as when modifying its
else branch to check for a new failure condition:
In addition, the implementation is not exception-safe. Thus, lock_ will not be released if a
later version of the increment() method throws an exception or calls a helper method that
throws an exception [Mue96].
Both of these modifications will cause the increment() method to return without releasing
the lock_. If the lock_ is not released, however, the Web server process will hang when
other threads block indefinitely while trying to acquire the lock_. Moreover, if these error
cases are rare, the problems with this code may not show up during system testing.
A concurrent application containing shared resources that are manipulated by multiple
threads concurrently.
Code that should not execute concurrently must be protected by some type of lock that is
acquired and released when control enters and leaves a critical section, respectively. If
programmers must acquire and release locks explicitly, however, it is hard to ensure that the
locks are released in all paths through the code. For example, in C++ control can leave a
scope due to a return, break, continue, or goto statement, as well as from an
unhandled exception being propagated out of the scope.
Define a guard class whose constructor automatically acquires a lock when control enters a
scope and whose destructor automatically releases the lock when control leaves the scope.
Instantiate instances of the guard class to acquire/release locks in method or block scopes
that define critical sections.
The implementation of the Scoped Locking idiom is straightforward.
1. Define a guard class that acquires and releases a particular type of lock in its
constructor and destructor, respectively. The constructor of the guard class stores a
pointer or reference to the lock and then acquires the lock. The destructor of the
guard class uses the pointer or reference stored by the constructor to release the
lock.
2. The following class illustrates a guard designed for the Thread_Mutex wrapper
facade (47):
A pointer to a lock, rather than a lock object, should be used in a guard class
implementation to prevent copying or assigning a lock, which is erroneous, as
discussed in the Wrapper Facade pattern (47).
31. In addition, it is useful to add a flag, such as the owner_ flag in the
Thread_Mutex_Guard example above, that indicates whether or not a guard
acquired the lock successfully. The flag can also indicate failures that arise from
'order of initialization bugs' if static/global locks are used erroneously [LGS99]. By
checking this flag in the guard's destructor, a subtle run-time error can be avoided
that would otherwise occur if the lock was released when it was not held by the
guard.
32. Let critical sections correspond to the scope and lifetime of a guard object. To protect
a critical region from concurrent access, scope it¡ªif this has not already been done¡ª
and create a guard object on the stack as the first statement within the scope. The
constructor of the guard class acquires the lock automatically. When leaving the
scope of the critical section, the guard's destructor is called automatically, which
releases the lock. Due to the semantics of C++ destructors, guarded locks will be
released even if C++ exceptions are thrown from within the critical section.
33. The Scoped Locking idiom resolves the original problems with the Hit_Counter
class in our multi-threaded Web server:
In this solution the guard ensures that the lock_ is acquired and released
automatically as control enters and leaves the increment() method,
respectively.
Explicit Accessors. One drawback with the Thread_Mutex_Guard interface described in
the Implementation section is that it is not possible to release the lock explicitly without
leaving the method or block scope.
For example, the following code fragment illustrates a situation where the lock could be
released twice, depending on whether or not the condition in the if statement evaluates
to true:
To prevent this erroneous use, programmers should not access the lock directly. Instead,
explicit accessor methods to the underlying lock can be defined in the in the lock's guard
class:
We revise the Thread_Mutex_Guard class as follows:
The acquire() and release() accessor methods track whether the lock has been
already released, and if so, the lock will not be released in the guard's destructor.
Using the revised Thread_Mutex_Guard our code will work correctly:
Strategized Scoped Locking. Defining a different guard for each type of lock is tedious, error prone, and excessive, because it may increase the memory footprint of applications or
components. Therefore, a common variant of the Scoped Locking idiom is to apply either the
parameterized type or polymorphic version of the Strategized Locking pattern (333).
Booch Components. The Booch Components [BV93] were one of the first C++ class
libraries to use the Scoped Locking idiom for multi-threaded C++ programs.
ACE [Sch97]. The Scoped Locking idiom is used extensively throughout the ADAPTIVE
Communication Environment framework, which defines an ACE_Guard implementation
similar to the Thread_Mutex_Guard class described in the Implementation and Variants
sections.
Threads.h++. The Rogue Wave Threads.h++ library defines a set of guard classes that are
modeled after the ACE Scoped Locking designs.
Java defines a programming feature called a synchronized block that implements the
Scoped Locking idiom in that language. Java compilers generate a corresponding block of
bytecode instructions where a monitorenter and a monitorexit bracket this block. To
ensure that the lock is always released, the compiler also generates an exception handler to
catch all exceptions thrown in the synchronized block [Eng99].
The Scoped Locking idiom offers the following benefit:
Increased robustness. By applying this idiom, locks are acquired and released automatically
when control enters and leaves critical sections defined by C++ method and block scopes.
This idiom increases the robustness of concurrent applications by eliminating common
programming errors related to synchronization and multi-threading.
There are two liabilities of applying the Scoped Locking idiom to concurrent applications
and components:
Potential for deadlock when used recursively. If a method that uses the Scoped Locking
idiom calls itself recursively, 'self-deadlock' will occur if the lock is not a 'recursive' mutex.
The Thread-Safe Interface pattern (345) describes a technique that avoids this problem. This
pattern ensures that only interface methods apply the Scoped Locking idiom, whereas
implementation methods do not apply it.
Limitations with language-specific semantics. The Scoped Locking idiom is based on a C++
language feature and therefore will not be integrated with operating system-specific system
calls. Thus, locks may not be released automatically when threads or processes abort or exit
inside a guarded critical section. Likewise, they will not be released properly if the standard
C longjmp() function is called because this function does not call the destructors of C++
objects as the run-time stack unwinds.
The following modification to increment() will prevent the Scoped Locking idiom from
working:
In general, therefore, it is inappropriate to abort or exit a thread or process within a
component. Instead, an exception-handling mechanism or error-propagation pattern should
be used [Mue96].
Excessive compiler warnings. The Scoped Locking idiom defines a guard object that is not
used explicitly within the scope, because its destructor releases the lock implicitly.
Unfortunately, some C++ compilers print 'statement has no effect' warnings when guards are
defined but not used explicitly within a scope. At best these warnings are distracting¡ªat
worst, they encourage developers to disable certain compiler warnings, which may mask
other warnings that indicate actual problems with the code. An effective way to handle this
problem is to define a macro that eliminates the warnings without generating additional code.
The following macro is defined in ACE [Sch97]:
The Scoped Locking idiom is a special case of a more general C++ idiom [Str97] and the
Execute Around Object [Hen00] idiom, in which a constructor acquires a resource and a
destructor releases the resource when a scope is entered and exited, respectively. When
these idioms are applied to concurrent applications, the resource that is acquired and
released is some type of lock.
##%%&&
The Strategized Locking design pattern parameterizes synchronization mechanisms that
protect a component's critical sections from concurrent access.
A key component for implementing high-performance Web servers is a file cache, which
maps URL path names to memory-mapped files or open file handles [HS98]. When a client
requests a URL that is in the cache, the Web server can transfer the file contents to the
client immediately without accessing slower secondary storage via multiple read() and
write() operations.
A file cache implementation for a portable high-performance should run efficiently on a range
of multi-threaded and single-threaded operating systems. One way to achieve this portability
is to develop multiple file cache classes:
These two implementations form part of a component family whose classes differ only in
their synchronization strategy. One component in the family¡ªclass File_Cache_ST¡ª
implements a single-threaded file cache with no locking. The other component¡ªclass
File_Cache_Thread_Mutex¡ªimplements a file cache that uses a mutex to serialize
multiple threads that access the cache concurrently. Maintaining separate implementations
of these file cache components can be tedious, however. In particular, future enhancements
and fixes must be added consistently in each component's implementation.
An application or system where components must run efficiently in a variety of different
concurrency architectures.
Components that run in multi-threaded environments must protect their critical sections from
concurrent client access. When integrating synchronization mechanisms with component
functionality two forces must be resolved:
??Different applications may require different synchronization strategies, such as
mutexes, readers/writer locks, or semaphores [McK95]. It should therefore be possible
to customize a component's synchronization mechanisms according to the requirements
of particular applications.
?In our example the synchronization strategy is hard-coded. To increase performance
on large-scale multi-processor platforms, a new class must therefore be written to
support a file cache implementation that uses a readers/writer lock instead of a
thread mutex. It is time-consuming, however, to customize an existing file cache
class to support new more efficient synchronization strategies.
??Adding new enhancements and bug fixes should be straightforward. In particular, to
avoid 'version-skew', changes should apply consistently and automatically to all
members in a component family.
?If there are multiple copies of the same basic file cache component, version-skew will
likely to occur because changes to one component may be applied inconsistently to
other component implementations. Applying each change manually is also error prone and non-scalable.
Parameterize a component's synchronization aspects by making them 'pluggable' types.
Each type objectifies a particular synchronization strategy, such as a mutex, readers/writer
lock, semaphore, or 'null' lock. Define instances of these pluggable types as objects
contained within a component that can use the objects to synchronize its method
implementations efficiently.
The Strategized Locking pattern can be implemented via five activities.
1. Define the component interface and implementation without concern for the
component's synchronization aspects.
Strategize the locking mechanisms. Many components have relatively simple
synchronization aspects that can be implemented using common locking strategies,
such as mutexes and semaphores. These synchronization aspects can be
strategized uniformly using either polymorphism or parameterized types. In general,
polymorphism should be used when the locking strategy is not known until runtime.
Conversely, parameterized types should be used when the locking strategy is known
at compile-time. As usual the trade-off is between the efficient run-time performance
of parameterized types versus the potential for run-time extensibility with
polymorphism.
Assuming the Scoped Locking idiom (325) is applied, the strategization of locks
involves two sub-activities:
1. Define an abstract interface for the locking mechanisms. To configure a
component with alternative locking mechanisms, all concrete implementations
of these mechanisms must employ an abstract interface with common
signatures for acquiring and releasing locks based on either polymorphism or
parameterized types.
??Polymorphism. In this strategy, define a polymorphic lock object that
contains dynamically-bound acquire() and release() methods.
Derive all concrete locks from this base class and override its methods to
define a concrete locking strategy, as outlined in implementation activity 5
(339).
??To implement a polymorphic lock object for our file cache example, we
first define an abstract locking class with virtual acquire() and
release() methods:
Parameterized types. In this strategy, we must ensure that all concrete
locks employ the same signature for acquiring and releasing locks. The
usual way to ensure this is to implement concrete locks using the
Wrapper Facade pattern (47).
2. Use the Scoped Locking Idiom (325) to define a guard class that is strategized
by its synchronization aspect. This design follows the Strategy Pattern
280
[GHJV95], in which the guard class serves as the context that holds a
particular lock and the concrete locks provide the strategies. The Scoped
Locking idiom can be implemented either with polymorphism or with
parameterized types.
??Polymorphism. In this approach, pass a polymorphic lock object to the
guard's constructor and define an instance of this lock object as a private
data member. To acquire and release the lock with which it is configured,
the implementation of the guard class can use the interface of the
polymorphic Lock base class defined in implementation sub-activity 2.1
(335).
??A Guard class that controls a polymorphic lock can be defined as
follows:
Parameterized types. In this approach, define a template guard class
that is parameterized by the type of lock that will be acquired and
released automatically.
??The following illustrates a Guard class that is strategized by a LOCK
template parameter:
Update the component interface and implementation. After synchronization
mechanisms are strategized, components can use these mechanisms to protect their
critical sections, either by acquiring or releasing a lock explicitly or by using the guard
class defined in implementation activity 2.2 (336). The latter approach follows the
Scoped Locking idiom (325). Depending on whether the polymorphic or
parameterized type strategy is used, the lock can be passed to the component either
as a parameter in its constructor or by adding a lock template parameter to the
component declaration. In either case, the lock passed to a component must follow
the signature expected by the guard class, as discussed in implementation activity
2.2 (336).
13. This version of our file cache component is passed a polymorphic lock
parameter:
If a C++ compiler supports default template arguments, it may be useful to add a
default LOCK to handle the most common use case. For example, we can define
the default LOCK as a readers/writer lock:
Revise the component implementation to avoid deadlock and remove unnecessary
locking overhead. If intra-component method invocations occur, developers must
design their component implementation carefully to avoid self-deadlock and
unnecessary synchronization overhead. The Thread-Safe Interface pattern (345)
provides a straightforward technique that prevents these problems.
57. Define a family of locking strategies with uniform interfaces that can support various
application-specific concurrency designs. Common locking strategies include
recursive and non-recursive mutexes, readers/writer locks, semaphores, and file
locks.
When applying the polymorphic approach, implement the locking strategies as
subclasses of the abstract class Lock, as discussed in implementation activity 2
(335). If parameterized types are used, ensure that all concrete lock implementations
follow the signature for locks defined in implementation activity 2 (335).
In addition to the Thread_Mutex locking strategy defined in Wrapper Facade
(47), the Null_Mutex is surprisingly useful. This class defines an efficient
locking strategy for single-threaded applications and components:
All methods in Null_Mutex are empty C++ inline functions that can be removed
completely by optimizing compilers. This class is an example of the Null Object
pattern [PLoPD3], which simplifies applications by defining a 'no-op' placeholder
that removes conditional statements in the component's implementation. A use of
Null_Mutex and other locking strategies appears in the Example Resolved
section.
If existing locking mechanisms have incompatible interfaces, use the Wrapper Facade (47)
or Adapter [GoF95] patterns to ensure the interfaces conform to the signatures expected by
the component's synchronization aspects.
The following class wraps the Thread_Mutex from the Implementation section of the
Wrapper Facade pattern (47), thereby connecting it to our polymorphic lock hierarchy:
We can apply the parameterized type form of the Strategized Locking pattern to implement a
file cache for Web server content that is tuned for various single-threaded and multi threaded concurrency models:
Note how in each configuration the Content_Cache interface and implementation require
no changes. This flexibility stems from the Strategized Locking pattern, which abstracts
synchronization aspects into 'pluggable' parameterized types. Moreover, the details of
locking have been Strategized via a C++ typedef. It is therefore straightforward to define a
Content_Cache object that does not expose synchronization aspects to applications:
Content_Cache cache;
Bridge strategy. Unfortunately, configuring the polymorphic file cache in implementation
activity 3 (337) differs from configuring the templatized file cache, because a polymorphic
lock implemented as a pointer cannot be passed as a parameter to the templatized
File_Cache and Guard classes. Instead, we need a 'real' object, rather than a pointer to
an object. Fortunately, the Bridge pattern [GoF95] can help us implement a family of locking
strategies that is applicable to both polymorphic and parameterized type approaches. To
apply this bridge strategy variant we simply define an additional abstraction class that
encapsulates, and can be configured with, a polymorphic lock. An instance of this
abstraction class then can be passed uniformly to both polymorphic and templatized
components.
Consider the hierarchy of polymorphic locks defined in implementation activity 2 (335) as
being a Bridge implementation class hierarchy. The following abstraction class then can
be used to encapsulate this hierarchy:
Note how this design allows us to initialize both our polymorphic and parameterized
File_Cache and Guard classes with a single Lock_Abstraction class that can be
configured with a concrete lock from our hierarchy of locking mechanisms.
As a result of using this variation of Strategized Locking, the family of locking mechanisms
becomes more reusable and easier to apply across applications. Be aware however that
while this scheme is flexible, it is also more complicated to implement. It should therefore be
used with care.
ACE [Sch97]. The Strategized Locking pattern is used extensively throughout the
ADAPTIVE Communication Environment framework. Most synchronization aspects of ACE
containers components, such as ACE_Hash_Map_Manager, can be strategized via
parameterized types.
Booch Components. The Booch Components [BV93] were one of the first C++ class
libraries to parameterize locking strategizes via templates.
The Dynix/PTX operating system applies the Strategized Locking pattern extensively
throughout its kernel.
ATL Wizards. The Microsoft ATL Wizard in Visual Studio uses the parameterized type
implementation of Strategized Locking, completed with default template parameters. In
addition, it implements a class similar to the Null_Mutex. If a COM class is implemented as
a single-threaded apartment a no-op lock class is used, whereas in multi-threaded
apartments a 'real' recursive mutex is used.
There are three benefits of applying the Strategized Locking pattern:
Enhanced flexibility and customization. It is straightforward to configure and customize a
component for certain concurrency models because the synchronization aspects of
components are strategized. If no suitable locking strategy is available for a new
concurrency model, the family of locking strategies can be extended without affecting
existing code.
Decreased maintenance effort for components. It is straightforward to add enhancements
and bug fixes to a component because there is only one implementation, rather than a
separate implementation for each concurrency model. This centralization of concerns helps
minimize version-skew.
Improved reuse. Components implemented using this pattern become less dependent on
specific synchronization mechanisms. They therefore become more reusable, because their
locking strategies can be configured orthogonally to their behavior.
There are two liabilities of applying the Strategized Locking pattern:
Obtrusive locking. If templates are used to parameterize locking aspects this will expose the
locking strategies to application code. Although this design is flexible, it also can be
obtrusive, particularly for compilers that do not support templates efficiently or correctly. One
way to avoid this problem is to apply the polymorphic strategy to vary component locking
behavior.
Over-engineering. Externalizing a locking mechanism by placing it in a component's
interface may actually provide too much flexibility in certain situations. For example,
inexperienced developers may try to parameterize a component with the wrong type of lock,
resulting in improper compile- or run-time behavior. Similarly, only a single type of
synchronization mechanism may be needed for a particular type of component. In this case
the flexibility of Strategized Locking is unnecessary. In general this pattern is most effective
when practical experience reveals a component's behavior to be orthogonal to its locking
strategy, and that locking strategies do indeed vary in semantically meaningful and efficient
ways.
The main synchronization mechanism in Java is the monitor. The Java language does not
provide 'conventional' concurrency control mechanisms, such as mutexes and semaphores,
to application developers. The Strategized Locking pattern therefore need not be applied to
Java directly.
It is possible, however, to implement different concurrency primitives, such as mutexes,
semaphores, and readers/writer locks in Java. For example, the util.concurrent
package in [Lea99a] defines various types of locks, such as readers/writer locks. The
implementation of these primitives then can be used as locking strategies to support various
application-specific concurrency use cases. Due to the lack of parameterized types in Java
specifications to date, only the polymorphic approach of Strategized Locking pattern could
be used to configure different synchronization strategies. In this case, Java implementations
of this pattern will be similar to the C++ versions described in this pattern.
##%%&&
The Thread-Safe Interface design pattern minimizes locking overhead and ensures that
intra-component method calls do not incur 'self-deadlock' by trying to reacquire a lock that is
held by the component already.
When designing thread-safe components when intra-component method calls, developers
must be careful to avoid self-deadlock and unnecessary locking overhead. For example,
consider a more complete implementation of the File_Cache component outlined in the
Strategized Locking pattern (333):
This implementation of File_Cache works efficiently only when strategized by a 'null' lock
such as the Null_Mutex described in the Strategized Locking pattern (333). If the
File_Cache implementation is strategized with a recursive mutex, however, it will incur
unnecessary overhead when it reacquires the mutex in the insert() method. Even worse,
if it is strategized with a non-recursive mutex, the code will 'self-deadlock' when the
lookup() method calls the insert() method. This self-deadlock occurs because
insert() tries to reacquire the LOCK that has been acquired by lookup() already.
It is therefore counter-productive to apply the Strategized Locking pattern to the
implementation of File_Cache shown above, because there are so many restrictions and
subtle problems that can arise. Yet the File_Cache abstraction can still benefit from the
flexibility and customization provided by Strategized Locking.
Components in multi-threaded applications that contain intra-component method calls.
Multi-threaded components often contain multiple publicly-accessible interface methods and
private implementation methods that can alter the component states. To prevent race
conditions, a lock internal to the component can be used to serialize interface method
invocations that access its state. Although this design works well if each method is self contained, component methods may call each other to carry out their computations. If this
occurs, the following forces will be unresolved in multi-threaded components that use
improper intra-component method invocation designs:
??Thread-safe components should be designed to avoid 'self-deadlock'. Self-deadlock
can occur if one component method acquires a non-recursive lock in the component
and then calls another component method that tries to reacquire the same lock.
??Thread-safe components should be designed to incur only minimal locking overhead,
for example to prevent race conditions on component state. If a recursive component
lock is selected to avoid the self-deadlock problem outlined above, however,
unnecessary overhead will be incurred to acquire and release the lock multiple times
across intra-component method calls.
Structure all components that process intra-component method invocations according two
design conventions:
??Interface methods check. All interface methods, such as C++ public methods, should
only acquire/release component lock(s), thereby performing synchronization checks at
the 'border' of the component. After the lock is acquired, the interface method should
forward immediately to an implementation method, which performs the actual method
functionality. After the implementation method returns, the interface method should
release the lock(s) before returning control to the caller.
??Implementation methods trust. Implementation methods, such as C++ private and
protected methods, should only perform work when called by interface methods. They
therefore trust that they are called with the necessary lock(s) held and should never
acquire or release lock(s). Implementation methods should also never call 'up' to
interface methods, because these methods acquire lock(s).
The Thread-Safe Interface pattern can be implemented using two activities:
1. Determine the interface and corresponding implementation methods. The interface
methods define the public API of the component. For each interface method, define a
corresponding implementation method.
2. The interface and implementation methods for File_Cache can be defined as
follows:
Program the interface and implementation methods. The bodies of the interface and
implementation methods are programmed according to the design conventions
described in the Solution section.
21. Our File_Cache implementation applies Thread-Safe Interface to minimize
locking overhead and prevent self-deadlock in class methods:
Thread-Safe Facade. This variant can be used if access to a whole subsystem or coarse grained component must be synchronized. A facade [GoF95] can be introduced as the entry
point for all client requests. The facade's methods correspond to the interface methods. The
classes that belong to the subsystem or component provide the implementation methods. If
these classes have their own internal concurrency strategies, refactoring may be needed to
avoid nested monitor lockout[2] [JS97a].
Nested monitor lockup occurs when a thread acquires object X's monitor lock without
relinquishing the lock already held on monitor Y, thereby preventing a second thread from
acquiring the monitor lock for Y. This can lead to deadlock because, after acquiring monitor
X, the first thread may wait for a condition to become true that can only change as a result of
actions by the second thread after it has acquired monitor Y. It may not be possible to
refactor the code properly to avoid nested monitor lockouts if the subsystem or component
cannot be modified, for example if it is a third-party product or legacy system. In this case,
Thread-Safe Facade should not be applied.
Thread-Safe Wrapper Facade. This variant helps synchronize access to a non-synchronized
class or function API that cannot be modified. A wrapper facade (47) provides the interface
methods, which encapsulate the corresponding implementation calls on the class or function
API with actions that acquire and release a lock. The wrapper facade thus provides a
synchronization proxy [POSA1] [GoF95] that serializes access to the methods of the class or
function API.
ACE [Sch97]. The Thread-Safe Interface pattern is used throughout the ADAPTIVE
Communication Environment framework, for example in its ACE_Message_Queue class.
The Dynix/PTX operating system applies the Thread-Safe Interface pattern in portions of its
kernel.
Java. The hash table implementation in java.util.Hashtable uses the Thread-Safe
Interface design pattern. Hashtable's interface methods, such as put(Object key,
Object value), acquire a lock before changing the underlying data structure, which
consists of an array of linked lists. The implementation method rehash() is called when the
load threshold is exceeded. A new larger hash table is created, all elements are moved from
the old to the new hash table and the old table is left to the garbage collector. Note that the
rehash() method is not protected by a lock, in contrast to the publicly accessible methods
such as put(Object key, Object value). Protecting rehash() by a lock would not
deadlock the program due to Java's reentrant monitors. It would, however diminish its
performance due to the locking overhead.
A more sophisticated use case was introduced in JDK 1.2 with the Collection classes,
which applies the Thread-Safe Wrapper Facade variant to make collection data structures
thread-safe. The java.util.Collections takes any class implementing the Map
interface and returns a SynchronizedMap, which is a different class implementing Map.
The methods of SynchronizedMap do no more than synchronize on an internal monitor
and then forward to the method of the original object. Developers can therefore choose
between fast or thread-safe variants of data structures, which only need be implemented
once.
Security checkpoints. You may encounter a real-life variation of the Thread-Safe Interface
pattern when entering a country or commercial office building that has a security guard at the
border or entrance. To be admitted, you must sign in. After being admitted, other people that
you interact with typically trust that you are supposed to be there.
There are three benefits of applying the Thread-Safe Interface pattern:
Increased robustness. This pattern ensures that self-deadlock does not occur due to intra component method calls.
Enhanced performance. This pattern ensures that locks are not acquired or released
unnecessarily.
Simplification of software. Separating the locking and functionality concerns can help to
simplify both aspects.
However, there are also four liabilities when applying the Thread-Safe Interface pattern:
Additional indirection and extra methods. Each interface method requires at least one
implementation method, which increases the footprint of the component and may also add
an extra level of method call indirection for each invocation. One way to minimize this
overhead is to inline the interface and/or implementation methods.
Potential deadlock. By itself, the Thread-Safe Interface pattern does not resolve the problem
of self-deadlock completely. For example, consider a client that calls an interface method on
component A, which then delegates to an implementation method that calls an interface
method on another component B. If the implementation of component B's method calls back
on an interface method of component A, deadlock will occur when trying to reacquire the
lock that was acquired by the first call in this chain.
Potential for misuse. Object-oriented programming languages, such as C++ and Java,
support class-level rather than object-level access control. As a result, an object can bypass
the public interface to call a private method on another object of the same class, thus
bypassing that object's lock. Therefore, programmer's should be careful to avoid invoking
private methods on any object of their class other than themselves.
Potential overhead. The Thread-Safe Interface pattern prevents multiple components from
sharing the same lock. Therefore synchronization overhead may increase because multiple
locks must be acquired, which also makes it harder to detect and avoid deadlocks.
Moreover, the pattern prevents locking at a finer granularity than the component, which can
increase lock contention, thereby reducing performance.
The Thread-Safe Interface pattern is related to the Decorator pattern [GoF95], which
extends an object transparently by attaching additional responsibilities dynamically. The
intention of the Thread-Safe Interface pattern is similar, in that it attaches robust and efficient
locking strategies to make components thread-safe. The primary difference is that the
Decorator pattern focuses on attaching additional responsibilities to objects dynamically,
whereas the Thread-Safe Interface pattern focuses on the static partitioning of method
responsibilities in component classes.
Components designed according to the Strategized Locking pattern (333) should employ the
Thread-Safe Interface pattern to ensure that the component will function robustly and
efficiently, regardless of the type of locking strategy that is selected.
Java implements locking at the method level via monitor objects (399) designated by the
synchronized keyword. In Java, monitors are recursive. The problem of self-deadlock
therefore cannot occur as long as developers reuse the same monitor, that is, synchronize
on the same object. However, the problem of nested monitor lockout [JS97a] [Lea99a] can
occur in Java if multiple nested monitors are used carelessly.
The problem of locking overhead depends on which Java Virtual Machine (JVM) is used. If a
specific JVM implements monitors inefficiently and monitors are acquired recursively, the
Thread-Safe Interface pattern may be able to help improve component run-time
performance.
##%%&&
The Double-Checked Locking Optimization design pattern reduces contention and
synchronization overhead whenever critical sections of code must acquire locks in a thread safe manner just once during program execution.
The Singleton pattern ensures a class has only one instance and provides a global access
point to that instance. The following C++ code shows the canonical implementation of
Singleton from [GoF95]:
Applications use the static instance() method to retrieve a pointer to the Singleton and
then invoke public methods:
Singleton::instance ()->method_1 ();
Unfortunately the canonical implementation of the Singleton pattern shown above is
problematic on platforms with preemptive multi-tasking or true hardware parallelism. In
particular, the Singleton constructor can be called multiple times if
??Multiple pre-emptive threads invoke Singleton::instance() simultaneously
before it is initialized and
??Multiple threads execute the dynamic initialization of the Singleton constructor within
the critical section.
At best calling the Singleton constructor multiple times will cause a memory leak. At worst
it can have disastrous consequences if singleton initialization is not idempotent.
To protect the critical section from concurrent access we could apply the Scoped Locking
idiom (345) to acquire and release a mutex lock automatically:
Assuming singleton_lock_ is initialized correctly, Singleton is now thread-safe. The
additional locking overhead may be excessive, however. In particular every call to
instance() now acquires and releases the lock, even though the critical section should be
executed just once. By placing the guard inside the conditional check, we can remove the
locking overhead:
Unfortunately, this solution does not provide thread-safe initialization because a race
condition in multi-threaded applications can cause multiple initializations of Singleton. For
example, consider two threads that simultaneously check for instance_ == 0. Both will
succeed, one will acquire the lock via the guard and the other will block. After the first
thread initializes Singleton and releases the lock, the blocked thread will obtain the lock
and erroneously initialize Singleton a second time.
A application containing shared resources accessed concurrently by multiple threads.
Concurrent applications must ensure that certain portions of their code execute serially to
avoid race conditions when accessing and modifying shared resources. A common way of
avoiding race conditions is to serialize access to the shared resources' critical sections via
locks, such as mutexes. Every thread that wants to enter a critical section must first acquire
a lock. If this lock is already owned by another thread, the thread will block until the lock is
released and the lock can be acquired.
The serialization approach outlined above can be inappropriate for objects or components
that require 'just once' initialization. For example, the critical section code in our Singleton
example must be executed just once during its initialization. However, every method call on
the singleton acquires and releases the mutex lock, which can incur excessive overhead
[PLoPD3]. To avoid this overhead, programmers of concurrent applications may revert to
using global variables rather than applying the Singleton pattern. Unfortunately, this 'solution'
has two drawbacks [LGS99]:
??It is non-portable because the order in which global objects defined in different files are
constructed is often not specified.
??It is overly resource consumptive because global variables will be created even if they
are not used.
Introduce a flag that provides a 'hint' about whether it is necessary to execute a critical
section before acquiring the lock that guards it. If this code need not be executed the critical
section is skipped, thereby avoiding unnecessary locking overhead.
The Double-Checked Locking Optimization pattern can be implemented via three activities:
1. Identify the critical section to be executed just once. This critical section performs
operations, such as initialization logic, that are executed just once in a program.
2. For example, a singleton is initialized just once in a program. The call to the
singleton's constructor is thus executed only once in a critical section, regardless
of the number of times the accessor method Singleton::instance() is
called.
4. Implement the locking logic. The locking logic serializes access to the critical section of
code that is executed just once. To implement this locking logic we can employ the
Scoped Locking idiom (345) to ensure that the lock is acquired automatically when
the appropriate scope is entered and released automatically when it goes out of
scope.
5. In accordance with the Scoped Locking idiom (345) a Thread_Mutex
singleton_lock_ is used to ensure that the singleton's constructor does not
execute concurrently.
7. This lock must be initialized prior to the first call to the code that is executed just once.
In C++, one way to ensure that a lock is initialized prior to its first use is to define it
as a static object, as shown in the Example section. Unfortunately the C++ language
specification does not guarantee the order of initialization of static objects that are
defined in separate compilation units. As a result, different C++ compiler and linker
platforms may behave inconsistently and the lock may not be initialized when it is first
accessed.
8. A better way to avoid this problem is to use the Object Lifetime Manager pattern
[LGS99]. This pattern defines a portable object manager component that governs the
entire lifetime of global or static objects as follows:
??The object manager creates these objects prior to their first use and
??It ensures they are destroyed properly at program termination
For example, the lock can be placed under the control of an object manager that will
ensure it is initialized before any singleton attempts to use the lock to serialize its
initialization. The object manager can also delete the singleton when the program
terminates, thereby preventing the memory and resource leaks that can otherwise
occur with the Singleton pattern [Vlis98a].
9. Implement the first-time-in flag. This flag indicates whether the critical section has been
executed already.
10. The Singleton::instance_ pointer is used as the first-time-in flag. If the flag
evaluates to true the critical section is skipped. If the flag also has a particular
application-specific purpose, as our Singleton::instance_ pointer is used, it
must be an atomic type that can be set without a partial read or write. The
following code for the Singleton example is thread-safe, but avoids
unnecessary locking overhead by placing the call to new within another
conditional test:
The first thread that acquires the singleton_lock_ will construct the
Singleton object and assign the pointer to instance_, which serves as the
first-time-in flag in this example. All threads that call instance() subsequently
will find instance_ is not equal to zero and will thus skip the initialization step.
30. The second check prevents a race condition if multiple threads try to initialize
Singleton simultaneously. This handles the case in which multiple threads
execute in parallel. In the code above these threads will queue up at the
singleton_lock_ mutex. When the queued threads finally obtain the mutex
singleton_lock_ they will find instance_ is not equal to zero and will then
skip the Singleton initialization.
31. This implementation of the Singleton::instance() method only incurs
locking overhead for threads that are active inside instance() when the
Singleton is first initialized. In subsequent calls to instance() the
instance_ pointer is not zero and thus singleton_lock_ is neither acquired
nor released.
Volatile Data. The Double-Checked Locking Optimization pattern implementation may
require modifications if a compiler optimizes the first-time-in flag by caching it in some way,
such as storing it in a CPU register. In this case, cache coherency may become a problem.
For example, copies of the first-time-in flag held simultaneously in registers by multiple
threads may become inconsistent if one thread's setting of the value is not reflected in other
threads' copies.
A related problem is that a highly optimizing compiler may consider the second check of
flag == 0 to be superfluous and optimize it away. A solution to both these problems is to
declare the flag as volatile data, which ensures the compiler will not perform aggressive
optimizations that change the program's semantics.
The use of volatile ensures that a compiler will not place the instance_ pointer into
a register, nor will it optimize away the second check in instance().
The downside of using volatile is that all access to will be through memory rather than
through registers, which may degrade performance.
Template Adapter. Another variation for the Double-Checked Locking Optimization pattern is
applicable when the pattern is implemented in C++. In this case, create a template adapter
that transforms classes to have singleton-like behavior and performs the Double-Checked
Locking Optimization pattern automatically.
The Singleton template is parameterized by the TYPE that will be accessed as a
singleton. The Double-Checked Locking Optimization pattern is applied automatically on
the singleton_lock_ within the instance() method.
The Singleton template adapter can also be integrated with the Object Lifetime
Manager pattern [LGS99], which ensures that dynamically allocated singletons are
deallocated automatically when an application process exits. This pattern can also
ensure that the static singleton_lock_ data member is initialized properly before its
first use.
Pre-initialization of Singletons. This variation is an alternative that may alleviate the need for
Double-Checked Locking Optimization. It does this by initializing all objects explicitly at
program start-up, for example in a program's main() function. Thus, there are no race
conditions because the initialization is constrained to occur within a single thread.
This solution is inappropriate, however, when expensive calculations must be performed that
may be unnecessary in certain situations. For instance, if a singleton is never actually
created during program execution, initializing it during program start-up will simply waste
resources. Pre-initialization can also break encapsulation by forcing application components
with singletons in their implementation to expose this information so the singletons can be
initialized explicitly. Likewise, pre-initialization makes it hard to compose applications using
components that are configured dynamically using the Component Configurator pattern (75).
ACE [Sch97]. The Double-Checked Locking Optimization pattern is used extensively
throughout the ACE framework. To reduce code duplication, ACE defines a reusable adapter
template called ACE_Singleton that is similar to the one shown in the Variants section and
is used to transform 'normal' classes into singletons. Although singletons are not the only
use of the Double-Checked Locking Optimization pattern in the ACE framework, they are a
common example that demonstrates the utility of the pattern.
Sequent Dynix/PTX. The Doubled-Checked Locking Optimization pattern is used in the
Sequent Dynix/PTX operating system.
POSIX and Linux. The Double-Checked Locking Optimization pattern can be used to
implement POSIX 'once' variables [IEEE96], which ensure that functions are invoked just
once in a program. This pattern has been used in the LinuxThreads pthread_once()
implementation to ensure its function-pointer parameter init_routine() is called only
once¡ªon its first call¡ªand not subsequently.
Andrew Birrell describes the use of the Double-Checked Locking Optimization pattern in
[Bir91]. Birrell refers to the first check of the flag as a 'lock hint'.
The Solaris 2.x documentation for pthread_key_create(3T), which is shown in the
Thread-Specific Storage pattern (475) Known Uses section, illustrates how to use the
Double-Checked Locking Optimization to initialize thread-specific data.
There are two benefits of using the Double-Checked Locking Optimization pattern:
Minimized locking overhead. By performing two first-time-in flag checks, the Double Checked Locking Optimization pattern minimizes overhead for the common case. After the
flag is set the first check ensures that subsequent accesses require no further locking.
Prevents race conditions. The second check of the first-time-in flag ensures that the critical
section is executed just once.
However, there are three liabilities of using the Double-Checked Locking Optimization
pattern that can arise if the pattern is used in software that is ported to certain types of
operating system, hardware, or compiler/linker platforms. However, because this pattern is
applicable to a large class of platforms, we outline techniques for overcoming these
limitations.
Non-atomic pointer or integral assignment semantics. If an instance_ pointer is used as
the flag in a singleton implementation, all bits of the singleton instance_ pointer must be
read and written atomically in a single operation. If the write to memory after the call to new
is not atomic, other threads may try to read an invalid pointer. This can result in sporadic
illegal memory accesses.
These scenarios are possible on systems where memory addresses straddle word alignment
boundaries, such as 32-bit pointers used on a computer with a 16 bit word bus, which
requires two fetches from memory for each pointer access. In this case it may be necessary
to use a separate, word-aligned integral flag¡ªassuming that the hardware supports atomic
word-based reads and writes¡ªrather than using an instance_ pointer.
Multi-processor cache coherency. Certain multi-processor platforms, such as the COMPAQ
Alpha and Intel Itanium, perform aggressive memory caching optimizations in which read
and write operations can execute 'out of order' across multiple CPU caches. On these
platforms, it may not be possible to use the Double-Checked Locking Optimization pattern
without further modifications because CPU cache lines will not be flushed properly if shared
data is accessed without locks held.
To use the Double-Checked Locking Optimization pattern correctly on these types of
hardware platforms, CPU-specific instructions, such as memory barriers to flush cache lines,
must be inserted into the Double-Checked Locking Optimization implementation. Note that a
serendipitous side-effect of using the template adapter variation of the Double-Checked
Locking Optimization pattern is that it centralizes the placement of these CPU-specific cache
instructions.
For example, a memory barrier instruction can be located within the instance()
method of the Singleton template adapter class:
As long as the Singleton template adapter is used uniformly, it is straightforward to
localize the placement of CPU-specific code without affecting applications. Conversely, if
the Double-Checked Locking Optimization pattern is hand-crafted into singletons at each
point of use much more effort is required to add this CPU-specific code.
Unfortunately, the need for CPU-specific code in implementations of the Double-Checked
Locking Optimization pattern makes this pattern inapplicable for Java applications. Java's
bytecodes are designed to be cross-platform and therefore its JVMs lack a memory barrier
instruction that can resolve the problem outlined in this liability.
Additional mutex usage. Regardless of whether a singleton is allocated on-demand, some
type of lock, such as the Thread_Mutex used in our examples, is allocated and retained for
the lifetime of the program. One technique for minimizing this overhead is to pre-allocate a
singleton lock within an object manager [LGS99] and use this lock to serialize all singleton
initialization. Although this may increase lock contention, it may not affect program
performance because each singleton will most likely acquire and release the lock only once
when its initialized.
The Double-Checked Locking Optimization pattern is a thread-safe variant of the Lazy
Evaluation pattern [Mey98] [Beck97]. This pattern is often used in programming languages
such as C that lack constructors in order to ensure components are initialized before their
state is accessed.
The first time that push() is called, stack_ is 0, which triggers its implicit initialization
via malloc().
##%%&&
The Active Object design pattern decouples method execution from method invocation to
enhance concurrency and simplify synchronized access to objects that reside in their own
threads of control.
Consider the design of a communication gateway,[1] which decouples cooperating
components and allows them to interact without having direct dependencies on each other.
As shown below, the gateway may route messages from one or more supplier processes to
one or more consumer processes in a distributed system.
The suppliers, consumers, and gateway communicate using TCP [Ste93], which is a
connection-oriented protocol. The gateway may therefore encounter flow control from the
TCP transport layer when it tries to send data to a remote consumer. TCP uses flow control
to ensure that fast suppliers or gateways do not produce data more rapidly than slow
consumers or congested networks can buffer and process the data. To improve end-to-end
quality of service (QoS) for all suppliers and consumers, the entire gateway process must
not block while waiting for flow control to abate over any one connection to a consumer. In
addition the gateway must scale up efficiently as the number of suppliers and consumers
increase.
An effective way to prevent blocking and improve performance is to introduce concurrency
into the gateway design, for example by associating a different thread of control for each
TCP connection. This design enables threads whose TCP connections are flow controlled to
block without impeding the progress of threads whose connections are not flow controlled.
We thus need to determine how to program the gateway threads and how these threads
interact with supplier and consumer handlers.
Clients that access objects running in separate threads of control.
Many applications benefit from using concurrent objects to improve their quality of service,
for example by allowing an application to handle multiple client requests simultaneously.
Instead of using a single-threaded passive object, which executes its methods in the thread
of control of the client that invoked the methods, a concurrent object resides in its own
thread of control. If objects run concurrently, however, we must synchronize access to their
methods and data if these objects are shared and modified by multiple client threads, in
which case three forces arise:
??Processing-intensive methods invoked on an object concurrently should not block the
entire process indefinitely, thereby degrading the quality of service of other concurrent
objects.
?For example, if one outgoing TCP connection in our gateway example is blocked due
to flow control, the gateway process still should be able to run other threads that
can queue new messages while waiting for flow control to abate. Similarly, if other
outgoing TCP connections are not flow controlled, it should be possible for other
threads in the gateway to send messages to their consumers independently of any
blocked connections.
??Synchronized access to shared objects should be straightforward to program. In
particular, client method invocations on a shared object that are subject to
synchronization constraints should be serialized and scheduled transparently.
?Applications like our gateway can be hard to program if developers use low-level
synchronization mechanisms, such as acquiring and releasing mutual exclusion
(mutex) locks explicitly. Methods that are subject to synchronization constraints,
such as enqueueing and dequeueing messages from TCP connections, should be
serialized transparently when objects are accessed by multiple threads.
??Applications should be designed to leverage the parallelism available on a
hardware/software platform transparently.
?In our gateway example, messages destined for different consumers should be sent
concurrently by a gateway over different TCP connections. If the entire gateway is
programmed to only run in a single thread of control, however, performance
bottlenecks cannot be alleviated transparently by running the gateway on a multi processor platform.
For each object exposed to the forces above, decouple method invocation on the object from
method execution. Method invocation should occur in the client's thread of control, whereas
method execution should occur in a separate thread. Moreover, design the decoupling so
the client thread appears to invoke an ordinary method.
In detail: A proxy [POSA1] [GoF95] represents the interface of an active object and a servant
[OMG98a] provides the active object's implementation. Both the proxy and the servant run in
separate threads so that method invocations and method executions can run concurrently.
The proxy runs in the client thread, while the servant runs in a different thread.
At run-time the proxy transforms the client's method invocations into method requests, which
are stored in an activation list by a scheduler. The scheduler's event loop runs continuously
in the same thread as the servant, dequeueing method requests from the activation list and
dispatching them on the servant. Clients can obtain the result of a method's execution via a
future returned by the proxy.
An active object consists of six components:
A proxy [POSA1] [GoF95] provides an interface that allows clients to invoke publicly accessible methods on an active object. The use of a proxy permits applications to program
using standard strongly-typed language features, rather than passing loosely-typed
messages between threads. The proxy resides in the client's thread.
When a client invokes a method defined by the proxy it triggers the construction of a method
request object. A method request contains the context information, such as a method's
parameters, necessary to execute a specific method invocation and return any result to the
client. A method request class defines an interface for executing the methods of an active
object. This interface also contains guard methods that can be used to determine when a
method request can be executed. For every public method offered by a proxy that requires
synchronized access in the active object, the method request class is subclassed to create a
concrete method request class.
A proxy inserts the concrete method request it creates into an activation list. This list
maintains a bounded buffer of pending method requests created by the proxy and keeps
track of which method requests can execute. The activation list decouples the client thread
where the proxy resides from the thread where the servant method is executed, so the two
threads can run concurrently. The internal state of the activation list must therefore be
serialized to protect it from concurrent access.
A scheduler runs in a different thread than its client proxies, namely in the active object's
thread. It decides which method request to execute next on an active object. This scheduling
decision is based on various criteria, such as ordering¡ªthe order in which methods are
called on the active object¡ªor certain properties of an active object, such as its state. A
scheduler can evaluate these properties using the method requests' guards, which
determine when it is possible to execute the method request [Lea99a]. A scheduler uses an
activation list to manage method requests that are pending execution. Method requests are
inserted in an activation list by a proxy when clients invoke one of its methods.
A servant defines the behavior and state that is modeled as an active object. The methods a
servant implements correspond to the interface of the proxy and method requests the proxy
creates. It may also contain other predicate methods that method requests can use to
implement their guards. A servant method is invoked when its associated method request is
executed by a scheduler. Thus, it executes in its scheduler's thread.
When a client invokes a method on a proxy it receives a future [Hal85] [LS88]. This future
allows the client to obtain the result of the method invocation after the servant finishes
executing the method. Each future reserves space for the invoked method to store its result.
When a client wants to obtain this result, it can rendezvous with the future, either blocking or
polling until the result is computed and stored into the future.
The class diagram for the Active Object pattern is shown below:
The behavior of the Active Object pattern can be divided into three phases:
??Method request construction and scheduling. A client invokes a method on the proxy.
This triggers the creation of a method request, which maintains the argument bindings
to the method as well as any other bindings required to execute the method and return
its result. The proxy then passes the method request to its scheduler, which enqueues it
on the activation list. If the method is defined as a two-way invocation [OMG98a], a
future is returned to the client. No future is returned if a method is a one-way, which
means it has no return values.
??Method request execution. The active object's scheduler runs continuously in a
different thread than its clients. The scheduler monitors its activation list and determines
which method request(s) have become runnable by calling their guard method. When a
method request becomes runnable the scheduler removes it, binds the request to its
servant, and dispatches the appropriate method on the servant. When this method is
called, it can access and update the state of its servant and create its result if it is a two way method invocation.
??Completion. In this phase the result, if any, is stored in the future and the active
object's scheduler returns to monitor the activation list for runnable method requests.
After a two-way method completes, clients can retrieve its result via the future. In
general, any clients that rendezvous with the future can obtain its result. The method
request and future can be deleted explicitly or garbage collected when they are no
longer referenced.
Five activities show how to implement the Active Object pattern.
1. Implement the servant. A servant defines the behavior and state being modeled as an
active object. In addition, a servant may contain predicate methods used to determine
when to execute method requests.
2. For each remote consumer in our gateway example there is a consumer handler
containing a TCP connection to a consumer process running on a remote
machine. Each consumer handler contains a message queue modeled as an
active object and implemented with an MQ_Servant. This active object stores
messages passed from suppliers to the gateway while they are waiting to be sent
to the remote consumer.[2] The following C++ class illustrates the MQ_Servant
class:
The put() and get() methods implement the message insertion and removal
operations on the queue, respectively. The servant defines two predicates,
empty() and full(), that distinguish three internal states: empty, full, and
neither empty nor full. These predicates are used to determine when put() and
get() methods can be called on the servant.
23. In general, the synchronization mechanisms that protect a servant's critical sections
from concurrent access should not be tightly coupled with the servant, which should
just implement application functionality. Instead, the synchronization mechanisms
should be associated with the method requests. This design avoids the inheritance
anomaly problem [MWY91], which inhibits the reuse of servant implementations if
subclasses require different synchronization policies than base classes. Thus, a
change to the synchronization constraints of the active object need not affect its
servant implementation.
24. The MQ_Servant class is designed to omit synchronization mechanisms from a
servant. The method implementations in the MQ_Servant class, which are
omitted for brevity, therefore need not contain any synchronization mechanisms.
26. Implement the invocation infrastructure. In this activity, we describe the infrastructure
necessary for clients to invoke methods on an active object. This infrastructure
consists of a proxy that creates method requests, which can be implemented via two
sub-activities.
1. Implement the proxy. The proxy provides clients with an interface to the
servant's methods. For each method invocation by a client, the proxy creates
a concrete method request. Each method request is an abstraction for the
method's context, which is also called the closure of the method. Typically, this
context includes the method parameters, a binding to the servant the method
will be applied to, a future for the result, and the code that executes the
method request.
2. In our gateway the MQ_Proxy provides the following interface to the
MQ_Servant defined in implementation activity 1 (375):
The MP_Proxy is a factory [GoF95] that constructs instances of method
requests and passes them to a scheduler, which queues them for
subsequent execution in a separate thread.
34. Multiple client threads in a process can share the same proxy. A proxy method
need not be serialized because it does not change state after it is created. Its
scheduler and activation list are responsible for any necessary internal
serialization.
35. Our gateway example contains many supplier handlers that receive and
route messages to peers via many consumer handlers. Several supplier
handlers can invoke methods using the proxy that belongs to a single
consumer handler without the need for any explicit synchronization.
37. Implement the method requests. Method requests can be considered as
command objects [GoF95]. A method request class declares an interface used
by all concrete method requests. It provides schedulers with a uniform
interface that allows them to be decoupled from specific knowledge about how
to evaluate synchronization constraints or trigger the execution of concrete
method requests. Typically, this interface declares a can_run() method that
defines a hook method guard that checks when it is possible to execute the
method request. It also declares a call() method that defines a hook for
executing a method request on the servant.
The methods in a method request class must be defined by subclasses. There
should be one concrete method request class for each method defined in the
proxy. The can_run() method is often implemented with the help of the
servant's predicates.
In our gateway example a Method_Request base class defines two
virtual hook methods, which we call can_run() and call():
We then define two subclasses of Method_Request: class Put
corresponds to the put() method call on a proxy and class Get
corresponds to the get() method call. Both classes contain a pointer to
the MQ_Servant. The Get class can be implemented as follows:
Note how the can_run() method uses the MQ_Servant's empty()
predicate to allow a scheduler to determine when the Get method request
can execute. When the method request does execute, the active object's
scheduler invokes its call() hook method. This call() hook uses the
Get method request's run-time binding to MQ_Servant to invoke the
servant's get() method, which is executed in the context of that servant.
It does not require any explicit serialization mechanisms, however,
because the active object's scheduler enforces all the necessary
synchronization constraints via the method request can_run() methods.
The proxy passes a future to the constructors of the corresponding method
request classes for each of its public two-way methods in the proxy that
returns a value, such as the get() method in our gateway example. This
future is returned to the client thread that calls the method, as discussed in
implementation activity 5 (384).
27. Implement the activation list. Each method request is inserted into an activation list.
This list can be implemented as a synchronized bounded buffer that is shared
between the client threads and the thread in which the active object's scheduler and
servant run. An activation list can also provide a robust iterator [Kof93] [CarEl95] that
allows its scheduler to traverse and remove its elements.
The activation list is often designed using concurrency control patterns, such as
Monitor Object (399), that use common synchronization mechanisms like condition
variables and mutexes [Ste98]. When these are used in conjunction with a timer
mechanism, a scheduler thread can determine how long to wait for certain operations
to complete. For example, timed waits can be used to bound the time spent trying to
remove a method request from an empty activation list or to insert into a full activation
list.[3] If the timeout expires, control returns to the calling thread and the method
request is not executed.
For our gateway example we specify a class Activation_List as follows:
The insert() and remove() methods provide a 'bounded-buffer'
producer/consumer [Grand98] synchronization model. This design allows a
scheduler thread and multiple client threads to remove and insert
Method_Requests simultaneously without corrupting the internal state of an
Activation_List. Client threads play the role of producers and insert
Method_Requests via a proxy. A scheduler thread plays the role of a consumer.
It removes Method_Requests from the Activation_List when their guards
evaluate to 'true'. It then invokes their call() hooks to execute servant
methods.
28. Implement the active object's scheduler. A scheduler is a command processor
[POSA1] that manages the activation list and executes pending method requests
whose synchronization constraints have been met. The public interface of a
scheduler often provides one method for the proxy to insert method requests into the
activation list and another method that dispatches method requests to the servant.
29. We define the following MQ_Scheduler class for our gateway:
A scheduler executes its dispatch() method in a different thread of control than its
client threads. Each client thread uses a proxy to insert method requests in an active
object scheduler's activation list. This scheduler monitors the activation list in its own
thread, selecting a method request whose guard evaluates to 'true,' that is, whose
synchronization constraints are met. This method request is then removed from the
activation list and executed by invoking its call() hook method.
58. In our gateway example the constructor of MQ_Scheduler initializes the
Activation_List and uses the Thread_Manager wrapper facade (47) to
spawn a new thread of control:
The Thread_Manager::spawn() method is passed a pointer to a static
MQ_Scheduler::svc_run() method and a pointer to the MQ_Scheduler
object. The svc_run() static method is the entry point into a newly created
thread of control, which runs the svc_run() method. This method is simply an
adapter [GoF95] that calls the MQ_Scheduler::dispatch() method on the
this parameter:
The dispatch() method determines the order in which Put and Get method
requests are processed based on the underlying MQ_Servant predicates
empty() and full(). These predicates reflect the state of the servant, such as
whether the message queue is empty, full, or neither.
72. By evaluating these predicate constraints via the method request can_run()
methods, a scheduler can ensure fair access to the MQ_Servant:
In our example the MQ_Scheduler::dispatch() implementation iterates
continuously, executing the next method request whose can_run() method
evaluates to true. Scheduler implementations can be more sophisticated, however,
and may contain variables that represent the servant's synchronization state.
99. For example, to implement a multiple-readers/single-writer synchronization policy a
prospective writer will call 'write' on the proxy, passing the data to write. Similarly,
readers will call 'read' and obtain a future as their return value. The active object's
scheduler maintains several counter variables that keep track of the synchronization
state, such as the number of read and write requests. The scheduler also maintains
knowledge about the identity of the prospective writers.
100. The active object's scheduler can use these synchronization state counters to
determine when a single writer can proceed, that is, when the current number of
readers is zero and no write request from a different writer is currently pending
execution. When such a write request arrives, a scheduler may choose to dispatch
the writer to ensure fairness. In contrast, when read requests arrive and the servant
can satisfy them because it is not empty, its scheduler can block all writing activity
and dispatch read requests first.
101. The synchronization state counter variable values described above are
independent of the servant's state because they are only used by its scheduler to
enforce the correct synchronization policy on behalf of the servant. The servant
focuses solely on its task to temporarily store client-specific application data. In
contrast, its scheduler focuses on coordinating multiple readers and writers. This
design enhances modularity and reusability.
102. A scheduler can support multiple synchronization policies by using the
Strategy pattern [GoF95]. Each synchronization policy is encapsulated in a separate
strategy class. The scheduler, which plays the context role in the Strategy pattern, is
then configured with a particular synchronization strategy it uses to execute all
subsequent scheduling decisions.
103. Determine rendezvous and return value policy. The rendezvous policy
determines how clients obtain return values from methods invoked on active objects.
A rendezvous occurs when an active object servant executing in one thread passes a
return value to the client that invoke the method running in another thread.
Implementations of the Active Object pattern often choose from the following
rendezvous and return value policies:
??Synchronous waiting. Block the client thread synchronously in the proxy until
the scheduler dispatches the method request and the result is computed and
stored in the future.
??Synchronous timed wait. Block for a bounded amount of time and fail if the
active object's scheduler does not dispatch the method request within that time
period. If the timeout is zero the client thread 'polls', that is, it returns to the caller
without queueing the method request if its scheduler cannot dispatch it
immediately.
??Asynchronous. Queue the method call and return control to the client
immediately. If the method is a two-way invocation that produces a result then
some form of future must be used to provide synchronized access to the value,
or to the error status if the method call fails.
The future construct allows two-way asynchronous invocations [ARSK00] that return
a value to the client. When a servant completes the method execution, it acquires a
write lock on the future and updates the future with its result. Any client threads that
are blocked waiting for the result are awakened and can access the result
concurrently. A future can be garbage-collected after the writer and all readers
threads no longer reference it. In languages like C++, which do not support garbage
collection, futures can be reclaimed when they are no longer in use via idioms like
Counted Pointer [POSA1].
In our gateway example the get() method invoked on the MQ_Proxy ultimately
results in the Get::call() method being dispatched by the MQ_Scheduler, as
shown in implementation activity 2 (378). The MQ_Proxy::get() method
returns a value, therefore a Message_Future is returned to the client that calls
it:
The Message_Future is implemented using the Counted Pointer idiom
[POSA1]. This idiom simplifies memory management for dynamically allocated
C++ objects by using a reference counted
Message_Future_Implementation body that is accessed solely through the
Message_Future handle.
In general a client may choose to evaluate the result value from a future immediately,
in which case the client blocks until the scheduler executes the method request.
Conversely, the evaluation of a return result from a method invocation on an active
object can be deferred. In this case the client thread and the thread executing the
method can both proceed asynchronously.
In our gateway example a consumer handler running in a separate thread may
choose to block until new messages arrive from suppliers:
Conversely, if messages are not available immediately, a consumer handler can
store the Message_Future return value from message_queue and perform
other 'book-keeping' tasks, such as exchanging keep-alive messages to ensure
its consumer is still active. When the consumer handler is finished with these
tasks, it can block until a message arrives from suppliers:
In our gateway example, the gateway's supplier and consumer handlers are local proxies
[POSA1] [GoF95] for remote suppliers and consumers, respectively. Supplier handlers
receive messages from remote suppliers and inspect address fields in the messages. The
address is used as a key into a routing table that identifies which remote consumer will
receive the message.
The routing table maintains a map of consumer handlers, each of which is responsible for
delivering messages to its remote consumer over a separate TCP connection. To handle
flow control over various TCP connections, each consumer handler contains a message
queue implemented using the Active Object pattern. This design decouples supplier and
consumer handlers so that they can run concurrently and block independently.
The Consumer_Handler class is defined as follows:
Supplier_Handlers running in their own threads can put messages in the appropriate
Consumer_Handler's message queue active object:
To process the messages inserted into its queue, each Consumer_Handler uses the
Thread_Manager wrapper facade (47) to spawn a separate thread of control in its
constructor:
This new thread executes the svc_run() method entry point, which gets the messages
placed into the queue by supplier handler threads, and sends them to the consumer over the
TCP connection:
Every Consumer_Handler object uses the message queue that is implemented as an
active object and runs in its own thread. Therefore its send() operation can block without
affecting the quality of service of other Consumer_Handler objects.
Multiple Roles. If an active object implements multiple roles, each used by particular types of
client, a separate proxy can be introduced for each role. By using the Extension Interface
pattern (141), clients can obtain the proxies they need. This design helps separate concerns
because a client only sees the particular methods of an active object it needs for its own
operation, which further simplifies an active object's evolution. For example, new services
can be added to the active object by providing new extension interface proxies without
changing existing ones. Clients that do not need access to the new services are unaffected
by the extension and need not even be recompiled.
Integrated Scheduler. To reduce the number of components needed to implement the Active
Object pattern, the roles of the proxy and servant can be integrated into its scheduler
component. Likewise, the transformation of a method call on a proxy into a method request
can also be integrated into the scheduler. However, servants still execute in a different
thread than proxies.
Here is an implementation of the message queue using an integrated scheduler:
By centralizing the point at which method requests are generated, the Active Object pattern
implementation can be simplified because it has fewer components. The drawback, of
course, is that a scheduler must know the type of the servant and proxy, which makes it hard
to reuse the same scheduler for different types of active objects.
Message Passing. A further refinement of the integrated scheduler variant is to remove the
proxy and servant altogether and use direct message passing between the client thread and
the active object's scheduler thread.
For example, consider the following scheduler implementation:
In this variant, there is no proxy, so clients create an appropriate type of message
request directly and call insert() themselves, which enqueues the request into the
activation list. Likewise, there is no servant, so the dispatch() method running in a
scheduler's thread simply dequeues the next message request and processes the
request according to its type.
In general it is easier to develop a message-passing mechanism than it is to develop an
active object because there are fewer components. Message passing can be more tedious
and error-prone, however, because application developers, not active object developers,
must program the proxy and servant logic. As a result, message passing implementations
are less type-safe than active object implementations because their interfaces are implicit
rather than explicit. In addition, it is harder for application developers to distribute clients and
servers via message passing because there is no proxy to encapsulate the marshaling and
demarshaling of data.
Polymorphic Futures [LK95]. A polymorphic future allows parameterization of the eventual
result type represented by the future and enforces the necessary synchronization. In
particular, a polymorphic future describes a typed future that client threads can use to
retrieve a method request's result. Whether a client blocks on a future depends on whether
or not a result has been computed.
The following class is a polymorphic future template for C++:
Timed method invocations. The activation list illustrated in implementation activity 3 (379)
defines a mechanism that can bound the amount of time a scheduler waits to insert or
remove a method request. Although the examples we showed earlier in the pattern do not
use this feature, many applications can benefit from timed method invocations. To
implement this feature we can simply export the timeout mechanism via schedulers and
proxies.
In our gateway example, the MQ_Proxy can be modified so that its methods allow
clients to bound the amount of time they are willing to wait to execute:
If timeout is 0 both get() and put() will block indefinitely until Message is either
removed from or inserted into the scheduler's activation list, respectively. If timeout
expires, the System_Ex exception defined in the Wrapper Facade pattern (47) is thrown
with a status() value of ETIMEDOUT and the client must catch it.
To complete our support for timed method invocations, we also must add timeout
support to the MQ_Scheduler:
Distributed Active Object. In this variant a distribution boundary exists between a proxy and
a scheduler, rather than just a threading boundary. This pattern variant introduces two new
participants:
??A client-side proxy plays the role of a stub, which marshals method parameters into a
method request that is sent across a network and executed by a servant in a separate
server address space.
??A server-side skeleton, which demarshals method request parameters before they are
passed to a server's servant method.
The Distributed Active Object pattern variant is therefore similar to the Broker pattern
[POSA1]. The primary difference is that a Broker usually coordinates the processing of many
objects, whereas a distributed active object just handles a single object.
Thread Pool Active Object. This generalization of the Active Object pattern supports multiple
servant threads per active object to increase throughput and responsiveness. When not
processing requests, each servant thread in a thread pool active object blocks on a single
activation list. The active object scheduler assigns a new method request to an available
servant thread in the pool as soon as one is ready to be executed.
A single servant implementation is shared by all the servant threads in the pool. This design
cannot therefore be used if the servant methods do not protect their internal state via some
type of synchronization mechanism, such as a mutex.
Additional variants of active objects can be found in [Lea99a], Chapter 5: Concurrency
Control and Chapter 6: Services in Threads.
ACE Framework [Sch97]. Reusable implementations of the method request, activation list,
and future components in the Active Object pattern are provided in the ACE framework. The
corresponding classes in ACE are called ACE_Method_Request,
ACE_Activation_Queue, and ACE_Future. These components have been used to
implement many production concurrent and networked systems [Sch96].
Siemens MedCom. The Active Object pattern is used in the Siemens MedCom framework,
which provides a black-box component-based framework for electronic medical imaging
systems. MedCom employs the Active Object pattern in conjunction with the Command
Processor pattern [POSA1] to simplify client windowing applications that access patient
information on various medical servers [JWS98].
Siemens FlexRouting - Automatic Call Distribution [Flex98]. This call center
management system uses the Thread Pool variant of the Active Object pattern. Services that
a call center offers are implemented as applications of their own. For example, there may be
a hot-line application, an ordering application, and a product information application,
depending on the types of service offered. These applications support operator personnel
that serve various customer requests. Each instance of these applications is a separate
servant component. A 'FlexRouter' component, which corresponds to the scheduler,
dispatches incoming customer requests automatically to operator applications that can
service these requests.
Java JDK 1.3 introduced a mechanism for executing timer-based tasks concurrently in the
classes java.util.Timer and java.util.TimerTask. Whenever the scheduled
execution time of a task occurs it is executed. Specifically, Timer offers different scheduling
functions to clients that allow them to specify when and how often a task should be
executed. One-shot tasks are straightforward and recurring tasks can be scheduled at
periodic intervals. The scheduling calls are executed in the client's thread, while the tasks
themselves are executed in a thread owned by the Timer object. A Timer internal task
queue is protected by locks because the two threads outlined above operate on it
concurrently.
The task queue is implemented as a priority queue so that the next TimerTask to expire
can be identified efficiently. The timer thread simply waits until this expiration. There are no
explicit guard methods and predicates because determining when a task is 'ready for
execution' simply depends on the arrival of the scheduled time.
Tasks are implemented as subclasses of TimerTask that override its run() hook method.
The TimerTask subclasses unify the concepts behind method requests and servants by
offering just one class and one interface method via TimerTask.run().
The scheme described above simplifies the Active Object machinery for the purpose of
timed execution. There is no proxy and clients call the scheduler¡ªthe Timer object¡ª
directly. Clients do not invoke an ordinary method and therefore the concurrency is not
transparent. Moreover, there are no return value or future objects linked to the run()
method. An application can employ several active objects by constructing several Timer
objects, each with its own thread and task queue.
Chef in a restaurant. A real-life example of the Active Object pattern is found in restaurants.
Waiters and waitresses drop off customer food requests with the chef and continue to
service requests from other customers asynchronously while the food is being prepared. The
chef keeps track of the customer food requests via some type of worklist. However, the chef
may cook the food requests in a different order than they arrived to use available resources,
such as stove tops, pots, or pans, most efficiently. When the food is cooked, the chef places
the results on top of a counter along with the original request so the waiters and waitresses
can rendezvous to pick up the food and serve their customers.
The Active Object pattern provides the following benefits:
Enhances application concurrency and simplifies synchronization complexity. Concurrency is
enhanced by allowing client threads and asynchronous method executions to run
simultaneously. Synchronization complexity is simplified by using a scheduler that evaluates
synchronization constraints to guarantee serialized access to servants, in accordance with
their state.
Transparently leverages available parallelism. If the hardware and software platforms
support multiple CPUs efficiently, this pattern can allow multiple active objects to execute in
parallel, subject only to their synchronization constraints.
Method execution order can differ from method invocation order. Methods invoked
asynchronously are executed according to the synchronization constraints defined by their
guards and by scheduling policies. Thus, the order of method execution can differ from the
order of method invocation order. This decoupling can help improve application performance
and flexibility.
However, the Active Object pattern encounters several liabilities:
Performance overhead. Depending on how an active object's scheduler is implemented¡ªfor
example in user-space versus kernel-space [SchSu95]¡ªcontext switching, synchronization,
and data movement overhead may occur when scheduling and executing active object
method invocations. In general the Active Object pattern is most applicable for relatively
coarse-grained objects. In contrast, if the objects are fine-grained, the performance
overhead of active objects can be excessive, compared with related concurrency patterns,
such as Monitor Object (399).
Complicated debugging. It is hard to debug programs that use the Active Object pattern due
to the concurrency and non-determinism of the various active object schedulers and the
underlying operating system thread scheduler. In particular, method request guards
determine the order of execution. However, the behavior of these guards may be hard to
understand and debug. Improperly defined guards can cause starvation, which is a condition
where certain method requests never execute. In addition, program debuggers may not
support multi-threaded applications adequately.
The Monitor Object pattern (399) ensures that only one method at a time executes within a
thread-safe passive object, regardless of the number of threads that invoke the object's
methods concurrently. In general, monitor objects are more efficient than active objects
because they incur less context switching and data movement overhead. However, it is
harder to add a distribution boundary between client and server threads using the Monitor
Object pattern.
It is instructive to compare the Active Object pattern solution in the Example Resolved
section with the solution presented in the Monitor Object pattern. Both solutions have similar
overall application architectures. In particular, the Supplier_Handler and
Consumer_Handler implementations are almost identical.
The primary difference is that the Message_Queue in the Active Object pattern supports
sophisticated method request queueing and scheduling strategies. Similarly, because active
objects execute in different threads than their clients, there are situations where active
objects can improve overall application concurrency by executing multiple operations
asynchronously. When these operations complete, clients can obtain their results via futures
[Ha185] [LS88].
On the other hand, the Message_Queue itself is easier to program and often more efficient
when implemented using the Monitor Object pattern than the Active Object pattern.
The Reactor pattern (179) is responsible for demultiplexing and dispatching multiple event
handlers that are triggered when it is possible to initiate an operation without blocking. This
pattern is often used in lieu of the Active Object pattern to schedule callback operations to
passive objects. Active Object also can be used in conjunction with the Reactor pattern to
form the Half-Sync/Half-Async pattern (423).
The Half-Sync/Half-Async pattern (423) decouples synchronous I/O from asynchronous I/O
in a system to simplify concurrent programming effort without degrading execution efficiency.
Variants of this pattern use the Active Object pattern to implement its synchronous task
layer, the Reactor pattern (179) to implement the asynchronous task layer, and a Producer Consumer pattern [Lea99a], such as a variant of the Pipes and Filters pattern [POSA1] or
the Monitor Object pattern (399), to implement the queueing layer.
The Command Processor pattern [POSA1] separates issuing requests from their execution.
A command processor, which corresponds to the Active Object pattern's scheduler,
maintains pending service requests that are implemented as commands [GoF95],
Commands are executed on suppliers, which correspond to servants. The Command
Processor pattern does not focus on concurrency, however. In fact, clients, the command
processor, and suppliers often reside in the same thread of control. Likewise, there are no
proxies that represent the servants to clients. Clients create commands and pass them
directly to the command processor.
The Broker pattern [POSA1] defines many of the same components as the Active Object
pattern. In particular, clients access brokers via proxies and servers implement remote
objects via servants. One difference between Broker and Active Object is that there is a
distribution boundary between proxies and servants in the Broker pattern, as opposed to a
threading boundary between proxies and servants in the Active Object pattern. Another
difference is that active objects typically have just one servant, whereas a broker can have
many servants.
##%%&&
The Monitor Object design pattern synchronizes concurrent method execution to ensure that
only one method at a time runs within an object. It also allows an object's methods to
cooperatively schedule their execution sequences.
Let us reconsider the design of the communication gateway described in the Active Object
pattern (369).[4]
The gateway process is a mediator [GoF95] that contains multiple supplier and consumer
handler objects. These objects run in separate threads and route messages from one or
more remote suppliers to one or more remote consumers. When a supplier handler thread
receives a message from a remote supplier, it uses an address field in the message to
determine the corresponding consumer handler. The handler's thread then delivers the
message to its remote consumer.
When suppliers and consumers reside on separate hosts, the gateway uses a connection oriented protocol, such as TCP [Ste93], to provide reliable message delivery and end-to-end
flow control. Flow control is a protocol mechanism that blocks senders when they produce
messages more rapidly than receivers can process them. The entire gateway should not
block while waiting for flow control to abate on outgoing TCP connections, however. In
particular, incoming TCP connections should continue to be processed and messages
should continue to be sent over any non-flow-controlled TCP connections.
To minimize blocking, each consumer handler can contain a thread-safe message queue.
Each queue buffers new routing messages it receives from its supplier handler threads. This
design decouples supplier handler threads in the gateway process from consumer handler
threads, so that all threads can run concurrently and block independently when flow control
occurs on various TCP connections.
One way to implement a thread-safe message queue is to apply the Active Object pattern
(369) to decouple the thread used to invoke a method from the thread used to execute the
method. Active Object may be inappropriate, however, if the entire infrastructure introduced
by this pattern is unnecessary. For example, a message queue's enqueue and dequeue
methods may not require sophisticated scheduling strategies. In this case, Implementing the
Active Object pattern's method request, scheduler and activation list participants incurs
unnecessary performance overhead, and programming effort.
Instead, the implementation of the thread-safe message queue must be efficient to avoid
degrading performance unnecessarily. To avoid tight coupling of supplier and consumer
handler implementations, the mechanism should also be transparent to implementors of
supplier handlers. Varying either implementation independently would otherwise become
prohibitively complex.
Multiple threads of control accessing the same object concurrently.
Many applications contain objects whose methods are invoked concurrently by multiple
client threads. These methods often modify the state of their objects. For such concurrent
applications to execute correctly, therefore, it is necessary to synchronize and schedule
access to the objects.
In the presence of this problem four forces must be addressed:
??To separate concerns and protect object state from uncontrolled changes, object oriented programmers are accustomed to accessing objects only through their interface
methods. It is relatively straightforward to extend this object-oriented programming
model to protect an object's data from uncontrolled concurrent changes, known as race
conditions. An object's interface methods should therefore define its synchronization
boundaries, and only one method at a time should be active within the same object.
??Concurrent applications are harder to program if clients must explicitly acquire and
release low-level synchronization mechanisms, such as semaphores, mutexes, or
condition variables [IEEE96]. Objects should therefore be responsible for ensuring that
any of their methods that require synchronization are serialized transparently, without
requiring explicit client intervention.
??If an object's methods must block during their execution, they should be able to
relinquish their thread of control voluntarily, so that methods called from other client
threads can access the object. This property helps prevent deadlock and makes it
possible to take advantage of concurrency mechanisms available on hardware and
software platforms.
??When a method relinquishes its thread of control voluntarily, it must leave its object in a
stable state, that is, object-specific invariants must hold. Similarly, a method must
resume its execution within an object only when the object is in a stable state.
Synchronize the access to an object's methods so that only one method can execute at any
one time.
In detail: for each object accessed concurrently by multiple client threads, define it as a
monitor object. Clients can access the functions defined by a monitor object only through its
synchronized methods. To prevent race conditions on its internal state, only one
synchronized method at a time can run within a monitor object. To serialize concurrent
access to an object's state, each monitor object contains a monitor lock. Synchronized
methods can determine the circumstances under which they suspend and resume their
execution, based on one or more monitor conditions associated with a monitor object.
There are four participants in the Monitor Object pattern:
A monitor object exports one or more methods. To protect the internal state of the monitor
object from uncontrolled changes and race conditions, all clients must access the monitor
object only through these methods. Each method executes in the thread of the client that
invokes it, because a monitor object does not have its own thread of control.[5]
Synchronized methods implement the thread-safe functions exported by a monitor object. To
prevent race conditions, only one synchronized method can execute within a monitor object
at any one time. This rule applies regardless of the number of threads that invoke the
object's synchronized methods concurrently, or the number of synchronized methods in the
object's class.
A consumer handler's message queue in the gateway application can be implemented
as a monitor object by converting its put() and get() operations into synchronized
methods. This design ensures that routing messages can be inserted and removed
concurrently by multiple threads without corrupting the queue's internal state.
Each monitor object contains its own monitor lock. Synchronized methods use this lock to
serialize method invocations on a per-object basis. Each synchronized method must acquire
and release an object's monitor lock when entering or exiting the object. This protocol
ensures the monitor lock is held whenever a synchronized method performs operations that
access or modify the state of its object.
Monitor condition. Multiple synchronized methods running in separate threads can schedule
their execution sequences cooperatively by waiting for and notifying each other via monitor
conditions associated with their monitor object. Synchronized methods use their monitor lock
in conjunction with their monitor condition(s) to determine the circumstances under which
they should suspend or resume their processing.
In the gateway application a POSIX mutex [IEEE96] can be used to implement the
message queue's monitor lock. A pair of POSIX condition variables can be used to
implement the message queue's not-empty and not-full monitor conditions:
?When a consumer handler thread attempts to dequeue a routing message from an
empty message queue, the queue's get() method must atomically release the
monitor lock and suspend itself on the not-empty monitor condition. It remains
suspended until the queue is no longer empty, which happens when a supplier
handler thread inserts a message into the queue.
?When a supplier handler thread attempts to enqueue a message into a full queue, the
queue's put() method must atomically release the monitor lock and suspend itself
on the not-full monitor condition. It remains suspended until the queue is no longer
full, which happens when a consumer handler removes a message from the
message queue.
Note that the not-empty and not-full monitor conditions both share the same monitor
lock.
The structure of the Monitor Object pattern is illustrated in the following class diagram:
The collaborations between participants in the Monitor Object pattern divide into four
phases:
??Synchronized method invocation and serialization. When client thread T1 invokes a
synchronized method on a monitor object, the method must first acquire the object's
monitor lock. A monitor lock cannot be acquired as long as another synchronized
method in thread T2 is executing within the monitor object. In this case, client thread T1
will block until the synchronized method acquires the lock. Once the synchronized
method called by T1 has finished executing, the monitor lock is released so that other
synchronized methods called by other threads can access the monitor object.
??Synchronized method thread suspension. If a synchronized method must block or
cannot otherwise make immediate progress, it can wait on one of its monitor conditions.
This causes it to 'leave' the monitor object temporarily [Hoare74]. The monitor object
implementation is responsible for ensuring that it is in a stable state before switching to
another thread. When a synchronized method leaves the monitor object, the client's
thread is suspended on that monitor condition and the monitor lock is released
atomically by the operating system's thread scheduler. Another synchronized method in
another thread can now execute within the monitor object.
??Monitor condition notification. A synchronized method can notify a monitor condition.
This operation awakens the thread of a synchronized method that had suspended itself
on the monitor condition earlier. A synchronized method can also notify all other
synchronized methods that suspended their threads earlier on a monitor condition. In
this case all the threads are awakened and one of them at a time can acquire the
monitor lock and run within the monitor object.
??Synchronized method thread resumption. Once a suspended synchronized method
thread is notified, its execution can resume at the point where it waited on the monitor
condition. The operating system thread scheduler performs this resumption implicitly.
The monitor lock is reacquired atomically before the notified thread 're-enters' the
monitor object and resumes its execution in the synchronized method.
Four activities illustrate how to implement the Monitor Object pattern.
1. Define the monitor object's interface methods. The interface of a monitor object exports
a set of methods to clients. Interface methods are often synchronized, that is, only
one of them at a time can be executed by a thread within a particular monitor object.
2. In our gateway example, each consumer handler contains a message queue and
a TCP connection. The message queue can be implemented as a monitor object
that buffers messages it receives from supplier handler threads. This buffering
helps prevent the entire gateway process from blocking whenever consumer
handler threads encounter flow control on TCP connections to their remote
consumers. The following C++ class defines the interface for our message queue
monitor object:
The Message_Queue monitor object interface exports four synchronized
methods. The empty() and full() methods are predicates that clients can use
to distinguish three internal queue states: empty, full, and neither empty nor full.
The put() and get() methods enqueue and dequeue messages into and from
the queue, respectively, and will block if the queue is full or empty.
32. Define the monitor object's implementation methods. A monitor object often contains
internal implementation methods that synchronized interface methods use to perform
the object's functionality. This design helps decouple the core monitor object
functionality from its synchronization and scheduling logic. It also helps avoid intra object deadlock and unnecessary locking overhead.
Two conventions, based on the Thread-Safe Interface pattern (345), can be used to
structure the separation of concerns between interface and implementation methods
in a monitor object:
??Interface methods only acquire and release monitor locks and wait upon or
notify certain monitor conditions. They otherwise forward control to
implementation methods that perform the monitor object's functionality.
??Implementation methods only perform work when called by interface methods.
They do not acquire and release the monitor lock, nor do they wait upon or notify
monitor conditions explicitly.
Similarly, in accordance with the Thread-Safe Interface pattern, implementation
methods should not call any synchronized methods defined in the class interface.
This restriction helps to avoid intra-object method deadlock or unnecessary
synchronization overhead.
In our gateway, the Message_Queue class defines four implementation methods:
Implementation methods are often non-synchronized. They must be careful when
invoking blocking calls, because the interface method that called the implementation
method may have acquired the monitor lock. A blocking thread that owned a lock
could therefore delay overall program progress indefinitely.
33. Define the monitor object's internal state and synchronization mechanisms. A monitor
object contains data members that define its internal state. This state must be
protected from corruption by race conditions resulting from unsynchronized
concurrent access. A monitor object therefore contains a monitor lock that serializes
the execution of its synchronized methods, as well as one or more monitor conditions
used to schedule the execution of synchronized method within a monitor object.
Typically there is a separate monitor condition for each of the following situations:
??Cases in which synchronized methods must suspend their processing to wait
for the occurrence of some event of state change; or
??Cases in which synchronized methods must resume other threads whose
synchronized methods have suspended themselves on the monitor condition.
A monitor object method implementation is responsible for ensuring that it is in a
stable state before releasing its lock. Stable states can be described by invariants,
such as the need for all elements in a message queue to be linked together via valid
pointers. The invariant must hold whenever a monitor object method waits on the
corresponding condition variable.
Similarly, when the monitor object is notified and the operating system thread
scheduler decides to resume its thread, the monitor object method implementation is
responsible for ensuring that the invariant is indeed satisfied before proceeding. This
check is necessary because other threads may have changed the state of the object
between the notification and the resumption. A a result, the monitor object must
ensure that the invariant is satisfied before allowing a synchronized method to
resume its execution.
A monitor lock can be implemented using a mutex. A mutex makes collaborating
threads wait while the thread holding the mutex executes code in a critical section.
Monitor conditions can be implemented using condition variables [IEEE96]. A
condition variable can be used by a thread to make itself wait until a particular event
occurs or an arbitrarily complex condition expression attains a particular stable state.
Condition expressions typically access objects or state variables shared between
threads. They can be used to implement the Guarded Suspension pattern [Lea99a].
In our gateway example, the Message_Queue defines its internal state, as
illustrated below:
A Message_Queue monitor object defines three types of internal state:
??Queue representation data members. These data members define the internal
queue representation. This representation stores the contents of the queue
in a circular array or linked list, together with book-keeping information
needed to determine whether the queue is empty, full, or neither. The
internal queue representation is manipulated only by the put_i(),
get_i(), empty_i(), and full_i() implementation methods.
??Monitor lock data member. The monitor_lock_ is used by a
Message_Queue's synchronized methods to serialize their access to the
state of the queue's internal representation. A monitor object's lock must be
held whenever its state is being changed to ensure that its invariants are
satisfied. This monitor lock is implemented using the platform-independent
Thread_Mutex class defined in the Wrapper Facade pattern (47).
??Monitor condition data members. The monitor conditions is_full_ and
is_empty_ are used by the put() and get() synchronized methods to
suspend and resume themselves when a Message_Queue leaves its full
and empty boundary conditions, respectively. These monitor conditions are
implemented using the platform-independent Thread_Condition class
defined in the Wrapper Facade pattern (47).
34. Implement all the monitor object's methods and data members. The following two
sub-activities can be used to implement all the monitor object methods and internal
state defined above.
1. Initialize the data members. This sub-activity initializes object-specific data
members, as well as the monitor lock and any monitor conditions.
2. The constructor of Message_Queue creates an empty queue and
initializes the monitor conditions not_empty_ and not_full_:
In this example, both monitor conditions share the same
monitor_lock_. This design ensures that Message_Queue state, such
as the message_count_, is serialized properly to prevent race conditions
from violating invariants when multiple threads try to put() and get()
messages on a queue simultaneously.
10. Apply the Thread-Safe Interface pattern. In this sub-activity, the interface and
implementation methods are implemented according to the Thread-Safe
Interface pattern (345).
11. In our Message_Queue implementation two pairs of interface and
implementation methods check if a queue is empty, which means it
contains no messages, or full, which means it contains max_messages_.
We show the interface methods first:
These methods illustrate a simple example of the Thread-Safe Interface
pattern (345). They use the Scoped Locking idiom (325) to acquire and
release the monitor lock, then forward immediately to their corresponding
implementation methods:
In accordance with the Thread-Safe Interface pattern, these
implementation methods assume the monitor_lock_ is held, so they
just check for the boundary conditions in the queue.
30. The put() method inserts a new Message, which is a class defined in
the Active Object pattern (369), at the tail of a queue. It is a synchronized
method that illustrates a more sophisticated use of the Thread-Safe
Interface pattern (345):
Note how this public synchronized put() method only performs the
synchronization and scheduling logic needed to serialize access to the
monitor object and wait while the queue is full. Once there is room in the
queue, put() forwards to the put_i() implementation method. This
inserts the message into the queue and updates its book-keeping
information. Moreover, the put_i() is not synchronized because the
put() method never calls it without first acquiring the monitor_lock_.
Likewise, the put_i() method need not check to see if the queue is full
because it is not called as long as full_i() returns true.
54. The get() method removes the message at the front of the queue and
returns it to the caller:
As before, note how the synchronized get() interface method performs
the synchronization and scheduling logic, while forwarding the dequeueing
functionality to the get_i() implementation method.
Internally, our gateway contains instances of two classes, Supplier_Handler and
Consumer_Handler. These act as local proxies [GoF95] [POSA1] for remote suppliers and
consumers, respectively. Each Consumer_Handler contains a thread-safe
Message_Queue object implemented using the Monitor Object pattern. This design
decouples supplier handler and consumer handler threads so that they run concurrently and
block independently. Moreover, by embedding and automating synchronization inside
message queue monitor objects, we can protect their internal state from corruption, maintain
invariants, and shield clients from low-level synchronization concerns.
Each Supplier_Handler runs in its own thread, receives messages from its remote
supplier and routes the messages to the designated remote consumers. Routing is
performed by inspecting an address field in each message, which is used as a key into a
routing table that maps keys to Consumer_Handlers.
Each Consumer_Handler is responsible for receiving messages from suppliers via its
put() method and storing each message in its Message_Queue monitor object:
To process the messages placed into its message queue by Supplier_Handlers, each
Consumer_Handler spawns a separate thread of control in its constructor using the
Thread_Manager class defined in the Wrapper Facade pattern (47), as follows:
This new Consumer_Handler thread executes the svc_run() entry point. This is a static
method that retrieves routing messages placed into its message queue by
Supplier_Handler threads and sends them over its TCP connection to the remote
consumer:
The SOCK_Stream's send() method can block in a Consumer_Handler thread. It will not
affect the quality of service of other Consumer_Handler or Supplier_Handler threads,
because it does not share any data with the other threads. Similarly,
Message_Queue::get() can block without affecting the quality of service of other threads,
because the Message_Queue is a monitor object. Supplier_Handlers can thus insert
new messages into the Consumer_Handler's Message_Queue via its put() method
without blocking indefinitely.
Timed Synchronized Method Invocations. Certain applications require 'timed' synchronized
method invocations. This feature allows them to set bounds on the time they are willing to
wait for a synchronized method to enter its monitor object's critical section. The Balking
pattern described in [Lea99a] can be implemented using timed synchronized method
invocations.
The Message_Queue monitor object interface defined earlier can be modified to support
timed synchronized method invocations:
If timeout is 0 then both get() and put() will block indefinitely until a message is
either inserted into or removed from a Message_Queue monitor object. If the time-out
period is non-zero and it expires, the Timedout exception is thrown. The client must be
prepared to handle this exception.
The following illustrates how the put() method can be implemented using the timed
wait feature of the Thread_Condition condition variable wrapper outlined in
implementation activity 3 (408):
While the queue is full this 'timed' put() method releases monitor_lock_ and
suspends the calling thread, to wait for space to become available in the queue or for
the timeout period to elapse. The monitor_lock_ will be re-acquired automatically
when wait() returns, regardless of whether a time-out occurred or not.
Strategized Locking. The Strategized Locking pattern (333) can be applied to make a
monitor object implementation more flexible, efficient, reusable, and robust. Strategized
Locking can be used, for example, to configure a monitor object with various types of
monitor locks and monitor conditions.
The following template class uses generic programming techniques [Aus98] to
parameterize the synchronization aspects of a Message_Queue:
Each synchronized method is then modified as shown by the following empty()
method:
To parameterize the synchronization aspects associated with a Message_Queue, we
can define a pair of classes, MT_Synch and NULL_SYNCH that typedef the appropriate
C++ traits:
To define a thread-safe Message_Queue, therefore, we simply parameterize it with the
MT_Synch strategy:
Similarly, to create a non-thread-safe Message_Queue, we can parameterize it with the
following Null_Synch strategy:
Note that when using the Strategized Locking pattern in C++ it may not be possible for a
generic component class to know what type of synchronization strategy will be configured for
a particular application. It is important therefore to apply the Thread-Safe Interface pattern
(345) as described in implementation activity 4.2 (411), to ensure that intra-object method
calls, such as put() calling full_i(), and put_i() , avoid self-deadlock and minimize
locking overhead.
Multiple Roles. If a monitor object implements multiple roles, each of which is used by
different types of clients, an interface can be introduced for each role. Applying the
Extension Interface pattern (141) allows clients to obtain the interface they need. This design
helps separate concerns, because a client only sees the particular methods of a monitor
object it needs for its own operation. This design further simplifies a monitor object's
evolution. For example, new services can be added to the active object by providing new
extension interface without changing existing ones. Clients that do not need access to the
new services are thus unaffected by the extension.
Dijkstra and Hoare-style Monitors. Dijkstra [Dij68] and Hoare [Hoare74] defined
programming language features called monitors that encapsulate functions and their internal
variables into thread-safe modules. To prevent race conditions a monitor contains a lock that
allows only one function at a time to be active within the monitor. Functions that want to
leave the monitor temporarily can block on a condition variable. It is the responsibility of the
programming language compiler to generate run-time code that implements and manages a
monitor's lock and its condition variables.
Java Objects. The main synchronization mechanism in Java is based on Dijkstra/Hoare style monitors. Each Java object can be a monitor object containing a monitor lock and a
single monitor condition. Java's monitors are simple to use for common use cases, because
they allow threads to serialize their execution implicitly via method-call interfaces and to
coordinate their activities via calls to wait(), notify(), and notifyAll() methods
defined on all objects.
For more complex use cases, however, the simplicity of the Java language constructs may
mislead developers into thinking that concurrency is easier to program than it actually is in
practice. In particular, heavy use of inter-dependent Java threads can yield complicated
inter-relationships, starvation, deadlock, and overhead. [Lea99a] describes many patterns
for handling simple and complex concurrency use cases in Java.
The Java language synchronization constructs outlined above can be implemented in
several ways inside a compliant Java virtual machine (JVM). JVM implementors must
choose between two implementation decisions:
??Implement Java threads internally in the JVM. If threads are implemented internally,
the JVM appears as one monolithic task to the operating system. In this case, the JVM
is free to decide when to suspend and resume threads and how to implement thread
scheduling, as long as it stays within the bounds of the Java language specification.
??Map Java threads them to native operating system threads. In this case Java monitors
can take advantage of synchronization primitives and scheduling behavior of the
underlying platform.
The advantage of an internal threads implementation is its platform-independence. However,
one of its disadvantages is its inability to take advantage of parallelism in the hardware. As a
result, an increasing number of JVMs are implemented by mapping Java threads to native
operating system threads.
ACE Gateway. The example from the Example Resolved section is based on a
communication gateway application contained in the ACE framework [Sch96], which uses
monitor objects to simplify concurrent programming and improve performance on
multiprocessors. Unlike the Dijkstra/Hoare and Java monitors, which are programming
language features, the Message_Queues used by Consumer_Handlers in the gateway
are reusable ACE C++ components implemented using the Monitor Object pattern. Although
C++ does not support monitor objects directly as a language feature, ACE implements the
Monitor Object pattern by applying other patterns and idioms, such as the Guarded
Suspension pattern [Lea99a] and the Scoped Locking (325) idiom, as described in the
Implementation section.
Fast food restaurant. A real-life example of the Monitor Object pattern occurs when
ordering a meal at a busy fast food restaurant. Customers are the clients who wait to place
their order with a cashier. Only one customer at a time interacts with a cashier. If the order
cannot be serviced immediately, a customer temporarily steps aside so that other customers
can place their orders. When the order is ready the customer re-enters at the front of the line
and can pick up the meal from the cashier.
The Monitor Object pattern provides two benefits:
Simplification of concurrency control. The Monitor Object pattern presents a concise
programming model for sharing an object among cooperating threads. For example, object
synchronization corresponds to method invocations. Similarly clients need not be concerned
with concurrency control when invoking methods on a monitor object. It is relatively
straightforward to create a monitor object out of most so-called passive objects, which are
objects that borrow the thread of control of its caller to execute its methods.
Simplification of scheduling method execution. Synchronized methods use their monitor
conditions to determine the circumstances under which they should suspend or resume their
execution and that of collaborating monitor objects. For example, methods can suspend
themselves and wait to be notified when arbitrarily complex conditions occur, without using
inefficient polling. This feature makes it possible for monitor objects to schedule their
methods cooperatively in separate threads.
The Monitor Object pattern has the following four liabilities:
The use of a single monitor lock can limit scalability due to increased contention when
multiple threads serialize on a monitor object.
Complicated extensibility semantics resulting from the coupling between a monitor object's
functionality and its synchronization mechanisms. It is relatively straightforward to decouple
an active object's (369) functionality from its synchronization policies via its separate
scheduler participant. However, a monitor object's synchronization and scheduling logic is
often tightly coupled with its methods' functionality. This coupling often makes monitor
objects more efficient than active objects. Yet it also makes it hard to change their
synchronization policies or mechanisms without modifying the monitor object's method
implementations.
It is also hard to inherit from a monitor object transparently, due to the inheritance anomaly
problem [MWY91]. This problem inhibits reuse of synchronized method implementations
when subclasses require different synchronization mechanisms. One way to reduce the
coupling of synchronization and functionality in monitor objects is to use Aspect-Oriented
Programming [KLM+97] or the Strategized Locking (333) and Thread-Safe Interface (345)
patterns, as shown in the Implementation and Variants section.
Nested monitor lockout. This problem is similar to the preceding liability. It can occur when a
monitor object is nested within another monitor object.
This code illustrates the canonical form of the nested monitor lockout problem in Java
[JS97a]. When a Java thread blocks in the monitor's wait queue, all its locks are held except
the lock of the object placed in the queue.
Consider what would happen if thread T1 made a call to Outer.process() and as a result
blocked in the wait() call in Inner.awaitCondition(). In Java, the Inner and Outer
classes do not share their monitor locks. The wait() statement in waitCondition() call
would therefore release the Inner monitor while retaining the Outer monitor. Another
thread T2 cannot then acquire the Outer monitor, because it is locked by the synchronized
process() method. As a result Outer.set cannot set Inner.cond_ to true and T1 will
continue to block in wait() forever.
Nested monitor lockout can be avoided by sharing a monitor lock between multiple monitor
conditions. This is straightforward in Monitor Object pattern implementations based on
POSIX condition variables [IEEE96]. It is surprisingly hard in Java due to its simple
concurrency and synchronization model, which tightly couples a monitor lock with each
monitor object. Java idioms for avoiding nested monitor lockout in Java are described in
[Lea99a] [JS97a].
The Monitor Object pattern is an object-oriented analog of the Code Locking pattern
[McK95], which ensures that a region of code is serialized. In the Monitor Object pattern, the
region of code is the synchronized method implementation.
The Monitor Object pattern has several properties in common with the Active Object pattern
(369). Both patterns can synchronize and schedule methods invoked concurrently on
objects, for example. There are two key differences, however:
??An active object executes its methods in a different thread than its client(s), whereas a
monitor object executes its methods in its client threads. As a result, active objects can
perform more sophisticated, albeit more expensive, scheduling to rearrange the order in
which their methods execute.
??Monitor objects often couple their synchronization logic more closely to their methods'
functionality. In contrast, it is easier to decouple an active object's functionality from its
synchronization policies, because it has a separate scheduler.
It is instructive to compare the Monitor Object pattern solution in the Example Resolved
section with the solution presented in the Active Object pattern. Both solutions have similar
overall application architectures. In particular, the Supplier_Handler and
Consumer_Handler implementations are almost identical. The primary difference is that
the Message_Queue itself is easier to program and often more efficient when implemented
using the Monitor Object pattern than the Active Object pattern.
If a more sophisticated queueing strategy is necessary, however, the Active Object pattern
may be more appropriate. Similarly, because active objects execute in different threads than
their clients, there are situations where active objects can improve overall application
concurrency by executing multiple operations asynchronously. When these operations
complete, clients can obtain their results via futures [Hal85] [LS88].
##%%&&
The Half-Sync/Half-Async architectural pattern decouples asynchronous and synchronous
service processing in concurrent systems, to simplify programming without unduly reducing
performance. The pattern introduces two intercommunicating layers, one for asynchronous
and one for synchronous service processing.
Performance-sensitive concurrent applications, such as telecommunications switching
systems and avionics mission computers, perform a mixture of synchronous and
asynchronous processing to coordinate different types of applications, system services, and
hardware. Similar characteristics hold for system-level software, such as operating systems.
The BSD UNIX operating system [MBKQ96] [Ste98] is an example of a concurrent system
that coordinates the communication between standard Internet application services, such as
FTP, INETD, DNS, TELNET, SMTP, and HTTPD, and hardware I/O devices, such as
network interfaces, disk controllers, end-user terminals, and printers.
The BSD UNIX operating system processes certain services asynchronously to maximize
performance. Protocol processing within the BSD UNIX kernel, for example, runs
asynchronously, because I/O devices are driven by interrupts triggered by network interface
hardware. If the kernel does not handle these asynchronous interrupts immediately,
hardware devices may malfunction and drop packets or corrupt memory buffers.
Although the BSD operating system kernel is driven by asynchronous interrupts, it is hard to
develop applications and higher-level system services using asynchrony mechanisms, such
as interrupts or signals. In particular, the effort required to program, validate, debug, and
maintain asynchronous programs can be prohibitive. For example, asynchrony can cause
subtle timing problems and race conditions when an interrupt preempts a running
computation unexpectedly.
To avoid the complexities of asynchronous programming, higher-level services in BSD UNIX
run synchronously in multiple processes. For example, FTP or TELNET Internet services that
use synchronous read() and write() system calls can block awaiting the completion of
I/O operations. Blocking I/O, in turn, enables developers to maintain state information and
execution history implicitly in the run-time stacks of their threads, rather than in separate
data structures that must be managed explicitly by developers.
Within the context of an operating system, however, synchronous and asynchronous
processing is not wholly independent. In particular, application-level Internet services that
execute synchronously within BSD UNIX must cooperate with kernel-level protocol
processing that runs asynchronously. For example, the synchronous read() system call
invoked by an HTTP server cooperates indirectly with the asynchronous reception and
protocol processing of data arriving on the Ethernet network interface.
A key challenge in the development of BSD UNIX was the structuring of asynchronous and
synchronous processing, to enhance both programming simplicity and system performance.
In particular, developers of synchronous application programs must be shielded from the
complex details of asynchronous programming. Yet, the overall performance of the system
must not be degraded by using inefficient synchronous processing mechanisms in the BSD
UNIX kernel.
A concurrent system that performs both asynchronous and synchronous processing services
that must intercommunicate.
Concurrent systems often contain a mixture of asynchronous and synchronous processing
services. There is a strong incentive for system programmers to use asynchrony to improve
performance. Asynchronous programs are generally more efficient, because services can be
mapped directly onto asynchrony mechanisms, such as hardware interrupt handlers or
software signal handlers.
Conversely, there is a strong incentive for application developers to use synchronous
processing to simplify their programming effort. Synchronous programs are usually less
complex, because certain services can be constrained to run at well-defined points in the
processing sequence.
Two forces must therefore be resolved when specifying a software architecture that
executes services both synchronously and asynchronously:
??The architecture should be designed so that application developers who want the
simplicity of synchronous processing need not address the complexities of asynchrony.
Similarly, system developers who must maximize performance should not need to
address the inefficiencies of synchronous processing.
??The architecture should enable the synchronous and asynchronous processing
services to communicate without complicating their programming model or unduly
degrading their performance.
Although the need for both programming simplicity and high performance may seem
contradictory, it is essential that both these forces be resolved in certain types of concurrent
systems, particularly large-scale or complex ones.
Decompose the services in the system into two layers [POSA1], synchronous and
asynchronous, and add a queueing layer between them to mediate the communication
between services in the asynchronous and synchronous layers.
In detail: process higher-layer services, such as long-duration database queries or file
transfers, synchronously in separate threads or processes, to simplify concurrent
programming. Conversely, process lower-layer services, such as short-lived protocol
handlers driven by interrupts from network interface hardware, asynchronously to enhance
performance. If services residing in separate synchronous and asynchronous layers must
communicate or synchronize their processing, allow them to pass messages to each other
via a queueing layer.
The structure of the Half-Sync/Half-Async pattern follows the Layers pattern [POSA1] and
includes four participants:
The synchronous service layer performs high-level processing services. Services in the
synchronous layer run in separate threads or processes that can block while performing
operations.
The Internet services in our operating system example run in separate application
processes. These processes invoke read() and write() operations to perform I/O
synchronously on behalf of their Internet services.
The asynchronous service layer performs lower-level processing services, which typically
emanate from one or more external event sources. Services in the asynchronous layer
cannot block while performing operations without unduly degrading the performance of other
services.
The processing of I/O devices and protocols in the BSD UNIX operating system kernel is
performed asynchronously in interrupt handlers. These handlers run to completion, that
is, they do not block or synchronize their execution with other threads until they are
finished.
The queueing layer provides the mechanism for communicating between services in the
synchronous and asynchronous layers. For example, messages containing data and control
information are produced by asynchronous services, then buffered at the queueing layer for
subsequent retrieval by synchronous services, and vice versa. The queueing layer is
responsible for notifying services in one layer when messages are passed to them from the
other layer. The queueing layer therefore enables the asynchronous and synchronous layers
to interact in a 'producer/consumer' manner, similar to the structure defined by the Pipes and
Filters pattern [POSA1].
The BSD UNIX operating system provides a Socket layer [Ste98]. This layer serves as
the buffering and notification point between the synchronous Internet service application
processes and the asynchronous, interrupt-driven I/O hardware services in the BSD
UNIX kernel.
External event sources generate events that are received and processed by the
asynchronous service layer. Common sources of external events for operating systems
include network interfaces, disk controllers, and end-user terminals.
The following class diagram illustrates the structure and relationships between these
participants:
Asynchronous and synchronous layers in the Half-Sync/Half-Async pattern interact by
passing messages via a queueing layer. We describe three phases of interactions that occur
when input arrives 'bottom-up' from external event sources:
??Asynchronous phase. In this phase external sources of input interact with the
asynchronous service layer via an asynchronous event notification, such as an interrupt
or signal. When asynchronous services have finished processing the input, they can
communicate their results to the designated services in the synchronous layer via the
queueing layer.
??Queueing phase. In this phase the queueing layer buffers input passed from the
asynchronous layer to the synchronous layer and notifies the synchronous layer that
input is available.
??Synchronous phase. In this phase the appropriate service(s) in the synchronous layer
retrieve and process the input placed into the queueing layer by service(s) in the
asynchronous layer.
The interactions between layers and pattern participants is reversed to form a 'top-down'
sequence when output arrives from services running in the synchronous layer.
This section describes the activities used to implement the Half-Sync/Half-Async pattern and
apply it to structure the concurrency architecture of higher-level applications, such as Web
servers [Sch97] and database servers, as well as to lower-level systems, such as the BSD
UNIX operating system. We therefore present examples from several different domains.
1. Decompose the overall system into three layers: synchronous, asynchronous, and
queueing. Three sub-activities can be used to determine how to decompose a system
architecture designed in accordance with the Half-Sync/Half-Async pattern.
1. Identify higher-level and/or long-duration services and configure them into the
synchronous layer. Many services in a concurrent system are easier to
implement when they are programmed using synchronous processing. These
services often perform relatively high-level or long-duration application
processing, such as transferring large streams of content in a Web server or
performing complex queries in a database. Services in the synchronous layer
should therefore run in separate processes or threads. If data is not available
the services can block at the queueing layer awaiting responses, under the
control of peer-to-peer application communication protocols.
2. Each Internet service shown in our BSD UNIX operating system example
runs in a separate application process. Each application process
communicates with its clients using the protocol associated with the
Internet service it implements. I/O operations within these processes can
be performed by blocking synchronously on TCP Sockets and waiting for
the BSD UNIX kernel to complete the I/O operations asynchronously.
4. Identify lower-level and/or short-duration services and configure them into the
asynchronous layer. Certain services in a system cannot block for prolonged
amounts of time. Such services typically perform lower-level or short-duration
system processing that interacts with external sources of events, such as end user terminals or interrupt-driven hardware network interfaces. To maximize
responsiveness and efficiency, these sources of events must be handled
rapidly and must not block the thread that services them. Their services should
be triggered by asynchronous notifications or interrupts from external event
sources and run to completion, at which point they can insert messages
containing their results into the queueing layer.
5. In our operating system example, processing of I/O device drivers and
communication protocols in the BSD UNIX kernel occurs in response to
asynchronous hardware interrupts. Each asynchronous operation in the
kernel runs to completion, inserting messages containing data and/or
control information into the Socket layer if it must communicate with an
application process running an Internet service in the synchronous layer.
7. Identify inter-layer communication strategies and configure them into the
queueing layer. The queueing layer is a mediator [GoF95] that decouples the
communication between services in the asynchronous and synchronous
layers. Thus these services do not access each other directly, but only via the
queueing layer. The communication-related strategies performed by the
queueing layer involve (de)multiplexing, buffering, notification, and flow
control. Services in the asynchronous and synchronous layers use these
queueing strategies to implement protocols for passing messages between the
synchronous and asynchronous layers [SC96].
8. In our BSD UNIX operating system example, the Sockets mechanism
[Ste98] defines the queueing layer between the synchronous Internet
service application processes and the asynchronous operating system
kernel. Each Internet service uses one or more Sockets, which are queues
maintained by BSD UNIX to buffer messages exchanged between
application processes, and the TCP/IP protocol stack and networking
hardware devices in the kernel.
2. Implement the services in the synchronous layer. High-level and/or long-duration
services in the synchronous layer are often implemented using either multi-threading
or multi-processing. Compared to a thread, a process maintains more state
information and requires more overhead to spawn, synchronize, schedule, and inter communicate. Implementing synchronous services in separate threads, rather than
separate processes, can therefore yield simpler and more efficient applications.
Multi-threading can reduce application robustness, however, because separate
threads within a process are not protected from one another. For instance, one faulty
thread can corrupt data shared with other threads in the process, which may produce
incorrect results, crash the process, or cause the process to hang indefinitely. To
increase robustness, therefore, application services can be implemented in separate
processes.
The Internet services in our BSD UNIX example are implemented in separate
processes. This design increases their robustness and prevents unauthorized
access to certain resources, such as files owned by other users.
3. Implement the services in the asynchronous layer. Lower-level and/or shorter-duration
services in the asynchronous layer often do not have their own dedicated thread of
control. Instead, they must borrow a thread from elsewhere, such as the operating
system kernel's 'idle thread' or a separate interrupt stack. To ensure adequate
response time for other system services, such as high-priority hardware interrupts,
these services must run asynchronously and cannot block for long periods of time.
The following are two strategies that can be used to trigger the execution of
asynchronous services:
??Asynchronous interrupts. This strategy is often used when developing
asynchronous services that are triggered directly by hardware interrupts from
external event sources, such as network interfaces or disk controllers. In this
strategy, when an event occurs on an external event source, an interrupt notifies
the handler associated with the event, which then processes the event to
completion.
In complex concurrent systems, it may be necessary to define a hierarchy of
interrupts to allow less critical handlers to be preempted by higher-priority ones.
To prevent interrupt handlers from corrupting shared state while they are being
accessed, data structures used by the asynchronous layer must be protected, for
example by raising the interrupt priority [WS95].
The BSD UNIX kernel uses a two-level interrupt scheme to handle network
packet processing [MBKQ96]. Time-critical processing is done at a high
priority and less critical software processing is done at a lower priority. This
two-level interrupt scheme prevents the overhead of software protocol
processing from delaying the servicing of high-priority hardware interrupts.
??Proactive I/O. This strategy is often used when developing asynchronous
services based on higher-level operating system APIs, such as the Windows NT
overlapped I/O and I/O completion ports [So198] or the POSIX aio_* family of
asynchronous I/O system calls [POSIX95]. In this strategy, I/O operations are
executed by an asynchronous operation processor. When an asynchronous
operation finishes, the asynchronous operation processor generates a
completion event. This event is then dispatched to the handler associated with
the event, which processes the event to completion.
??For example, the Web server in the Proactor pattern (215) illustrates an
application that uses the proactive I/O mechanisms defined by the Windows
NT system call API. This example underscores the fact that asynchronous
processing and the Half-Sync/Half-Async pattern can be used for higher level applications that do not access hardware devices directly.
Both of these asynchronous processing strategies share the constraint that a handler
cannot block for a long period of time without disrupting the processing of events from
other external event sources.
4. Implement the queueing layer. After services in the asynchronous layer finish
processing input arriving from external event sources, they typically insert the
resulting messages into the queueing layer. The appropriate service in the
synchronous layer will subsequently remove these messages from the queueing layer
and process them. These roles are reversed for output processing. Two
communication-related strategies must be defined when implementing the queueing
layer:
0. Implement the buffering strategy. Services in the asynchronous and
synchronous layers do not access each other's memory directly¡ªinstead, they
exchange messages via a queueing layer. This queueing layer buffers
messages so that synchronous and asynchronous services can run
concurrently, rather than running in lockstep via a 'stop-and-wait' flow control
protocol. The buffering strategy must therefore implement an ordering,
serialization, notification, and flow-control strategy. Note that the Strategy
pattern [GoF95] can be applied to simplify the configuration of alternative
strategies.
??Implement the ordering strategy. Simple queueing layers store their
messages in the order they arrive, that is, 'first-in, first-out' (FIFO). The
first message that was placed in the queue by a service in one layer is
thus the first message to be removed by a service in the other layer. FIFO
ordering is easy to implement, but may result in priority inversions
[SMFG00] if high-priority messages are queued behind lower-priority
messages. Therefore, more sophisticated queueing strategies can be
used to store and retrieve messages in 'priority' order.
??Implement the serialization strategy. Services in the asynchronous and
synchronous layer can execute concurrently. A queue must therefore be
serialized to avoid race conditions when messages are inserted and
removed concurrently. This serialization is often implemented using
lightweight synchronization mechanisms, such as mutexes [Lew95]. Such
mechanisms ensure that messages can be inserted into and removed
from the queueing layer's message buffers without corrupting its internal
data structures.
??Implement the notification strategy. It may be necessary to notify a
service in one layer when messages addressed to it arrive from another
layer. The notification strategy provided by the queueing layer is often
implemented using more sophisticated and heavyweight synchronization
mechanisms, such as semaphores or condition variables [Lew95]. These
synchronization mechanisms can notify the appropriate services in the
synchronous or asynchronous layers when data arrives for them in the
queueing layer. The Variations section outlines several other notification
strategies based on asynchronous signals and interrupts.
??Implement the flow-control strategy. Systems cannot devote an
unlimited amount of resource to buffer messages in the queueing layer. It
may therefore be necessary to regulate the amount of data passed
between the synchronous and asynchronous layers. Flow control is a
technique that prevents synchronous services from flooding the
asynchronous layer at a rate greater than that at which messages can be
transmitted and queued on network interfaces [SchSu93].
Services in the synchronous layer can block. A common flow control
policy simply puts a synchronous service to sleep if it produces and
queues more than a certain number of messages. After the asynchronous
service layer empties the queue to below a certain level, the queueing
layer can awaken the synchronous service to continue its processing.
In contrast, services in the asynchronous layer cannot block. If they can
produce an excessive number of messages, a common flow-control policy
allows the queueing layer to discard messages until the synchronous
service layer finishes processing the messages in its queue. If the
messages are associated with a reliable connection-oriented transport
protocol, such as TCP [Ste93], senders will time-out eventually and
retransmit discarded messages.
1. Implement the (de)multiplexing mechanism. In simple implementations of the
Half-Sync/Half-Async pattern, such as the OLTP servers described in the
Example section of the Leader/Followers pattern (447), there is only one
queue in the queueing layer. This queue is shared by all services in the
asynchronous and synchronous layers and any service can process any
request. This configuration alleviates the need for a sophisticated
(de)multiplexing mechanism. In this case, a common implementation is to
define a singleton [GoF95] queue that all services use to insert and remove
messages.
In more complex implementations of the Half-Sync/Half-Async pattern,
services in one layer may need to send and receive certain messages to
particular services in another layer. A queueing layer may therefore need
multiple queues, for example one queue per service. With multiple queues,
more sophisticated demultiplexing mechanism are needed to ensure
messages exchanged between services in different layers are placed in the
appropriate queue. A common implementation is to use some type of
(de)multiplexing mechanism, such as a hash table [HMPT89] [MD91], to place
messages into the appropriate queue(s).
The Message_Queue components defined in the Monitor Object (399)
and Active Object (369) patterns illustrate various strategies for
implementing a queueing layer:
??The Monitor Object pattern ensures that only one method at a time
executes within a queue, regardless of the number of threads that
invoke the queue's methods concurrently, by using mutexes and
condition variables. The queue executes its methods in its client
threads, that is, in the threads that run the synchronous and
asynchronous services.
??The Active Object pattern decouples method invocations on the queue
from method execution. Multiple synchronous and asynchronous
services can therefore invoke methods on the queue concurrently.
Methods are executed in a different thread than the threads that run
the synchronous and asynchronous services.
The See Also sections of the Active Object (369) and Monitor Object (399)
patterns discuss the pros and cons of using these patterns to implement a
queueing layer.
Chapter 1, Concurrent and Networked Objects, and other patterns in this book, such as
Proactor (215), Scoped Locking (325), Strategized Locking (333), and Thread-Safe Interface
(345), illustrate various aspects of the design of a Web server application. In this section, we
explore the broader system context in which Web servers execute, by outlining how the BSD
UNIX operating system [MBKQ96] [Ste93] applies the Half-Sync/Half-Async pattern to
receive an HTTP GET request via its TCP/IP protocol stack over Ethernet.
BSD UNIX is an example of an operating system that does not support asynchronous I/O
efficiently. It is therefore not feasible to implement the Web server using the Proactor pattern
(215). We instead outline how BSD UNIX coordinates the services and communication
between synchronous application processes and the asynchronous operating system kernel.
In particular, we describe:[6]
??The synchronous invocation of a read() system call by a Web server application (the
HTTPD process).
??The asynchronous reception and protocol processing of data arriving on the Ethernet
network interface.
??The synchronous completion of the read() call, which returns control and the GET
request data back to the HTTPD process.
These steps are shown in the following figure:
As shown in this figure, the HTTPD process invokes a read() system call on a connected
socket handle to receive an HTTP GET request encapsulated in a TCP packet. From the
perspective of the HTTPD process, the read() system call is synchronous, because the
process invokes read() and blocks until the GET request data is returned. If data is not
available immediately, however, the BSD UNIX kernel puts the HTTPD process to sleep until
the data arrives from the network.
Many asynchronous steps occur to implement the synchronous read() system call,
however. Although the HTTPD process can sleep while waiting for data, the BSD UNIX
kernel cannot sleep, because other application processes, such as the FTP and TELNET
services and I/O devices in the kernel, require its services to run concurrently and efficiently.
After the read() system call is issued the application process switches to 'kernel mode' and
starts running privileged instructions, which direct it synchronously into the BSD UNIX
networking subsystem. Ultimately, the thread of control from the application process ends in
the kernel's soreceive() function. This function processes input for various types of
sockets, such as datagram sockets and stream sockets, by transferring data from the socket
queue to the application process. The soreceive() function thus defines the boundary
between the synchronous application process layer and the asynchronous kernel layer for
outgoing packets.
There are two ways in which the HTTPD process's read() system call can be handled by
soreceive(), depending on the characteristics of the Socket and the amount of data in the
socket queue:
??Completely synchronous. If the data requested by the HTTPD process is in the socket
queue, the soreceive() function can copy it immediately and the read() system call
will complete synchronously.
??Half-synchronous and half-asynchronous. If the data requested by the HTTPD process
is not yet available, the kernel calls the sbwait() function to put the process to sleep
until the requested data arrives.
After sbwait() puts the process to sleep, the BSD UNIX scheduler will switch to another
process context that is ready to run. From the perspective of the HTTPD process, however,
the read() system call appears to execute synchronously. When packet(s) containing the
requested data arrive, the kernel will process them asynchronously, as described below.
When enough data has been placed in the socket queue to satisfy the HTTPD process'
request, the kernel will wake this process and complete its read() system call. This call
then returns synchronously so that the HTTPD process can parse and execute the GET
request.
To maximize performance within the BSD UNIX kernel, all protocol processing is executed
asynchronously, because I/O devices are driven by hardware interrupts. For example,
packets arriving at the Ethernet network interface are delivered to the kernel via interrupt
handlers initiated asynchronously by the Ethernet hardware. These handlers receive packets
from devices and trigger subsequent asynchronous processing of higher-layer protocols,
such as IP and TCP. Ultimately, valid packets containing application data are queued at the
Socket layer, where the BSD UNIX kernel schedules and dispatches the waiting HTTPD
process to consume this data synchronously.
For example, the 'half-async' processing associated with an HTTPD process's read()
system call starts when a packet arrives at an Ethernet network interface, which triggers an
asynchronous hardware interrupt. All incoming packet processing is performed in the context
of an interrupt handler. During an interrupt, the BSD UNIX kernel cannot sleep or block,
because there is no application process context and no dedicated thread of control. The
Ethernet interrupt handler therefore 'borrows' the kernel's thread of control. Similarly, the
BSD UNIX kernel borrows the threads of control of application processes when they invoke
system calls.
If the packet is destined for an application process, it is passed up to the transport layer,
which performs additional protocol processing, such as TCP segment reassembly and
acknowledgments. Eventually, the transport layer appends the data to the receive socket
queue and calls sbwakeup(), which represents the boundary between the asynchronous
and synchronous layers for incoming packets. This call wakes up the HTTPD process that
was sleeping in soreceive() waiting for data on that socket queue. If all the data
requested by the HTTPD process has arrived, soreceive() will copy it to the buffer
supplied by HTTPD, allowing the system call to return control to the Web server. The
read() call thus appears to be synchronous from the perspective of the HTTPD process,
even though asynchronous processing and context switching were performed while this
process was asleep.
Asynchronous Control with Synchronous Data I/O. The HTTPD Web server described in the
Implementation section 'pulls' messages synchronously from the queueing layer at its
discretion, thereby combining control and data activities. On some operating system
platforms, however, it is possible to decouple control and data so that services in the
synchronous layer can be notified asynchronously when messages are inserted into the
queueing layer. The primary benefit of this variant is that higher-level 'synchronous' services
may be more responsive, because they can be notified asynchronously.
The UNIX signal-driven I/O mechanism [Ste98] implements this variant of the Half Sync/Half-Async pattern. The UNIX kernel uses the SIGIO signal to 'push' control to a
higher-level application process when data arrives on one of its Sockets. When a
process receives this control notification asynchronously, it can then 'pull' the data
synchronously from socket queueing layer via read().
The disadvantage of using asynchronous control, of course, is that developers of higher level services must now face many of the asynchrony complexities outlined in the Problem
section.
Half-Async/Half-Async. This variant extends the previous variant by propagating
asynchronous control notifications and data operations all the way up to higher-level
services in the 'synchronous' layer. These higher-level services may therefore be able to
take advantage of the efficiency of the lower-level asynchrony mechanisms.
For example, the real-time signal interface defined in the POSIX real-time programming
specification [POSIX95] supports this variant. In particular, a buffer pointer can be
passed to the signal handler function dispatched by the operating system when a real time signal occurs. Windows NT supports a similar mechanism using overlapped I/O and
I/O completion ports [Sol98]. In this case, when an asynchronous operation completes,
its associated overlapped I/O structure indicates which operation has completed and
passes any data along. The Proactor pattern (215) and Asynchronous Completion
Token pattern (261) describe how to structure applications to take advantage of
asynchronous operations and overlapped I/O.
The disadvantage of this variant is similar to that of the previous variant. If most or all
services can be driven by asynchronous operations, the design may be modeled better by
applying the Proactor pattern (215) rather than the Half-Sync/Half-Async pattern.
Half-Sync/Half-Sync. This variant provides synchronous processing to lower-level services. If
the asynchronous layer is multi-threaded, its services can run autonomously and use the
queueing layer to pass messages to the synchronous service layer. The benefits of this
variant are that services in the asynchronous layer may be simplified, because they can
block without affecting other services in this layer.
Microkernel operating systems, such as Mach [B190] or Amoeba [Tan95], typically use
this variant. The microkernel runs as a separate multi-threaded 'process' that exchanges
messages with application processes. Similarly, multi-threaded operating system
macrokernels, such as Solaris [EKBF+92], can support multiple synchronous I/O
operations in the kernel.
Multi-threading the kernel can be used to implement polled interrupts, which reduce the
amount of context switching for high-performance continuous media systems by dedicating a
kernel thread to poll a field in shared memory at regular intervals [CP95]. In contrast, single threaded operating system kernels, such as BSD UNIX, restrict lower-level kernel services to
use asynchronous I/O and only support synchronous multi-programming for higher-level
application processes.
The drawback to providing synchronous processing to lower-level services, of course, is that
it may increase overhead, thereby degrading overall system performance significantly.
Half-Sync/Half-Reactive. In object-oriented applications, the Half-Sync/Half-Async pattern
can be implemented as a composite architectural pattern that combines the Reactor pattern
(179) with the Thread Pool variant of the Active Object pattern (369). In this common variant,
the reactor's event handlers constitute the services in the 'asynchronous' layer[7] and the
queueing layer can be implemented by an active object's activation list. The servants
dispatched by the scheduler in the active object's thread pool constitute the services in the
synchronous layer. The primary benefit of this variant is the simplification it affords. This
simplicity is achieved by performing event demultiplexing and dispatching in a single threaded reactor that is decoupled from the concurrent processing of events in the active
object's thread pool.
The OLTP servers described in the Example section of the Leader/Followers pattern
(447) apply this variant. The 'asynchronous' service layer uses the Reactor pattern (179)
to demultiplex transaction requests from multiple clients and dispatch event handlers.
The handlers insert requests into the queueing layer, which is an activation list
implemented using the Monitor Object pattern (399). Similarly, the synchronous service
layer uses the thread pool variant of the Active Object pattern (369) to disseminate
requests from the activation list to a pool of worker threads that service transaction
requests from clients. Each thread in the active object's thread pool can block
synchronously because it has its own run-time stack.
The drawback with this variant is that the queueing layer incurs additional context switching,
synchronization, data allocation, and data copying overhead that may be unnecessary for
certain applications. In such cases the Leader/Followers pattern (447) may be a more
efficient, predictable, and scalable way to structure a concurrent application than the Half Sync/Half-Async pattern.
UNIX Networking Subsystems. The BSD UNIX networking subsystem [MBKQ96] and the
UNIX STREAMS communication framework [Ris98] use the Half-Sync/Half-Async pattern to
structure the concurrent I/O architecture of application processes and the operating system
kernel. I/O in these kernels is asynchronous and triggered by interrupts. The queueing layer
is implemented by the Socket layer in BSD UNIX [Ste98] and by Stream Heads in UNIX
STREAMS [Rago93]. I/O for application processes is synchronous.
Most UNIX network daemons, such as TELNETD and FTPD, are developed as application
processes that invoke read() and write() system calls synchronously [Ste98]. This
design shields application developers from the complexity of asynchronous I/O processed by
the kernel. However, there are hybrid mechanisms, such as the UNIX SIGIO signal, that can
be used to trigger synchronous I/O processing via asynchronous control notifications.
CORBA ORBs. MT-Orbix [Bak97] uses a variation of the Half-Sync/Half-Async pattern to
dispatch CORBA remote operations in a concurrent server. In MT-Orbix's ORB Core a
separate thread is associated with each socket handle that is connected to a client. Each
thread blocks synchronously, reading CORBA requests from the client. When a request is
received it is demultiplexed and inserted into the queueing layer. An active object thread in
the synchronous layer then wakes up, dequeues the request, and processes it to completion
by performing an upcall to the CORBA servant.
ACE. The ACE framework [Sch97] applies the 'Half-Sync/Half-Reactive' variant of the Half Sync/Half-Async pattern in an application-level gateway that routes messages between
peers in a distributed system [Sch96]. The ACE_Reactor is the ACE implementation of the
Reactor pattern (179) that demultiplexes indication events to their associated event handlers
in the 'asynchronous' layer. The ACE Message_Queue class implements the queueing
layer, while the ACE Task class implements the thread pool variant of the Active Object
pattern (369) in the synchronous service layer.
Conduit. The Conduit communication framework [Zweig90] from the Choices operating
system project [CIRM93] implements an object-oriented version of the Half-Sync/Half-Async
pattern. Application processes are synchronous active objects, an Adapter Conduit serves
as the queueing layer, and the Conduit micro-kernel operates asynchronously,
communicating with hardware devices via interrupts.
Restaurants. Many restaurants use a variant of the Half-Sync/Half-Async pattern. For
example, restaurants often employ a host or hostess who is responsible for greeting patrons
and keeping track of the order in which they will be seated if the restaurant is busy and it is
necessary to queue them waiting for an available table. The host or hostess is 'shared' by all
the patrons and thus cannot spend much time with any given party. After patrons are seated
at a table, a waiter or waitress is dedicated to service that table.
The Half-Sync/Half-Async pattern has the following benefits:
Simplification and performance. The programming of higher-level synchronous processing
services are simplified without degrading the performance of lower-level system services.
Concurrent systems often have a greater number and variety of high-level processing
services than lower-level services. Decoupling higher-level synchronous services from
lower-level asynchronous processing services can simplify application programming,
because complex concurrency control, interrupt handling, and timing services can be
localized within the asynchronous service layer. The asynchronous layer can also handle
low-level details that may be hard for application developers to program robustly, such as
interrupt handling. In addition, the asynchronous layer can manage the interaction with
hardware-specific components, such as DMA, memory management, and I/O device
registers.
The use of synchronous I/O can also simplify programming, and may improve performance
on multi-processor platforms. For example, long-duration data transfers, such as
downloading a large medical image from a hierarchical storage management system
[PHS96], can be simplified and performed efficiently using synchronous I/O. In particular,
one processor can be dedicated to the thread that is transferring the data. This enables the
instruction and data cache of that CPU to be associated with the entire image transfer
operation.
Separation of concerns. Synchronization policies in each layer are decoupled. Each layer
therefore need not use the same concurrency control strategies. In the single-threaded BSD
UNIX kernel, for example, the asynchronous service layer implements synchronization via
low-level mechanisms, such as raising and lowering CPU interrupt levels. In contrast,
application processes in the synchronous service layer implement synchronization via
higher-level mechanisms, such as monitor objects (399) and synchronized message queues.
Legacy libraries, such as X Windows and older RPC toolkits, are often not re-entrant.
Multiple threads of control cannot therefore invoke these library functions concurrently
within incurring race conditions. To improve performance or to take advantage of
multiple CPUs, however, it may be necessary to perform bulk data transfers or database
queries in separate threads. In this case, the Half-Sync/Half-Reactive variant of the Half Sync/Half-Async pattern can be applied to decouple the single-threaded portions of an
application from its multi-threaded portions.
For example, an application's X Windows GUI processing could run under the control of
a reactor. Similarly, long data transfers could run under the control of an active object
thread pool. By decoupling the synchronization policies in each layer of the application
via the Half-Sync/Half-Async pattern, non-re-entrant functions can continue to work
correctly without requiring changes to existing code.
Centralization of inter-layer communication. Inter-layer communication is centralized at a
single access point, because all interaction is mediated by the queueing layer. The queueing
layer buffers messages passed between the other two layers. This eliminates the
complexities of locking and serialization that would otherwise be necessary if the
synchronous and asynchronous service layers accessed objects in each other's memory
directly.
The Half-Sync/Half-Async pattern also has the following liabilities:
A boundary-crossing penalty may be incurred from context switching, synchronization, and
data copying overhead when data is transferred between the synchronous and
asynchronous service layers via the queueing layer. For example, most operating systems
implement the Half-Sync/Half-Async pattern by placing the queueing layer at the boundary
between the user-level and kernel-level protection domains. A significant performance
penalty can be incurred when crossing this boundary [HP91].
One way of reducing this overhead is to share a region of memory between the synchronous
service layer and the asynchronous service layer [DP93]. This 'zero-copy' design allows the
two layers to exchange data directly, without copying data into and out of the queueing layer.
[CP95] presents a set of extensions to the BSD UNIX I/O subsystem that minimizes
boundary-crossing penalties by using polled interrupts to improve the handling of
continuous media I/O streams. This approach defines a buffer management system that
allows efficient page re-mapping and shared memory mechanisms to be used between
application processes, the kernel, and its devices.
Higher-level application services may not benefit from the efficiency of asynchronous I/O.
Depending on the design of operating system or application framework interfaces, it may not
be possible for higher-level services to use low-level asynchronous I/O devices effectively.
The BSD UNIX operating system, for example, prevents applications from using certain
types of hardware efficiently, even if external sources of I/O support asynchronous
overlapping of computation and communication.
Complexity of debugging and testing. Applications written using the Half-Sync/Half-Async
pattern can incur the same debugging and testing challenges described in Consequences
sections of the Proactor (215) and Reactor (179) patterns.
The Proactor pattern (215) can be viewed as an extension of the Half-Sync/Half-Async
pattern that propagates asynchronous control and data operations all the way up to higher level services. In general, the Proactor pattern should be applied if an operating system
platform supports asynchronous I/O efficiently and application developers are comfortable
with the asynchronous I/O programming model.
The Reactor pattern (179) can be used in conjunction with the Active Object pattern (369) to
implement the Half-Sync/Half-Reactive variant of the Half-Sync/Half-Async pattern. Similarly,
the Leader/Followers (447) pattern can be used in lieu of the Half-Sync/Half-Async pattern if
there is no need for a queueing layer between the asynchronous and synchronous layers.
The Pipes and Filters pattern [POSA1] describes several general principles for implementing
producer-consumer communication between components in a software system. Certain
configurations of the Half-Sync/Half-Async pattern can therefore be viewed as instances of
the Pipes and Filters pattern, where filters contain entire layers of many finer-grained
services. Moreover, a filter could contain active objects, which could yield the Half-Sync/Half Reactive or Half-Sync/Half-Sync variants.
The Layers [POSA1] pattern describes the general principle of separating services into
separate layers. The Half-Sync/Half-Async pattern can thus be seen as a specialization of
the Layers pattern whose purpose is to separate synchronous processing from
asynchronous processing in a concurrent system by introducing two designated layers for
each type of service.
##%%&&
The Leader/Followers architectural pattern provides an efficient concurrency model where
multiple threads take turns sharing a set of event sources in order to detect, demultiplex,
dispatch, and process service requests that occur on the event sources.
Consider the design of a multi-tier, high-volume, on-line transaction processing (OLTP)
system [GR93]. In this design, front-end communication servers route transaction requests
from remote clients, such as travel agents, claims processing centers, or point-of-sales
terminals, to back-end database servers that process the requests transactionally. After a
transaction commits, the database server returns its results to the associated communication
server, which then forwards the results back to the originating remote client. This multi-tier
architecture is used to improve overall system throughput and reliability via load balancing
and redundancy, respectively. It also relieves back-end servers from the burden of managing
different communication protocols with remote clients.
One way to implement OLTP servers is to use a single-threaded event processing model
based on the Reactor pattern (179). However, this model serializes event processing, which
degrades the overall server performance when handling long-running or blocking client
request events. Likewise, single-threaded servers cannot benefit transparently from multi processor platforms.
A common strategy for improving OLTP server performance is to use a multi-threaded
concurrency model that processes requests from different clients and corresponding results
simultaneously [HPS99]. For example, we could multi-thread an OLTP back-end server by
creating a thread pool based on the Half-Sync/Half-Reactive variant of the Half-Sync/Half Async pattern (423). In this design, the OLTP back-end server contains a dedicated network
I/O thread that uses the select() [Ste98] event demultiplexer to wait for events to occur on
a set of socket handles connected to front-end communication servers.
When activity occurs on handles in the set, select() returns control to the network I/O
thread and indicates which socket handles in the set have events pending. The I/O thread
then reads the transaction requests from the socket handles, stores them into dynamically
allocated requests, and inserts these requests into a synchronized message queue
implemented using the Monitor Object pattern (399). This message queue is serviced by a
pool of worker threads. When a worker thread in the pool is available, it removes a request
from the queue, performs the designated transaction, and then returns a response to the
front-end communication server.
Although the threading model described above is used in many concurrent applications, it
can incur excessive overhead when used for high-volume servers, such as those in our
OLTP example. For instance, even with a light workload, the Half-Sync/Half-Reactive thread
pool design will incur a dynamic memory allocation, multiple synchronization operations, and
a context switch to pass a request message between the network I/O thread and a worker
thread. These overheads make even the best-case latency unnecessarily high [PRS+99].
Moreover, if the OLTP back-end server is run on a multi-processor, significant overhead can
occur from processor cache coherency protocols required to transfer requests between
threads [SKT96].
If the OLTP back-end servers run on an operating system platform that supports
asynchronous I/O efficiently, the Half-Sync/Half-Reactive thread pool can be replaced with a
purely asynchronous thread pool based on the Proactor pattern (215). This alternative will
reduce much of the synchronization, context switching, and cache coherency overhead
outlined above by eliminating the network I/O thread. Unfortunately, many operating systems
do not support asynchronous I/O and those that do often support it inefficiently.[8] Yet, it is
essential that high-volume OLTP servers demultiplex requests efficiently to threads that can
process the results concurrently.
An event-driven application where multiple service requests arriving on a set of event
sources must be processed efficiently by multiple threads that share the event sources.
Multi-threading is a common technique to implement applications that process multiple
events concurrently. However, it is hard to implement high-performance multi-threaded
server applications. These applications often process a high volume of multiple types of
events, such as CONNECT, READ, and WRITE events in our OLTP example, that arrive
simultaneously. To address this problem effectively, three forces must be resolved:
??Service requests can arrive from multiple event sources, such as multiple TCP/IP
socket handles [Ste98], that are allocated for each connected client. A key design force,
therefore, is determining efficient demultiplexing associations between threads and
event sources. In particular, associating a thread for each event source may be
infeasible due to the scalability limitations of applications or the underlying operating
system and network platforms.
?For our OLTP server applications, it may not be practical to associate a separate
thread with each socket handle. In particular, as the number of connections
increase significantly, this design may not scale efficiently on many operating
system platforms.
??To maximize performance, key sources of concurrency-related overhead, such as
context switching, synchronization, and cache coherency management, must be
minimized. In particular, concurrency models that allocate memory dynamically for each
request passed between multiple threads will incur significant overhead on conventional
multi-processor operating systems [SchSu95].
?Implementing our OLTP servers using the Half-Sync/Half-Reactive thread pool variant
(423) outlined in the Example section requires memory to be allocated dynamically
in the network I/O thread to store incoming transaction requests into the message
queue. This design incurs numerous synchronizations and context switches to
insert the request into, or remove the request from, the message queue, as
illustrated in the Monitor Object pattern (399).
??Multiple threads that demultiplex events on a shared set of event sources must
coordinate to prevent race conditions. Race conditions can occur if multiple threads try
to access or modify certain types of event sources simultaneously.
?For instance, a pool of threads cannot use select() concurrently to demultiplex a set
of socket handles because the operating system will erroneously notify more than
one thread calling select() when I/O events are pending on the same set of
socket handles [Ste98]. Moreover, for bytestream-oriented protocols, such as TCP,
having multiple threads invoking read() or write() on the same socket handle
will corrupt or lose data.
Structure a pool of threads to share a set of event sources efficiently by taking turns
demultiplexing events that arrive on these event sources and synchronously dispatching the
events to application services that process them.
In detail: design a thread pool mechanism that allows multiple threads to coordinate
themselves and protect critical sections while detecting, demultiplexing, dispatching, and
processing events. In this mechanism, allow one thread at a time¡ªthe leader¡ªto wait for an
event to occur on a set of event sources. Meanwhile, other threads¡ªthe followers¡ªcan
queue up waiting their turn to become the leader. After the current leader thread detects an
event from the event source set, it first promotes a follower thread to become the new
leader. It then plays the role of a processing thread, which demultiplexes and dispatches the
event to a designated event handler that performs application-specific event handling in the
processing thread. Multiple processing threads can handle events concurrently while the
current leader thread waits for new events on the set of event sources shared by the
threads. After handling its event, a processing thread reverts to a follower role and waits to
become the leader thread again.
There are four key participants in the Leader/Followers pattern:
Handles are provided by operating systems to identify event sources, such as network
connections or open files, that can generate and queue events. Events can originate from
external sources, such as CONNECT events or READ events sent to a service from clients, or
internal sources, such as time-outs. A handle set is a collection of handles that can be used
to wait for one or more events to occur on handles in the set. A handle set returns to its
caller when it is possible to initiate an operation on a handle in the set without the operation
blocking.
OLTP servers are interested in two types of events¡ªCONNECT events and READ
events¡ªwhich represent incoming connections and transaction requests, respectively.
Both front-end and back-end servers maintain a separate connection for each client,
where clients of front-end servers are the so-called 'remote' clients and front-end servers
themselves are clients of back-end servers. Each connection is a source of events that
is represented in a server by a separate socket handle. Our OLTP servers use the
select() event demultiplexer, which identifies handles whose event sources have
pending events, so that applications can invoke I/O operations on these handles without
blocking the calling threads.
An event handler specifies an interface consisting of one or more hook methods [Pree95]
[GoF95]. These methods represent the set of operations available to process application specific events that occur on handle(s) serviced by an event handler.
Concrete event handlers specialize the event handler and implement a specific service that
the application offers. In particular, concrete event handlers implement the hook method(s)
responsible for processing events received from a handle.
For example, concrete event handlers in OLTP front-end communication servers receive
and validate remote client requests, and then forward requests to back-end database
servers. Likewise, concrete event handlers in back-end database servers receive
transaction requests from front-end servers, read/write the appropriate database records
to perform the transactions, and return the results to the front-end servers. All network
I/O operations are performed via socket handles, which identify various sources of
events.
At the heart of the Leader/Followers pattern is a thread pool, which is a group of threads that
share a synchronizer, such as a semaphore or condition variable, and implement a protocol
for coordinating their transition between various roles. One or more threads play the follower
role and queue up on the thread pool synchronizer waiting to play the leader role. One of
these threads is selected to be the leader, which waits for an event to occur on any handle in
its handle set. When an event occurs, the current leader thread promotes a follower thread
to become the new leader. The original leader then concurrently plays the role of a
processing thread, which demultiplexes that event from the handle set to an appropriate
event handler and dispatches the handler's hook method to handle the event. After a
processing thread is finished handling an event, it returns to playing the role of a follower
thread and waits on the thread pool synchronizer for its turn to become the leader thread
again.
Each OLTP server designed using the Leader/Followers pattern can have a pool of
threads waiting to process transaction requests that arrive on event sources identified by
a handle set. At any point in time, multiple threads in the pool can be processing
transaction requests and sending results back to their clients. One thread in the pool is
the current leader, which waits for a new CONNECT or READ event to arrive on the handle
set shared by the threads. When this occurs, the leader thread becomes a processing
thread and handles the event, while one of the follower threads in the pool is promoted
to become the new leader.
The following class diagram illustrates the structure of participants in the Leader/Followers
pattern. In this structure, multiple threads share the same instances of thread pool, event
handler, and handle set participants. The thread pool ensures the correct and efficient
coordination of the threads:
The collaborations in the Leader/Followers pattern divide into four phases:
??Leader thread demultiplexing. The leader thread waits for an event to occur on any
handle in the handle set. If there is no current leader thread, for example, due to events
arriving faster than the available threads can service them, the underlying operating
system can queue events internally until a leader thread is available.
??Follower thread promotion. After the leader thread has detected a new event, it uses
the thread pool to choose a follower thread to become the new leader.
??Event handler demultiplexing and event processing. After helping to promote a follower
thread to become the new leader, the former leader thread then plays the role of a
processing thread. This thread concurrently demultiplexes the event it detected to the
event's associated handler and then dispatches the handler's hook method to process
the event. A processing thread can execute concurrently with the leader thread and any
other threads that are in the processing state.
??Rejoining the thread pool. After the processing thread has run its event handling to
completion, it can rejoin the thread pool and wait to process another event. A
processing thread can become the leader immediately if there is no current leader
thread. Otherwise, the processing thread returns to playing the role of a follower thread
and waits on the thread pool synchronizer until it is promoted by a leader thread.
A thread's transitions between states can be visualized in the following diagram:
Six activities can be used to implement the Leader/Followers pattern:
1. Choose the handle and handle set mechanisms. A handle set is a collection of handles
that a leader thread can use to wait for an event to occur on a set of event sources.
Developers often choose the handles and handle set mechanisms provided by the
underlying operating system, rather than implementing them from scratch. Four sub activities help with choosing the handle and handle set mechanisms:
1. Determine the type of handles. There are two general types of handles:
??Concurrent handles. This type allows multiple threads to access a
handle to an event source concurrently without incurring race conditions
that can corrupt, lose, or scramble the data [Ste98]. For instance, the
Socket API for record-oriented protocols, such as UDP, allows multiple
threads to invoke read() or write() operations on the same handle
concurrently.
??Iterative handles. This type requires multiple threads to access a
handle to an event source iteratively because concurrent access will incur
race conditions. For instance, the Socket API for bytestream-oriented
protocols, such as TCP, does not guarantee that read() or write()
operations respect application-level message boundaries. Thus,
corrupted or lost data can result if I/O operations on the Socket are not
serialized properly.
2. Determine the type of handle set. There are two general types of handle sets:
??Concurrent handle set. This type can be acted upon concurrently, for
example, by a pool of threads. Each time it becomes possible to initiate
an operation on a handle in the set without blocking the operation, a
concurrent handle set returns that handle to one of its calling threads. For
example, the Win32 WaitForMultipleObjects() function [Sol98]
supports concurrent handle sets by allowing a pool of threads to wait on
the same set of handles simultaneously.
??Iterative handle set. This type returns to its caller when it is possible to
initiate an operation on one or more handles in the set without the
operation(s) blocking. Although an iterative handle set can return multiple
handles in a single call, it can only be called by one thread at a time. For
example, the select() [Ste98] and poll() [Rago93] functions support
iterative handle sets. Thus, a pool of threads cannot use select() or
poll() to demultiplex events on the same handle set concurrently
because multiple threads can be notified that the same I/O events are
pending, which elicits erroneous behavior.
The following table summarizes representative examples for each combination
of concurrent and iterative handles and handle sets:
3. Determine the consequences of selecting certain handle and handle set
mechanisms. In general, the Leader/Followers pattern is used to prevent
multiple threads from corrupting or losing data erroneously, such as invoking
read operations on a shared TCP byte stream socket handle concurrently or
invoking select() on a shared handle set concurrently. However, some
applications need not guard against these problems. In particular, if the handle
and handle set mechanisms are both concurrent, many of the subsequent
implementation activities can be skipped.
As discussed in implementation activities 1.1 (456) and 1.2 (457), the
semantics of certain combinations of protocols and network programming APIs
support concurrent multiple I/O operations on a shared handle. For example,
UDP support in the Socket API ensures a complete message is always read or
written by one thread or another, without the risk of a partial read() or of data
corruption from an interleaved write(). Likewise, certain handle set
mechanisms, such as the Win32 WaitForMultipleObjects() function
[Sol98], return a single handle per call, which allows them to be called
concurrently by a pool of threads.[9]
In these situations, it may be possible to implement the Leader/Followers
pattern by simply using the operating system's thread scheduler to
(de)multiplex threads, handle sets, and handles robustly, in which case,
implementation activities 2 through 6 can be skipped.
4. Implement an event handler demultiplexing mechanism. In addition to calling
an event demultiplexer to wait for one or more events to occur on its handle
set, such as select(), a Leader/Followers pattern implementation must
demultiplex events to event handlers and dispatch their hook methods to
process the events. In general, two alternative strategies can be used to
implement this mechanism:
??Program to a low-level operating system event demultiplexing
mechanism. In this strategy, the handle set demultiplexing mechanisms
provided by the operating system are used directly. Thus, a
Leader/Followers implementation must maintain a demultiplexing table
that is a manager [Som97] containing a set of <handle, event handler,
event types> tuples. Each handle serves as a 'key' that associates
handles with event handlers in its demultiplexing table, which also stores
the type of event(s), such as CONNECT and READ, that each event handler
will process. The contents of this table are converted into handle sets
passed to the native event demultiplexing mechanism, such as
select() [Ste98] or WaitForMultipleObjects() [Sol98].
??Implementation activity 3.3 of the Reactor pattern (179) illustrates how
to implement a demultiplexing table.
??Program to a higher-level event demultiplexing pattern. In this
strategy, developers leverage higher-level patterns, such as Reactor
(179), Proactor (215), and Wrapper Facade (47). These patterns help to
simplify the Leader/Followers implementation and reduce the effort
needed to address the accidental complexities of programming to native
operating system handle set demultiplexing mechanisms directly.
Moreover, applying higher-level patterns makes it easier to decouple the
I/O and demultiplexing aspects of a system from its concurrency model,
thereby reducing code duplication and maintenance effort.
??In our OLTP server example, an event must be demultiplexed to the
concrete event handler associated with the socket handle that
received the event. The Reactor pattern (179) supports this activity,
therefore it can be applied to simplify the implementation of the
Leader/Followers pattern. In the context of the Leader/Followers
pattern, however, a reactor demultiplexes just one handle at a time to
its associated concrete event handler, regardless of how many
handles have events pending on them. Demultiplexing only one
handle at a time can maximize the concurrency among a pool of
threads and simplify a Leader/Followers pattern implementation by
alleviating its need to manage a separate queue of pending events.
2. Implement a protocol for temporarily (de)activating handles in a handle set. When an
event arrives, the leader thread performs three steps:
??It deactivates the handle from consideration in the handle set temporarily
??It promotes a follower thread to become the new leader and
??It continues to process the event.
Deactivating the handle from the handle set avoids race conditions that could occur
between the time when a new leader is selected and the event is processed. If the
new leader waits on the same handle in the handle set during this interval, it could
demultiplex the event a second time, which is erroneous because the dispatch is
already in progress. After the event is processed, the handle is reactivated in the
handle set, which allows the leader thread to wait for an event to occur on it or any
other activated handles in the set.
In our OLTP example, a handle deactivation and reactivation protocol can be
provided by extending the Reactor interface defined in implementation activity 2
of the Reactor pattern (179):
Implement the thread pool. To promote a follower thread to the leader role, as well as
to determine which thread is the current leader, an implementation of the
Leader/Followers pattern must manage a pool of threads. A straightforward way to
implement this is to have all the follower threads in the set simply wait on a single
synchronizer, such as a semaphore or condition variable. In this design, it does not
matter which thread processes an event, as long as all threads in the pool that share
the handle set are serialized.
4. For example, the LF_Thread_Pool class shown below can be used for the
back-end database servers in our OLTP example:
The constructor of LF_Thread_Pool caches the reactor passed to it. By default,
this reactor implementation uses select(), which supports iterative handle
sets. Therefore, LF_Thread_Pool is responsible for serializing multiple threads
that take turns calling select() on the reactor's handle set.
36. Application threads invoke join() to wait on a handle set and demultiplex new
events to their associated event handlers. As shown in implementation activity 4
(462), this method does not return to its caller until the application terminates or
join() times out. The promote_new_leader() method promotes one of the
follower threads in the set to become the new leader, as shown in implementation
activity 5.2 (464).
37. The deactivate_handle() method and the reactivate_handle() method
deactivate and reactivate handles within a reactor's handle set. The
implementations of these methods simply forward to the same methods defined
in the Reactor interface shown in implementation activity 2 (459).
38. Note that a single condition variable synchronizer followers_condition_ is
shared by all threads in this thread pool. As shown in implementation activities 4
(462) and 5 (463), the implementation of LF_Thread_Pool uses the Monitor
Object pattern (399).
40. Implement a protocol to allow threads to initially join (and later rejoin) the thread pool.
This protocol is used in the following two cases:
??After the initial creation of a pool of threads that retrieve and process events;
and
??After a processing thread completes and is available to handle another event.
If no leader thread is available, a processing thread can become the leader
immediately. If a leader thread is already available, a thread can become a follower
by waiting on the thread pool's synchronizer.
Our back-end database servers can implement the following join() method of
the LF_Thread_Pool to wait on a handle set and demultiplex new events to
their associated event handlers:
Within the for loop, the calling thread alternates between its role as a leader,
processing, and follower thread. In the first part of this loop, the thread waits
until it can be a leader, at which point it uses the reactor to wait for an event on
the shared handle set. When the reactor detects an event on a handle, it will
demultiplex the event to its associated event handler and dispatch its
handle_event() method to promote a new leader and process the event. After
the reactor demultiplexes one event, the thread re-assumes its follower role.
These steps continue looping until the application terminates or a timeout occurs.
41. Implement the follower promotion protocol. Immediately after a leader thread detects
an event, but before it demultiplexes the event to its event handler and processes the
event, it must promote a follower thread to become the new leader. Two sub-activities
can be used to implement this protocol:
0. Implement the handle set synchronization protocol. If the handle set is iterative
and we blindly promote a new leader thread, it is possible that the new leader
thread will attempt to handle the same event that was detected by the previous
leader thread that is in the midst of processing the event. To avoid this race
condition, we must remove the handle from consideration in the handle set
before promoting a follower to new leader and dispatching the event to its
concrete event handler. The handle must be reactivated in the handle set after
the event has been dispatched and processed.
1. An application can implement concrete event handlers that subclass from
the Event_Handler class defined in implementation activity 1.2 of the
Reactor pattern (179). Likewise, the Leader/Followers implementation can
use the Decorator pattern [GoF95] to create an LF_Event_Handler
class that decorates Event_Handler. This decorator promotes a new
leader thread and activates/deactivates the handler in the reactor's handle
set transparently to the concrete event handlers.
Determine the promotion protocol ordering. Several ordering strategies can be
used to determine which follower thread to promote:
??LIFO order. In many applications, it does not matter which of the
follower threads is promoted next because all threads are equivalent
peers. In this case, the leader thread can promote follower threads in last in, first-out (LIFO) order. The LIFO protocol maximizes CPU cache affinity
[SKT96] [MB91] by ensuring that the thread waiting the shortest time is
promoted first [Sol98], which is an example of the Fresh Work Before
Stale pattern [Mes96].
Cache affinity can improve system performance if the thread that blocked
most recently executes essentially the same code and data when it is
scheduled to run again. Implementing a LIFO promotion protocol requires
an additional data structure, however, such as a stack of waiting threads,
rather than just using a native operating system synchronization object,
such as a semaphore.
??Priority order. In some applications, particularly real-time applications,
threads may run at different priorities. In this case, therefore, it may be
necessary to promote follower threads according to their priority. This
protocol can be implemented using some type of priority queue, such as a
heap [BaLee98]. Although this protocol is more complex than the LIFO
protocol, it may be necessary to promote follower threads according to
their priorities in order to minimize priority inversion [SMFG00].
??Implementation-defined order. This ordering is most common when
implementing handle sets using operating system synchronizers, such as
semaphores or condition variables, which often dispatch waiting threads
in an implementation-defined order. The advantage of this protocol is that
it maps onto native operating system synchronizers efficiently.
??Our OLTP back-end database servers could use the following simple
protocol to promote follower thread in whatever order they are
queued by a native operating system condition variable:
As shown in implementation activity 5.1 (463), the
promote_new_leader() method is invoked by a
LF_Event_Handler decorator before it forwards to the concrete
event handler that processes an event.
42. Implement the event handlers. Application developers must decide what actions to
perform when the hook method of a concrete event handler is invoked by a
processing thread in the Leader/Followers pattern implementation. Implementation
activity 5 in the Reactor pattern (179) describes various issues associated with
implementing concrete event handlers.
The OLTP back-end database servers described in the Example section can use the
Leader/Followers pattern to implement a thread pool that demultiplexes I/O events from
socket handles to their event handlers efficiently. In this design, there is no designated
network I/O thread. Instead, a pool of threads is pre-allocated during database server
initialization:
These threads are not bound to any particular socket handle. Thus, all threads in this pool
take turns playing the role of a network I/O thread by invoking the LF
Thread_Pool::join() method:
As shown in implementation activity 4 (462), the join() method allows only the leader
thread to use the Reactor singleton to select() on a shared handle set of Sockets
connected to OLTP front-end communication servers. If requests arrive when all threads are
busy, they will be queued in socket handles until threads in the pool are available to execute
the requests.
When a request event arrives, the leader thread deactivates the socket handle temporarily
from consideration in select()'s handle set, promotes a follower thread to become the
new leader, and continues to handle the request event as a processing thread. This
processing thread then reads the request into a buffer that resides in the runtime stack or is
allocated using the Thread-Specific Storage pattern (475).[10] All OLTP activities occur in the
processing thread. Thus, no further context switching, synchronization, or data movement is
necessary until the processing completes. When it finishes handling a request, the
processing thread returns to playing the role of a follower and waits on the synchronizer in
the thread pool. Moreover, the socket handle it was processing is reactivated in the
Reactor singleton's handle set so that select() can wait for I/O events to occur on it,
along with other Sockets in the handle set.
Bound Handle/Thread Associations. The earlier sections in this pattern describe unbound
handle/thread associations, where there is no fixed association between threads and
handles. Thus, any thread can process any event that occurs on any handle in a handle set.
Unbound associations are often used when a pool of worker threads take turns
demultiplexing a shared handle set.
A variant of the Leader/Followers pattern uses bound handle/thread associations. In this
variant, each thread is bound to its own handle, which it uses to process particular events.
Bound associations are often used in the client-side of an application when a thread waits on
a socket handle for a response to a two-way request it sent to a server. In this case, the
client application thread expects to process the response event on this handle in the same
thread that sent the original request.
In the bound handle/thread association variant, therefore, the leader thread in the thread
pool may need to hand-off an event to a follower thread if the leader does not have the
necessary context to process the event. After the leader detects a new event, it checks the
handle associated with the event to determine which thread is responsible for processing it.
396
If the leader thread discovers that it is responsible for the event, it promotes a follower thread
to become the new leader Conversely, if the event is intended for another thread, the leader
must hand-off the event to the designated follower thread. This follower thread can then
temporally disable the handle and process the event. Meanwhile, the current leader thread
continues to wait for another event to occur on the handle set.
The following diagram illustrates the additional transition between the following state and the
processing state:
The leader/follower thread pool can be maintained implicitly, for example, using a
synchronizer, such as a semaphore or condition variable, or explicitly, using a container and
the Manager pattern [Som97]. The choice depends largely on whether the leader thread
must notify a specific follower thread explicitly to perform event hand-offs.
A detailed discussion of the bounded handle/thread association variant and its
implementation appears in [SRPKB00].
Relaxing Serialization Constraints. There are operating systems where multiple leader
threads can wait on a handle set simultaneously. For example, the Win32 function
WaitForMultipleObjects() [Sol98] supports concurrent handle sets that allow a pool of
threads to wait on the same set of handles concurrently. Thus, a thread pool designed using
this function can take advantage of multi-processor hardware to handle multiple events
concurrently while other threads wait for events.
Two variations of the Leader/Followers pattern can be applied to allow multiple leader
threads to be active simultaneously:
??Leader/followers per multiple handle sets. This variation applies the conventional
Leader/Followers implementation to multiple handle sets separately. For instance, each
thread is assigned a designated handle set. This variation is particularly useful in
applications where multiple handle sets are available. However, this variant limits a
thread to use a specific handle set.
??Multiple leaders and multiple followers. In this variation, the pattern is extended to
support multiple simultaneous leader threads, where any of the leader threads can wait
on any handle set. When a thread re-joins the thread pool it checks if a leader is
associated with every handle set already. If there is a handle set without a leader, the
re-joining thread can become the leader of that handle set immediately.
Hybrid Thread Associations. Some applications use hybrid designs that implement both
bound and unbound handle/thread associations simultaneously. Likewise, some handles in
an application may have dedicated threads to handle certain events, whereas other handles
can be processed by any thread. Thus, one variant of the Leader/Follower pattern uses its
event hand-off mechanism to notify certain subsets of threads, according to the handle on
which event activity occurs.
For example, the OLTP front-end communication server may have multiple threads
using the Leader/Followers pattern to wait for new request events from clients. Likewise,
it may also have threads waiting for responses to requests they invoked on back-end
servers. In fact, threads play both roles over their lifetime, starting as threads that
dispatch new incoming requests, then issuing requests to the back-end servers to satisfy
the client application requirements, and finally waiting for responses to arrive from the
back-end server.
Hybrid Client/Servers. In complex systems, where peer applications play both client and
server roles, it is important that the communication infrastructure processes incoming
requests while waiting for one or more replies. Otherwise, the system can deadlock because
one client dedicates all its threads to block waiting for responses.
In this variant, the binding of threads and handles changes dynamically. For example, a
thread may be unbound initially, yet while processing an incoming request the application
discovers it requires a service provided by another peer in the distributed system. In this
case, the unbound thread dispatches a new request while executing application code,
effectively binding itself to the handle used to send the request. Later, when the response
arrives and the thread completes the original request, it becomes unbound again.
Alternative Event Sources and Sinks. Consider a system where events are obtained not only
through handles but also from other sources, such as shared memory or message queues.
For example, in UNIX there are no event demultiplexing functions that can wait for I/O
events, semaphore events, and/or message queue events simultaneously. However, a
thread can either block waiting for one type of event at the same time. Thus, the
Leader/Followers pattern can be extended to wait for more than one type of events
simultaneously:
??A leader thread is assigned to each source of events¡ªas opposed to a single leader
thread for the complete system.
??After the event is received, but before processing the event, a leader thread can select
any follower thread to wait on this event source.
A drawback with this variant, however, is that the number of participating threads must
always be greater than the number of event sources. Therefore, this approach may not scale
well as the number of event sources grows.
ACE Thread Pool Reactor framework [Sch97]. The ACE framework provides an object oriented framework implementation of the Leader/Followers pattern called the 'thread pool
reactor' (ACE_TP_Reactor) that demultiplexes events to event handlers within a pool of
threads. When using a thread pool reactor, an application pre-spawns a fixed number of
threads. When these threads invoke the ACE_TP_Reactor's handle_events() method,
one thread will become the leader and wait for an event. Threads are considered unbound
by the ACE thread pool reactor framework. Thus, after the leader thread detects the event, it
promotes an arbitrary thread to become the next leader and then demultiplexes the event to
its associated event handler.
CORBA ORBs and Web servers. Many CORBA implementations, including Chorus COOL
ORB [SMFG00] and TAO [SC99], use the Leader/Followers pattern for both their client-side
connection model and the server-side concurrency model. In addition, The JAWS Web
server [HPS99] uses the Leader/Followers thread pool model for operating system platforms
that do not allow multiple threads to simultaneously call accept() on a passive-mode
socket handle.
Transaction monitors. Popular transaction monitors, such as Tuxedo, operate traditionally
on a per-process basis, for example, transactions are always associated with a process.
Contemporary OLTP systems demand high-performance and scalability, however, and
performing transactions on a per-process basis may fail to meet these requirements.
Therefore, next-generation transaction services, such as implementations of the CORBA
Transaction Service [OMG97b], employ bound Leader/Followers associations between
threads and transactions.
Taxi stands. The Leader/Followers pattern is used in everyday life to organize many airport
taxi stands. In this use case, taxi cabs play the role of the 'threads,' with the first taxi cab in
line being the leader and the remaining taxi cabs being the followers. Likewise, passengers
arriving at the taxi stand constitute the events that must be demultiplexed to the cabs,
typically in FIFO order. In general, if any taxi cab can service any passenger, this scenario is
equivalent to the unbound handle/thread association described in the main Implementation
section. However, if only certain cabs can service certain passengers, this scenario is
equivalent to the bound handle/thread association described in the Variants section.
The Leader/Followers pattern provides several benefits:
Performance enhancements. Compared with the Half-Sync/Half-Reactive thread pool
approach described in the Example section, the Leader/Followers pattern can improve
performance as follows:
??It enhances CPU cache affinity and eliminates the need for dynamic memory allocation
and data buffer sharing between threads. For example, a processing thread can read
the request into buffer space allocated on its run-time stack or by using the Thread Specific Storage pattern (475) to allocate memory.
??It minimizes locking overhead by not exchanging data between threads, thereby
reducing thread synchronization. In bound handle/thread associations, the leader thread
demultiplexes the event to its event handler based on the value of the handle. The
request event is then read from the handle by the follower thread processing the event.
In unbound associations, the leader thread itself reads the request event from the
handle and processes it.
??It can minimize priority inversion because no extra queueing is introduced in the
server. When combined with real-time I/O subsystems [KSL99], the Leader/Followers
thread pool model can reduce sources of non-determinism in server request processing
significantly.
??It does not require a context switch to handle each event, reducing the event
dispatching latency. Note that promoting a follower thread to fulfill the leader role does
require a context switch. If two events arrive simultaneously this increases the
dispatching latency for the second event, but the performance is no worse than Half Sync/Half-Reactive thread pool implementations.
Programming simplicity. The Leader/Follower pattern simplifies the programming of
concurrency models where multiple threads can receive requests, process responses, and
demultiplex connections using a shared handle set.
However, the Leader/Followers pattern has the following liabilities:
Implementation complexity. The advanced variants of the Leader/Followers pattern are
harder to implement than Half-Sync/Half-Reactive thread pools. In particular, when used as
a multi-threaded connection multiplexer, the Leader/Followers pattern must maintain a pool
of follower threads waiting to process requests. This set must be updated when a follower
thread is promoted to a leader and when a thread rejoins the pool of follower threads. All
these operations can happen concurrently, in an unpredictable order. Thus, the
Leader/Follower pattern implementation must be efficient, while ensuring operation
atomicity.
Lack of flexibility. Thread pool models based on the Half-Sync/Half-Reactive variant of the
Half-Sync/Half-Async pattern (423) allow events in the queueing layer to be discarded or re prioritized. Similarly, the system can maintain multiple separate queues serviced by threads
at different priorities to reduce contention and priority inversion between events at different
priorities. In the Leader/Followers model, however, it is harder to discard or reorder events
because there is no explicit queue. One way to provide this functionality is to offer different
levels of service by using multiple Leader/Followers groups in the application, each one
serviced by threads at different priorities.
Network I/O bottlenecks. The Leader/Followers pattern, as described in the Implementation
section, serializes processing by allowing only a single thread at a time to wait on the handle
set. In some environments, this design could become a bottleneck because only one thread
at a time can demultiplex I/O events. In practice, however, this may not be a problem
because most of the I/O-intensive processing is performed by the operating system kernel.
Thus, application-level I/O operations can be performed rapidly.
The Reactor pattern (179) often forms the core of Leader/Followers pattern implementations.
However, the Reactor pattern can be used in lieu of the Leader/Followers pattern when each
event only requires a short amount of time to process. In this case, the additional scheduling
complexity of the Leader/Followers pattern is unnecessary.
The Proactor pattern (215) defines another model for demultiplexing asynchronous event
completions concurrently. It can be used instead of the Leader/Followers pattern:
??When an operating system supports asynchronous I/O efficiently and
??When programmers are comfortable with the asynchronous inversion of control
associated with the Proactor pattern
The Half-Sync/Half-Async (423) and Active Object (369) patterns are two other alternatives
to the Leader/Followers pattern. These patterns may be a more appropriate choice than the
Leader/Followers pattern:
??When there are additional synchronization or ordering constraints that must be
addressed by reordering requests in a queue before they can be processed by threads
in the pool and/or
??When event sources cannot be waited for by a single event demultiplexer efficiently
The Controlled Reactor pattern [DeFe99] includes a performance manager that controls the
use of threads for event handlers according to a user's specification and may be an
alternative when controlled performance is an important objective.
##%%&&
The Thread-Specific Storage design pattern allows multiple threads to use one 'logically
global' access point to retrieve an object that is local to a thread, without incurring locking
overhead on each object access.
Consider the design of a multi-threaded network logging server that remote client
applications use to record information about their status centrally within a distributed system.
Unlike the logging server shown in the Reactor pattern example (179), which demultiplexed
all client connections iteratively within a single thread, this logging server uses a thread-per connection [Sch97] concurrency model to process requests concurrently.
In the thread-per-connection model a separate thread is created for each client connection.
Each thread reads logging records from its associated TCP Socket, processes these records
and writes them to the appropriate output device, such as a log file or a printer.
Each logging server thread is also responsible for detecting and reporting any low-level
network conditions or system errors that occur when performing I/O. Many operating
systems, such as UNIX and Windows NT, report this low-level information to applications via
a global access point, called errno. When an error or unusual condition occurs during
system calls, such as read() or write(), the operating system sets errno to indicate
what has happened and returns a specific status value, such as -1. Applications must test
for these return values and then check errno to determine what type of error or unusual
condition occurred.
Consider the following C code fragment that receives client logging records from a TCP
socket handle set to non-blocking mode [Ste98].
If recv() returns -1 the logging server logger code checks errno to determine what
happened and decide how to proceed.
Although implementing errno at global scope works reasonably well for single-threaded
applications, it can incur subtle problems for multi-threaded applications. In particular, race
conditions in preemptive multi-threaded systems can cause an errno value set in one
thread to be interpreted erroneously in other threads. If multiple threads execute the
logger() function simultaneously erroneous interactions may occur.
For example, assume that thread T1 invokes a non-blocking recv() call that returns -1 and
sets errno to EWOULDBLOCK, which indicates that no data is currently queued on the
Socket. Before T1 can check for this case, however, it is preempted and thread T2 starts
running.
Assuming that T2 is then interrupted by an asynchronous signal, such as SIGALRM, it sets
errno to EINTR. If T2 is preempted immediately because its time-slice is finished, T1 will
falsely assume its recv() call was interrupted and perform the wrong action:
One apparent solution to this problem is to apply the Wrapper Facade pattern (47) to
encapsulate errno with an object wrapper that contains a lock. The Scoped Locking idiom
(325) can then be used to acquire the lock before setting or checking errno and to release
it afterwards. Unfortunately, this design will not solve the race condition problem, because
setting and checking the global errno value is not atomic. Instead, it involves the following
two activities:
1. The recv() call sets errno.
2. The application checks errno to determine what action to take.
A more robust way to prevent race conditions is to improve the errno locking protocol. For
example, the recv() system call could acquire a global errno_lock before it sets errno.
Subsequently, when recv() returns, the application releases the errno_lock after it tests
the value of errno. This solution is error-prone, however, because applications may forget
to release errno_lock, causing starvation and deadlock. Also, because applications may
need to check the status of errno frequently, the extra locking overhead will degrade
performance significantly, particularly when an application happens to run in a single threaded configuration.
What is needed therefore is mechanism that transparently gives each thread its own local
copy of 'logically global' objects, such as errno.
Multi-threaded applications that frequently access data or objects that are logically global but
whose state should be physically local to each thread.
Multi-threaded applications can be hard to program due to the complex concurrency control
protocols needed to avoid race conditions, starvation and deadlocks [Lea99a]. Due to
locking overhead, multi-threaded applications also often perform no better than single threaded applications. In fact, they may perform worse, particularly on multi-processor
platforms [SchSu95]. Two forces can arise in concurrent programs:
??Multi-threaded applications should be both easy to program and efficient. In particular,
access to data that is logically global but physically local to a thread should be atomic
without incurring locking overhead for each access.[11]
?As described in the Example section, operating systems often implement errno as a
'logically global' variable that developers program as if it were an actual global
variable. To avoid race conditions, however, the memory used to store errno is
allocated locally, once per thread.
??Many legacy libraries and applications were written originally assuming a single thread
of control. They therefore often pass data implicitly between methods via global objects,
such as errno, rather than passing parameters explicitly. When retrofitting such code
to run in multiple threads it is often not feasible to change existing interfaces and code
in legacy applications.
?Operating systems that return error status codes implicitly in errno cannot be
changed easily to return these error codes explicitly without causing existing
applications and library components to break.
Introduce a global access point for each thread-specific object, but maintain the 'real' object
in storage that is local to each thread. Let applications manipulate these thread-specific
objects only through their global access points.
The Thread-Specific Storage pattern is composed of six participants.
A thread-specific object is an instance of an object that can be accessed only by a particular
thread.
For example, in operating systems that support multi-threaded processes, errno is an
int that has a different instance in each thread.
A thread identifies a thread-specific object using a key that is allocated by a key factory.
Keys generated by the key factory are assigned from a single range of values to ensure that
each thread-specific object is 'logically' global.
For example, a multi-threaded operating system implements errno by creating a
globally-unique key. Each thread uses this key to access its own local instance of errno
implicitly.
A thread-specific object set contains the collection of thread-specific objects that are
associated with a particular thread. Each thread has its own thread-specific object set.
Internally, this thread-specific object set defines a pair of methods, which we call set() and
get(), to map the globally-managed set of keys to the thread-specific objects stored in the
set. Clients of a thread-specific object set can obtain a pointer to a particular thread-specific
object by passing a key that identifies the object as a parameter to get(). The client can
inspect or modify the object via the pointer returned by the get() method. Similarly, clients
can add a pointer to a thread-specific object into the object set by passing the pointer to the
object and its associated key as parameters to set().
An operating system's threads library typically implements the thread-specific object set.
This set contains the errno data, among other thread-specific objects.
A thread-specific object proxy [GoF95] [POSA1] can be defined to enable clients to access a
specific type of thread-specific object as if it were an ordinary object. If proxies are not used,
clients must access thread-specific object sets directly and use keys explicitly, which is
tedious and error-prone. Each proxy instance stores a key that identifies the thread-specific
object uniquely. Thus, there is one thread-specific object per-key, per-thread.
A thread-specific object proxy exposes the same interface as its associated thread-specific
object. Internally, the interface methods of the proxy first use the set() and get() methods
provided by its thread-specific object set to obtain a pointer to the thread-specific object
designated by the key stored in the proxy. After a pointer to the appropriate thread-specific
object has been obtained, the proxy then delegates the original method call to it.
For example, errno is implemented as a preprocessor macro that plays the role of the
proxy and shields applications from thread-specific operations.
Application threads are clients that use thread-specific object proxies to access particular
thread-specific objects that reside in thread-specific storage. To an application thread, the
method appears to be invoked on an ordinary object, when in fact it is invoked on a thread specific object. Multiple application threads can use the same thread-specific object proxy to
access their unique thread-specific objects. A proxy uses the identifier of the application
thread that calls its interface methods to differentiate between the thread-specific objects it
encapsulates.
For example, the thread that runs the logger function in the Example section is an
application thread.
The following class diagram illustrates the general structure of the Thread-Specific Storage
pattern:
The participants in the Thread-Specific Storage pattern can be modeled conceptually as a
two-dimensional matrix that has one row per key and one column per thread. The matrix
entry at row k and column t yields a pointer to the corresponding thread-specific object.
Creating a key is analogous to adding a row to the matrix; creating a new thread is
analogous to adding a column.
A thread-specific object proxy works in conjunction with the thread-specific object set to
provide application threads with a type-safe mechanism to access a particular object located
at row k and column t. The key factory maintains a count of how many keys have been used.
A thread-specific object set contains the entries in one column.
Note that the model above is only an analogy. In practice, implementations of the Thread Specific Storage pattern do not use two-dimensional matrices, because keys are not
necessarily consecutive integers. The entries in the thread-specific object set may also
reside in their corresponding thread, rather than in a global two-dimensional matrix. It is
helpful to visualize the structure of the Thread-Specific Storage pattern as a two-dimensional
matrix, however. We therefore refer to this metaphor in the following sections.
There are two general scenarios in the Thread-Specific Storage pattern: creating and
accessing a thread-specific object. Scenario I describes the creation of a thread-specific
object:
??An application thread invokes a method defined in the interface of a thread-specific
object proxy.
??If the proxy does not yet have an associated key it asks the key factory to create a new
key. This key identifies the associated thread-specific object uniquely in each thread's
object set. The proxy then stores the key, to optimize subsequent method invocations
by application threads.
??The thread-specific object proxy creates a new object dynamically. It then uses the
thread-specific object set's set() method to store a pointer to this object in the location
designated by the key.
??The method that was invoked by the application thread is then executed, as shown in
Scenario II.
Scenario II describes how an application thread accesses an existing thread-specific object:
??An application thread invokes a method on a thread-specific object proxy.
??The thread-specific object proxy passes its stored key to the get() method of the
application thread's thread-specific object set. It then retrieves a pointer to the
corresponding thread-specific object.
??The proxy uses this pointer to delegate the original method call to the thread-specific
object. Note that no locking is necessary, because the object is referenced through a
pointer that is accessed only within the client application thread itself.
Implementing the Thread-Specific Storage pattern centers on implementing thread-specific
object sets and thread-specific object proxies. These two components create the
mechanisms for managing and accessing objects residing in thread-specific storage. We
therefore describe their implementation¡ªincluding potential alternatives¡ªas two separate
activities, starting with thread-specific object sets and then covering thread-specific object
proxies.
The thread-specific objects themselves, as well as the application code that accesses them,
are defined by application developers. We therefore do not provide general implementation
activities for these pattern participants. In the Example Resolved section, however, we use
our multi-threaded logging server example to illustrate how applications can program the
Thread-Specific Storage pattern effectively.
1. Implement the thread-specific object sets. This activity is divided into six sub-activities:
1. Determine the type of the thread-specific objects. In terms of our two dimensional matrix analogy, a thread-specific object is an entry in the matrix
that has the following properties:
??Its row number corresponds to the key that uniquely identifies the
'logically global' object.
??Its column number corresponds to a particular application thread
identifier.
To make implementations of the Thread-Specific Storage pattern more
generic, a pointer to a thread-specific object is stored rather than storing the
object itself. These pointers are often 'loosely typed', such as C/C++ void
*'s, so that they can point to any type of object. Although loosely typed void
*'s are highly flexible, they are hard to program correctly. Implementation
activity 2 (491) therefore describes several strategies to encapsulate void
*'s with less error-prone, strongly-typed proxy classes.
2. Determine where to store the thread-specific object sets. In terms of our two dimensional matrix analogy, the thread-specific object sets correspond to
matrix columns, which are allocated one per application thread. Each
application thread identifier therefore designates one column in our conceptual
two-dimensional matrix. Each thread-specific object set can be stored either
externally to all threads or internally to its own thread:
There are pros and cons for each strategy:
??External to all threads. This strategy maps each application thread's
identifier to a global table of thread-specific object sets that are stored
externally to all threads. Note that an application thread can obtain its own
thread identifier by calling an API in the threading library. Implementations
of external thread-specific object sets can therefore readily determine
which thread-specific object set is associated with a particular application
thread.
Depending on the implementation of the external table strategy, threads
can access thread-specific object sets in other threads. At first, this
design may appear to defeat the whole point of the Thread-Specific
Storage pattern, because the objects and pointers themselves do not
reside in thread-specific storage. It may be useful, however, if the thread specific storage implementation can recycle keys when they are no longer
needed, for example if an application no longer needs to access a global
object, such as errno, for some reason.
A global table facilitates access to all thread-specific object sets from one
'clean-up' thread, to remove the entries corresponding to the recycled
key. Recycling keys is particularly useful for Thread-Specific Storage
pattern implementations that support only a limited number of keys. For
example, Windows NT has a limit of 64 keys per process. Real-time
operating systems often support even fewer keys.
One drawback of storing thread-specific object sets in a global table
external to all threads is the increased overhead for accessing each
thread-specific object. This overhead stems from the synchronization
mechanisms needed to avoid race conditions every time the global table
containing all the thread-specific object sets is modified. In particular,
serialization is necessary when the key factory creates a new key,
because other application threads may be creating keys concurrently.
After the appropriate thread-specific object set is identified, however, the
application thread need not perform any more locking operations to
access a thread-specific object in the set.
??Internal to each thread. This strategy requires each thread to store a
thread-specific object set with its other internal state, such as its run-time
thread stack, program counter, general-purpose registers, and thread
identifier. When a thread accesses a thread-specific object, the object is
retrieved by using its associated key as an index into the thread's internal
thread-specific object set. Unlike the external strategy described above,
no serialization is required when the thread-specific object set is stored
internally to each thread. In this case all accesses to a thread's internal
state occurs within the thread itself.
Storing the thread-specific object set locally in each thread requires more
state per-thread, however, though not necessarily more total memory
consumption. As long as the growth in size does not increase the cost of
thread creation, context switching or destruction significantly, the internal
thread-specific object strategy can be more efficient than the external
strategy.
If an operating system provides an adequate thread-specific storage
mechanism, thread-specific object sets can be implemented internally to each
thread via the native operating system mechanism. If not, thread-specific
object sets can be implemented externally using a two-level mapping strategy.
In this strategy, one key in the native thread-specific storage mechanism is
dedicated to point to a thread-specific object set implemented externally to
each thread.
3. Define a data structure to map application thread identifiers to thread-specific
object sets. In terms of the two-dimensional matrix analogy, application thread
identifiers map to the columns in the matrix that represent thread-specific
object sets.
Application thread identifiers can range in value from very small to very large.
A large range in values presents no problem for object sets that reside
internally to each thread. In this case the thread identifier is associated
implicitly with the corresponding thread-specific object set contained within the
thread's state. Thus, there is no need to implement a separate data structure
to map application thread identifiers to thread-specific object sets.
For thread-specific object sets residing externally to all threads, however, it
may be impractical to have a fixed-size array with an entry for every possible
thread identifier value. This is one reason why the two-dimensional matrix
analogy is just a conceptual model rather than a realistic implementation
strategy. In this case it may be more space efficient to use a dynamic data
structure that maps thread identifiers to thread-specific object sets.
For example, a common strategy is to compute a hash function using the
thread identifier to obtain an offset to a hash table. The entry at this offset
contains a chain of tuples that map thread identifiers to their corresponding
thread-specific object sets.
4. Define the data structure that maps keys to thread-specific objects within a
thread-specific object set. In terms of the two-dimensional matrix analogy, this
mapping identifies a particular matrix entry (the thread-specific object)
according to its row (the key) at a particular column (the thread-specific object
set associated with a particular application thread identifier). For both the
external and internal thread-specific object set implementations, we must
select either a fixed-sized or a variable-sized data structure for this mapping.
The thread-specific object set can be stored in a fixed-size array if the range of
thread-specific key values is relatively small and contiguous. The POSIX
Pthreads standard [IEEE96], for example, defines a standard macro,
_POSIX_THREAD_KEYS_MAX, that sets the maximum number of keys
supported by a Pthreads implementation. If the size defined by this macro is
small and fixed, for example 64 keys, the lookup time can be O(1) by indexing
into the thread-specific object set array directly using the key that identifies a
thread-specific object.
Some thread-specific storage implementations provide a range of thread specific keys that is large and nearly unbounded, however. Solaris threads, for
example, have no predefined limit on the number of thread-specific storage
keys in an application process. Solaris therefore uses a variable-sized data
structure, such as a hash table to map keys to thread-specific objects.
Although this data structure is more flexible than a fixed-size array, it can
increase the overhead of managing the thread-specific object set when a
many keys are allocated.
The following code shows how thread-specific object sets can be
implemented internally within each thread using a fixed-sized array of
thread-specific objects that are stored as void *'s. Using this internal
design means that it is not necessary to map application thread identifiers
to thread-specific object sets. Instead, we need only provide a data
structure that maps keys to thread-specific objects within a thread-specific
object set.
All the C code examples shown in this Implementation section are
adapted from a publicly available user-level library implementation
[Mue93] of POSIX Pthreads [IEEE96]. For example, the object_set_
data structure corresponding to implementation activity 1.4 (487) is
contained within the following thread_state struct. This struct is
used by the Pthreads library implementation to store the state of each
thread:
In addition to keeping track of the array of pointers to thread-specific
storage objects, an instance of thread_state also includes other thread
state. This includes a pointer to the thread's stack and space to store
thread-specific registers that are saved and restored during a context
switch. Our Pthreads implementation also defines several macros to
simplify its internal programming:
The pthread_self() function used by the INTERNAL_ERRNO macro is
an internal implementation subroutine that returns a pointer to the context
of the currently active thread.
5. Define the key factory. In our two-dimensional matrix analogy, keys
correspond to rows in the matrix. The key factory creates a new key that
identifies a 'logically global' object (row) uniquely. The state of this object will
physically reside in storage that is local to each thread.
For a particular object that is logically global yet physically local to each
thread, the same key value k is used by all threads to access their
corresponding thread-specific object. The count of the number of keys
currently in use can therefore be stored globally to all threads.
The code below illustrates our Pthreads library implementation of the
pthread_key_create() key factory. Keys are represented by integer
values:
A static variable keeps track of the current key count within the thread specific storage implementation:
The total_keys_ variable is incremented automatically every time a
new thread-specific key is required, which is equivalent to adding a new
row to our conceptual two-dimensional matrix. Next, we define the key
factory itself:
The pthread_key_create() function is a key factory. It allocates a
new key that identifies a thread-specific data object uniquely. This function
requires no internal synchronization, because it must be called with an
external lock held, as shown by the TS_Proxy in implementation activity
2.1 (492).
When a key is created, the pthread_key_create() function allows the
calling thread to associate a thread_exit_hook with the new key. This
hook is a pointer to a function that will be used to delete any dynamically
allocated thread-specific objects that are associated with the key. When a
thread exits, the Pthreads library calls this function pointer automatically
for each key that has registered an exit hook.
To implement this feature, an array of function pointers to 'thread exit
hooks' can be stored as a static global variable in the Pthreads library:
For each key, an application can either register the same function pointer,
a different function pointer, or any combination of function pointers. When
each thread exits, the Pthreads implementation calls the same function
that was registered when each key was created. Applications often
implement thread exit hooks as follows because deleting dynamically
allocated thread-specific objects is common:
Define methods to store and retrieve thread-specific objects from a thread specific object set. In terms of the matrix analogy, these two methods set and
get the value of matrix entries. The set() method stores a void* at matrix
entry [k,t], whereas the get() method retrieves a void* at matrix entry
[k,t]. In thread-specific storage implementations, k is passed as a key
argument and t is the implicit thread identifier returned by a call to
pthread_self().
7. The pthread_setspecific() function is a set() method that stores a
void* using the key passed by the client application thread that calls it:
Similarly, the pthread_getspecific() function retrieves a void*
using the key passed by the client application thread:
In this implementation, neither function requires any locks to access its
thread-specific object set, because the set resides internally within the
state of each thread.
2. Implement thread-specific object proxies. In theory, the thread-specific object sets
written in C above are sufficient to implement the Thread-Specific Storage pattern. In
practice, however, it is undesirable to rely on such low-level C function APIs for two
reasons:
??Although the thread-specific storage APIs of popular threading libraries, such
as POSIX Pthreads, Solaris threads, and Win32 threads, are similar, their
semantics differ subtly. For example, Win32 threads, unlike POSIX Pthreads and
Solaris threads, do not provide a reliable way to deallocate objects allocated in
thread-specific storage when a thread exits. In Solaris threads, conversely, there
is no API to delete a key. These diverse semantics make it hard to write code
that runs portably on all three platforms.
??The POSIX Pthreads, Solaris, and Win32 thread-specific storage APIs store
pointers to thread-specific objects as void*'s. Although this approach provides
maximal flexibility, it is error-prone because void*'s eliminate type-safety.
To overcome these limitations the Thread-Specific Storage pattern defines a thread specific object proxy. Each proxy applies the Proxy pattern [GoF95] [POSA1] to
define an object that acts as a 'surrogate' for a thread-specific object. Application
threads that invoke methods on a proxy appear to access an ordinary object, when in
fact the proxy forwards the methods to a thread-specific object. This design shields
applications from knowing when or how thread-specific storage is being used. It also
allows applications to use higher-level, type-safe, and platform-independent wrapper
facades (47) to access thread-specific objects managed by lower-level C function
APIs.
The implementation of thread-specific object proxies can be divided into three sub activities:
3. Define the thread-specific object proxy interfaces. For the thread-specific
object there are two strategies for designing a proxy interface, polymorphism
or parameterized types:
??Polymorphism. In this strategy an abstract proxy class declares and
implements the data structures and methods that every proxy supports.
Examples include the key that the thread-specific object set associates
with a particular 'logically global' object, or the lock needed to avoid race
conditions when creating this key.
Access to the concrete methods offered by thread-specific objects is
provided by subclasses of the general proxy, with one class for each type
of thread-specific object. Before forwarding client application requests to
the corresponding methods in the thread-specific object, a proxy first
retrieves an object pointer from the thread-specific object set via the key
stored in the proxy.
Using polymorphism to implement a proxy is a common strategy
[POSA1]. It can incur overhead, however, due to the extra level of
indirection caused by dynamic binding.
??Parameterized types. In this strategy the proxy can be parameterized
by the types of objects that will reside in thread-specific storage. As with
the polymorphism strategy described above, the proxy mechanism only
declares and implements the data structures and methods every proxy
supports. It also performs all necessary operations on the thread-specific
object set before invoking the designated method on the thread-specific
object. Parameterization can remove the indirection associated with
polymorphism, which can improve the proxy's performance.
A key design problem that arises when using the parameterized type
strategy is selecting a convenient mechanism to access the methods of
thread-specific objects encapsulated by a proxy. In particular, different
types of thread-specific objects have different interfaces. The mechanism
for accessing these objects cannot therefore define any concrete service
methods. This differs from the polymorphism strategy described above.
One way to address this problem is to use smart pointers [Mey95], such
as operator->, also known as the C++ arrow operator [Str97]. This
operator allow client application threads to access the proxy as if they
were accessing the thread-specific object directly. The operator->
method receives special treatment from the C++ compiler. It first obtains a
pointer to the appropriate type of thread-specific object, then delegates
the original method invoked on it.
Another generic way to access the methods of thread-specific objects is
to apply the Extension Interface pattern (141). This solution introduces a
generic method for the proxy that allows clients to retrieve the concrete
interfaces supported by the configured thread-specific object.
4. In our example we use C++ parameterized types to define a type-safe
template that applications can use to instantiate thread-specific object
proxies with concrete thread-specific objects:
This thread-specific proxy template is parameterized by the type of object
that will be accessed via thread-specific storage. In addition, it defines the
C++ smart pointer operator-> to access a thread-specific object of type
TYPE.
34. Implement the creation and destruction of the thread-specific object proxy.
Regardless of whether we apply the polymorphism or parameterized type
strategy to define the thread-specific object proxy, we must manage the
creation and destruction of thread-specific object proxies.
35. The constructor for our thread-specific object proxy template class is
minimal, it simply initializes the object's data members:
In general, a proxy's constructor does not allocate the key or a new thread specific object instance in the constructor for two reasons:
??Thread-specific storage semantics. A thread-specific object proxy is
often created by a thread, for example the application's main thread, that
is different from thread(s) that use the proxy. Thus, there is no benefit
from pre-initializing a new thread-specific object in its constructor because
this instance will only be accessible by the thread that created it.
??Deferred creation. On some operating systems, keys are limited
resources and should not be allocated until absolutely necessary. Their
creation should therefore be deferred until the first time a method of the
proxy is invoked. In our example implementation this point of time occurs
in the operator-> method.
The destructor for the thread-specific object proxy presents us with several
tricky design issues. The 'obvious' solution is to release the key that was
allocated by the key factory. There are several problems with this approach,
however:
??Non-portability. It is hard to write a proxy destructor that releases keys
portably. For example, Solaris threads, unlike Win32 and POSIX
Pthreads, lacks an API to release keys that are not needed.
??Race conditions. One reason that Solaris threads do not provide an
API to release keys is that it hard to implement efficiently and correctly.
The problem is that each thread maintains independent copies of the
objects referenced by a key. Only after all threads have exited and the
memory reclaimed can a key be released safely.
As a result of these problems the proxy's destructor is generally a 'noop'. This
means that we do not recycle keys in this implementation. In lieu of a
destructor, therefore, we implement a thread-exit hook function, as discussed
in implementation activity 1.5 (489). This hook is dispatched automatically by
the thread-specific storage implementation when a thread exits. It deletes the
thread-specific object, thereby ensuring that the destructor of the object is
invoked.
Note that the cleanup_hook() is defined as a static method in the
TS_Proxy class. By defining this method as static, it can be passed as
a pointer-to-function thread exit hook to pthread_key_create().
40. Implement the access to the thread-specific object. As we discussed earlier,
there are two general strategies¡ªpolymorphism and parameterized types¡ªfor
accessing the methods of a thread-specific object that is represented by a
proxy.
When using the polymorphism strategy, the interface of each concrete proxy
must include all methods offered by the thread-specific object that is
represented by this class. Method implementations in a concrete proxy
generally perform four steps:
??Create a new key, if no such thread-specific object has been created
yet. We must avoid race conditions by preventing multiple threads from
creating a new key for the same TYPE of thread-specific object
simultaneously. We can resolve this problem by applying the Double Checked Locking Optimization pattern (365).
??The method must next use the key stored by the proxy to get the
thread-specific object via its thread-specific object set.
??If the object does not yet exist, it is created 'on-demand'.
??The requested operation is forwarded to the thread-specific object.
Any operation results are returned to the client application thread.
To avoid repeating this code within each proxy method, we recommend
introducing a helper method in the thread-specific object proxy base class that
implements these general steps.
When using parameterized types to instantiate a generic proxy, the smart
pointer and Extension Interface pattern (365) strategies described in
implementation activity 2.1 (492) can be applied to implement a general
access mechanism for any thread-specific object's methods. Analogously to
the polymorphism strategy, the general access mechanism must follow the
implementation steps described above.
By using the parameterized type strategy and overloading the C++ arrow
operator, operator->, applications can invoke methods on instances of
TS_Proxy as if they were invoking a method on an instance of the TYPE
parameter. The C++ arrow operator controls all access to the thread specific object of class TYPE. It performs most of the work, as follows:
The TS_Proxy template is a proxy that transforms ordinary C++ classes
into type-safe classes whose instances reside in thread-specific storage. It
combines the operator-> method with C++ features, such as templates,
inlining, and overloading. In addition, it uses common concurrency control
patterns and idioms, such as Double-Checked Locking Optimization (353),
Scoped Locking (325), and Strategized Locking (333).
The Double-Checked Locking Optimization pattern is used in operator-
> to test the once_ flag twice. Although multiple threads could access the
same instance of TS_Proxy simultaneously, only one thread can validly
create a key via the pthread_key_create() method. All threads then
use this key to access their associated thread-specific object of the
parameterized class TYPE. The operator-> method therefore uses a
keylock_ of type Thread_Mutex to ensure that only one thread at a
time executes pthread_key_create().
After key_ is created, no other locking is needed to access thread-specific
objects, because the pthread_getspecific() and
pthread_setspecific() functions both retrieve the thread-specific
object of class TYPE from the state of the client application thread, which
is independent from other threads. In addition to reducing locking
overhead, the implementation of class TS_Proxy shown above shields
application code from the fact that objects are local to the calling thread.
The implementation of the extension interface and polymorphic proxy are
similar to the generic smart pointer approach shown above. The polymorphic
proxy approach simply forwards to a method of the thread-specific object and
returns the result. Similarly, the extension interface approach returns an
extension interface from the thread-specific object and passes this back to the
client.
The following application is similar to our original logging server from the Example section.
The logger() function shown below is the entry point to each thread that has its own
unique connection to a remote client application. The main difference is that the logger()
function uses the TS_Proxy template class defined in implementation activity 2.3 (496) to
access the errno value.
This template is instantiated by the following Error_Logger class:
The Error_Logger class defines the type of the 'logically' global, but 'physically' thread specific, logger object, which is created via the following TS_Proxy thread-specific object
proxy template:
The logger() function is called by each connection handling thread in the logging server.
We use the SOCK_Stream class described in the Wrapper Facade pattern (47) to read data
from the network connection, instead of accessing the lower-level C Socket API directly:
Consider the call to the my_logger->last_error() method above. The C++ compiler
generates code that replaces this call with two method calls. The first is a call to the
TS_Proxy::operator->, which returns the appropriate Error_Logger instance residing
in thread-specific storage. The compiler then generates a second method call to the
last_error() method of the Error_Logger object returned by the previous call. In this
case, TS_Proxy behaves as a proxy that allows an application to access and manipulate
the thread-specific error value as if it were an ordinary C++ object.
Starting with JDK 1.2, Java supports the Thread-Specific Storage pattern via class
java.lang.ThreadLocal. An object of class java.lang.ThreadLocal is a thread specific object proxy, which corresponds to one row in our two-dimensional matrix analogy.
ThreadLocal objects are often created as static variables in a central location so that they
are broadly visible. A ThreadLocal internal hash table maintains the entries for the thread specific objects, one per-thread. These entries are of type Object, which means that the
hash table does not know the concrete type of the objects it holds. Applications must
therefore maintain that knowledge and perform the necessary downcasting, which has the
pros and cons discussed in implementation activities 1.1 (484) and 2 (491).
A Java application thread can set the value of a ThreadLocal object foo by calling
foo.set(newValue). The foo object of type Thread-Local uses the thread identifier to
return the thread's current object entry from the hash table. The hash table is a normal data
structure, but by calling Collections.synchronizedMap(hashtable) wraps a thread safe layer around hashtable. This feature combines the Decorator [GoF95] and Thread Safe Interface patterns (345) to ensure that an existing Java collection will be serialized
correctly.
The class java.lang.InheritableThreadLocal is an extension of the ThreadLocal
class. This subclass allows a child thread to inherit all thread-specific objects from its parent
thread, with values preset to its current parent's values.
Widely-used examples of the Thread-Specific Storage pattern are operating system
platforms, such as Win32 and Solaris, that support the errno mechanism. The following
definition of errno is defined by Solaris in <errno.h>:
he ___errno() function invoked by this macro can be implemented as follows, based
upon the low-level C thread-specific storage functions we described in implementation
activity 1 (484):
The Win32 GetLastError() and SetLastError() functions implement the Thread Specific Storage pattern in a similar manner.
In the Win32 operating system API, windows are owned by threads [Pet95]. Each thread
that owns a window has a private message queue where the operating system enqueues
user interface events. Event processing API calls then dequeue the next message on the
calling thread's message queue residing in thread-specific storage.
The Active Template Library from COM uses the Extension Interface approach to
implement the Thread-Specific Storage pattern.
OpenGL [NDW93] is a C API for rendering three-dimensional graphics. The program
renders graphics in terms of polygons that are described by making repeated calls to the
glVertex() function to pass each vertex of the polygon to the library. State variables set
before the vertices are passed to the library determine precisely what OpenGL draws as it
receives the vertices. This state is stored as encapsulated global variables within the
OpenGL library or on the graphics card itself. On the Win32 platform, the OpenGL library
maintains a unique set of state variables in thread-specific storage for each thread using the
library.
Thread-specific storage is used within the ACE framework [Sch97] to implement its error
handling scheme, which is similar to the approach described in the Example Resolved
section. In addition, ACE implements the type-safe thread-specific object proxy using C++
templates, as described in implementation activity 2 (491). The ACE thread-specific storage
template class is called ACE_TSS.
Local telephone directory services. A real-life example of the Thread-Specific Storage
pattern is found in telephone directory services. For example, in the United States, the
'logically global' number 411 can be used to connect with the local directory assistance
operator for a particular area code or region.
There are four benefits of using the Thread-Specific Storage pattern:
Efficiency. The Thread-Specific Storage pattern can be implemented so that no locking is
necessary to access thread-specific data. For example, by placing errno into thread specific storage, each thread can reliably and efficiently set and test the completion status of
methods called within that thread, without using complex synchronization protocols. This
design eliminates locking overhead for data shared within a thread, which is faster than
acquiring and releasing a mutex [EKBF+92].
Reusability. Applying the Wrapper Facade pattern (47) and decoupling the reusable the
Thread-Specific Storage pattern code from application-specific classes can shield
developers from subtle and non-portable thread-specific key creation and allocation logic.
For example, the Double-Checked Locking Optimization pattern (365) can be integrated into
a reusable thread-specific object proxy component to prevent race conditions automatically.
Ease of use. When encapsulated with wrapper facades, thread-specific storage is relatively
straightforward for application programmers to use. For example, thread-specific storage can
be hidden completely at the source-code level by abstractions, such as the thread-specific
object proxy templates, or macros, such as errno. Changing a class to or from a thread specific class therefore simply requires changing the way in which an object of the class is
defined.
Portability. Thread-specific storage is available on most multi-threaded operating systems
platforms. It can be implemented conveniently on platforms that lack it, such as VxWorks or
pSoS. Furthermore, thread-specific object proxies can encapsulate platform-dependent
operations behind a uniform and portable interface. Porting an application to another thread
library, such as the TLS interfaces in Win32, therefore only requires changing the TS_Proxy
class, rather than application code that uses the class.
However, the following are liabilities of using the Thread-Specific Storage pattern:
It encourages the use of (thread-specific) global objects. Many applications do not require
multiple threads to access thread-specific data via a common access point. In this case, data
should be stored so that only the thread owning the data can access it.
Consider our logging server that uses a pool of threads to handle incoming logging
records from clients. In addition to writing the logging records to persistent storage, each
thread can log the number and type of services it performs. This logging mechanism
could be accessed as a global Error_Logger object via thread-specific storage.
However, a simpler approach, though potentially less efficient and more obtrusive, is to
represent each logger thread as an active object (369), with an instance of the
Error_Logger stored as a data member rather than in thread-specific storage. In this
case, the Error_Logger can be accessed as a data member by active object methods
or passed as a parameter to all external methods or functions called by the active object.
It obscures the structure of the system. The use of thread-specific storage potentially makes
an application harder to understand, by obscuring the relationships between its components.
For example, it is not obvious from examining the source code of our logging server that
each thread has its own instance of Error_Logger, because my_logger resembles an
ordinary global object. In some cases it may be possible to eliminate the need for thread specific storage, by representing relationships between components explicitly via
containment or aggregation relationships.
It restricts implementation options. Not all languages support parameterized types or smart
pointers, and not all application classes offer Extension Interfaces (141). 'Elegant'
implementation solutions for the thread-specific object proxy therefore cannot be applied for
all systems. When this occurs, less elegant and less efficient solutions, such as
polymorphism or low-level functions, must be used to implement the Thread-Specific
Storage pattern.
Thread-specific objects such as errno are often used as per-thread singletons [GoF95]. Not
all uses of thread-specific storage are singletons, however, because a thread can have
multiple instances of a type allocated from thread-specific storage. For example, each
instance of an active object (369) implemented via an ACE_Task [Sch97] stores a thread specific cleanup hook.
The Thread-Specific Storage pattern is related to the Data Ownership pattern [McK95],
where a thread mediates client access to an object.
##%%&&
The Lookup pattern describes how to find and access resources, whether local or distributed, by using a lookup service as a mediating instance.
Consider a system that consists of several services implemented as remote objects using CORBA [OMG04a]. To access one of the distributed services, a client typically needs to obtain a reference to the object that provides the service. An object reference identifies the remote object that will receive the request. Object references can be passed around in the system as parameters to operations, as well as the results of requests. A client can therefore obtain a reference to a remote object in the system from another object. However, how can a client acquire an initial reference to an object that the client wants to access?
For example, in a system that provides a distributed transaction service, a client may want to obtain a reference to the transaction manager so that it can participate in distributed transactions. How can a server make the transaction manager object reference that it created widely available? How can a client obtain the transaction manager object reference without having a reference to any other object?
Systems where resource users need to find and access local and distributed resources.
Resource providers may offer one or more resources to resource users. Over time additional resources may be added or existing resources may be removed by the resource provider. One way the resource provider can publish the availability of existing resources to interested resource users is by periodically sending a broadcast message. Such messages need to be sent on a periodic basis to ensure that new resource users joining the system become aware of available resources. Conversely, a resource user could send a broadcast message requesting all available resource providers to respond. Once the resource user receives replies from all available resource providers, it can then choose the resource(s) it needs. However, both of these approaches can be quite costly and inefficient, since they generate lots of messages, which proliferate across the network in the case of a distributed system. To address this problem of allowing resource providers to publish resources and for resource users to find these resources in an efficient and inexpensive manner requires the resolution of the following forces:
?	Availability. A resource user should be able to find out on demand what resources are available in its environment.
?	Bootstrapping. A resource user should be able to obtain an initial reference to a resource provider that offers the resource.
?	Location independence. A resource user should be able to acquire a resource from a resource provider independent of the location of the resource provider. Similarly, a resource provider should be able to provide resources to resource users without having knowledge of the location of the resource users.
?	Simplicity. The solution should not burden a resource user when finding resources. Similarly, the solution should not burden a resource provider providing the resources.
Provide a lookup service that allows resource providers to make resources available to resource users. The resource provider advertises resources via the lookup service along with properties that describe the resources that the resource providers provide. Allow resource users to first find the advertised resources using the properties, then retrieve the resources, and finally use the resources.
For resources that need to be acquired before they can be used, the resource providers register references to themselves together with properties that describe the resources that the resource providers provide. Allow resource users to retrieve these registered references from the resource providers and use them to acquire the available resources from the referenced resource providers.
For resources, such as concurrently reusable services, that do not need to be first acquired and can instead be accessed directly, the resource providers register references to the resources together with properties that describe the resources. Allow resource users to directly access the resources without first interacting with the resource providers.
The lookup service serves as a central point of communication between resource users and resource providers. It allows resource users to access resource providers when resources need to be explicitly acquired from the resource providers. In addition, the lookup service allows resource users to directly access resources that do not need to be acquired from resource providers. In both cases, the resource users need not know about the location of the resource providers. Similarly, the resource providers don't need to know the location of the resource users that want to acquire and access the resources that they provide.
The following participants form the structure of the Lookup pattern:
?	A resource user uses a resource.
?	A resource is an entity such as a service that provides some type of functionality.
?	A resource provider offers resources and advertises them via the lookup service.
?	A lookup service provides the capability for resource providers to advertise resources via references to themselves, and for resource users to find these references.
The following CRC cards describe the responsibilities and collaborations of the participants.
The following class diagram illustrates the structure of the Lookup pattern.
The resource user depends on all three other participants: the lookup service to find the resource provider, the resource provider to acquire the resource, and the resource to actually access it.
There are three sets of interactions in the Lookup pattern.
Scenario I In the first interaction, a resource provider advertises a resource with the lookup service. It is assumed that the resource provider already knows the access point of the lookup service. For the interactions necessary when the resource provider has no access point to the lookup service, refer to Scenario III. On advertisement of the resource, the resource provider registers a reference to itself with the lookup service, together with some properties that are descriptive of the type of resources that the resource provider provides.
Scenario II In the second scenario, a resource user finds a resource provider using a lookup service, and includes the following interactions:
?	The resource user queries the lookup service for the desired resource using one or more properties, such as resource description, interface type, and location.
?	The lookup service responds with the reference to the resource provider, which provides the desired resource.
?	The resource user uses the reference of the resource provider to acquire and access the resource.
Scenario III In distributed systems the access point of the lookup service might not be known to the resource user and the resource provider. In such cases the access point might be configured via the runtime environment of the application, or the application might use a bootstrapping protocol to find the access point. The bootstrapping protocol may be a broadcast, multicast or a combination of several unicast messages.
The necessary steps are:
?	The resource provider or resource user searches for a lookup service via a bootstrapping protocol.
?	The lookup service responds announcing its access point.
The following sequence diagram shows these interactions, which are valid for the resource provider as well as the resource user. In the diagram, the resource provider uses broadcast as a bootstrapping protocol.
There are four steps involved in implementing the Lookup pattern:
1.	Determine the interface for a lookup service. A lookup service should facilitate advertisement and lookup of resources, either directly or through their resource providers. It should provide an interface that allows resource providers to register and unregister references. The reference is associated with properties that describe the resource offered. The lookup service keeps a list of the registered references and their associated properties. These properties can be used by the lookup service to select one or more resources or resource providers based on queries sent by the resource user. In the simplest case, the properties may just contain the name or type of a single resource that the resource provider provides. Queries from resource users on the lookup service return either a valid reference, or an error code when no matching resource could be found.
Different policies can be defined for the lookup service. For example, the lookup service may support bindings with duplicate names or properties. The lookup service should also provide an interface that allows resource users to retrieve a list of all available resource providers. The search criteria used by the resource users can be a simple query-by-name, or a more complex query mechanism as described in implementation step 5, Determine a query language.
2.	Determine whether to register resource references or resource provider references. Depending on the kind of resource and its acquisition strategy, either the reference to the resource provider or the reference to the resource is registered with the lookup service. If a resource must first be acquired explicitly by a resource user, then the reference of the resource provider providing that resource should be registered with the lookup service, together with properties that describe the resource provided by the resource provider. It may be advantageous to require explicit acquisition of a resource to control the type of resource user that can access the resource.
When a resource provider provides multiple resources, the reference of a resource provider can be such that it identifies the resource as well. This can be useful in enabling the resource provider to associate acquisition requests with desired resources. See the Multi-faceted Resource Provider variant for details.
On the other hand, if resources need not be explicitly acquired and can be directly accessed by resource users, then references to the resources along with properties describing them can be registered with the lookup service. For example, in the case of concurrently reusable resources, resources can be made available directly to resource users by registering references to the resources with the lookup service. Examples of such resources include read-only objects and Web Services [W3C04]. Such resources either do not have any synchronization issues, or can synchronize access themselves.
3.	Implement the lookup service. Internally, the lookup service can be implemented in many different ways. For example, the lookup service may keep the registered references and their meta information in some kind of a tree data structure, helpful when complex dependencies must be modelled, or in a simple hash map. For noncritical and heavily changing resource advertisements, the information may be stored transiently, while for critical advertisements the associations should be made persistent, with an appropriate backend persistency mechanism.
 For example, the CORBA implementation Orbix [Iona04] uses the COS Persistent State Service to persist the name bindings in its Naming Service, which is an implementation of the lookup service. Other CORBA implementations such as TAO [Schm98] [OCI04] persist the bindings using memory-mapped files.
4.	Provide the lookup service access point. To communicate with the lookup service, an access point is necessary, such as a Java reference, a C++ pointer or reference, or a distributed object reference, which typically includes information such as the host name and the port number where the lookup service is running. This information can be published to resource providers and resource users by several means, such as writing to a file that can be accessed by resource providers and resource users, or through well-defined environment variables.
 For example, a lot of CORBA implementations publish the access point of the Naming Service using property or configuration files, which can be accessed by clients.
If an access point is not published by the lookup service, it will be necessary to design a bootstrapping protocol that can allow resource providers and resource users to obtain the access point. Such a bootstrapping protocol is typically designed using a broadcast or a multicast protocol. The resource provider or user sends an initial request for a reference to a lookup service using the bootstrapping protocol. On receiving the request, typically one or more lookup services send a reply back to the requestor, passing their access points. The resource provider can then contact the lookup service to publish its reference. Similarly, a resource user can contact the lookup services to obtain references to registered resource providers.
 In CORBA, a client or server can acquire the access point of a Naming Service using the resolve_initial_references() call on the ORB. Internally, the ORB may use a broadcast protocol to acquire the access point of the Naming Service, such as an object reference.
5.	Determine a query language. The lookup service may optionally support a query language that allows resource users to search for resources using complex queries. For example, a query language could be based on a property sheet that describes the type of resource in which a resource user is interested. When using the lookup service to query a resource, the resource user may submit a list of properties that should be satisfied by the requested resource. The lookup service can then compare the list of properties submitted by the resource user against the properties of the available resources. If a match is found, the reference to the corresponding resource or resource provider is returned to the resource user.
 The CORBA Trading Service [OMG04f] is a lookup service that allows properties to be specified corresponding to a resource that is registered with it. Resource providers are server applications that register CORBA objects with it. The object references will point to the server application, but will also identify the CORBA object as a resource. A client, as resource user, can build an arbitrarily complex query using a criterion that is matched against the properties of the registered CORBA objects. The client is returned a reference to the CORBA object in the server application.
Consider the example in which a client wants to obtain an initial reference to a transaction manager in a distributed CORBA environment. Using the Lookup pattern, a lookup service should be implemented. Most CORBA implementations provide such a lookup service, either in the form of a Naming Service, a Trading Service, or both. These services are accessible via the Internet Inter-ORB Protocol (IIOP) and provide well-defined CORBA interfaces.
In our example, the server that creates a transaction manager is a resource provider. The server should first obtain a reference to the Naming Service, then use it to register the reference of the created transaction manager. The reference contains the access point of the server, and it identifies the registered resource in the server. The C++ code below shows how a server can obtain the reference to the Naming Service and then register the transaction manager with it.
Once the transaction manager has been registered with the Naming Service, a client can obtain its object reference from the Naming Service. The C++ code below shows how a client can do this.
Once the initial reference to the transaction manager has been obtained by the client, the client can then use it to invoke operations, as well as to obtain references to other CORBA objects and services.
Self-registering Resources. The participants, resource provider and resource, can be implemented by the same software artifact. In this case, since there is no distinction between a resource and a resource provider, the reference that is registered with the lookup service will be that of the resource. For example, a resource such as a distributed service can directly register with the lookup service, providing a reference to itself.
Multi-faceted Resource Provider. When a resource provider offers more than one resource that needs to be acquired, the registered reference of the resource provider can also identify the resource. The resource provider registers a unique reference to itself for each resource that it advertises in the lookup service. The reference that is registered with the lookup service would correspond to the resource provider, but would indirectly refer to a unique resource provided by that resource provider.
Resource Registrar. A separate entity can be responsible for registering references with the lookup service other than the one that actually provides the resources. That is, the resource providers need not be the one that register references with the lookup service; this responsibility can be handled by a separate entity called a resource registrar.
Federated Lookup. Several instances of the lookup service can be used together to build a federation of lookup services. The instances of lookup services in a federation cooperate to provide resource users with a wider spectrum of resources. The Half-Object Plus Protocol [Mesz95] pattern describes how to separate lookup service instances while still keeping them synchronized.
A federated lookup service can be configured to forward requests to other lookup services if it cannot fulfill the requests itself. This widens the scope of queries and allows a resource user to gain access to additional resources it was not able to reach before. The lookup services in a federation can be in the same or different location domains.
Replicated Lookup. The lookup service can be used to build faulttolerant systems. Replication is a well-known concept in providing fault tolerance and can be applied at two levels using lookup service:
?	Firstly, the lookup service itself can be replicated. Multiple instances of a lookup service can serve to provide both load balancing and fault tolerance. The Proxy [GoF95] pattern can be used to hide the selection of a lookup service from the client. For example, several ORB implementations provide smart proxies [HoWo03] on the client side that can be used to hide the selection of a particular lookup service from among the replicated instances of all the lookup services. 
?	Secondly, both the resources and the resource providers whose reference are registered with a lookup service can also be replicated. A lookup service can be extended to support multiple registrations of resource providers for the same list of properties, such as the same name in the case of the CORBA Naming Service. The lookup service can be configured with various strategies [GoF95] to allow dispatch of the appropriate resource provider upon request from a resource user. For example, a lookup service could use a round-robin strategy to alternate between multiple instances of a transaction manager that are registered with it using the same list of properties. This type of replication is used by Borland [Borl04] to extend the scalability of their CORBA implementation, Visibroker.
There are several benefits of using the Lookup pattern:
?	Availability. Using the lookup service, a resource user can find out on demand what resources are available in its environment. Note that a resource or its corresponding resource provider may no longer be available, but its reference may not have been removed from the lookup service. See the Dangling References liability for further details.
?	Bootstrapping. The lookup service allows a resource user to obtain initial resources. In distributed systems a bootstrapping protocol allows the resource user to find the lookup service and then use it to find other distributed services.
?	Location independence. The lookup service provides location transparency by shielding the location of the resource providers from the resource users. Similarly, the pattern shields the location of the resource users from the resource providers.
?	Configuration simplicity. Distributed systems based on a lookup service need little or no manual configuration. No files need to be shared or transferred in order to distribute, find and access remote objects. The use of a bootstrapping protocol is a key feature for ad hoc networking scenarios, in which the environment changes regularly and cannot be predetermined.
?	Property-based selection. Resources can be chosen based on properties. This allows for fine-grained matching of user needs with resource advertisements.
There are some liabilities of using the Lookup pattern:
?	Single point of failure. One consequence of the Lookup pattern is the danger of a single point of failure. If an instance of a lookup service crashes, the system can lose the registered references along with the associated properties. Once the lookup service is restarted, the resource providers would need to re-register the resources with it unless the lookup service has persistent state. This can be both tedious and error-prone, since it requires resource providers to detect that the lookup service has crashed and then restarted. In addition, a lookup service can also act as a bottleneck, affecting system performance. A better solution, therefore, is to introduce replication of the lookup service, as discussed in the Variants section.
?	Dangling references. Another consequence of the Lookup pattern is the danger of dangling references. The registered references in the lookup service can become outdated as a result of the corresponding resource providers being terminated or moved. In this case the Leasing (149) pattern, as applied in Jini [Sun04c], can help, by forcing the resource providers to prolong the ¡®lease¡¯ on their references regularly if they do not want their entries removed automatically.
?	Unwanted replication. Problems can occur when similar resources with the same properties are advertised but replication is not wanted. Depending on the implementation of the lookup service, multiple instances of the same resource may be registered erroneously, or one resource provider may overwrite the registration of a previous resource provider. Enforcing that at least one of the properties is unique can avoid this problem.
CORBA [OMG04a]. The Common Object Services Naming Service and Trading Service implements lookup services. Whereas the query language of the Naming Service is quite simple, using just names, the query language of the Trading Service is powerful and supports complex queries for components.
LDAP [HoSm97]. The Lightweight Directory Access Protocol (LDAP), defines a network protocol and information model for accessing information directories. An LDAP server allows the storage of almost any kind of information in the form of text, binary data, public key certificates, URLs, and references. Often LDAP servers are protected by permissions, as they contain critical information that must be secured from unauthorized access. LDAP clients can query the information stored on LDAP servers. Large organizations typically centralize their user databases of e-mail, Web and file sharing servers using LDAP directories.
JNDI [Sun04f]. The Java Naming and Directory Interface (JNDI) is an interface in Java that provides naming and directory functionality to applications. Using JNDI, Java applications can store and retrieve named Java objects of any type. In addition, JNDI provides querying functionality by allowing resource users to look up Java objects using their attributes. Using JNDI also allows integration with existing naming and directory services such as the CORBA Naming Service, RMI registry [Sun04g], and LDAP.
Jini [Sun04c]. Jini supports ad hoc networking by allowing services to join a network without requiring any pre-planning, installation, or human intervention, and by allowing users to discover devices on the network. Jini services are registered with Jini's lookup service, and these services are accessed by users using Jini's discovery protocol. To increase network reliability, the Jini lookup service regularly broadcasts its availability to potential clients.
COM+ [Ewal01]. The Windows Registry is a lookup service that allows resource users to retrieve registered components based on keys. A key can be either a ProgId, a GUID (Global Unique IDentifier) or the name and version of a component. The registry allows then to retrieve the associated components.
UDDI The Universal Description, Discovery, and Integration protocol (UDDI) [UDDI04] is a key building block of Web Services [W3C04]. The UDDI allows publishers of Web Services to advertise their service and clients to search for a matching Web Service. The advertisements contain service descriptions and point to detailed technical specifications that define the interfaces to the services.
Peer-to-peer networks Peer-to-Peer (P2P) networking technologies, such as JXTA [JXTA04], support advertisement and discovery of peers and resources. Resources in the context of P2P are often files and services
DNS [Tane02]. The Domain Name Service (DNS) is responsible for the coordination and mapping of domain names to and from IP addresses. It consists of a hierarchy of name servers that host the mapping. It is therefore a good example of how a federated lookup works. Clients query any nearby name server, sending a UDP packet containing a name. If the nearby name server can resolve it, it returns the IP address. If not, it queries the next name server in the hierarchy using a well-defined protocol.
Grid computing [BBL02] [Grid04] [JAP02]. Grid computing is about the sharing and aggregation of distributed resources such as processing time, storage, and information. A grid consists of multiple computers linked to form one virtual system. Grid computing uses the Lookup pattern to find distributed resources. Depending on the project, the role of the lookup service in the system is often referred to as ¡®resource broker¡¯, ¡®resource manager¡¯, or ¡®discovery service¡¯.
Eclipse plug-in registry [IBM04b]. Eclipse is an open, extensible IDE that provides a universal tool platform. Its extensibility is based on a plug-in architecture. Eclipse includes a plug-in registry that implements the Lookup pattern. The plug-in registry holds a list of all discovered plug-ins, extension points, and extensions and allows clients to locate these by their identity. The plug-in registry itself can be found by clients through several framework classes.
Telephone directory service The Lookup pattern has a real-world known use case in the form of telephone directory services. A person X may want to obtain the phone number of person Y. Assuming person Y has registered his/her phone number with a lookup service, in this case a telephone directory service, person X can then call this directory service and obtain the phone number of person Y. The telephone directory service will have a well-known phone number, for example 411 or a Web site [ATT04], thus allowing person X to contact it.
Receptionist [TwAl83]. A receptionist at a company can be considered as a real-life example of the Lookup pattern. The receptionist manages the contact information in the form of the phone numbers of all the employees of the company. When a caller wishes to contact an employee, they first speak to the receptionist, who then provides the ¡®reference¡¯ of the person being sought. Similarly, if a new person joins the company or an existing employee leaves, their contact information is typically updated by the receptionist to handle future queries.
The Activator [Stal00] pattern registers activated components with a lookup service to provide resource users with access to them. In many cases the references retrieved from a lookup service are actually references to factories, implementing the Abstract Factory [GoF95] pattern. This decouples the location of components from their activation.
The Sponsor-Selector [Wall97] pattern decouples the responsibilities of resource selection from resource recommendation and hands these responsibilities to two participants, the selector and the sponsor respectively. Sponsor-Selector can be used in Lookup to improve the finding of a resource that matches the resource user's demand. The role of the sponsor would coincide with the resource provider, while the lookup service would be the selector.
The Service Locator pattern [ACM01] encapsulates the complexity of JNDI lookups for EJB [Sun04b] home objects and the creation of business objects.
##%%&&
The Lazy Acquisition pattern defers resource acquisitions to the latest possible time during system execution in order to optimize resource use.
Consider a medical Picture Archiving and Communication System (PACS) that provides storage of patient data. The data includes both patient details such as address information as well as medical history. In addition, the data can include digitized images originating from various sources, such as X-rays and computer tomography scanners. In addition to supporting different sources of patient data, the PACS system must provide efficient access to the data. Such data is typically accessed by physicians and radiologists for diagnosis and treatment purposes.
A PACS system is typically built using a three-tier architecture. The middle tier of the system maintains business objects that represent the patient data. Since the data must be persisted, these business objects and the data they contain are mapped to some persistent store, typically a database. When a physician queries for a particular patient, the data is fetched and the corresponding business object is created. This business object is delivered to the presentation layer of the system, which extracts the relevant information and presents it to the physician. The time it takes to fetch the relevant patient data and create the business objects is proportional to the size of the patient data. For a patient with a long medical history and several digitized images corresponding to various medical examinations, fetching all the data and creating the corresponding business object can take a lot of time. Since high performance is a typical nonfunctional requirement of such systems, a delay in fetching patient records can be a big problem.
How can the PACS system be designed so that retrieval of patient information is quick regardless of the number of images in the patient's record?
A system with restricted resources that must satisfy high demands, such as throughput and availability.
Limited resource availability is a constraint faced by all software systems. In addition, if the available resources are not managed properly, it can lead to bottlenecks in the system and can have a significant impact on system performance and stability. To ensure that resources are available when they are needed, most systems acquire the resources at start-up time. However, early acquisition of resources can result in high acquisition overheads, and can also lead to wastage of resources, especially if the resources are not needed immediately.
Systems that have to acquire and manage expensive resources need a way of reducing the initial cost of acquiring the resources. If these systems were to acquire all resources up front, a lot of overhead would be incurred and a lot of resources would be consumed unnecessarily.
To address these problems requires resolution of the following forces:
?	Availability. Acquisition of resources should be controlled such that it minimizes the possibility of resource shortage and ensures that a sufficient number of resources are available when needed.
?	Stability. Resource shortage can lead to system instability, and therefore resources should be acquired in a way that has minimum impact on the stability of the system.
?	Quick system start-up. Acquisition of resources at system start-up should be done in a way that optimizes the system start-up time. 
?	Transparency. The solution should be transparent to the resource user.
Acquire resources at the latest possible time. The resource is not acquired until it becomes unavoidable to do so. When the initial request for a resource is made by the resource user, a resource proxy is created and returned. When the resource user tries to access the resource, the resource proxy acquires the actual resource and then redirects the access request of the resource user to the resource. The resource user is therefore dependent on the resource proxy, but as this provides the same interface as the resource, whether the resource proxy or the resource is accessed is transparent to the resource user.
By using a proxy to represent resources that are potentially expensive to acquire, the overall cost of acquiring a set of resources can be minimized. In addition, by not acquiring a large number of resources up front, the total number of resources that need to be managed simultaneously is also minimized.
The following participants form the structure of the Eager Acquisition pattern:
?	A resource user acquires and uses resources.
?	A resource is an entity such as a connection or memory.
?	A resource proxy intercepts resource acquisitions by the resource user and hands the lazily acquired resources to the resource user.
?	A resource provider manages and provides several resources.
The following CRC cards describe the responsibilities and collaborations of the participants.
The following class diagram illustrates the structure of the Lazy Acquisition pattern.
The class diagram shows that the resource user depends on the resource proxy. Since the resource proxy provides the same interface as the resource, whether the resource proxy or the resource is accessed is transparent to the user.
Scenario I In this scenario, the resource provider not only provides resources, but also acts as a factory for creating resource proxies.
When the resource user tries to acquire a resource from the resource provider, the resource provider creates and returns a resource proxy to the resource user, instead of the actual resource.
Scenario II The key dynamics of Lazy Acquisition is the acquisition of the resource by the resource proxy on the first access by the resource user. Initially, the resource proxy does not own the resource. Only at first access is the actual resource acquired. All subsequent access to the resource are forwarded by the resource proxy to the actual resource. The resource user does not notice the level of indirection provided by the resource proxy.
The implementation of this pattern is described by the following steps:
1.	Identify resources that need to be acquired lazily. Using profiling and system analysis, identify resources with one or more of the following properties:
o	Resources that are expensive to acquire,
o	Resources that are available only in limited number, and
o	Resources that remain unused for a long time after acquisition.
Review each identified resource and its use, and decide whether through the lazy acquisition of the resource overall resource availability, system stability, and system start-up can be improved. Apply the following implementation steps to each of the identified resources.
2.	Define the resource proxy interface. For every resource that needs to be acquired lazily, define a resource proxy as Virtual Proxy [GoF95] [POSA1] whose interface is identical to that of the resource.
3.	Implement the resource proxy. Implement the resource proxy such that it hides the lazy acquisition of the resource. The resource proxy will acquire the resource only when the resource is actually accessed by the resource user. Once the actual resource has been acquired, the resource proxy should use delegation to handle all resource access requests. Depending upon the resource, the resource proxy may also be responsible for initializing the resource after acquiring it.
4.	Define the acquisition strategy. Define the strategy by which the resource is actually obtained from the resource provider by the resource proxy. The Strategy [GoF95] pattern can be applied to configure different types of strategy. A simple strategy could be to delay acquisition of the resource until the resource is accessed by the resource user. An alternative strategy could be to acquire the resource based on some state machine. For example, the instantiation of a component might trigger the resource proxy of another existing component to acquire its corresponding resources. In addition, the resource proxy may also provide the ability to switch off lazy acquisition entirely, in which case the resource will be acquired immediately.
5.	Implement a proper resource release strategy for the lazily acquired resources. If the resources are not to be released by the resource user explicitly, then use either the Evictor (168) pattern or Leasing (149) pattern for automatically releasing the resources. For this, the lazily acquired resource needs either to implement the EvictionInterface, or be registered with a lease provider, respectively.
Consider the example of a medical Picture Archiving and Communication System (PACS). In order to solve the problem of quick retrieval of patient information, use the Lazy Acquisition pattern.
When a request is made to fetch all the information for a particular patient, create a query that does not fetch any image data. For each image in the patient record, create an image proxy in the business object that is returned. The presentation layer will process the business object and present all the information. For all the image proxies that it encounters, it will create links in the presentation that is generated. When an image corresponding to such a link needs to be viewed, it can then be fetched (lazily) from the file system. Images are stored on the file system directly, as storing images with their large amounts of binary data in a database is typically inefficient.
Using this solution optimizes fetching of textual patient data, which is typically not large. This can provide the physician with a good summary of the patient's medical history. If the physician wishes to view any image from the patient's record, that can be fetched lazily and on demand.
The sample code below shows the class PatientManager, which queries the database for the patient record. It fetches all data except the images.
The returned PatientRecord holds a list of MedicalExams, which in turn reference an image each. The class MedicalExam cannot differentiate between a regularly loaded image and a lazily loaded image, as the interface to it is the same.
The Image interface can be implemented using various strategies. The ImageProxy class below implements the lazy acquisition of the image by loading it from the file system only when actually accessed.
Note that the ImageProxy needs to have information about how to retrieve the lazily-acquired image from the file system.
Some specialized patterns derived from Lazy Acquisition are:
Lazy Instantiation [BiWa04b]. Defer the instantiation of objects/ components until the instance is accessed by a user. As object instantiation is very often linked with dynamic memory allocation, and memory allocations are typically very expensive, lazy instantiation saves cost up front, especially for objects that are not accessed. However, using lazy instantiation can incur a dramatic overhead in situations in which a burst of users concurrently access objects, leading to a high-demand situation.
Lazy Loading. Defer the loading of a shared library until the program elements contained in that library are accessed. The Component Configurator Pattern [POSA2] can be used to implement this. Lazy Loading is often combined with Lazy Instantiation, since objects need to be instantiated when loaded. Lazy Load [Fowl02] describes how to defer loading the state of an object from the database until a client is actually interested in that object.
Lazy State [MoOh97]. Defer the initialization of the state of an object until the state is accessed. Lazy State is often used in situations in which a large volume of state information is accessed rarely. This pattern becomes even more powerful in combination with Flyweight [GoF95], or Memento [GoF95]. In networks of objects, the Lazy Propagator [FeTi97] describes how dependent objects can determine when they are affected by state changes and therefore need to update their state
Lazy Evaluation [Pryc02]. Lazy evaluation means that an expression is not evaluated until the expression's result is needed for the evaluation of another expression. Lazy evaluation of parameters allows functions to be partially evaluated, resulting in higher-order functions that can then be applied to the remaining parameters. Using lazy evaluation can significantly improve the performance of the evaluation, as unnecessary computations are avoided. In programming languages such as Java or C++, evaluation of subconditions in a Boolean expression is done using lazy evaluation, in the form of short-circuiting operators such as ¡®&&¡¯.
Lazy Initialization [Beck97]. Initialize the parts of your program the first time they are accessed. This pattern has the benefit of avoiding overhead in certain situations, but has the liability of increasing the chance of accessing uninitialized parts of the program.
Variable Allocation [NoWe00]. Allocate and deallocate variable-sized objects as and when you need them. This specialization applies Lazy Acquisition specifically to memory allocations and deallocations.
Semi-Lazy Acquisition. Instead of acquiring a resource lazily as late as possible or at the beginning, the resource can also be acquired at three additional times. The idea is that you don't obtain the resource in the beginning but you also don't wait until the resource is actually needed¡ªyou load the resource some time in between. An example could be a network management system in which a topology tree of the network needs to be built.
There are three options:
?	Build it when the application starts.
o	For: the tree is available as soon as the application is initialized.
o	Against: slow start-up time.
?	Build it when the user requests it.
o	For: fast start-up time.
o	Against: the user has to wait for the tree to be constructed.
?	Build it after the application has started and before the user requests it.
o	For: fast start-up time and tree is available when needed.
The last option is commonly used in network management systems.
There are several benefits of using the Lazy Acquisition pattern:
?	Availability. Using Lazy Acquisition ensures that not all resources are acquired up front. This helps to minimize the possibility that the system will run short of resources, or will acquire resources when they are not needed.
?	Stability. Using Lazy Acquisition ensures that resources are acquired only when needed. This avoids needless acquisition of resources up front, thus reducing the possibility of resource exhaustion and making the system more stable.
?	Optimal system start-up. Using Lazy Acquisition ensures that resources that are not needed immediately are acquired at a later stage. This helps to optimize system start-up time.
?	Transparency. Using Lazy Acquisition is transparent to the resource user. The resource proxy hides the actual acquisition of the resource from the resource user.
There are some liabilities of using the Lazy Acquisition pattern:
?	Space overhead. The pattern incurs a slight space overhead, as additional memory is required for proxies resulting from the indirection.
?	Time overhead. The execution of the lazy acquisitions can introduce a significant time delay when a resource is acquired, and also overhead during regular program execution, due to the additional level of indirection. For real-time systems such behavior might not be acceptable.
?	Predictability. The behavior of a lazy acquisition system can become unpredictable. If multiple parts of a system defer resource acquisition as late as possible, it can lead to bursts when all parts of the system attempt to acquire resources at the same time.
Singleton Singletons [GoF95], objects that exist uniquely in a system, are usually instantiated using lazy instantiation. In some cases Singletons are accessed by several threads. The Double-Checked Locking [POSA2] idiom can be used to avoid race conditions between threads during instantiation.
Haskell [Thom99]. The Haskell language, like other functional programming languages, allows lazy evaluation of expressions. Haskell only evaluates as much of a program as is required to get the answer. Using this demand-driven evaluation, data structures in Haskell are evaluated just enough to deliver the answer, and parts of them may not be evaluated at all.
Java 2 platform, Enterprise Edition (J2EE) [Sun04b]. Enterprise JavaBeans (EJB) containers in J2EE application servers [Iona04] host many different components simultaneously. To avoid resource exhaustion, they need to ensure that only components that are actually used by clients are active, while others should be inactive. A typical solution is the application of Lazy Loading and Lazy Instantiation for the components and their state. This saves valuable resources and assures scalability. Also, Java Server Pages (JSPs) are typically compiled into servlets by many J2EE application servers only when they are actually accessed, rather than when they are deployed.
Ad hoc networking [Sun04c] [UPnP04]. In ad hoc networking only temporal relationships between devices and their components exist, so that it becomes too expensive to hold on to resources that are not actually needed currently. This means that components need to be loaded, instantiated, destroyed, and unloaded regularly. Ad hoc networking frameworks therefore need to offer mechanisms such as lazy loading and lazy instantiation. It is also possible to run lazy discovery of devices¡ªthe application will not be notified until the discovered device list changes from the last discovery run by the underlying framework [IrDA04].
Operating systems A common behavior of operating systems is to defer the complete loading of application libraries until they are actually needed. For example, on most Unix systems such as Solaris [Sun04h] and Linux, an environment variable called LD_BIND_NOW can be used to specify whether or not the shared objects (.so files) should be loaded using a lazy model. Under a lazy loading model, any dependencies that are labeled for lazy loading will be loaded only when explicitly referenced. By taking advantage of a function call's lazy binding, the loading of a dependency is delayed until it is first referenced. This has the additional advantage that objects that are never referenced will never be loaded.
.NET Remoting [Ramm02]. In .NET Remoting so-called ¡®singleton remote objects¡¯ are objects that can be used by multiple clients, but only one instance of the respective object type can exist at the same time in the server. Those singleton remote objects are only instantiated on first access, even though clients might obtain references to them before the first access.
COM+ [Ewal01]. Just-in-Time (JIT) activation is an automatic service provided by COM+ that can help to use server resources more efficiently, particularly when scaling up your application to do high-volume transactions. When a component is configured as being ¡®JIT activated¡¯, COM+ will at times deactivate an instance of it while a client still holds an active reference to the object. The next time the client calls a method on the object, which it still believes to be active, COM+ will reactivate the object transparently to the client, just in time. JIT activation is supported for COM+ and .NET based applications. .NET applications have to set configuration attributes of the .NET System.EnterpriseServices package.
JIT compilation JIT compilation is heavily used in today's Java virtual machines (JVM). The compilation of the regular Java byte code into fast machine-specific assembler code is done just-in-time. One of the virtual machines that supports this feature is the IBM J9 JVM [IBM02]. The opposite of JIT compilation is ahead-of-time (AOT) compilation.
Java [Sun04a]. JVM implementations optimize Java class loading typically by loading the classes when the code of that class is to be first executed. This behavior is clearly following the Lazy Loading pattern.
Manufacturing [VBW97]. Just-in-Time manufacturing, as used in many industries such as the automobile industry, follows the same pattern. Parts of an assembly are manufactured as they are needed. This saves the cost of fixed storage.
Eclipse plug-in [IBM04b]. Eclipse is a universal tool platform¡ªan open extensible IDE for anything and nothing in particular. Its extensibility is based on a plug-in architecture that allows every user to become a contributor of plug-ins. While the plug-in declarations that determine the visualization of the plug-ins' features are loaded eagerly, the actual logic, which is contained in Java archives (JAR), is loaded lazily on first use of any of the plug-in's functionality.
Heap compression The research by [GKVI+03] employs a special garbage collector for the Java programming language that allows a heap smaller than the application's footprint to be used, by compressing temporarily unused objects in the heap. When such an object is accessed again, it lazily uncompresses portions of the object, thereby allocating the necessary memory.
FrameMaker [Adob04]. The desktop publishing program FrameMaker does not open and read referenced graphic files until the page containing the graphic is first displayed. The graphic is then rendered on the screen, and the rendered image is stored in temporary storage. When the page is displayed again, Framemaker checks for any updates to the referenced file. If no changes have been made to that file, it reuses the rendered image from temporary storage.
The Eager Acquisition (53) pattern can be conceived as the opposite of Lazy Acquisition. Eager Acquisition describes the concept of acquiring resources up front to avoid acquisition overheads at the first access by clients.
Since both Lazy Acquisition and Eager Acquisition can be suboptimal in some use cases, the Pooling (97) pattern combines both into one pattern to optimize resource usage.
The Lazy Optimization [Auer96] pattern can help to tune performance once the program is running correctly and the system design reflects the best understanding of how the program should be structured.
The Thread-Specific Storage [POSA2] pattern uses a proxy to shield the creation of keys that identify the associated thread-specific object uniquely in each thread's object set. If the proxy does not yet have an associated key when accessed, it asks the key factory to create a new key.
##%%&&
The Eager Acquisition pattern describes how run-time acquisition of resources can be made predictable and fast by eagerly acquiring and initializing resources before their actual use.
Consider an embedded telecommunication application with soft real-time constraints, such as predictability and low latency in execution of operations. Assume the application is deployed on a commercial off-the-shelf (COTS) operating system such as Linux, primarily for cost as well as portability reasons.
In most operating systems, operations such as dynamic memory allocation can be very expensive. The time it takes for memory allocations via operations such as new() or malloc() depends on the implementation of the operations. In most operating systems, including Real-Time Operating Systems (RTOS), the time it takes to execute dynamic memory allocations varies.
The main reasons for this are:
?	Memory allocations are protected by synchronization primitives.
?	Memory management, such as compaction of smaller memory segments to larger ones, consumes time.
If memory compaction is not done often enough, memory allocations can easily cause memory fragmentation. However, most operating systems, including some RTOS such as VxWorks [Wind04], do not provide such memory management, which can leave an application susceptible to memory fragmentation.
A system that must satisfy high predictability and performance in resource acquisition time.
Systems with soft real-time constraints need to be stringent about how and when they acquire resources. Examples of such systems include critical industrial systems, highly scalable Web applications, or even the graphical user interface (GUI) of a desktop application. In each case, the users of such systems make certain assumptions about the predictability, latency, and performance of the system. For example, in the case of the GUI of a desktop application, a fast response is expected by the user and any delay in response can be irritating. However, if the execution of any user-initiated request results in expensive resource acquisition, such as dynamic acquisition of threads and memory, it can result in unpredictable time overheads. How can resources be acquired in systems with soft real-time constraints while still fulfilling the constraints?
To solve the problem, the following forces must be resolved:
?	Performance. Resource acquisition by resource users must be fast.
?	Predictability. Resource acquisition by resource users must be predictable¡ªit should take the same amount of time each time a resource is acquired.
?	Initialization overhead. Resource initialization at application run time needs to be avoided.
?	Stability. Resource exhaustion at run time needs to be avoided.
?	Fairness. The solution must be fair with respect to other resource users trying to acquire resources.
Eagerly acquire a number of resources before their actual use. At a time before resource use, optimally at start-up, the resources are acquired from the resource provider by a provider proxy. The resources are then kept in an efficient container. Requests for resource acquisition from resource users are intercepted by the provider proxy, which accesses the container and returns the requested resource.
The time at which the resources are acquired can be configured using different strategies. These strategies should take into account different factors, such as when the resources will be actually used, the number of resources, their dependencies, and how long it takes to acquire the resources. Options are to acquire at system start-up, or at a dedicated, possibly calculated, time after system start-up. Regardless of what strategy is used, the goal is to ensure that the resources are acquired and available before they are actually used.
The following participants form the structure of the Eager Acquisition pattern:
?	A resource user acquires and uses resources.
?	A resource is an entity such as memory or a thread.
?	A provider proxy intercepts resource acquisitions by the user and hands the eagerly-acquired resources to the resource user in constant time.
?	A resource provider provides and manages several resources.
The following CRC cards describe the responsibilities and collaborations of the participants.
The following class diagram visualizes the structure of the Eager Acquisition pattern.
The resource user does not acquire the resource directly from the resource provider, but acquires it via the provider proxy.
Scenario I The resource user creates a provider proxy, which it later uses to acquire the resource. The resource is acquired by the provider proxy before its use. At the latest, the resource is acquired when the resource user actually tries to acquire it.
Scenario II The resource user acquires a resource, but is intercepted by the provider proxy. The following sequence diagram shows how the resource user acquires the resource.
The provider proxy intercepts the acquisition request and returns an eagerly-acquired resource. The resource user can now access and use the resource.
Five steps are involved in implementing the Eager Acquisition pattern.
1.	Select resources to acquire eagerly. Determine the kind of resources to be eagerly acquired in order to guarantee predictable behavior of the overall system. Determine resources that are expensive, such as connections, memory, and threads. The acquisition of such resources is most likely to introduce unpredictability, and therefore they serve as ideal candidates for eager acquisition.
2.	Estimate resource usage. Estimate the amount of resources a resource user will acquire during its lifetime. Perform test runs and measure the maximum resource usage, if it is not known up front. If you cannot predict the resource use, make it a configuration parameter, so it can be tuned easily after deployment. Provide operator information, such as log entries, for the actual resource utilization of eagerly acquired resources.
3.	Implement the provider proxy. A provider proxy is responsible for the transparent integration of the Eager Acquisition pattern. It can be included in an actual design in several ways, for example by using a Virtual Proxy [GoF95] [POSA1] or Interceptor [POSA2]. A provider proxy can be obtained from an Abstract Factory [GoF95] by the resource user. When transparency is not required, the resource user acquires the resource directly from the resource provider, in which case a provider proxy is not required.
4.	Implement a container. Implement a container, such as a hash map, to hold the eagerly-acquired resources. The container should allow predictable lookups. For example, a hash map can provide lookup in constant time.
5.	Determine a timing strategy. Decide on a strategy, depending on when the provider proxy eagerly acquires the resources:
o	At system start-up. Implement a hook so that the code for the eager acquisition is executed at start-up time. The advantage of acquiring resources at system start-up is that the run time behavior of the system is not influenced, although it must be possible to predict the resource usage.
o	Proactively during run time. Use Reflection [POSA1] to detect system state that might lead to a need for resource acquisition by the resource user in the future. Proactive behavior has the advantage of being able to address acquisition requirements more closely. However, the cost of proactive behavior is higher complexity and execution overhead for continuous system monitoring.
6.	Determine initialization semantics. Decide on how to initialize acquired resources to avoid initialization overhead. For some resources, a complete initialization on acquisition by the provider proxy is impossible. In such cases the initialization overhead during run time should be taken into account.
Consider the embedded telecommunication application described earlier. To make the application predictable, three options exist:
?	Implement the application objects as global variables, which is basically eager instantiation of objects.
?	Put the objects on the stack, avoiding dynamic memory allocation altogether.
?	Implement a memory pool that eagerly acquires memory from the operating system up front, after system start-up but before the first time dynamic memory allocation is needed. The memory pool is used by application objects to acquire their memory.
The first option has the disadvantage of losing control over initialization and release of memory. Further, many developers consider this design to be a poor one because of the high coupling it introduces into a system. Class static variables allow the location of definition to be more visible, but still have some of the same coupling problems, as well as issues regarding timing of initialization and release of memory. Using class static variables also hard-wires the number of instances of a class, which reduces the adaptability of a piece of software, as well as its ability to accommodate runtime/load-time variability.
The second option, working only with objects allocated on the stack, demands large stack sizes. Additionally, the lifecycle of the objects would have to map the call stack, otherwise objects could go out of scope while still being used.
The third option is much more flexible regarding the initialization of application objects, but has some restrictions. As described earlier, synchronization overhead and compaction of variable-sized memory blocks are the main reasons for poor predictability of memory acquisition time. The memory pool can only become more predictable than the operating system if it is able to avoid synchronization and management of variable-sized memory allocations.
To avoid synchronization overhead, memory pools need either to be dedicated to a thread, or to internalize thread-specific issues using, for instance, Thread-Specific Storage [POSA2]. For details, see also the Thread-Local Memory Pool [Somm02] pattern.
The following C++ class implements a simple memory pool for fixed-size blocks of memory without synchronization. It is expected to be used only by a single thread. The memory it provides is acquired eagerly in its constructor. For support of multiple block sizes, the memory pool internal management of blocks would need to be extended, or separate memory pools, one for each block size, would need to be instantiated.
An instance of the Memory_Pool class should be created for each thread that is spawned and needs to allocate memory dynamically. The eager acquisition of the memory blocks in the constructor, as well as acquisition of block-size memory in the acquire() method, can throw a bad_alloc exception.
The acquire() method uses the memory that has been eagerly acquired in the constructor of Memory_Pool. Ideally, the constructor should allocate enough memory initially so that all subsequent requests by acquire() can be fulfilled. However, in case there is insufficient memory to service a request by the acquire() method, additional memory would need to be acquired from the operating system. Therefore, acquire() would need to handle such cases as well. Of course, implementing acquisition only is not sufficient, and we also need to implement a proper disposal method [Henn03].
The above code only works for structures that are really just C-like. For real classes, the memory pool must be better integrated with the new and delete operators, such as:
This would allow the struct above to be allocated as:
The price of optimization in this particular case is that for deletion, the traditional new/delete symmetry needs to be broken. Therefore, destroy the object explicitly and return its memory to the pool explicitly:
For a more detailed discussion on how to integrate custom memory management techniques, see [Meye98].
Even though the above implementation makes certain assumptions and imposes some restrictions, it has the advantage of increased predictability of dynamic memory acquisitions. Furthermore, memory fragmentation is avoided, by allocating only fixed-size blocks. For alternative implementations, see the memory pool implementations of ACE [Schm02] [Schm03a] and Boost [Boos04].
The following are some specializations of the Eager Acquisition pattern:
Eager Instantiation. In this case objects are instantiated eagerly and managed in a container. When the application, as resource user, requests new objects, new instances can be handed back from the list.
Eager Loading. Eager Loading applies eager acquisition to the loading of libraries, such as shared objects on Unix platforms, or dynamically linked libraries on Win32. The libraries are loaded up front, in contrast to Lazy Acquisition (38).
Static Allocation. Static Allocation, which is also known as Fixed Allocation [NoWe00], or Pre-Allocation, applies Eager Acquisition to the allocation of memory. Fixed Allocation is especially useful in embedded and real-time systems. In such systems memory fragmentation and predictability of the system behavior are more important than dynamic memory allocations.
Proactive Resource Allocation [Cros02]. Resource acquisitions can be made up front based on indications derived from resource usage by reflection techniques instead of purely basing it on estimations.
There are several benefits of using the Eager Acquisition pattern:
?	Predictability. The availability of resources is predictable, as requests for resource acquisitions from the user are intercepted and served instantly. Variation in delay in resource acquisition, incurred by the operating system, for example, is avoided.
?	Performance. As resources are already available when needed, they can be acquired within a short and constant time.
?	Flexibility. Customization of the resource acquisition strategy can easily be applied. Interception of resource acquisition from the user allows for strategized acquisition of resources by the provider proxy. This is very helpful in avoiding side effects such as memory fragmentation.
?	Transparency. As the resources are eagerly acquired from the resource provider without requiring any user involvement, the solution is transparent to the user.
There are some liabilities of using the Eager Acquisition pattern:
?	Management responsibility. Management of eagerly-acquired resources becomes an important aspect, as not all resources might immediately be associated with a resource user, and therefore need to be organized. Caching (83) and Pooling (97) patterns can be used to provide possible management solutions.
?	Static configuration. The system becomes more static, as the number of resources has to be estimated up front. Overly-eager acquisitions must be avoided to guarantee fairness among resource users and to avoid resource exhaustion.
?	Over-acquisition. Too many resources might be acquired up front by a subsystem that might not need all of them. This can lead to unnecessary resource exhaustion. However, properly tuned resource acquisition strategies can help to address this problem. Pooling (97) can also be used to keep a limit on the number of resources that are eagerly acquired.
?	Slow system start-up. If many resources are acquired and initialized at system start-up, a possibly long delay due to eager acquisitions is incurred by the system. If resources are not eagerly acquired at system start-up, but later, there is still an overhead associated with it.
Ahead-of-time compilation [Hope02] [NewM04] is commonly used by Java virtual machines to avoid compilation overheads during execution.
Pooling (97). Pooling solutions, such as connection or thread pools typically pre-acquire a number of resources, such as network connections, or threads, to serve initial requests quickly.
Application servers [Sun04b]. In general, a servlet container of an application server offers no guarantee about when servlets are loaded or the order in which they are loaded. However, an element called <load-on-startup> can be specified for a servlet in the deployment descriptor, causing the container to load that servlet at start-up.
NodeB [Siem03]. In the software of the Siemens UMTS base station ¡®NodeB¡¯ the connections to various system parts are eagerly acquired at system start-up. This avoids unpredictable resource acquisition errors and delays during system run time.
Hamster [EvCa01]. A real-world known use is a hamster. It acquires as many fruits as possible before eating them in its burrow. The hamster stores the food in its cheek pouch. Unfortunately, no numbers are available regarding its estimations about how much it acquires eagerly.
Eclipse plug-in [IBM04b]. Eclipse is a universal tool platform¡ªan open extensible IDE for anything and nothing in particular. Its extensibility is based on a plug-in architecture that allows every user to become a contributor of plug-ins. While the plug-in declarations that determine the visualization are loaded eagerly, the actual logic, which is contained in Java archives (JAR), is loaded lazily on first use of the any of the plug-in's functionality.
The opposite of Eager Acquisition is Lazy Acquisition (38), which allocates resources just in time, at the moment the resources are actually used.
The Pooling pattern (97) combines the advantages of Eager Acquisition and Lazy Acquisition into one pattern.
The Caching (83) pattern can be used to manage eagerly-acquired resources.
##%%&&
The Partial Acquisition pattern describes how to optimize resource management by breaking up acquisition of a resource into multiple stages. Each stage acquires part of the resource, dependent upon system constraints such as available memory and the availability of other resources.
Consider a network management system that is responsible for managing several network elements. These network elements are typically represented in a topology tree. A topology tree provides a virtual hierarchical representation of the key elements of the network infrastructure. The network management system allows a user to view such a tree, as well as get details about one or more network elements. Depending on the type of the network element, its details may correspond to a large amount of data. For example, the details of a complex network element may include information about its state as well as the state of its components.
The topology tree is typically constructed at application start-up or when the application is restarting and recovering from a failure. In the first case, the details of all the network elements, along with their components and subcomponents, are usually fetched from the physical network elements. In the latter case, this information can be obtained from a persistent store as well as from the physical network elements. However, in either case obtaining all this information can have a big impact on the time it takes for the application to start up or recover. This is because completely creating or recovering a network element would require creating or recovering all its components. In addition, since each component can in turn be comprised of additional subcomponents, creating or recovering a component would in turn require creating or recovering all its subcomponents. Therefore the size of the resulting hierarchical topology tree, as well as the time it takes to create or recover all its elements, can be hard to predict.
Systems that need to acquire resources efficiently. The resources are characterized by either large or unknown size.
Highly robust and scalable systems must acquire resources efficiently. A resource can include local as well as remote resources. Eager acquisition (53) of resources can be essential to satisfy resource availability and accessibility constraints. However, if these systems were to acquire all resources up front, a lot of overhead would be incurred and a lot of resources would be consumed unnecessarily. On the other hand, it may not be possible to lazily acquire all the resources, since some of the resources may be required immediately at application start-up or recovery. To address these conflicting requirements of resource acquisition requires resolution of the following forces:
?	Availability. Acquisition of resources should be influenced by parameters such as available system memory, CPU load, and availability of other resources.
?	Flexibility. The solution should work equally well for resources of fixed size and for resources of unknown or unpredictable size.
?	Scalability. The solution should be scalable with the size of the resources. 
?	Performance. The acquisition of resources should have a minimal impact on system performance.
Split the acquisition of a resource into two or more stages. In each stage, acquire part of the resource. The amount of resources to acquire at each stage should be configured using one or more strategies. For example, the amount of resources to acquire at each stage can be governed by a strategy that takes into account available buffer space and required response time, as well as availability of dependent resources. Once a resource has been partially acquired, the resource user may start using it on the assumption that there is no need to have the entire resource available before it can be used.
Patterns such as Eager Acquisition (53) and Lazy Acquisition (38) can be used to determine when to execute one or more stages to partially acquire a resource. However, the Partial Acquisition pattern determines in how many stages a resource should be acquired, together with the proportion of the resource that should be acquired in each stage.
The following participants form the structure of the Partial Acquisition pattern:
?	A resource user acquires and uses resources.
?	A resource is an entity such as audio/video content. A resource is acquired in multiple stages.
?	A resource provider manages and provides several resources.
The following CRC cards describe the responsibilities and collaborations of the participants.
The dependencies between the participants are shown in the following class diagram.
Scenario I The sequence diagram below shows how the resource user partially acquires a resource in a series of acquisition steps. When all parts have been acquired, it accesses the resource.
Scenario II In some cases the resource user might not know when the next part of the resource is ready to be acquired. For example, a resource can be created incrementally, such as the arrival of network packets of a message stream. In such cases the user might not want to block on the acquisition, but be informed about the occurrence of such an event. The Reactor [POSA2] pattern is very useful for such scenarios. Here is how it works dynamically.
The reactor is triggered by some event from the resource provider about the availability of (parts of) the resource. The reactor in turn dispatches this event to the resource user, which in turn performs a partial acquisition. For more details, refer to [POSA2].
There are six steps involved in implementing the Partial Acquisition pattern.
1.	Determine the number of stages. The number of stages in which a resource should be acquired depends on system constraints such as available memory and CPU, as well as other factors such as timing constraints and the availability of dependent resources. For a resource of unknown or unpredictable size it may not be possible to determine the number of stages that it would take to acquire the entire resource. In this case, the number of stages would not have an upper bound, and a new stage would be executed until the resource has been completely acquired.
 In the case of the network management system in our example, the number of stages could correspond to the number of hierarchy levels in the topology tree. At each stage, an entire level of the hierarchy can be constructed by obtaining the details of the components of that level. If a level in the hierarchy is complex and contains a large number of components, obtaining the details of all the components of that level can be further divided into two or more stages.
2.	Select an acquisition strategy. Determine when each stage of resource acquisition should be executed. Patterns such as Lazy Acquisition (38) and Eager Acquisition (53) can be used to control when one or more stages of resource acquisition should be executed. For example, Eager Acquisition can be used to acquire an initial part of the resource. The remaining parts can then be acquired using Lazy Acquisition, or can be acquired some time in between, after the system has started but before a user requests them.
 In the case of the network management system, Eager Acquisition (53) can be used to fetch the details of the network elements, but not of its internal components. The details of the components and sub-components of a network element can be fetched using Lazy Acquisition (38) when the user selects the network element in the GUI and tries to view the details of its components.
3.	Determine how much to acquire. Configure strategies to determine how much to acquire partially at each stage. Different strategies can be configured to determine how much of a resource should be acquired in each stage. If the size of a resource is deterministic then a simple strategy can evenly distribute the amount of the resource to acquire at each stage among all the stages of resource acquisition. A more complex strategy would take into account available system resources. Thus, for example, if sufficient memory is available, such a strategy would acquire a large part of the resource in a single stage. At a later stage, if system memory is low, the strategy would acquire a smaller part of the resource.
Such an adaptive strategy can also be used if the size of the resource to be acquired is unknown or unpredictable. Additional strategies can be configured that make use of other parameters such as required response time. If there are no system constraints, another strategy could be used to acquire greedily as much of the resource as is available. Such a strategy would ensure that the entire resource is acquired in the shortest possible time. A good understanding of the application semantics is necessary to determine the appropriate strategies that should be used. 
4.	Introduce a buffer (optional). Determine whether the partially-acquired resource should be buffered. Buffering a resource can be useful if the size of the resource is unknown, or if the entire resource needs to be consolidated in one place before being used. If a resource needs to be buffered, the amount of buffer space to allocate should be determined to ensure the entire resource can fit. For a resource of unknown or unpredictable size, a buffer size should be allocated that is within the system constraints (such as available memory) but sufficiently large to handle the upper bound on the size of the resources in the system.
5.	Implement an acquisition trigger. Set up a mechanism that is responsible for executing each stage of resource acquisition. Such a mechanism would then be responsible for acquiring different parts of a resource in multiple stages. Patterns such as Reactor [POSA2] can be used to implement such a mechanism. For example, a reactor can be used to acquire parts of a resource as they become available. An alternative mechanism can be set up that acquires parts of the resource proactively [POSA2].
6.	Handle error conditions and partial failures. Error conditions and partial failures are characteristic of distributed systems. When using Partial Acquisition, it is possible that an error occurs after one or more stages have completed. As a result, part of a resource may have been acquired, but the attempt to acquire the subsequent parts of the resource would fail. Depending upon the application semantics, such a partial failure may or may not be acceptable. For example, if the resource being acquired in multiple stages is the contents of a file, then a partial failure would make inconsistent the data that has already been acquired successfully.
 On the other hand, in the case of the network management system in our example, the failure to obtain the details of one of the subcomponents will not have an impact on the details of the remaining components acquired successfully. A partial failure could still make the details of successfully acquired components available to the user.
One possible way to handle partial failures is to use the Coordinator (111) pattern. This pattern can help to ensure that either all stages of resource acquisition are completed, or none are.
Consider the example of a network management system that is responsible for managing a network of several network elements. The network elements themselves consist internally of many components, such as their CPU board, the connection switch, and the memory. Loading the details of those components can take a long time. Using the Partial Acquisition pattern, the acquisition of the details of the network elements, along with their components, can be split into multiple stages. In the initial stage only the details of the network elements will be fetched from the physical network and a database. The details of the components of the network elements will not be fetched.
The topology manager of the network management system provides details about the network element and its components to the visualization subsystem, so that they can be displayed. The topology manager retrieves the information from the database or from the physical network elements, depending on the kind of data, static configuration or current operating parameters. The Java code below shows how a TopologyManager can defer fetching the details of the components of a network element to a later stage.
The actual retrieval of network element components is done asynchronously by an Active Object [POSA2]. For this, requests are queued with the Scheduler, which runs in the active object's thread. The requests will fetch the network element's components when triggered by the Scheduler.
The main logic for retrieving the details of the components of the network element is contained in the FetchNEComponents class. It implements the Request interface, so that it can be scheduled by the Scheduler.
The Scheduler will invoke canRun() to check if the Request can be executed. If it returns true it invokes call(), else it reschedules the request.
The interactions between the classes are illustrated in the following sequence diagram. When the user selects a network element, the topology manager creates a request that acquires the shelf and card information step by step. At each step the retrieved information is handed to the user interface for visualization. On the selection of a shelf of the same network element by the user, the details are returned quickly, as they have been fetched in the background.
Using Partial Acquisition, the topological tree is therefore constructed in multiple stages. The result is a significant increase in performance when accessing network elements. The first request for details of a network element, which is usually done at start-up, triggers the partial acquisition of the details of the components of the network element and makes them available by the time the user requests them. In addition, further logic can also be added such that the partial acquisition of the details of a network element can trigger the retrieval of the neighboring network elements. To integrate Partial Acquisition transparently, an Interceptor [POSA2] can be used (see Variants).
Transparent Partial Acquisition. An interceptor can be introduced that would intercept a resource user's requests for resource acquisition and, using configured strategies, acquire an initial part of a resource. It would then acquire the remaining parts of the resource in additional stages transparent to the resource user. For instance, in the motivating example, an interceptor can be used to intercept requests and only fetch the network elements. The interceptor would not fetch the subcomponents immediately, as they can be fetched at a later stage transparent to the resource user.
There are several benefits of using the Partial Acquisition pattern:
?	Reactive behavior. The Partial Acquisition pattern allows acquisition of resources that become available slowly or partially. If this partial acquisition is not done, the resource user would have to wait an undefined time before the resource became completely available.
?	Scalability. The Partial Acquisition pattern allows the size of the resource being acquired to be scalable. The number of stages in which a resource is acquired can be configured depending upon the size of the resource being acquired.
?	Configurability. The Partial Acquisition pattern can be configured with one or more strategies to determine in how many stages to acquire a resource, as well as how much of the resource to acquire at each stage.
There are some liabilities of using the Partial Acquisition pattern:
?	Complexity. User algorithms that handle the resources need to be prepared to handle only partially-acquired resources. This can add a certain level of complexity to applications. In addition, using the Partial Acquisition pattern results in error handling becoming more complex. If one stage fails, an error-handling strategy must assure that the complete activity is restarted or corrected. On the other hand, error handling also becomes more robust. If a stage acquisition fails, the same stage can be reloaded without the necessity for a complete acquisition restart.
?	Overhead. The Partial Acquisition pattern requires a resource to be acquired in more than one stage. This can result in an overhead of additional calls being made to acquire different parts of the same resource.
Incremental image loading Most modern Web browsers such as Netscape [Nets04], Internet Explorer [Micr04], or Mozilla [Mozi04] implement Partial Acquisition by supporting incremental loading of images. The browsers first download the text content of a Web page and at the same time create markers where the images of the page will be displayed. The browsers then download and display the images incrementally, during which the user can read the text content of the page.
Socket input [Stev03]. Reading from a socket also typically makes use of the Partial Acquisition pattern. Since data is typically not completely available at the socket, multiple read operations are performed. Each read operation partially acquires data from the socket and stores it in a buffer. Once all the read operations complete, the buffer contains the final result.
Data-driven protocol-compliant applications Data-driven applications that adhere to specific protocols also make use of the Partial Acquisition pattern. Typically such applications follow a particular protocol to obtain data in two or more stages. For example, an application handling CORBA IIOP (Internet Inter-ORB Protocol) [OMG04a] requests typically reads the IIOP header in the first step to determine the size of the request body. It then reads the contents of the body in one or more steps. Note that such applications therefore use partially-acquired resources. In the case of an application handling an IIOP request, the application makes uses of the IIOP header obtained in the first stage.
Heap compression The research by [GKVI+03] employs a special garbage collector for the Java programming language that allows the use of a heap smaller than the application's footprint. The technique compresses temporarily unused objects in the heap. When a compressed object is accessed again, only a part of the object is uncompressed, resulting in partial allocation of the memory. The remaining parts of the object are uncompressed in subsequent stages, as and when required.
Audio/Video streaming [Aust02]. When decoding audio and video streams the streams are acquired in parts. For videos, the algorithms typically acquire one or several frames at once, which are decoded, buffered, and displayed.
The Reactor [POSA2] pattern is designed to demultiplex and dispatch events efficiently. In the case of I/O operations it allows multiple resources that are acquired partially to be served reactively. Buffering can be used to keep partially-acquired resources in each stage. This can be especially useful if the size of the resource being acquired is unknown.
##%%&&
The Caching pattern describes how to avoid expensive re-acquisition of resources by not releasing the resources immediately after their use. The resources retain their identity, are kept in some fast-access storage, and are re-used to avoid having to acquire them again.
Consider a network management system that needs to monitor the state of many network elements. The network management system is typically implemented as a three-tier system. End-users interact with the system using the presentation layer, typically a GUI. The middle-tier, comprising the business logic, interacts with the persistence layer, and is also responsible for communicating with the physical network elements. Because a typical network may consist of thousands of network elements, it is very expensive to set up persistent connections between the middle tier, the application server, and all the network elements. On the other hand, an end-user can select any of the network elements using the GUI to obtain details of the network element. The network management system must be responsive to the user request, and hence should provide low latency between user selection of a network element and the visualization of its properties.
Establishing a new network connection for every network element selected by the user and destroying it after use incurs overhead in the form of CPU cycles inside the application server. In addition, the average time to access a network element can be too large.
Systems that repeatedly access the same set of resources and need to optimize for performance.
Repetitious acquisition, initialization, and release of the same resource causes unnecessary overhead. In situations in which the same component or multiple components of a system access the same resource, repetitious acquisition and initialization incurs cost in terms of CPU cycles and overall system performance. The cost of acquisition, access, and release of frequently used resources should be reduced to improve performance.
To address the problem the following forces need to be resolved:
?	Performance. The cost of repetitious resource acquisition, initialization, and release must be minimized.
?	Complexity. The solution should not make acquisition and release of resources more complex and cumbersome. In addition, the solution should not add unnecessary levels of indirection to the access of resources.
?	Availability. The solution should allow resources to be accessible even when the resource providers are temporarily unavailable.
?	Scalability. The solution should be scalable with regard to the number of resources.
Temporarily store the resource in a fast-access buffer called a cache.
Subsequently, when the resource is to be accessed again, use the cache to fetch and return the resource instead of re-acquiring it from the resource provider, such as an operating system that is hosting resources. The cache identifies resources by their identity, such as pointer, reference, or primary key.
To retain frequently-accessed resources and not release them helps to avoid the cost of re-acquisition and release of resources. Using a cache eases the management of components that access the resources.
When resources in a cache are no longer needed, they are released. The cache implementation determines how and when to evict resources that are no longer needed. This behavior can be controlled by strategies.
The following participants form the structure of the Caching pattern:
?	A resource user uses a resource.
?	A resource is an entity such as a connection.
?	A resource cache buffers resources that resource users release.
?	A resource provider owns and manages several resources.
The following CRC cards show how the participants interact with each other
The following class diagram illustrates the structure of the Caching pattern.
The resource provider, for example an operating system, hosts the resources that are initially acquired by the resource user. The resource user then accesses the resource. When no longer needed, the resource is released to the cache. The resource user uses the cache to acquire resources that it needs to access again. Acquisition of a resource from the cache is cheaper with respect to CPU utilization and latency compared to acquisition from the resource provider.
The following figure shows how the resource user acquires a resource from the resource provider. The resource is then accessed by the user. After the resource has been used it is put into a cache instead of releasing it to the resource provider.
When the resource user needs to access the same resource again, it uses the cache to retrieve it.
To implement the Caching pattern, the following steps should be followed:
1.	Select resources. Select resources that benefit from caching. These are typically resources that are expensive to acquire and are used frequently. Caching is very often introduced as an optimization technique after identification of performance bottlenecks.
In distributed systems two forms of caching can exist: client-side and server-side caching. Client-side caching is useful to save the bandwidth and time it takes to transmit server data repeatedly to the client. On the other hand, server-side caching is useful when many client requests lead to repeated acquisitions and release of the same resource in the server.
2.	Determine a caching interface. When resources are released and reacquired from the cache directly by the resource user, a proper interface must be designed. This interface needs to offer a release() and an acquire() method.
3.	The interface above relies on the availability of a separate resource ID, but this might not be the case for every resource. In some cases the ID of a resource might need to be calculated from properties of the resource.[1] 
The release() method is called by the resource user when it releases the resource to the cache, instead of releasing it to the resource provider.
4.	Implement the cache. The implementation of the acquire() and release() methods of the Cache interface provides the key functionality of the cache.
 The following code snippet shows the implementation of the release() method that is called on resource release by the resource user.
1.	The release() method adds the resource to the map so that a later call on acquire can find it via its ID. For optimization reasons it is advisable to use a hash map, since that can perform look-up in almost constant time. The Comparand pattern [CoHa01] gives some ideas of how to perform comparisons between IDs. Depending on the kind of resource, the ID of the resource has to be determined first. In the case of our example, the resource can identify itself.
The acquire() method of the cache implementation should be responsible for looking up the resource from the map based on the ID. As a variant, when acquisition of the resource from the cache fails, which means that the resource with the required identity has not been found, the cache itself can acquire the resource from the resource provider. For more details, please refer to the Transparent Cache variant in the Variants section
 The following piece of code shows an implementation of the acquire() method.
1.	Determine how to integrate the cache (optional). If the cache has to be integrated transparently, use an Interceptor [POSA2] or a Cache Proxy [POSA1]. Introducing an Interceptor or a Cache Proxy reduces the complexity of releasing resources explicitly to and re-acquiring resources from the cache by making the operations transparent. See the Transparent Cache variant in the Variants section for information on how to make the operations transparent. However, even with this approach, the level of indirection as a result of the look-up in the cache cannot be avoided.
2.	Decide on an eviction strategy. The resources stored in the cache require storage space. When not used for a long time, it becomes inefficient to hold on to those resources. Therefore use an Evictor (168) to remove resources that are no longer being used. The integration of the Evictor can be done in multiple ways. For example, an Evictor can be invoked in the context of the release() method, or can be invoked by a timer at regular intervals. Of course, this behavior influences overall predictability. In addition, the Evictor can be configured with different strategies, such as Least Recently Used (LRU), Least Frequently Used (LFU), and other domain-specific strategies. The Strategy [GoF95] pattern can be used for this.
3.	Ensure consistency. Many resources have states associated with them that must be properly initialized on resource creation. In addition, when resources are accessed using write operations, consistency needs to be ensured between the original resource and the resource in the cache that is mirroring it. When the original changes, callbacks are used to tell the mirror to update its copy. When the mirror changes, most caches apply a strategy called ¡®write-through¡¯. Using this strategy, changes to the mirror are applied both to the original and to the mirror directly. This functionality is typically implemented by an entity called a Synchronizer. Thus the Synchronizer becomes an important participant of the pattern. Some caches further optimize this functionality by introducing more complex logic for keeping the original and its mirror in the cache consistent.
Use a Strategy [GoF95] to decide on when to synchronize. In some cases only special operations, such as write operations, need to be synchronized immediately, whereas in other cases a periodic update might be advisable. Also, synchronization might be triggered by external events, such as updates of the original by other resource users.
 In our motivating example, if the physical data at the network element changes, the memory representation of the network element that is cached must change. Similarly, if the user changes a setting of a network element, the change must be reflected at the physical network element.
Consider the network management system that needs to monitor the state of many network elements. The middle tier of the network management system will use the Caching pattern to implement a cache of connections to the network elements. On user access of a specific network element, the connection to the network element is acquired. When the connection is no longer needed, it is added to the cache. Later, when a new request for the connection arrives, the connection is acquired from the cache, thus avoiding high acquisition cost.
Subsequent connections to other network elements will then be established when the user first accesses them. When the user switches context to another network element, the connection is put back into the connection cache. If a user accesses the same network element, the connection will be reused. No delay will occur on the first access of the reused connections.
Transparent Cache. When the cache must be integrated transparently, the cache can use Lazy Acquisition (38) to acquire the resources that the client requests. This is only possible if the cache has knowledge of how to acquire and initialize such resources. By lazily acquiring resources initially, the resource user can remain oblivious of the existence of the cache.
Read-Ahead Cache. In a situation in which repetitious Partial Acquisition (66) is used to acquire resources, the system can be designed efficiently if a read-ahead cache is used. The read-ahead cache can acquire resources before they are actually used, ensuring that the resources are available when needed.
Cached Pool. A combination of a cache and a pool can be used to provide a sophisticated resource management solution. When a resource needs to be released from the cache, instead of returning it to the resource provider, it can be put into a pool. The cache therefore serves as an intermediate storage for the resource. The cache is configured with a timeout¡ªonce the time expires, the resource loses its identity and goes into the pool. The advantage is a small optimization¡ªif the same resource is required and the resource has not yet been returned to the pool, you avoid the cost of initialization (the virtue of caching).
Layered Cache. In complex systems, the Caching pattern is often applied at several levels. For example, the Websphere Application Server (WAS) [IBM04a] applies caching at multiple levels. WAS provides the ability to cache the results of EJB methods actively. Storing commands in a cache for reuse by subsequent callers allows requests to be handled in the business logic tier rather than in the data tier, where processing is more expensive. WAS also provides a prepared statement cache that can be configured with the backend database for dynamic/static SQL statement processing. Depending on application data access patterns, the WAS prepared statement cache can improve application performance. To increase the performance of JNDI operations, WAS employs caching to reduce the number of remote calls to the name server for lookup operations. Finally, WAS provides data access beans that offer caching of database query results.
Caching adds some performance overhead due to an additional level of indirection, but overall there is a performance gain, since resources are acquired faster.
There are several benefits of using the Caching pattern:
?	Performance. Fast access to frequently used resources is an explicit benefit of caching. Unlike Pooling (97), caching ensures that the resources maintain their identities. Therefore, when the same resource needs to be accessed again, the resource need not be acquired or fetched from somewhere¡ªit is already available.
?	Scalability. Avoiding resource acquisition and release is an implicit benefit of caching. Caching by its nature is implemented by keeping frequently-used resources. Therefore, just like Pooling, Caching helps to avoid the cost of resource acquisition and release. This especially pays off in frequent usage, therefore improving scalability.
?	Usage complexity. Caching ensures that the complexity of acquiring and releasing resources from a resource user perspective does not increase, apart from the additional step required to check the availability of the resource in the cache.
?	Availability. Caching resources increases availability of resources whose resource providers are temporarily unavailable, since the cached resources are still available.
?	Stability. Since Caching reduces the number of releases and re-acquisitions of resources, it reduces the chance of memory fragmentation leading to greater system stability. This is similar to Pooling.
There are some liabilities of using the Caching pattern:
?	Synchronization complexity. Depending on the type of resource, complexity increases, because consistency between the state of the cached resource and the original data, which the resource is representing, needs to be ensured. This complexity becomes even worse in clustered environments, which in some extreme cases might render the pattern useless.
?	Durability. Changes to the cached resource can be lost when the system crashes. However, this problem can be avoided if a synchronized cache is used.
?	Footprint. The run-time footprint of the system is increased, as possibly unused resources are cached. However, if an Evictor (168) is used, the number of such unused cached resources can be minimized.
Caches are not a good idea if the application requires data to be always available on the expensive-to-access resources. For example, interrupt-driven I/O intensive applications as well as embedded systems often have no hardware memory caches.
A general note on optimizations: Caching should be applied carefully when other means, such as optimizing the acquisition of the resource itself, cannot be further improved. Caching can introduce some implementation complexity, complicating the maintenance of the overall solution, and increase the overall resource consumption, for example of memory, because cached resources are not released. Therefore consider the trade-off between performance, resource consumption, and complexity before applying Caching.
Hardware cache [Smit82]. Almost every central processing unit (CPU) has a hardware memory cache associated with it. The cache not only reduces mean memory access time, but also helps reduce bus traffic. Cache memory is typically faster than RAM by a factor of at least two.
Caching in file systems and distributed file systems [NW088] [Smit85]. Distributed file systems use Caching on the server side to avoid reading file blocks from disk all the time. The most frequently- used files are kept in the server's memory for quick access. File systems also use caching of file blocks on the client side to avoid network traffic and the latency incurred when the data has to be fetched from the server.
Data transfer object [Fowl02]. Middleware technologies such as CORBA [OMG04a] and Java RMI [Sun04g] allow the remote transfer of objects, as opposed to pure remote method invocation on a remote object. The remote object is actually transferred by value between the client and the server when methods are invoked on them locally. This is done to minimize the number of expensive remote method invocations. The object is held locally in a cache and represents the actual remote object. Though this approach improves performance, synchronization between the local copy and the remote original of the object must be implemented by the user.
Web proxy [Squi04]. A Web proxy is a proxy server located between a Web browser and Web servers. The Web proxy is accessed by the Web browser every time a Web page is accessed from any Web server in the World Wide Web. The Web proxy caches a Web page that is fetched by a browser from a Web server, and returns it when a subsequent request is sent for the same page. As a result, a Web proxy helps to reduce the number of Web page requests sent over the Internet by keeping frequently requested Web pages in its local cache.
Web browsers. Most popular Web browsers such as Netscape [Nets04] and Internet Explorer [Micr04] cache frequently-accessed Web pages. If a user accesses the same page, the browsers fetch the contents of the page from cache, thus avoiding the expensive retrieval of the contents from the Web site. Timestamps are used to determine how long to maintain the pages in cache and when to evict them.
Paging [Tane01] [NoWe00]. Modern operating systems keep pages in memory to avoid expensive reads from swap space on disk. The pages that are kept in memory can be regarded as being kept in a cache. Only when a page is not found in the cache does the operating system fetch it from disk.
File cache [KiSa92]. File caches improve performance and allow the files and directories of a mounted network drive to be available locally when off-line without a network connection. The files and directories are cached and synchronized against the original when connection is made to the network. Latest versions of the Microsoft operating system support file caches through a feature known as ¡®Offline Files¡¯.
.NET [Rich02]. The .NET data sets can be viewed as in-memory databases. They are instantiated locally and filled by a SQL query on the database with one or more tables, using an SqlDataAdapter. From then on, clients can access the data in object-oriented fashion. The changes will only be reflected in the original database after the SqlDataAdapter is used to update them explicitly. Consistency with changes in the original data source is not automatically ensured.
Enterprise JavaBeans (EJB) [Sun04b]. Entity Beans of EJB represent database information in the middle tier, the application server. This avoids expensive data retrieval (resource acquisition) from the database.
Object cache [Orac03] [Shif04]. An object cache applies the pattern to the paradigm of object orientation. In this case, the resources are objects that have a certain cost associated with them when created and initialized. The object cache allows such expensive operations to be avoided when the resource's use allows for caching.
Data cache [RoDe90] [Newp04]. A data cache applies the pattern to data. Data is viewed as a resource that is in some cases hard to acquire. For example, such data could include a complex and expensive calculation, or some information that needs to be retrieved from secondary storage. This pattern allows fetched data to be reused to avoid expensive re-acquisition of data when it is needed again.
iMerge [Luce03]. The iMerge EMS is an element management system for the iMerge VoIP (Voice over Internet Protocol) hardware system, which uses SNMP as its communication interface. It uses Caching to optimize visualization and provisioning of lines between network elements.
The Pooling (97) pattern is similar to Caching, but has a major difference. The main idea behind Pooling is reuse of a resource that does not have an identity. It helps to avoid the cost of re-acquisition and release of resources. A resource is typically without any identity. The Caching pattern is different than the Pooling pattern, since Caching maintains the identity of a resource in memory. The difference allows Pooling to acquire resources transparently, whereas Caching expects the resource user to acquire the resource.
The Eager Acquisition (53) pattern typically uses Caching to manage eagerly-acquired resources.
An Evictor (168) can be used for eviction of cached data.
The Resource Lifecycle Manager (128) pattern can use Caching internally to provide fast access to stateful resources.
A Cache Proxy [POSA1] can be used to hide caching effects. Smart Proxies [HoWo03] in TAO [Schm98] [OCI04] intercepting remote invocations are especially designed for this.
The Cache Management [Gran98] pattern focuses on how to combine a cache with the Manager [Somm98] pattern, where the Manager pattern centralizes access, creation and destruction of objects. The description is more specific to objects and database connections, both in the context of Java.
The Page Cache [TMQH+03] pattern is a specialized cache for improving response times when dynamically-generated Web pages are accessed. A page cache is associated with a Web server, which uses it to store accessed pages indexed by their URL. When the same URL is requested, the Web server queries the cache and returns the cached page instead of dynamically generating its content again.
##%%&&
The Pooling pattern describes how expensive acquisition and release of resources can be avoided by recycling resources that are no longer needed. Once the resources are recycled and pooled, they lose their identity and state.
Consider a simple Web-based e-commerce application that allows users to select and order one or more items. Assume that the solution is implemented in Java using a three-tier architecture. The clients consist of Web browsers that communicate with a Java servlet engine such as Tomcat. The business logic is implemented by one or more Java servlets that execute in the Java servlet engine. The servlets themselves connect to a database using a Java Database Connection (JDBC) interface. The following figure shows such a set-up with two servlets executing in the servlet engine and connecting to a database.
Most of the Web pages of the catalog are dynamically generated and depend on the database for their contents. For example, to obtain a list of available items in the catalog along with their pricing, a servlet connects to the database and executes a query. The servlet uses the results of the query to generate the HTML page that is displayed to the user. Similarly, if a user makes a purchase and enters payment details via an HTML form, the servlet connects to the database and updates it.
To execute SQL statements on the database, a servlet needs to obtain a connection to it. A trivial implementation would create a connection to the database for every request of the servlet. Such a solution can be very inefficient, since it incurs a delay in creating a connection to the database for every user request. In addition, such a solution is expensive, since it results in potentially thousands of connections being created within a short time period.
An optimization could be to store a connection in the session context of the servlet engine. This would allow reuse of a connection within session. Multiple requests from a user belonging to the same session would use the same connection to the database. However, an on-line catalog can potentially be accessed by hundreds of users simultaneously. So even with this solution a large number of database connections would be needed to meet the requirements.
Systems that continuously acquire and release resources of the same or similar type, and have to fulfill high scalability and efficiency demands.
Many systems require fast and predictable access to resources. Such resources include network connections, instances of objects, threads, and memory. Besides providing fast and predictable access to resources, systems also require the solution to scale across the number of resources used, as well as the number of resource users. In addition, individual user requests should experience very little variation in their access time. Thus the acquisition time for resource A should not vary significantly from the acquisition time for resource B, where A and B are resources of the same type.
To solve the above-mentioned problems, the following forces need to be resolved:
?	Scalability. Released resources should be reused to avoid the overhead of re-acquisition.
?	Performance. Wastage of CPU cycles in repetitious acquisition and release of resources should be avoided. 
?	Predictability. The acquisition time of a resource by a resource user should be predictable even though direct acquisition of the resource from the resource provider can be unpredictable.
?	Simplicity. The solution should be simple to minimize application complexity.
?	Stability. Repetitious acquisition and release of resources can also increase the risk of system instability. For example, repetitious acquisition and release of memory can lead to memory fragmentation on operating systems without sophisticated memory management. The solution should minimize system instability.
Patterns such as Eager Acquisition (53) and Lazy Acquisition (38) resolve only a subset of these forces. For example, Eager Acquisition allows for predictable resource acquisition, whereas Lazy Acquisition focuses more on avoiding unnecessary acquisition of resources.
In summary, how can resource acquisition and access be made efficient while ensuring that predictability and stability are not compromised?
Manage multiple instances of one type of resource in a pool. This pool of resources allows for reuse when resource users release resources they no longer need. The released resources are then put back into the pool.
To increase efficiency, the resource pool eagerly acquires a fixed number of resources after creation. If the demand exceeds the available resources in the pool, it lazily acquires more resources. To free unused resources several alternatives exist, such as those documented by Evictor (168) or Leasing (149).
When a resource is released and put back into the pool, it should be made to lose its identity by the resource user or the pool, depending on the strategy used. Later, before the resource is reused, it needs to be re-initialized. If a resource is an object, providing a separate initialization interface can be useful. The resource identity, in many cases a pointer or a reference, is not used for identification by the resource user or the pool.
The following participants form the structure of the Pooling pattern:
?	A resource user acquires and uses resources. 
?	A resource is an entity such as memory or a thread.
?	A resource pool manages resources and gives them to resource users in response to acquisition requests.
?	A resource provider, such as an operating system, owns and manages resources.
The following CRC cards describe the responsibilities and collaborations of the participants.
The structure of the pattern is shown by the following class diagram.
The resource user only depends on the resource pool and the actual resources. Resources are acquired by the pool from the resource provider.
The interactions between the participants are shown in the following sketch.
The interaction between the participants varies slightly depending on whether the resource pool eagerly acquires resources up front or not.
Assuming the pool does acquire the resources up front, subsequent acquisition requests from resource users are served from those resources. Resource users release resources to the resource pool when no longer needed. The resources are then recycled by the pool and returned on new acquisition requests.
A slightly more complex situation is described in the following sequence diagram, in which an acquisition request by a user leads to the acquisition of a resource from the resource provider. Acquisition on demand is done because no pre-acquired resources are available. The resource user accesses and uses the resource. When no longer needed, the user releases the resource back to the resource pool. The resource pool uses statistical data about resource usage to determine when to evict the resources from the pool. The statistical data includes use characteristics, such as last use and frequency of use. The second part of the figure shows a scenario in which the resource pool decides to evict the resource based on statistical data after it has been released by the resource user.
Eight steps are involved in implementing the Pooling pattern.
1.	Select resources. Identify resources that would benefit from being pooled in a resource pool. Simplify resource management by grouping resources into separate pools by their types. Grouping resources of different types into the same pool can complicate their management, as it makes multiple sub-pools and ad hoc lookups necessary.
2.	Determine the maximum size of the pool. To avoid resource exhaustion, define the maximum number of resources that are maintained by the resource pool. The maximum number of resources available in the resource pool equals the number of eagerly-acquired resources plus the number of resources acquired on-demand, for details refer to Lazy Acquisition (38). The maximum number of resources is typically set at initialization time of the pool, but it is also possible to base it on configurable parameters such as the current system load.
3.	Determine the number of resources to acquire eagerly. To minimize resource acquisition time at run-time, it is recommended to eagerly acquire at least the average number of resources typically used. This reduces acquisitions during the routine execution of the application. User requirements, along with system analysis, help to determine the average number of resources to be acquired. It is important to remember that too many eagerly-acquired resources can be a liability, as they lead to additional resource contention, and should therefore be avoided. See also the Eager Acquisition (53) pattern.
4.	Define a resource interface. Provide an interface that all pooled resources need to implement. The interface facilitates the maintenance of a collection of resources.
5.	An implementation of the Resource interface maintains context information that is used to determine whether to evict the resource, as well as when to evict it. The context information includes timestamps and usage counts.
6.	For legacy code integration, where it may not be possible to have the resource implement the Resource interface, an Adapter [GoF95] class can be introduced. The adapter class can implement the Resource interface and then wrap the actual resource. The context information can then be maintained by the adapter class.
7.	Define a pool interface. Provide an interface for the acquisition and release of resources by resource users.
8.	An implementation of the Pool interface would maintain a collection of Resource objects. When a resource user tries to acquire a resource, the resource would be fetched from this collection. Similarly, when the resource user releases the resource, it would be returned back to the collection.
1.	The code above shows one way of doing creation, initialization and eviction of connections. Of course, it is also possible to make those operations flexible, for example by using Strategy [GoF95] and Abstract Factory [GoF95].
The following class diagram shows the structure of the abovementioned classes.
2.	Provide eviction of resources. A large number of resources in the pool indicate that many unused resources exist, wasting space and degrading performance. To minimize system performance degradation, the pool size should be reduced to a reasonable size by releasing some resources from the pool.
3.	Determine resource recycling semantics. Resource recycling varies depending on the type of the resource. For example, recycling of a thread requires cleaning up of its stack and initialization of memory. In the case of recycling objects, Philip Bishop and Nigel Warren offer in their Java Tip [BiWa04a] a way to decompose objects into multiple smaller objects, which are then recycled one by one.
4.	Determine failure strategies. In the case of any recoverable failure of resource acquisition or release, exceptions and/or error messages should be handled. If it is impossible to recover from a failure, exceptions should be thrown or a ¡®null¡¯ resource, such as a NULL pointer for malloc(), should be handed back to the resource user. In case recycling fails, there are other patterns that can help minimize the impact of failure. For example, the Java Tip [BiWa04a] mentioned above also describes how to recycle broken objects by dividing complex objects into smaller objects. These smaller objects are either recycled, or re-created if broken, then reassembled into the complex object.
In the case of the Web-based shop example, the database connections are pooled in a connection pool. On every request by the user, the servlet acquires a connection from the connection pool, which it uses to access the database with queries and updates. After access, the connection is released to the connection pool in the context of the current request. As the number of concurrent requests is smaller than the number of current users, fewer connections are needed in total and expensive acquisition cost is avoided.
The figure above shows how individual servlets acquire connections from the connection pool to connect to the database. If the connection pool has no connection available and the maximum number of connections is not reached, new connections are created on demand.
The following specializations reflect applications of this pattern to specific areas such as memory allocations, connection management, or object technology. Some of those specializations have been documented in existing literature.
Connection Pooling [HaBr01]. Connection Pooling is regularly used in scalable frameworks and applications that need to access remote services. Such remote services can be database services, for example via JDBC (Java Database Connectivity), or Web servers, via HTTP.
Thread Pooling [PeSo97]. Thread Pooling is commonly used in environments that are inherently asynchronous. Highly scalable as well as real-time systems use Thread Pooling as their key mechanism for managing multiple threads. Using thread pooling allows such systems to avoid resource exhaustion by running too many threads. A typical thread pool starts with a limited number of eagerly-acquired threads and lazily acquires more threads if it runs short of thread instances.
Component Instance Pooling [VSW02]. Component Instance Pooling is typically used by application servers. Application servers optimize their resource use by pre-instantiating a limited amount of commonly-used components to serve the initial demand quickly. When these components are used up, the application server instantiates new components as additional clients request them. If a client stops using a component, the component is put back into the pool.
Pooled Allocation [NoWe00]. In this pattern a pre-allocated pool of memory blocks is recycled when memory blocks are returned. Typically, pooled allocators do not release the memory until system shut-down. Often multiple pools are installed, each with memory blocks of a different size. This avoids wasting of large memory blocks in requests for small blocks.
Object Pool [Gran98]. Object Pool manages the reuse of objects of a particular type that are expensive to create, or for which only a limited number can be created. An Object Pool is different than Pooling, because it assumes that every resource can be represented as an object, whereas Pooling does not make the restriction. The implementation section of Pooling uses objects only because they more easily convey the key idea. Note that in reality resources can be represented by objects. However, the key to note is that those are only representations of resources, not the actual resources. For example, a connection is a resource that need not be represented as an object to make use of the Pooling pattern.
The following variants describe related patterns that are derived from
Pooling by extending or changing the problem and the solution.
Mixed Pool. Resources of different types are mixed in one pool. The interface of the pool might need to be extended to allow resource users to differentiate their requests for resource acquisitions. All resources are managed by the same strategy. The interface has to allow for the acquisition of resources of individual types.
Sub-Pools. In some situations it is advisable to subdivide the pool of resources into multiple smaller pools. For example, in the case of a thread pool, you could partition the thread pool into multiple sub-pools. Each sub-pool could be associated with a specific range of priorities [OMG04b]. This ensures that thread acquisitions for low-priority tasks do not cause resource exhaustion when high-priority tasks need to be executed.
There are several benefits of using the Pooling pattern:
?	Performance. The Pooling pattern can improve the performance of an application, since it helps reduce the time spent in costly release and re-acquisition of resources. In cases in which the resource has already been acquired by the resource pool, acquisition by the resource user becomes very fast.
?	Predictability. The average number of resource acquisitions by the resource user executes in a deterministic time if those resources are acquired eagerly. With a proper eviction strategy in place, most acquisition requests can be served from the pool. As lookup and release of previously-acquired resources is predictable, resource acquisition by the resource user becomes more predictable when compared to always acquiring a resource from the resource provider.
?	Simplicity. No additional memory management routines need to be invoked by the resource user. The resource user acquires and releases resources either transparently, or directly to and from the resource pool.
?	Stability and scalability. New resources are created if demand exceeds the available resources. Due to recycling, the resource pool delays resource eviction based on the eviction strategy. This saves costly release and re-acquisition of resources, which increases overall system stability and scalability.
?	Sharing. Using the pool as a Mediator [GoF95], unused resources can be shared between resource users. This also benefits memory use and can result in an overall reduction in the memory footprint of the application.
The pool does not synchronize access to the resources. Therefore if synchronization is required, it must be provided by the resource user.
There are also some liabilities of using the Pooling pattern:
?	Overhead. The management of resources in a pool consumes a certain amount of CPU cycles. However, these CPU cycles are typically less than the CPU cycles required for release and re-acquisition of the resources.
?	Complexity. Resource users have to explicitly release resources back to the resource pool. Patterns such as Leasing (149) can be used to address this.
?	Synchronization. In concurrent environments, acquisition requests to the pool have to be synchronized to avoid race conditions and the possibility of corrupting the associated state.
JDBC connection pooling [BEA02]. Java Database Connectivity (JDBC) connections are managed using connection pooling. JDBC Specification Version 2 includes a standard mechanism for introducing connection pooling. The management interface is called javax.sql. DataSource and provides a factory for pooled connection objects.
COM+ [Ewal01]. The COM+ Services runtime, as part of the Windows operating system, supports object pooling for COM+ objects and .NET applications. .NET applications have to set configuration attributes of the .NET System.EnterpriseServices package.
EJB application servers [Iona04] [IBM04a]. Application servers that are based on component technology use Component Instance Pooling to manage the number of component instances efficiently.
Web servers [Apac02]. Web servers have to serve hundreds, if not thousands, of concurrent requests. Most of them are served quickly, so creation of new threads per request is inefficient. Therefore most Web servers use Thread Pooling to efficiently manage the threads. Threads are reused after completing a request.
Flyweight [GoF95] uses sharing to support a large number of objects efficiently. The Flyweight objects can be maintained using Pooling.
This pattern is also known as Resource Pool [BiWa04a].
The Caching pattern (83) is related to Pooling, but Caching is about managing resources with identity. In the case of Caching, the resource user cares about which of the cached resources are returned. In the case of Pooling, the resource user does not care about the identity of the resource, since all resources in the pool are equal.
##%%&&
The Coordinator pattern describes how to maintain system consistency by coordinating the completion of tasks involving multiple participants, each of which can include a resource, a resource user and a resource provider. The pattern presents a solution such that in a task involving multiple participants, either the work done by all of the participants is completed or none are. This ensures that the system always stays in a consistent state.
Consider a large-scale system that is distributed over a network of processing nodes. The subsystem deployed on each node comprises of one or more services that need to be updated frequently. Assuming that shutting down the system to update all the services is not a feasible alternative, the most common solution is to implement the Deployer [Stal00] pattern. Using the Deployer pattern, the services deployed in each subsystem can be dynamically updated without requiring the system to be shut down.
To allow dynamic update of each subsystem requires a software download component to be present on each node. To update the overall system configuration, a central administration component is used that communicates with the software download component on each node. The central component typically sends update requests to each software download component, which then dynamically updates the subsystem deployed on the respective node.
Consider the scenario in which a global update needs to be made to the system configuration. As a result, the central administration component needs to send update requests to multiple software download components. Assume that the first few software download components successfully update their respective subsystems, but that one of the software download components fails to update its subsystem. The result would be that only part of the system would be successfully updated. If the global update requires all subsystems to be successfully updated, such a scenario would leave the system in an inconsistent state. The subsystem that failed to update could be left with an old and incompatible version of the software, making the entire system unusable.
Coordination of multiple participants that work on the same task. The task is divided among the participants, which then work independently to contribute to the total success of the task.
Many systems execute tasks that involve more than one participant. A participant is an active entity that can include both resource users and resource providers. In addition, in some cases resources, such as services, can be active and therefore directly participate in a task. The participants can be co-located in the same process, or can span multiple processes as well as multiple nodes. Each participant executes part of the task in a sequence. For the task to succeed as a whole, the work performed by each participant must succeed. If a task is successful, the changes made should keep the system in a consistent state. However, consider what would happen if the work performed by one of the participants were to fail. If the work is to be performed at a later stage in the task execution sequence, then there will be many participants that have already completed their work successfully. The work of these participants will have introduced changes to the system. However, the participant that failed will not be able to make the necessary changes to the system. As a result, inconsistent changes will be made to the system. This can lead to unstable and even malfunctioning systems. Partial failure in such systems is therefore worse than total failure.
One possible solution is to introduce point-to-point communication among the participants. The participants could communicate the results of their work to each other and take the steps necessary to keep the system state consistent. However, such a solution would require the participants to be aware of all other participants involved in the task, which is not practical. In addition, such a solution would scale poorly with the number of participants.
To address these problems requires resolution of the following forces:
?	Consistency. A task should either create a new and valid state of a system, or, if any failure occurs, should return all data to its state before the task was started.
?	Atomicity. In a task involving two or more participants, either the work done by all the participants should be completed, or none should be, even though the participants are independent of one another.
?	Location transparency. The solution should work in distributed systems, even though distributed systems are more prone to partial failures than total ones.
?	Scalability. The solution should be scalable with the number of participants without degrading performance significantly.
?	Transparency. The solution should be transparent to the system user and should require a minimum number of code changes.
Introduce a coordinator that is responsible for the execution and completion of a task by all the participants. The work performed by each of the participants is split into two parts or phases: prepare and commit.
In the first phase, the coordinator asks each participant to prepare for the work to be done. This phase must be used by each participant to check for consistency and also to determine if the execution would result in failure. If the prepare phase of a participant does not return successfully, the coordinator stops the execution sequence of the task. The coordinator asks all participants that have successfully completed the prepare phase to abort and restore the state prior to the start of the task. Since none of the participants have made any persistent changes, the system state remains consistent.
If all the participants complete the prepare phase successfully, the coordinator then initiates the commit phase for each of the participants. The participants do the actual work in this phase. Since each participant has indicated in the prepare phase that the work would succeed, the commit phase succeeds, leading to the overall success of the task.
The following elements form the structure of the Coordinator pattern:
?	A task is a unit of work that involves multiple participants.
?	A participant is an active entity that does a part of the work of a task. A participant can include resource users, resource providers, and resources.
?	A coordinator is an entity that is responsible for coordinating the completion of a task as a whole.
?	A client is the initiator of a task. The client directs the coordinator to execute a task.
The following CRC cards describe the responsibilities and collaborations of the elements of the Coordinator pattern.
The dependencies between the participants are illustrated in the following class diagram.
There are two main interactions in the Coordinator pattern. The first is between the client and the coordinator, and the second is between the coordinator and the participants.
Scenario I. The interactions start with the client directing the coordinator to begin a task. Following this, all participants involved in the task register with the coordinator. The client then directs the coordinator to commit the task. At this point the coordinator asks each participant to execute the prepare phase. If a participant indicates failure in the prepare phase, the coordinator aborts the entire task. On the other hand, if the prepare phase of all the participants succeeds, the coordinator initiates the commit phase of all the participants.
The following sequence diagram shows a scenario involving two participants in which the prepare phase of both the participants succeeds.
Scenario II. The following sequence diagram shows a scenario involving two participants in which the prepare phase of the second participant fails. The coordinator subsequently aborts the task, asking the first participant that succeeded in the prepare phase to revert any changes that it might have made.
There are three steps involved in implementing the Coordinator pattern.
1.	Identify participants. Any participant whose work needs to be coordinated by the coordinator must first be identified. In the context of resource management, a participant can be a resource user, a resource provider, or a resource itself. A resource user can be a participant if it actively tries to acquire, use, and release multiple resources, and therefore requires coordination. A resource provider can be a participant if it requires coordination when trying to provide one or more resources to resource users. A resource, such as a service, can be a participant if it is active and can therefore directly participate in a task.
In addition, any other entity that uses or manages resources can be regarded as a participant, and therefore would need to be coordinated. For example, in the case of Lazy Acquisition (38), a resource proxy can be regarded as a participant that requires coordination. The resource proxy may need to acquire the resource it represents from multiple sources, and therefore the resource acquisition task must be properly coordinated. Similarly, in the case of Eager Acquisition (53), a provider proxy can be regarded as a participant that requires coordination.
2.	Define the coordination interface. A coordination interface should be defined that will be implemented by all participants that will execute part of the work of a task.
3.	The prepare() method is used by the coordinator to initiate the prepare phase of each participant. This phase is used by each participant to check for consistency, and also to determine if the execution would result in failure. A return value of true indicates that the prepare phase succeeded, and therefore that the commit phase will also succeed. A return value of false indicates that the prepare phase failed, and that therefore the participant will not be able to execute the commit phase successfully.
If any of the participants return false in the prepare phase, the coordinator will abort the execution of the task. It will call the abort() method of each participant that has already returned successfully from the prepare phase. This would indicate to the participants that the task is being aborted and that they should perform any clean-up that is necessary.
4.	Define the coordinator interface. The coordinator interface should provide the means for a client to begin or terminate a task. In addition, it should allow participants of the task to register. Before the participants can register themselves, they must be able to discover the coordinator. To facilitate this the participants can use the Lookup (21) pattern.
5.	The beginTask() method is used by the client to define the beginning of a task. At this point the coordinator does not do anything. Once a task has begun, the participants of the task then register with the coordinator using the register() method. Once all the participants have registered, the client executes the commitTask() method on the coordinator. This serves as an indicator of two things to the coordinator¡ªfirstly, that all participants of the task have registered, and secondly, that the coordinator can begin coordinating the execution of the task by the participants. The coordinator now uses the two-phase commit protocol defined by Coordination interface to ensure that the participants complete the task.
6.	Handle error conditions. Using the coordinator allows the participants to indicate whether or not the work they are responsible for will succeed. If any participant indicates that its work will not succeed, the coordinator can abort the task without causing any persistent inconsistencies in the system state. If all participants indicate success in the prepare phase, the coordinator goes ahead with the commit phase on all of the participants. However, there is still a possibility that one or more participants may fail in the commit phase, even though their prepare phases completed successfully. This could be, for example, a result of factors beyond the control of the participants, such as a connection being disconnected. To handle such error conditions, participants may optionally maintain state information prior to executing their part of the task. If the commit on one of the participants fails, the coordinator can call a rollback() method on all of the remaining committed participants. This would give the participants a chance to restore their state prior to the execution of the task. Note, however, that such functionality requires state information to be maintained and can be expensive. For more details on this, please see Three-phase Commit in the Variants section.
7.	Consider the example of a distributed system with multiple nodes. To allow dynamic update of the entire system, each subsystem contains a software download component. To update the overall system configuration, a central administration component is used that communicates with the software download component on each node.
8.	Using the Coordinator pattern, the task that needs to be executed is therefore the updating of the entire system. The central administration component is the client controlling the execution of the task. Each software download component acts as a participant of the task and implements the Coordination interface. The sub-tasks that the participants perform correspond to the update of the respective subsystems.
9.	A coordinator is introduced that will be used by the central administration component to begin and commit the task. Once the task has begun, all the software download components are registered with the coordinator. This can be done by the software download components themselves, or by the central administration component¡ªsee the Variants section. When all the participants are registered, the central administration component asks the coordinator to begin the task of updating the entire system.
10.	When the coordinator has been asked to commit the task, it will begin the two-phase commit protocol. The coordinator first initiates the prepare phase of each software download component. During this phase each software download component checks for consistency and also determine whether the execution of the update would result in failure. Each software download component then downloads the software update, but does not install it. If an error occurs while downloading the software, the prepare phase for that software download component returns failure.
11.	If any of the software download components does not return success in the prepare phase, the coordinator terminates the execution of the task. The coordinator then asks any software download components that succeeded in the prepare phase to abort. No changes are made in any of the subsystems, and therefore the system state remains consistent.
12.	If all the software download components return successfully from the prepare phase, the coordinator initiates the commit phase. Each software download component installs the downloaded software and updates its respective subsystem during this phase. Since each software download component indicated in the prepare phase that the update would succeed, the commit phase succeeds, leading to the overall success of the system update.
Note that exception handling has been left out of the code presented in this section for the sake of brevity. If an exception is thrown during the prepare phase, it should typically be regarded as failure and the entire task aborted. If an exception is thrown during the commit phase, the changes made to the system should be rolled back¡ªsee Three-Phase-Commit in the Variants section.
Third-Party Registration. The registration of the participants with the coordinator need not be done by the participants themselves. It can be done by a third party, including the client. This is useful if the client needs to control who the participants in a task should be.
Participant Adapter. A participant of a task need not implement the coordination interface directly. Instead, the object can be contained inside an Adapter [GoF95], which then implements the coordination interface. The coordinator will invoke the prepare() method on the adapter object, which will then be responsible for performing consistency checks and determining whether the task would succeed. When the coordinator invokes the commit() method on the adapter object, the adapter object delegates the request to the actual object. This variant makes it easier to integrate legacy code without requiring existing classes to implement the coordination interface.
Three-phase Commit. To handle the possibility of failure in the commit phase, a third phase can be introduced to supplement the two-phase commit protocol. Each participant needs to maintain state information prior to executing its part of the task. If the commit on one of the participants fails, the coordinator executes the third phase by invoking a rollback() method on all the committed participants. This gives the participants a chance to restore their state prior to the execution of the task. If the commit phase of all the participants succeeds, the coordinator executes the third phase, but invokes a different method, clear(), on all the participants. This method allows the participants to discard the state they were maintaining in case of failure of the commit phase.
There are several benefits of using the Coordinator pattern:
?	Atomicity. The Coordinator pattern ensures that in a task involving two or more participants, either the work done by all of the participants is completed, or none is. The prepare phase ensures that all participants will be able to complete their work. If any participant returns failure during the prepare phase, the task is not executed, ensuring that no work is done by any of the participants.
?	Consistency. The Coordinator pattern ensures that the system state remains consistent. If a task executes successfully, it creates a new and valid state of the system. On the other hand, if any failure occurs, the Coordinator pattern ensures that all data is returned to its state before the task was started. Since each task is considered to be atomic, either all participants complete their work, or no participants do any work. In both cases, the end result leaves the system in a consistent state.
?	Scalability. The solution is scalable with the number of participants. Increasing the number of participants does not affect the execution of a task. With an increase in the number of participants, the likelihood of a failure of one of the participants increases. However, using the pattern, such a failure can be detected in the prepare phase, ensuring that the system stays in a consistent state.
?	Transparency. Using the Coordinator pattern is transparent to the user, since two-phase the execution of a task is not visible to the user.
There are some liabilities of using the Coordinator pattern:
?	Overhead. The Coordinator pattern requires that each task be split into two phases, resulting in the execution sequence involving all the participants being repeated twice. If the participants are distributed, this implies a doubling of the number of remote calls. This in turn can result in a potential of loss of transparency.
?	Additional responsibility. The Coordinator pattern imposes additional responsibility on the participants by requiring them to register with the Coordinator. If the participants are distributed, the registration process in turn results in the participants making remote calls.
Java Authentication and Authorization Service (JAAS) [Sun04e]. The Login Context of the JAAS implements the Coordinator pattern. JAAS supports the notion of dynamically-configurable stacked login modules that perform authentication. To guarantee that either all login modules succeed, or none succeed, the login context performs the authentication steps in two phases. In the first phase, or the ¡®login¡¯ phase, the login context invokes the configured login modules and instructs each to attempt only the authentication process. If all the necessary login modules successfully pass this phase, the login context then enters the second phase and invokes the configured login modules again, instructing each to formally ¡®commit¡¯ the authentication process. During this phase, each login module associates the relevant authenticated principals and credentials with the subject. If either the first phase or the second phase fails, the login context invokes the configured login modules and instructs each to abort the entire authentication attempt. Each login module then cleans up any relevant state it has associated with the authentication attempt.
Transaction services Many transaction models and software implement the Coordinator pattern. The Object Transaction Service (OTS) [OMG04e] is a distributed transaction processing service specified by the Object Management Group (OMG). One of the principal interfaces provided by the specification is that of a coordinator that is responsible for coordinating distributed transactions. Many implementations of the OTS specification are available, including ITS from Inprise, Orbix 6 from Iona [Iona04], and ArjunaTS from Arjuna Solutions Ltd.
Java Transaction Service (JTS) [Sun04d] implements the Java mapping of the OMG OTS specification. JTS includes a Transaction Manager that plays the role of coordinator. The JTS Transaction Manager is responsible for coordinating database transactions. The participants in these transactions implement transaction-protected resources, such as relational databases, and are called ¡®resource managers¡¯.
Distributed Transaction Coordinator (DTC) [Ewal01] as part of the Microsoft COM+ Services is a transaction server implementing the Coordinator pattern.
Databases [GaRe93]. Databases make extensive use of the Coordinator pattern and the two-phase commit protocol. If, for example, a database update operation involves several dependent tables, and the computer system fails when the database update is only partially completed, using two-phase commit would ensure that the database is not left in an inconsistent or an inoperable state.
Software installation [Zero04b] [FIK04]. Most software installation programs implement the Coordinator pattern. In the prepare phase, checks are performed to ensure that the installation task would execute smoothly. This includes, for example, checking for necessary hard disk space and memory. If the prepare phase succeeds, the commit phase executes the installation of the software. This becomes especially important in distributed software installation.
Real-world example I A real-world example of the Coordinator pattern is a priest conducting a marriage ceremony. The priest is the coordinator while the bride and groom are the participants. In the prepare phase, the priest first asks the bride and groom, ¡®Do you take this person to be your spouse?¡¯ Only if they both respond ¡®I do¡¯ does the priest commit them to marriage.
Real-world example II Another real-world example could be a purchasing deal between two parties managed by a broker, who plays the role of a coordinator. In the prepare phase, the broker ensures that the first party puts an item to sell on the table, while the other party brings the money they are ready to offer for the item. If both parties are satisfied with what the other has to offer, the broker initiates the commit phase in which they exchange the money for the item.
The Command Processor [POSA1] pattern describes techniques for remembering and undoing a sequence of operations. It can be used as a means of encapsulating sub-tasks to be performed by the participants.
The Master-Slave [POSA1] pattern models the partitioning of work into sub-tasks and coordinating them. A master component distributes the work to slave components and computes a final result from the results returned by the slaves. Master-Slave is different than Coordinator, as it uses a ¡®divide and conquer¡¯ strategy followed by the integration of results, whereas Coordinator is about the consistency of state achieved after each sub-task. The Coordinator pattern can extend the Master-Slave pattern to ensure for consistency before the results of the sub-tasks are integrated.
##%%&&
The Resource Lifecycle Manager pattern decouples the management of the lifecycle of resources from their use by introducing a separate Resource Lifecycle Manager, whose sole responsibility is to manage and maintain the resources of an application.
Consider a distributed system that needs to service thousands of clients. As a consequence, thousands of network connections are established between the clients and the server, as shown in the figure below.
The server typically provides one or more services to the clients. A client invokes a request on the server using its connection, and the response to the synchronous invocation on the remote service is sent back via the same connection as a result. If the request needs to be decoupled from the response and some form of asynchronous communication is required between the client and the server, then the server would need to open a new connection to the client and use it to initiate a call-back to the client. For example, the server may use a call-back to notify a client about events in the service.
Maintaining thousands of network connections becomes even more complex in the case of real-time systems for which stringent QoS requirements exist. As a result, the server connection policies are typically quite complex. Connections might be associated with properties, such as the priority of requests to use the connection.
As connections depend on multiple low-level resources, they should be freed when no longer needed to ensure system stability. As a result, complex lifecycle scenarios for each connection must be managed by the application. This complexity in managing the lifecycles of connections can affect the core functionality of the application, and as a consequence can make its business logic hard to understand and maintain due to the entangled connection-management code.
Large and complex systems that require simplified management of the lifecycle of their resources.
Building large-scale systems is challenging. Making large-scale systems robust and scalable is even more challenging. The most important aspect of making large-scale systems robust and scalable is the way in which resources are managed. Resources in systems can be of many different types, such as network connections, threads, synchronization primitives, servants and so on. Network connections represent communication channels between client applications and distributed application services. Managing them efficiently requires the ability to determine when to establish connections and when to release them. Threads are especially important in large-scale systems, since they provide asynchronous behavior between different parts of an application, for example decoupling UI interaction from typical client functionality and service provisioning. However, managing threads effectively can be quite challenging, since it involves close monitoring of their execution, and the ability to determine when to create new threads or destroy threads that are no longer needed. Similarly, synchronization primitives such as locks and tokens are typically needed to synchronize the asynchronous parts of an application and to allow for their internal coordination and interaction. However, when and how these synchronization primitives are created and released is both important and very challenging to implement.
Solving the problem of managing the lifecycle of resources in an effective and efficient manner requires resolution of the following forces:
?	Availability. The number of available resources typically does not grow at the same rate as the size of the overall system. In large systems, therefore, managing resources efficiently and effectively is important to ensure that they are available when needed by users.
?	Scalability. As systems become large, the number of resources that need to be managed also grows, and can become much more difficult to manage directly by the users.
?	Complexity. Large systems typically have complex interdependencies between resources that can be very difficult to track. Maintaining and tracking these interdependencies is important to allow proper and timely release of resources when they are no longer needed.
?	Performance. Many optimizations are typically made to ensure that large systems don't face any performance bottlenecks. However, providing such optimizations can be quite complex if performed by individual resource users.
?	Stability. If resource users have to manage resource lifecycle issues, they might forget to free resources, leading to system instability in the long term. In addition, it should be possible to control the acquisition of resources, to ensure that there is no starvation of available resources at the system level, also leading to instability.
?	Interdependencies. In complex systems resources of the same or different types might depend on each other. This means that the lifecycle of resources are interdependent and need to be managed appropriately. 
?	Flexibility. Management of the resource lifecycle should be flexible, allowing support for different strategies. A strategy should provide a hook to allow configuration of resource management behavior.
?	Transparency. Resource lifecycle management should be transparent to the resource user. In particular, the resource user should not have to deal with any of the complexities of managing resources.
Separate resource usage from resource management. Introduce a separate Resource Lifecycle Manager (RLM) whose sole responsibility is to manage and maintain the resources of a resource user.
Resource users can use the RLM to retrieve and get access to specific resources. If a resource that is requested by a resource user does not yet exist, the RLM can initiate its creation. In addition, the RLM allows users to request explicit creation of resources.
An RLM has knowledge of current resource use and can therefore reject a request for resource acquisition from a resource user. For example, if the system is running low on available memory, then the RLM can reject a resource user's request to allocate memory.
The RLM also controls the disposal of the resources it manages, either transparently for the resource users or at their explicit request. The RLM maintains its resources on the basis of appropriate policies that take into account available computing resources such as memory, connections, and file handles.
An RLM can be responsible for a single type of resource, or for multiple types of resource. If interdependencies between resources exist, the RLMs for individual resources have to work in concert. That means that they have to maintain dependencies between resources. This can be done by a central RLM having the full responsibility over individual as well as dependent resources, or by a separate RLM that only deals with the interdependencies, while leaving the management of resources of the same type to resource-specific RLMs. In strongly-layered architectures a cascade of RLMs can be used, such that an RLM is present at every abstraction level, for example at the OS, framework, and application levels.
The following participants form the structure of the Resource
Lifecycle Manager pattern:
?	A resource user acquires and uses resources.
?	A resource is an entity such as a network connection or a thread.
?	A resource lifecycle manager manages the lifecycle of resources, including their creation/acquisition, reuse, and destruction.
?	A resource provider, such as an operating system, owns and manages resources. The resource provider might itself be a resource lifecycle manager at the same or different abstraction level.
The following CRC cards describe the responsibilities and collaborations of the participants.
The following class diagram illustrates the structure of the Resource Lifecycle Manager pattern.
The interactions between the participants are shown in the following sketch.
Scenario I. The dynamics of the pattern consists of the following activities: first, the system starts and initializes the RLM. The resource user needs a resource, and therefore tries to acquire the resource from the RLM. The RLM accepts the acquisition request and acquires the resource according to the resource-acquisition strategy. The resource is therefore passed to the user, which now uses it, accessing the resource.
When the resource is no longer used by the user, it hands it back to the RLM. The RLM checks dependencies of the resource on other resources and decides either to recycle the resource or to evict it.
These steps are illustrated in the following sequence diagram.
Scenario II. When a resource of the same type needs to be accessed again, the resource user can acquire it from the RLM again. The RLM can now apply Pooling (97) and Caching (83) as optimizations to avoid expensive acquisitions from the resource provider. See the sequence diagram below.
Seven steps are involved in implementing the Resource Lifecycle Manager pattern.
1.	Determine resources that need to be managed. A developer needs to first identify all resources whose lifecycle needs to be managed. Since resources can be of many different types, an application can provide multiple RLMs for different types of resource, for example an RLM for handling computing resources such as processes, threads, file handles, and connections, and another for maintaining application components. On the other hand, a single RLM can also handle resources of different types. Such a solution can be effective when complex interdependencies also need to be maintained among different types of resource (see also implementation step 4). If only a single instance of an RLM is needed, it should be implemented as a Singleton [GoF95].
2.	Define resource creation and acquisition semantics. A developer needs to determine how resources will be created or acquired by the RLM. 
This includes determination both of when resources will be created or acquired, as well as how resources will be created or acquired. Patterns such as Eager Acquisition (53), Lazy Acquisition (38), and Partial Acquisition (66) can be used to control when resources will be acquired, while patterns such as Factory Method [GoF95] and Abstract Factory [GoF95] can control how resources are created. Note that resources are typically acquired by the RLM so that it can have full control over the lifecycle of those resources. However, it may also be the case that certain resources are not acquired or created by the RLM, but may still need to be managed by the RLM.
To ensure system stability, the RLM may reject acquisition requests from resource users for various reasons, including the situation in which available resources become scarce.
An implementation of the Pooling (97) pattern can manage resources that are typically created up-front during the initialization of the RLM, using either Eager Acquisition or Partial Acquisition. Eager Acquisition acquires a resource completely and before it is ever accessed¡ªthus the resource is usable immediately after its acquisition. However, it can take a long time to fully create or acquire large resources. Partial Acquisition can help reduce up-front acquisition time by performing step-wise resource acquisition. Using Lazy Acquisition, the entire acquisition of a resource is deferred until it is actually accessed.
3.	Define resource management semantics. One of the principal responsibilities of the RLM is to manage resources efficiently and effectively. Frequent acquisition and release of resources can be expensive, so the RLM typically uses patterns such as Caching (83) and Pooling to optimize the management of resources. Pooling can be used to keep a fixed number of resources continuously available. This strategy is particularly useful for managing resources such as threads and connections, because the users of these resources typically do not rely on their identity on re-acquisition. Caching, in contrast, is advisable if resources must keep their identity.
 For example, Caching (83) is mostly applied for stateful application components involved in a task, because they must keep their state and hence their identity. Once the tasks are completed, the components are not needed until the same tasks are again executed. To avoid degrading the quality of service of an application, it can therefore be helpful to remove unused components temporarily from memory, so that the space and the computing resources they occupy become available for components that are in use. The Passivation [VSW02] pattern describes passivation and re-activation of components. Using both Caching and Passivation patterns together can help to limit total resource consumption.
4.	Handle resource dependencies. In many applications resources are dependent on each other. The first step therefore is to isolate the different resource lifecycles into separate RLMs to ease maintenance. However, optimizations based on dependent resources will be hard to apply.
 In the example above, dependent resources can include application services that have been accessed via a specific connection. In real-time environments, servants implementing the application services are often directly associated with prioritized connections from clients, so that priorities can be obeyed end-to-end. The removal of any such connection therefore influences the behavior of the servant, including its used resources. The servant might become inaccessible if dependent connections are evicted. Thus an RLM with a common responsibility should be considered for managing connections, servants, and their interdependencies.
5.	The exact applicability of such an RLM, as well as its implementation, is heavily dependent on the application context as well as on the types of resource. One possible solution is to group interdependent resources into a given context. The creation of multiple dependent resources can be controlled by the Builder [GoF95] pattern. Such a grouping can be used to control resource acquisition, access and release of interdependent resources. For example, a strategy can be configured that ensures that if a resource is released, all its dependent resources in the group are also automatically released.
6.	Define resource release semantics. When resources are no longer needed they should be automatically released by the RLM. Patterns such as Leasing (149) and Evictor (168) can be used to control when and how resources are released. An Evictor allows for a controlled removal of less-frequently used resources from the cache. To prevent any release of resources that are still referenced, an RLM can use the Leasing pattern, allowing it to specify the time for which the resources will be available to resource users. Once this expires, the resources can be safely and automatically released by the RLM. Additionally, a Garbage Collector [JoLi96] can be used to identify unused resources¡ªresources that are not referenced by any resource user or other resource¡ªand evict them. For safe eviction of resources, consider using a Disposal Method [Henn03].
7.	Define resource access semantics. Resources that have been created or acquired need to be easily accessible. The RLM can use patterns such as Lookup (21) to allow easy access to resources.
8.	Configure strategies. For each of the above steps, the RLM should allow the configuration of different strategies for controlling how the lifecycle of resources is managed. For example, if a resource is expensive, it should be acquired as late as possible using Lazy Acquisition and released as early as possible using Evictor. Measurement of the expense of a resource is application dependent. For example, a resource can be considered expensive if it consumes a large amount of memory, or is a contentious resource with high demand among resource users. On the other hand, a resource that is relatively less expensive and is used frequently should be acquired early using Eager Acquisition and retained through the lifetime of the application. In dynamic environments, reflection [POSA1] mechanisms can be employed to adapt configuration strategies according to the environment. When dependencies between resources have to be managed, use the Coordinator (111) pattern to synchronize their release.
Introduce a component that is responsible for the resource lifecycle management and hence free the application from this responsibility.
This decouples the management of connections from the business logic of the application.
The resource lifecycle manager is introduced in the client and server parts of the distributed application. The clients then use the resource lifecycle manager to request new connections to the server. Once these connections are given to the clients, their lifecycle is managed by the resource lifecycle manager.
In the following paragraphs a concrete implementation of the resource lifecycle manager is presented. The ResourceLifecycleManager class provides the primary interface for the Resource Lifecycle Manager pattern. Using this class one can acquire and release resources of different types. The ResourceLifecycleManager makes use of multiple resource pools to manage the lifecycle of resources. If a resource is not found in the appropriate pool, a factory is used to create the resource. Once a resource has been created, it must be added to a resource group by the resource user to allow it to be properly managed. It is assumed that each resource belongs to at least one group. Note that for the sake of brevity, all error and exception handling has been left out of the code.
The acquire() method tries to acquire a resource of the given type. The method first finds the resource pool corresponding to the given type. It tries to fetch the resource from the pool if possible. If a resource is not available in the pool, the method finds the appropriate factory for the resource type and uses it to create the resource. Note that it is up to the factory to be configured with different strategies and policies to determine when the resource is actually created or acquired.
The release() method releases the resource as well as any dependent resources. The dependent resources are those that belong to the ResourceGroup to which the resource belongs. The ResourceLifecycleManager internally uses the Coordinator (111) pattern to ensure either that all resources are released or none are. Each resource acts as a participant in the two-phase protocol initiated by the ResourceLifecycleManager, which acts as the coordinator.
For all group-related functionality, the following set of methods is offered by the ResourceLifecycleManager class.
The createGroup() method creates a group identified by a group ID and adds resources to it. The addResourceToGroup() method delegates the addition of resources to a method of the ResourceGroup class. Every resource to be managed has to implement the Resource interface.
The hook method [Pree94] beforeEviction() is called before the resource is evicted. A resource should first determine if it is in a state to allow itself to be evicted. If it is not, this method should return false and the resource will not be evicted. Otherwise, the resource should do any necessary cleanup before it gets evicted and should return true. A Connection resource implements the Resource interface as follows:
The hook method beforeEviction() first checks whether the resource can be evicted. For this it uses the Boolean flag consistentState, which indicates whether the resource is in a state to be evicted. In our example, the connection can be authenticated using a security token. The security token also implements the Resource interface, and is represented as secondary resource on which the connection depends.
An application that uses authenticated connections and wants to delegate the lifecycle management of both resources would use the ResourceLifecycleManager as follows:
This code provides interface methods explicitly for group-related resources. If the application is not to be burdened with the additional responsibility of group management, a Wrapper Facade [POSA2] can be used to hide the complexity.
There are several benefits of using this pattern:
?	Efficiency. The management of resources by individual users can be inefficient. The Resource Lifecycle Manager pattern allows coordinated and centralized lifecycle management of resources. This in turn allows for better application of optimizations and a reduction in overall complexity.
?	Scalability. Using the Resource Lifecycle Manager pattern allows for more efficient management of resources, allowing applications to make better use of available resources, which in turn allows higher application load.
?	Performance. The Resource Lifecycle Manager pattern can ensure that various levels of optimizations are enabled to achieve maximum performance from the system. By analyzing resource use and availability, it can use different strategies to optimize system performance.
?	Transparency. The Resource Lifecycle Manager pattern makes management of resources transparent to resource users. Different strategies can be configured to control resource creation and acquisition, management and release. By decoupling resource usage from resource management, the RLM reduces complexity of use and thereby makes the life of a resource user easier. 
?	Stability. The Resource Lifecycle Manager pattern can ensure that a resource is allocated to a user only when a sufficient number of resources are available. This can help make the system more stable by avoiding situations in which a resource user can acquire resources from the system directly, causing resource starvation.
?	Control. The Resource Lifecycle Manager pattern allows better control over the management of interdependent resources. By maintaining and tracking interdependencies among resources, the RLM allows proper and timely release of resources when they are no longer needed
There are some liabilities of using this pattern:
?	Single point of failure. A bug or error in the RLM can lead to the failure of large parts of the application. Redundancy concepts help only partly, as complexity is further increased and the performance is further constrained.
?	Flexibility. When individual resource instances need special treatment, the Resource Lifecycle Manager pattern might be too inflexible.
Component container A container manages the lifecycle of application components and provides application-independent services. Further, it manages the lifecycle of resources used by the components (see also Container and Managed Resource patterns in [VSW02]). Container functionality is offered by many technologies, including J2EE Enterprise JavaBeans (EJB) [Sun04b], CORBA Component Model (CCM) [OMG04d], and COM+ [Ewal01]. All implement an RLM. Similarly, Java 2 Connector Architecture (JCA) [Sun04i] defines a framework for integrating a user-defined RLM with an EJB-based RLM.
Remoting middleware. Middleware technologies such as CORBA [OMG04a] and .NET Remoting [Ramm02] implement an RLM at multiple levels. Middleware ensures the proper lifecycle management of resources such as connections, threads, synchronization primitives, and servants implementing the remote services.
Current developments such as Ice [Zero04a] prove that CORBA and .NET are not the sole examples for middleware frameworks that implement RLM functionality. RLM is a proven concept in all middleware frameworks.
Grid computing [BBL02] [Grid04]. Grid computing is about sharing and aggregation of distributed resources such as processing time, storage, and information. A grid consists of multiple computers linked together as one system. Participating computers that offer resources in the grid have to manage their local resources by some means. Often a resource lifecycle manager is used to make the resources in a grid available and fulfill local as well as distributed requests to those resources.
The Object Lifetime Manager [LGS01] pattern is specialized for the management of singleton objects as resources in operating systems that do not support static destructors properly, such as real-time operating systems.
The Garbage Collector [JoLi96] pattern is specialized for the eviction of unused objects and their associated resources. Garbage collection is therefore not a complete RLM, as it does not deal with creation, allocation or acquisition of resources.
The Pooling (97) pattern focuses specifically on the recycling of resources, and does cover the complete lifecycle of resources
The Caching (83) pattern is specialized for avoiding expensive acquisitions and associated initialization of resources.
The Manager [Somm98] pattern focuses on the management of objects, rather than on general resource management.
The Abstract Manager [Lieb01] pattern focuses on the management of business objects in enterprise systems, rather than on general resource management.
The Resource Exchanger [SaCa96] pattern describes how resources are shared between resource users. It uses a generator, such as a network driver, and an acceptor, such as a server. The generator initially acquires the resource, which is then exchanged with the acceptor. This sets the resource to a state that is read later by the generator. The pattern keeps resource utilization stable by using the resource exchanger to manage the resources generated by the resource generator.
##%%&&
The Leasing pattern simplifies resource release by associating time-based leases with resources when they are acquired. The resources are automatically released when the leases expire and are not renewed.
Consider a system consisting of several distributed objects implemented using CORBA [OMG04a]. To allow clients to access these distributed objects, the server containing the objects publishes the references to the objects in a lookup service (21) such as a CORBA Naming Service [OMG04c]. Clients can then query the lookup service to obtain references to the objects. For example, consider a distributed quoter service object registered with the CORBA Naming Service. The quoter service provides stock quotes to any clients that connect to it. A client queries the Naming Service, obtains a reference to the quoter service, then communicates directly with the quoter service to obtain stock quotes.
Consider what would happen if the server containing the quoter service were to crash and never come back up. The quoter service would no longer be available, but its reference would never be removed from the Naming Service. This can create two problems. First, clients would still be able to obtain a reference for the quoter service from the Naming Service. However, the reference would be invalid, and therefore any requests sent by the clients would typically result in an exception being thrown. Secondly, lacking any explicit means to remove the invalid object reference, unused resources such as invalid object references would build up at the lookup service over a period of time.
Systems where resource use needs to be controlled to allow timely release of unused resources.
Highly robust and scalable systems must manage resources efficiently. A resource can be of many types, including local as well as distributed services, database sessions and security tokens. In a typical use case, a resource user retrieves the interface of a resource provider and then asks the provider for one or more resources. Assuming that the provider grants the resource, the resource user can then start using the resources. Over a period of time, however, the resource user may no longer require some of these resources. Unless the resource user explicitly terminates its relationship with the provider and releases the resources, the unused resources will continue to be consumed needlessly. This in turn can have a degrading effect on the performance of both the resource user and provider. It can also affect resource availability for other resource users.
In systems in which the resource user and the resource provider are distributed, it is also possible that over a period of time the provider machine may crash, or that the provider may no longer offer some of its resources. Unless the resource user is explicitly informed about the resources becoming unavailable, the resource user may continue to reference invalid resources.
The net result of all of this is a build-up of resources on the resource user side that may never get freed. One solution to this problem is to use some kind of monitoring tool that periodically checks a user's resource use as well as the state of the resources used by the resource user. The tool can then recommend to the resource user possible resources that can be freed. However, this solution is both tedious and error-prone. In addition, a monitoring tool may hinder performance.
To solve this problem in an effective and efficient manner requires resolution of the following forces:
?	Simplicity. The management of resources for a resource user should be simple, by making it optional for the resource user to explicitly release the resources it no longer needs.
?	Availability. Resources not used by a resource user, or no longer available, should be freed as soon as possible to make them available to new resource users. For example, resources associated with a network connection should be released once the connection is broken.
?	Optimality. The system load caused by unused resources must be minimized.
?	Actuality. A resource user should not use an obsolete version of a resource when a new version becomes available.
Introduce a lease for every resource that is used by a resource user. A lease is granted by a grantor and is obtained by a holder. A lease grantor is typically the resource provider, while a lease holder is typically the resource user. A lease specifies a time duration for which the resource user can use the resource.
If the resource is held by the resource user, then once the time duration elapses, the lease is said to have expired and the corresponding resource is freed by the resource user. On the other hand, if the resource is held by the resource provider and the resource user holds a reference to the resource, then on lease expiration the resource reference becomes invalid and is released by the resource user. In addition, the resource provider can also release the resource.
While a lease is active, the lease holder can cancel it, in which case the corresponding resource is also freed. Before a lease expires, the lease holder can apply to renew the lease from the lease grantor. If the lease is renewed, the corresponding resource continues to be available.
The following participants form the structure of the Leasing pattern:
?	A resource provides some type of functionality or service.
?	A lease provides a notion of time that can be associated with the availability of a resource.
?	A grantor grants a lease for a resource. The grantor of the lease is typically identical to the resource provider.
?	A holder obtains a lease for a resource. The holder of the lease is typically identical to the resource user.
The following CRC cards describe the responsibilities and collaborations of the participants.
The dependencies between the participants of the Leasing pattern are shown in the following class diagram.
In the following sequence diagrams the grantor of the lease is the same as the resource provider, and the holder of the lease is the same as the resource user.
Scenario I In the first step the resource user acquires the resource from the resource provider. In this step, the lease is created and the duration of the lease is negotiated between the lease requestor and the lease grantor. The resource user is returned the resource and a corresponding lease. From that point on, the resource user can access the resource.
Scenario II The following sequence diagram shows the scenario in which a lease has expired and the resource user is informed by the resource provider.
In response, the resource user has multiple options. For example, the resource user may try to renew the lease by interacting with the resource provider. The first part of the sequence diagram shows this scenario. Alternatively, if the lease is an active entity, it may be able to initiate its own renewal. See the Variants section for more details.
If the lease no longer needs to be renewed, the timeout can be accepted by the resource user, and consequently the resource user can release the resource. This scenario is shown in the second part of the sequence diagram.
If the lease is an active entity, it may be able to initiate the resource release process by using the Evictor (168) pattern. See the Variants section for more details.
Scenario III In the case in which the resource is held by the resource provider and the resource user holds only a reference to the resource provider, the interactions on lease expiration are slightly different. When the lease expires, the resource user only needs to release the reference to the resource, whereas the actual resource is released by the resource provider. The following sequence diagram illustrates this.
Four steps are involved in implementing the Leasing pattern.
1.	Determine resources with which to associate leases. A lease should be associated with any resource whose availability is time-based. This includes resources that are short-lived, resources that are not used continuously. and resources that are updated frequently with newer versions. 
2.	Determine lease creation policies. A lease is created by the lease grantor for every resource used. If a resource can be shared by multiple resource users, multiple leases will be created for the resource. A lease can be created by the lease grantor using a factory, as described by Abstract Factory [GoF95]. Lease creation requires specification of the duration for which the lease is to be granted. The duration may depend on the type of resource, the requested duration and the policies of the lease grantor. The lease requestor and the lease grantor may negotiate the duration for which the lease should be granted.
A resource user may want to pass the resource, along with the associated lease, to another resource user. The lease creation policies can be used to specify whether or not this is supported. If a resource and its corresponding leases can be passed to other resource users, then the lease needs to provide operations allowing its ownership to be changed.
3.	Provide lease holder functionality. Each lease holder must implement a notification interface that can be used by the lease grantor to notify the lease holder of lease expiry. When the resource user acquires a resource and the corresponding lease from the lease grantor, it will register this interface with the lease grantor.
4.	The lease_expired() method is a call-back method that is used by the lease grantor to notify the lease holder of a lease that has expired. In the implementation of this method, the resource user can release the resource whose lease has expired.
5.	Provide lease grantor functionality. Once a lease has been created, the grantor needs to maintain a mapping between the lease and the corresponding resource. This allows the grantor to keep track of the time for which the resource is being used, and to determine the resources that are still available for which new leases can be granted.
In addition, to support the notification of resource users, a mapping of the lease to the corresponding resource user is necessary. The Observer [GoF95] pattern describes how to implement such notifications.
6.	Determine lease responsibility. If a lease can be renewed, it is necessary to determine who is responsible for renewing it. If the lease is an active entity, it may automatically renew itself, or the renewal process may require re-negotiation between the grantor and the holder. A re-negotiation of the lease may result in new policies for the lease, including the duration for which the lease is renewed.
7.	Determine lease expiration policies. Once a lease expires and is not renewed, the resource associated with it needs to be released. This can be done automatically, for example by using the Evictor (168) pattern, or may require some intervention on the part of the resource user. Similarly, the lease grantor needs to remove the mapping between the lease, the resource and the resource user. Typically, the lease contains some kind of Asynchronous Completion Token [POSA2] with information about the holder that it uses to allow proper clean-up in the grantor when the lease expires.
Consider the example in which a distributed quoter service object needs to be available to CORBA clients. The server containing the quoter service object registers the object with a CORBA Naming Service. The Naming Service is therefore a resource provider, while the resource is the service object reference that is registered with the Naming Service. The server containing the quoter service object is the resource user. The server and the Naming Service negotiate the lease details, including the duration for which the quoter service object reference needs to be registered, as well as policies regarding renewal of the lease. Once these negotiations are completed, the Naming Service registers the quoter service object reference with it and creates a lease of the agreed duration. The Naming Service is the lease grantor, while the server is the lease holder.
While the lease has not expired, the Naming Service keeps the quoter service object reference and makes it available to any clients that request it. Once the lease expires, the server may need to explicitly renew the lease, to indicate a continued interest in keeping the quoter service object reference available to clients. If the server does not renew the lease, the Naming Service automatically removes the quoter service object reference and releases any additional resources associated with it.
The C++ code below shows how a server can register a quoter service object reference with the Naming Service. In this example, the LookupService provides a Wrapper Facade [POSA2] around the Naming Service, so that the standard interface of the CORBA Naming Service need not be changed. The LookupService serves as a lease grantor.
To publish the quoter service, the application binds the hosted quoter object with the lookup service. When the leasing time is accepted a valid lease is returned. The lease can be renewed via the Lease interface before its expiration. When the lease expires the registered object of the type LeaseHolder is notified by the lease.
The following code shows how the LookupService_Impl class wraps the CORBA Naming Service. After checking for an acceptable duration and creating a corresponding lease, the actual binding is delegated to the standard Naming Service. Note that the Reactor [POSA2] pattern is used for the notion of timers. The LookupService_Impl class therefore implements not only the LookupService interface, but also the Event_Handler interface. This allows the lease to register itself with the reactor to receive timeouts on lease expiration.
The following code shows how a lease could be implemented. The lease will be created by the lookup service. It maintains the association between the name used to register and the lease holder.
An object of the LeaseHolder_Impl class is handed over to the lookup service on registration of the quoter object. It will receive a notifciation when the lease expires. It can prolong the lease, or prepare to have the registration of the quoter object removed from the Lookup service.
Specific lease creation and expiration policies can yield various variants of the Leasing pattern.
Active Lease. When the lease is an active entity, the lease can take over the responsibility for notifying the lease holder of lease expiration. It can either release the resource directly, or use Evictor (168) to trigger the release of the resource.
Auto-renewable Lease. An active lease may be created with a policy to automatically renew itself when it expires. In this case the lease maintains enough information about the holder and the grantor to renew itself. Automatic renewals with short lease durations are preferable to a single, longer, lease, since each lease renewal offers the opportunity for the lease holder to update the resource it holds if the resource has changed. A further variation on this is to limit the number of automatic renewals based on some negotiation between the lease grantor and the lease holder.
Leasing Delegate. The renewal of a lease need not be done automatically by the lease or the lease holder¡ªit can be done by a separate object. This frees the lease holder from the responsibility of renewing leases when they expire.
Non-expiring Lease. A lease may be created with no expiration. In this case, the holder must cancel the lease explicitly when it no longer needs the resource associated with the lease. This variant, however, loses many of the benefits of using the Leasing pattern, but allows integration of legacy systems in which the notion of leasing cannot be introduced easily.
Self-reminding Leases. Callbacks can be used to inform lease holders about expiring leases, to give them a chance to renew them. This can help lease holders who do not want to, or are not capable of, determining when a lease will expire.
Invalidating Leases. The Leasing pattern allows invalid resources such as object references to be released in a timely manner. The pattern can be extended using Invalidation [YBS99] to allow invalid resources to be released explicitly. If a resource becomes invalid, the resource provider can send an invalidation signal to the lease grantor, which can then propagate the signal to all the lease holders. The lease holders can then cancel the leases, allowing the resource to be released. Note that Invalidation can result in additional complexity and dependencies between the resource provider, the lease grantor, and the lease holders. It should therefore only be used when it is not sufficient to wait for the lease duration to expire, and is instead necessary to release resources as soon as they become invalid.
There are several benefits of using the Leasing pattern:
?	Simplicity. The Leasing pattern simplifies the management of resources for the resource user. Once the lease on a resource expires and is not renewed by the resource user, the resource can be automatically released.
?	Efficiency of use. A resource provider can control resource use more efficiently through time-based leases. By bounding resource use to a time-based lease, the resource provider can ensure that unused resources are not wasted by releasing them as soon as possible, allowing them to be granted to new resource users. This can lead to minimization of the overall system load caused by unused resources.
?	Versioning. The Leasing pattern allows older versions of resources to be replaced with newer versions with relative ease. The resource provider can supply the resource user with a new version of a resource at the time of lease renewal.
?	Enhanced system stability. The Leasing pattern helps to increase system stability by ensuring that resource users do not access invalid resources. In addition, the Leasing pattern helps to reduce wastage of unused resources, and therefore helps to avoid shortage of resources.
There are some liabilities of using the Leasing pattern:
?	Additional overhead. The Leasing pattern requires an additional object, a lease, to be created for every resource that is granted by a resource provider to a resource user. Creating a pool of lease objects and reusing them with different resource allocations can help to simplify this problem. In addition, on lease expiry, the lease grantor needs to send a notification to the resource user, adding additional overhead.
?	Additional application logic. The Leasing pattern requires the application logic to support the concept of leases as the glue between the resource providers and resource users. Application architects therefore need to keep in mind that resources are not unlimited, and that they are not available all the time. Putting repetitious leasing code into a framework alleviates the coding complexity. 
?	Timer watchdog. Both the resource provider and the resource user need to be able to determine when a lease will expire. This requires support for some kind of a timer mechanism, which may not be available in some legacy systems. If however the legacy systems are event-based applications, they can be made timer-aware with very little overhead.
Jini [Sun04c]. Sun's Jini technology makes extensive use of the Leasing pattern by using it in two ways. First, it couples each service with a lease object that specifies the duration of time for which a client can use that service. Once the lease expires and is not renewed by the client, the service corresponding to the lease object is no longer available to the client. Second, it associates a lease object with each registration of a service with the Jini lookup service. If a lease expires and the corresponding service does not renew the lease, the service is removed from the lookup service.
.NET Remoting [Ramm02]. Remote objects in .NET Remoting are managed by the Leasing Distributed Garbage Collector. Leases can be renewed in three ways: firstly, the lease is renewed implicitly on every new request. Secondly, clients and servers can explicitly renew the lease using the Renew() method on the ILease interface. Remote objects can influence their leasing-based lifecycle management by implementing the ILease interface. Thirdly, the remote objects can also transfer lease management to a lease provider¡ªa ¡®sponsor¡¯¡ª which has to implement the ISponsor interface. When a lease expires, the sponsor is asked for an extension to the lease. Leases are most useful for client-activated objects, as singletons are typically configured to live forever.
Software licenses [Macr04]. A software license can be regarded as a lease between the software and the user. A user may obtain a license for using a particular software. The license itself may be obtained, for example, from a license server, and is usually for a set period of time. Once the period of time expires, the user must renew the license or else the software can no longer be used.
Dynamic Host Configuration Protocol [DHCP04]. The purpose of DHCP is to enable individual computers on an IP network to extract their configuration settings from a DHCP server. The motivation behind this is to reduce the work necessary to administer a large IP network. The most significant piece of information distributed in this manner is the IP address. In that context a DHCP lease is the time that the DHCP server grants permission to the DHCP client to use a particular IP address. The lease time is typically set by the system administrator. Note that DHCP clients themselves monitor when a lease expires¡ªthey are not triggered by the DHCP server.
File caching Some network file systems such as SODA [KoMa95] use leases as extensions to traditional protocols such as NFS in order to assure the consistency of cached file blocks in distributed file systems, achieving improvements in performance and scalability.
Magazine/newspaper subscriptions A real-world known use of the Leasing pattern is magazine and newspaper subscriptions. In this case, the subscription represents the lease, which usually expires after a set period of time. The subscription must be renewed by the subscriber or the subscription terminates. In some cases, the subscriber may set up automatic renewal, for example by providing bank account information or credit card information.
Web-based e-mail accounts Many Web-based e-mail accounts, for example MSN Hotmail [MSN04] or GMX [GMX04], become inactive automatically if not used for a certain period of time. In this case, the time duration can be regarded as a lease whose renewal requires use of the e-mail account.
The most common implementation of the Leasing pattern relies on event-based callbacks to signal lease expiration. An event-based application typically uses one or more event dispatchers or Reactors [POSA2], such as the Windows Message Queue, InterViews' Dispatcher [LiCa87], or the ACE Reactor [Schm02] [Schm03a]. Event dispatchers typically provide an interface to register event handlers, such as timer handlers that are called on timer expiration.
In applications that do not contain an event loop, the Active Object [POSA2] pattern can be used to substitute timer handling. The Active Object has its own thread of control and can either instrument the OS or run its event loop to signal leases via callbacks on timer expiry.
To make leasing transparent to the resource user, the Proxy [GoF95] [POSA2] pattern can be employed. The resource proxy can handle lease renewals, policy negotiations, and lease cancellations that would otherwise normally be done by the user. CORBA Smart Proxies [Schm04] provide the appropriate abstraction in CORBA, while Smart Pointer is the counterpart to this in C++.
For concurrent resources, a resource provider has to manage multiple resource users holding leases to the same resource. Reference counting [Henn01] is a typical solution for keeping track of the event for which no leases are held by any resource user and the resource can be evicted.
##%%&&
The Evictor pattern describes how and when to release resources to optimize resource management. The pattern allows different strategies to be configured to determine automatically which resources should be released, as well as when those resources should be released.
Consider a network management system that is responsible for managing several network elements. These network elements are typically represented as a topology tree. A topology tree provides a virtual hierarchical representation of the key elements of the network infrastructure. The network management system allows a user to view such a tree, as well as to get details about one or more network elements. Depending on the type of the network element, its details may correspond to a large amount of data. For example, the details of a complex network element may include information about its state as well as the state of its components.
The topology tree is typically constructed at application start-up or when the user asks to view the tree of network elements, or some time in between. Similarly, the details of all the network elements can be fetched as the tree is constructed, or can be deferred until the user requests it. Regardless of when the details of the network elements are brought into memory, keeping these details in memory over time can be quite expensive. If the details of a network element are never accessed by the user again, they will consume valuable resources in the form of memory. On the other hand, the user may request the details of the same network elements, and therefore keeping the details in memory can improve performance. If the details of a network element that is frequently accessed by a user are not cached, it can lead to expensive calls to the real network element to get its details. This in turn can degrade system performance.
The relevant questions are: when is the right time to release resources, and which resources should be released?
Systems in which the release of resources needs to be controlled to ensure their efficient management.
Highly robust and scalable systems must manage resources efficiently. Over time, an application acquires many resources, some of which are only used once. If an application keeps on acquiring resources without ever releasing them, it will lead to performance degradation and system instability. To avoid this, the application may immediately release resources after using them. But the application may need to use the same resources again, which would require re-acquisition of those resources. However, re-acquisition of resources can itself be expensive, and should therefore be avoided by keeping frequently used resources in memory.
To address these conflicting requirements of resource management requires the resolution of the following forces:
?	Optimality. The frequency of use of a resource should influence the lifecycle of a resource.
?	Configurability. Resource release should be determined by parameters such as type of resource, available memory and CPU load. 
?	Transparency. The solution should be transparent to the resource user.
Monitor the use of a resource and control its lifecycle using some form of strategy such as Least Recently Used (LRU) or Least Frequently Used (LFU). Each time a resource is used, it is marked by the application. A resource that is not recently used or not frequently used does not get marked. Periodically or on demand the application selects the resources that are not marked and releases or evicts them. Resources that are marked continue to stay in memory, since they are used frequently.
Alternatively, other strategies can be used to determine which resources to evict. For example, for memory-constrained applications, the size of resources can be used to determine which resource(s) to evict. In such case, a high-memory consuming resource may be evicted even if it has been used recently.
The following participants form the structure of the Evictor pattern:
?	A resource user uses a resource, and can include an application or an operating system.
?	A resource is an entity such as a service that provides some type of functionality.
?	An evictor evicts resources based on one or more eviction strategies.
?	An eviction strategy describes the criteria that should be used to determine whether or not a resource should be evicted.
The following CRC cards describe the responsibilities and collaborations of the participants.
The participants of the Evictor pattern form the following class diagram.
The resource user first acquires the resource. Depending on the resource, the resource user registers it with the Evictor. The Evictor monitors the resource from then on. When the Evictor detects that a resource should be evicted according to its eviction strategy, it queries the resource to see if it can be evicted. If the resource can be evicted, it is given a chance to clean up, and is then removed.
The sequence of steps in most implementations stays the same. Some variations can occur when, for example, resources can not be marked, and as a result the Evictor has to intercept requests to resources to get statistical information on which to base its eviction strategies.
Four steps are involved in implementing the Evictor pattern.
1.	Define the eviction interface. An eviction interface should be defined that will be implemented by all resources able to be evicted.
2.	The method isEvictable() can be used to determine whether an object is evictable¡ªsee step 2 for details. The info() method is used by the Evictor to extract strategy-specific information from the object, to determine whether or not to evict it¡ªsee step 4 for details. The beforeEviction() method serves as a hook method [Pree94] that can be called by the Evictor before it evicts an object. This gives the object a chance to release any resources it may have acquired.
For example, the EJB Session Bean [Sun04b] and Entity Bean interfaces include a method called ejbPassivate() that is called just before an entity or a session bean is evicted. This gives the bean a chance to release any acquired resources.
3.	Determine evictable resources. The developer must determine which resources can and should be evicted. For example, resources that are always required by an application, or those that can not be reacquired, should not be evicted. Any resource that can be evicted must implement the eviction interface. Prior to evicting the resource, the application should call the interface, giving the resource a chance to do any necessary clean-up, including persisting any necessary state.
 In the Java interface described above, the application can use the method isEvictable() to indicate whether a resource can be evicted. If the method returns true, the resource is considered by the Evictor as a possible candidate for eviction. If the method returns false, the Evictor ignores the resource.
4.	Determine an eviction strategy. Based on application requirements, different eviction strategies can be used to determine when to evict resources, as well as which of the evictable resources to evict. Some of the common strategies used to determine which resources to evict include Least Recently Used (LRU) and Least Frequently Used (LFU).
In addition, a user-defined strategy can be used that may take different parameters. For example, an eviction strategy may take into account how expensive it is to re-acquire an evicted resource. Using such a strategy, resources that are less expensive to re-acquire may be evicted, even if they have been more frequently used when compared to resources that are more expensive to re-acquire.
5.	Define the use of eviction in the system. The business logic of evicting resources needs to be added to the Evictor. This includes determining how and when resources should be evicted, as well as actually marking resources to be evicted. Typically, the Evictor exists as a separate object or component in the application and is configured with the necessary eviction strategy by the application. For example, an application may choose to evict resources only when available memory goes below a specific threshold. A different application might implement a more proactive policy and could periodically evict resources even if memory does not go below a threshold.
An Evictor can use the Interceptor [POSA2] pattern to intercept user access to an object. The Interceptor can mark the object as being recently used in a manner that is completely transparent to the resource user. Periodically or on demand, the Evictor will typically query all evictable objects to determine which object(s), if any, to evict. In the Java interface described above, the Evictor will invoke the info() method on each object and use the information it receives, in the context of the eviction strategy, to determine whether or not to evict the object. For the actual eviction a Disposal Method [Henn03], sometimes also referred to as Explicit Termination Method [Bloc01], can be used. The Disposal Method pattern describes how classes should provide an explicit method to allow them to be cleaned up before being evicted.
Consider the example of a network management system that is responsible for managing a network of several network elements. The details of the network elements can be fetched at system start-up or when the resource user makes a request for them. Without a priori knowledge of a user's intentions, resource use needs to be optimized so that only those network elements that are frequently accessed by the resource user are kept in memory. The eviction strategy used for this example is therefore to evict network elements that have not been accessed by the resource user for a threshold amount of time.
Each network element needs to implement the Eviction interface:
Similarly, all network element components and sub-components need to implement the Eviction interface, so that they can recursively release any resources when they are evicted.
The Evictor can be implemented as an object that runs in its own thread of control. This allows it to periodically check if there are any network elements that have not been accessed for a threshold time.
Note that the information returned by the info() method and the way in which the Evictor interprets the information is application-specific, and can be tailored according to the eviction strategy that needs to be deployed.
Deferred Eviction. The process of evicting one or more objects can be refined into a two-step process. Instead of removing the objects immediately, they can be first put into some kind of a FIFO queue. When the queue is full the object at the head of the queue is evicted. The queue therefore serves as an intermediate holder of objects to be evicted. thereby giving the objects a ¡®second chance¡¯ in a similar way to Caching. If any of the objects are accessed prior to being removed from the queue, they need not incur the cost of creation and initialization. This variant, of course, incurs the cost of maintaining a queue, and also the resources associated with keeping the evicted objects in the queue.
Evictor with Object Pool. An object pool can be used to hold evicted objects. In this variant, when an object is evicted, instead of removing it from memory completely, it simply loses its identity and becomes an anonymous object. This anonymous object is then added to the object pool provided that it is not full. If the object pool is full, the object is removed from memory. When a new object needs to be created, an anonymous object from the queue can be dequeued and given a new identity. This reduces the cost of object creation. The size of the object pool should be set according to available memory. For more details, refer to the Pooling (97) pattern.
Eviction Wrapper. An object that is evictable need not implement the Eviction interface directly. Instead, the object can be contained inside a Wrapper Facade [POSA2] that then implements the Eviction interface. The Evictor will invoke the beforeEviction() method on the wrapper object, which in turn is responsible for evicting the actual object. This variant makes it easier to integrate legacy code without requiring existing classes to implement the Eviction interface. An example of this variant is the use of reference objects as wrapper objects in Java. See the Known Uses section for further details of this example.
There are several benefits of using the Evictor pattern:
?	Scalability. The Evictor pattern allows an application to keep an upper bound on the number of resources that are being used, and that are hence in memory at any given time. This allows an application to scale without impacting total memory consumption.
?	Low memory footprint. The Evictor pattern allows an application to control, via configurable strategies, which resources should be kept in memory and which resources should be released. By keeping only the most essential resources in memory, an application can be kept lean as well as more efficient.
?	Transparency. Using the Evictor pattern makes resource release completely transparent to the resource user.
?	Stability. The Evictor pattern reduces the probability of resource exhaustion and thus increases the stability of an application.
There are some liabilities of using the Evictor pattern:
?	Overhead. The Evictor pattern requires additional business logic to determine which resources to evict, and to implement the eviction strategy. In addition, the actual eviction of resources can incur a significant execution overhead.
?	Re-acquisition penalty. If an evicted resource is required again, the resource needs to be re-acquired. This can be expensive and can hinder application performance. The probability of this happening can be reduced by fine-tuning the strategy used by the Evictor to determine which resources to evict.
Enterprise JavaBeans (EJB) [Sun04b]. The EJB specification defines an activation and deactivation mechanism that can be used by the container to swap beans out from memory to secondary storage, freeing memory for other beans that need to be activated. The bean instances must implement the ejbPassivate() method and release any acquired resources. This method is called by the container immediately before swapping out the bean.
.NET [Rich02]. The Common Language Runtime (CLR) of .NET uses a garbage collector internally to release unused objects. The garbage collector categorizes objects in three generations, depending on the lifetime of the object. If an object wants to be informed before it is completely evicted, it must implement the Finalize() method. .NET suggests the use of a Disposal Method [Henn03] Dispose(), which clients should use to release objects explicitly.
Java [Sun04a]. The release of JDK 1.2 introduced the concept of reference objects that can be used to evict an object when heap memory is running low or when the object is no longer being used. A program can use a reference object to maintain a reference to another object in such a way that the latter may still be reclaimed by the garbage collector when memory is running low. In addition, the Reference Objects API defines a data structure called a reference queue, into which the Garbage Collector [NoWe00] places the reference object prior to evicting it. An application can use the reference queue to take necessary action when specific objects become softly, weakly, or phantomly reachable and hence ready to be evicted.
CORBA [OMG04a]. To manage memory consumption in an application, a Servant Manager typically keeps an upper bound on the total number of instantiated servants. If the number of servants reaches a specified limit, the Servant Manager can evict an instantiated servant before instantiating a servant for the current request [HeVi99].
Paging [Tane01]. Most operating systems that support virtual memory make use of the concept of paging. The memory used by a process is partitioned into pages. When the OS runs short of memory, unused pages are evicted from memory and paged out to disk. When a page is accessed again, the OS copies the required page from disk into main memory. This allows the OS to keep an upper bound on the total number of pages in main memory. Paging is different than swapping: swapping evicts all pages of a process at once, while paging evicts only individual pages.
The Leasing pattern (149) describes how the use of resources can be bounded by time, allowing unused resources to be released automatically. The Lazy Acquisition (38) pattern describes how resources can be (re-)acquired at the latest possible time during system run-time in order to optimize resource usage.
The Resource Exchanger [SaCa96] pattern describes how to reduce a server's load in allocating and managing resources. While the Evictor pattern deals with releasing resources to optimize memory management, the Resource Exchanger pattern deals with exchanging resources to minimize resource allocation overhead.
##%%&&
When starting to build a (distributed) application . . .
. . . we need an initial structure for the software being developed.   
Requirements and constraints inform the functionality, quality
of service, and deployment aspects of a software system, but
do not themselves suggest a concrete structure to guide development. Without precise and reasoned insight into a system¡¯s
scope and application domain, however, its realization is likely
to be a ¡®big ball of mud¡¯ that is hard to understand and convey
to customers, and a poor architectural basis on which to build.
A list of requirements shows the problem domain of an application,
but not its solution domain. Yet requirements in a working system
must be addressed by concrete software entities. If these entities and
their interactions are unrelated to the application¡¯s core business,
however, it will be hard to understand and communicate what the
system actually does. Similarly, it will be hard to meet system quality
of service requirements, since they cannot be mapped clearly to the
software elements where they are relevant. Without a clear vision
of a system¡¯s application domain, therefore, software architects can not determine if their designs are correct, complete, coherent, and
sufficiently bounded to serve as the basis for development.
Therefore:
Create a model that defines and scopes a system¡¯s business
responsibilities and their variations: model elements are abstractions meaningful in the application domain, while their roles and
interactions reflect the domain workflow.
The domain model serves as the foundation for a system¡¯s software
architecture, which becomes an expression of the model as it evolves.
A DOMAIN MODEL provides the initial step in transforming requirements
into a sustainable software architecture and implementation. Defining a precise model for the structure and workflow of an application
domain, including their variations, helps to map requirements to concrete software entities and check whether requirements are complete
and self-consistent. Missing requirements can be identified, fuzzy
requirements clarified, and unnecessary requirements removed. As
a result, the responsibilities and boundaries of a system¡¯s architecture are scoped properly. A well-formed domain model also makes
it easier meet a system¡¯s quality of service requirements, because
they can be assigned to the specific elements and workflows to which
they apply in the model. A domain model also fosters communication between software professionals, domain experts, and customers,
because its elements are based on the terminology used in the application domain.
In general, a domain model is created using an appropriate method,
such as Domain-Driven Design [Evans03] or Domain Analysis [CLF93].
Several specialized methods support the expression of variabilities
in a domain, such as Commonality/Variability Analysis [Cope98]
and Feature Modeling [CzEi02]. Domain-specific patterns can further
support the creation of a domain model: they offer representations
for recurring arrangements of common abstractions and workflows
within a domain, including their potential variations. Domain-specific
patterns are documented in a variety of domains, such as telecommunication, health care, and corporate finance [Fow97] [Ris01] [PLoPD1]
[PLoPD2] [PLoPD3] [PLoPD4] [PLoPD5].
Once the domain model has matured to the point where it adequately
portrays the functional responsibilities of an application, as well as
their variations, the next step is to transform the model into a concrete architecture that expresses and supports this functionality, and
which addresses a range of quality of service requirements such as
performance, scalability, availability, adaptability, and extensibility.
Several patterns help to arrange and connect the elements of a
domain model to support specific styles of computation. For example,
PIPES AND FILTERS (200) is suitable for applications that process
data streams, SHARED REPOSITORY (202) helps to organize data-driven
applications, and BLACKBOARD (205) is appropriate for applications that
operate on incomplete or fuzzy data, or for which no deterministic
solution algorithm is known or feasible.
Other patterns help group and separate elements of a domain
model to support specific aspects of system adaptation, extension,
and evolution. For example, LAYERS (185) groups elements of the
domain model that share similar responsibilities, properties, or granularity into separate layers, so that each layer can evolve independently. MODEL-VIEW-CONTROLLER (188) and PRESENTATION-ABSTRACTION CONTROL (191) separate user interfaces from domain functionality,
to support customer-specific interface adaptations without the need
to change or modify the realization of business logic. MICROKERNEL
(194) partitions applications into core functionality, version-specific
functionality, and version-specific APIs, to support different product variants. REFLECTION (197) objectifies specific aspects of a system¡¯s
structure and behavior to supports runtime flexibility in terms of how
its functionality executes and/or can be used by its clients. Finally,
DATABASE ACCESS LAYER (538) decouples application functionality from
a relational database, to make it easy to replace the database.
In production systems, several of the patterns outlined above can be
applied in combination to form a structural baseline architecture for
an application. For example, a MODEL-VIEW-CONTROLLER arrangement
may be combined with REFLECTION and a SHARED REPOSITORY-based
computational model.
Typically, each self-contained and coherent entity or responsibility
within the application¡¯s baseline is represented as a separate DOMAIN
OBJECT, to provide a defined software realization that addresses its
specific functional, operational, and developmental requirements.
In a distributed system, the domain objects in an application¡¯s base line can communicate via middleware. For example, BROKER (237)
supports applications whose components communicate via remote
method invocation, MESSAGING (221) supports systems in which components exchange asynchronous messages, and PUBLISHER-SUBSCRIBER
(234) mediates communication between components that coordinate
their processing via notifications of changes to their state.
##%%&&
When transforming a DOMAIN MODEL (182) into a technical software
architecture, or when realizing BROKER (237), DATABASE ACCESS LAYER
(438), MICROKERNEL (194), or HALF-SYNC/HALF-ASYNC (359) . . .
. . . we must support the independent development and evolution of
different system parts.
Regardless of the interactions and coupling between different
parts of a software system, there is a need to develop and evolve
them independently, for example due to system size and time to-market requirements. However, without a clear and reasoned
separation of concerns in the system¡¯s software architecture,
the interactions between the parts cannot be supported appropriately, nor can their independent development.
The challenge is to find a balance between a design that partitions
the application into meaningful, tangible parts that can be developed
and deployed independently, but does not lose itself in a myriad of
detail so that the architecture vision is lost and operational issues
such as performance and scalability are not addressed appropriately. An ad hoc, monolithic design is not a feasible way to resolve
the challenge. Although it allows quality of service aspects to be
addressed more directly, it is likely to result in a spaghetti structure
that degrades developmental qualities such as comprehensibility and
maintainability.
Therefore:
Define one or more layers for the software under development,
with each layer having a distinct and specific responsibility.
Assign the functionality of the system to the respective layers, and let
the functionality of a particular layer only build on the functionality
offered by the same or lower layers. Provide all layers with an interface
that is separate from their implementation, and within each layer
program using these interfaces only when accessing other layers.
A LAYERS architecture defines a horizontal partitioning of a software¡¯s
functionality according to a (sub)system-wide property, such that
each group of functionalities is clearly encapsulated and can evolve
independently. The specific partitioning criteria can be defined along
various dimensions, such as abstraction, granularity, hardware distance, and rate of change. For example, a layering that partitions
an architecture into presentation, application logic, and persistent
data follows the abstraction dimension. A layering that introduces a
business object layer whose entities are used by a business process
layer follows the granularity dimension, while one that suggests an
operating system abstraction layer, a communication protocol layer,
and a layer with application functionality follows the hardware distance dimension. Using rate of change as a layering criteria separates
functionalities that evolve independently of one another.
In most applications we find multiple dimensions combined. For
example, decomposing an application into presentation, application
logic, and persistent data layers is a layering according to both levels
of abstraction and rate of change. User interfaces tend to change
at a higher rate than application logic, which evolves faster than
data schemes such as tables in a relational database. Regardless of
which layering dimensions an application follows, each layer uses the
functionality offered by lower layers to realize its own functionality.
A key challenge is to find the ¡®right¡¯ number of layers. Too few layers
may not separate sufficiently the different issues in the system that
can evolve independently. Conversely, too many layers can fragment
a software architecture into bits and pieces without a clear vision
and scope, which makes it hard to evolve them at all. In addition,
the more layers are defined, the more levels of indirection must cross
in an end-to-end control flow, which can introduce performance
penalties¡ªespecially when layers are remote.
Typically, each self-contained and coherent responsibility within a
layer is realized as a separate DOMAIN OBJECT, to further partition
the layer into tangible parts that can be developed and evolved
independently.
Split each layer into an EXPLICIT INTERFACE (281) that publishes the
interfaces of those domain objects whose functionality should be
accessible by other layers, and connect it with an ENCAPSULATED
IMPLEMENTATION (313) that realizes this functionality. This separation
of concerns minimizes inter-layer coupling: each layer only depends
on layer interfaces, which makes it possible to evolve a layer implementation with minimal impact on other layers, and also to provide
remote access to a layer. A BRIDGE (436) or an OBJECT ADAPTER (438)
supports the separation of the explicit interface of a layer from its
encapsulated implementation.
Control and data can flow in both directions in layered systems.
For example, data is exchanged between adjacent layers in layered
protocol stacks such as TCP/IP or UDP/IP. However, LAYERS defines
an acyclic downward dependency: lower layers must not depend on
functions provided by higher layers. Such a design avoids accidental
structural complexity, and supports the use of lower layers in other
applications independently of the higher layers. Therefore, control
flow that originates from the ¡®bottom¡¯ of the stack is often instigated
via an OBSERVER-based (399) callback infrastructure. Lower layers
can pass data and service requests to higher layers via notifications realized as COMMANDS (412) or MESSAGES (420), without becoming
dependent on specific functions in their interfaces.
##%%&&
When transforming a DOMAIN MODEL (182) into a technical software
architecture, or specifying an agent in a PRESENTATION-ABSTRACTION CONTROL (191) configuration . . .
. . . we must consider that the user interface of an application changes
more frequently than its domain functionality.
User interfaces are prone to change requests: some must support
multiple look-and-feel skins, others must address specific customer preferences. However, changes to a user interface must
not affect an application¡¯s core functionality, which is generally independent of its presentation, and also changes less
frequently.
Changes to a user interface should be both easy and local to the
modified interface part. A changeable user interface must however not
degrade the application¡¯s quality of service: at any time it must display
the current state of computation, and respond to state changes
immediately. To further complicate matters, in a system that supports
multiple look-and-feel skins, each skin can change at a different rate,
which requires additional decoupling of different user interface parts.
Therefore:
Divide the interactive application into three decoupled parts:
processing, input, and output. Ensure the consistency of the
three parts with the help of a change propagation mechanism.
Encapsulate the application¡¯s functional core inside a model whose
implementation is independent of specific user interface look-and feel and mechanics. For each aspect of the model to be presented in
the application¡¯s user interface, introduce one or more self-contained
views. Associate each view with a set of separate controllers that
receive user input and translate this input into requests for either the
model or the associated view. Let users interact with the application
solely through the controllers.
Connect the model, view, and controller components via a change
propagation mechanism: when the model changes its state, notify all
views and controllers about this change so that they can update their
state accordingly and immediately via the model¡¯s APIs.
A MODEL-VIEW-CONTROLLER arrangement separates responsibilities of an
application that tend to change at a different rate, to support their
independent evolution.
The model defines the functional heart of the interactive application,
thus its internal structure depends strongly on the application¡¯s
specific domain responsibilities. Often the model is partitioned into
one or more application DOMAIN OBJECTS (208), one for each self contained responsibility. The implementation of the model should
not rely on specific I/O data formats or view and controller APIs, to
avoid having to change the model when the user interface changes.
Each coherent piece of information that is presented in the application¡¯s user interface is encapsulated within a self-contained view,
together with functionality to retrieve the respective data from the
model, transform this data into its output format, and display the
output in the user interface. This self-containment allows views to
evolve without affecting one another or the model. Two typical types
of view are TEMPLATE VIEW (345) and TRANSFORM VIEW (347). A TEMPLATE
VIEW renders model information into a predefined output format. A
TRANSFORM VIEW creates its output by rendering each data element
individually that it retrieves from the model.
Each view of the system is associated with one more controllers to
manipulate the model¡¯s state. A controller receives input through
an associated input device such as a keyboard or a mouse, and
translates it into requests to its associated view or the model. There
are three common types of controller: a controller associated with a
specific function in the application¡¯s user interface, a PAGE CONTROLLER
(337) that handles all requests issued by a specific form or page in the
user interface, and a FRONT CONTROLLER (339) that handles all requests
on the model. A controller per function is most suitable if the model
supports a wide range of functions. A PAGE CONTROLLER is appropriate
for form-based or page-based user interfaces in which each form or
page offers a set of related functions. A FRONT CONTROLLER is most
usable if the application publishes functions to the user interface
whose execution can differ for each specific request, such as the
HTTP protocol of a Web application.
The requests issued by controllers may be encapsulated into COMMAND
(412) objects that are passed to a dedicated COMMAND PROCESSOR (343)
for execution. Such a design allows controllers to change transparently to both the views and the model. In addition, it supports the
treatment of requests as first class objects, which in turn enables
an application to offer ¡®house-keeping¡¯ services like undo/redo and
request scheduling.
If a controller is in doubt over which concrete command to create, for
example in a workflow-driven application, an APPLICATION CONTROLLER
(341) helps to avoid dependencies on the model¡¯s internal state.
In most applications, multiple controllers are active at the same
time, but each user input can only be processed by one particular
controller. A CHAIN OF RESPONSIBILITY (440) that connects all controllers
simplifies the dispatching of the ¡®right¡¯ controller in response to a
specific input.
Using WRAPPER FACADES (459) for accessing low-level device driver APIs
and graphical libraries enables the views and controllers to be kept
independent of the system¡¯s platform, as well as of its input and
output devices. DATA TRANSFER OBJECTS (418) help to encapsulate the
data that views and controllers retrieve from the model.
To support efficient collaboration between model, views, and controllers without breaking the model¡¯s independence of user interface
aspects, connect them via an OBSERVER (405) arrangement. The model
is a subject, while the views and controllers are its observers. When
the model changes its state, it notifies all registered views and controllers, which in turn update their own state by retrieving the
corresponding data from the model.
##%%&&
When transforming a DOMAIN MODEL (182) into a technical software
architecture . . .
. . . we must at times consider that different functional responsibilities
of an application can require different user interface paradigms.
A human-computer interface allows users to interact with an
application via a specific ¡®paradigm,¡¯ such as forms or menus
and dialogs. However, some applications are best operated via a
distinct interface paradigm for each functionality type on offer.
For example, in a robot control system, the functionality for defining
a mission requires a different user interface than the functionality
for controlling a mobile robot during a mission. Yet we must ensure
that all functions and their user interfaces form a coherent system.
In addition, changes to any user interface should neither affect the
implementation of its corresponding functionality, nor that of other
functions and their associated user interfaces. Similarly, changes
to the implementations of a distinct function should not affect user
interfaces and implementations of other functions.
Therefore:
Structure the interactive application as a hierarchy of decoupled
agents: one top-level agent, several intermediate-level agents,
and many bottom-level agents. Each agent is responsible for a
specific functionality of the application and provides a specialized user interface for it.
Bottom-level agents implement self-contained functionality with
which users can interact, for example administration, error handling,
and data manipulation. Mid-level agents coordinate multiple related
bottom-level agents, for example all views that visualize a particular
type of data. The top-level agent provides core functionality that is
shared by all agents, such as access to a data base.
Split each agent into three parts. A presentation part defines the
agent¡¯s user interface. An abstraction part provides agent-specific
domain functionality. A control part connects the presentation with
the abstraction and allows the agent to communicate with other
agents. Connect the agents in the hierarchy via their controls.
Users interact with an agent via its presentation. All user requests
to the respective functionality in its abstraction are mediated by the
agent¡¯s control. If a user action requires accessing or coordinating
other agents, mediate this request to the controls of these agents,
either up or down the hierarchy, and from there to their abstractions.
A PRESENTATION-ABSTRACTION-CONTROL architecture helps to connect multiple self-contained subsystems, or even whole applications, with
specialized human-computer interaction models to a coherent (distributed) system. The downside of such an arrangement is its complexity: multiple user interfaces must be provided, and actions
instigated by a specific user interface must be coordinated carefully
and explicitly if control flow spans multiple subsystems and causes
reactions or view changes in their associated user interfaces. Consequently, a PRESENTATION-ABSTRACTION-CONTROL architecture only pays off
if a software system cannot be implemented by a single user interface
paradigm.
To specify a PRESENTATION-ABSTRACTION-CONTROL (PAC) architecture, identify all the self-contained responsibilities the application should offer
to its users. Each responsibility is then encapsulated within a separate bottom-level agent. If several agents share functionality or
need coordination, factor out this (coordination) functionality into an
intermediate-level agent. There can be multiple levels of intermediate level agents within a PAC architecture. Functionality shared by all
agents is provided by the top-level agent. Such decoupling supports
independent modification of agents without affecting other agents,
and allows each agent to provide its own user interface. Provide all
agents with a MODEL-VIEW-CONTROLLER (188) architecture: the abstraction corresponds with the model and its partitioning into DOMAIN
OBJECTS (208), and the presentation to the views and controllers.
Changes to an agent¡¯s interface will therefore affect its realization.
Decouple an agent¡¯s abstraction from its presentation via a control
component that is a MEDIATOR (410) with a twofold responsibility.
First, it must route all user requests from the agent¡¯s presentation to
the appropriate functionality in its abstraction. It must also route all
change propagation notifications from the abstraction to the views in
the presentation. Second, the control must coordinate the cooperation
between agents. If a user request received by a particular agent cannot
be handled by the agent alone, the control routes the request to the
controls of appropriate higher- or lower-level agents, together with its
associated input data. Results are returned in the same way, but in
reverse. Similarly, the control of an agent can receive requests and
data from the controls of other agents. The requests to be routed can
be encapsulated inside COMMAND (412) objects, and the data inside
DATA TRANSFER OBJECTS (418). Controls are the key to a loose coupling
between agents: if an agent¡¯s abstraction changes, effects on other
agents are limited to their controls.
To keep agents consistent with one another, connect them via an
OBSERVER (405) arrangement. An agent that is dependent on the state
of its associated higher- or lower-level agents registers its control
as a subscriber of the other agents¡¯ controls, which play the role of
subjects. Whenever one of these ¡®subject¡¯ agents changes its state, its
control notifies the control of the ¡®observing¡¯ agents, which can then
react appropriately to update their own state.
##%%&&
When transforming a DOMAIN MODEL (182) into a technical software
architecture . . .
. . . we must design support for functional scalability and adaptability
in different deployment scenarios.
Some applications exist in multiple versions. Each version offers
a different set of functionality to its users, or differs from other
versions in specific aspects, such as its user interface. Despite
their differences, however, all versions of the application should
be based on a common architecture and functional core.
The goal is to avoid architectural drift between the versions of the
application and to minimize development and maintenance effort for
shared functionality. In addition, upgrading one version of the application to another by adding and removing features, or by changing
their implementation, should require no or only minimal modifications to the system. Similarly, it should be easy to provide a particular
application version with different user interfaces, and also to run
the version on different platforms, allowing clients to use it most
appropriately within their specific environments.
Therefore:
Compose different versions of the application by extending a
common but minimal core via a ¡®plug-and-play¡¯ infrastructure.
A microkernel implements the functionality shared by all application
versions and provides the infrastructure for integrating version specific functionality. Internal servers implement self-contained
version-specific functionality, and external servers version-specific
user interfaces or APIs. Configure a specific application version by
connecting the corresponding internal servers with the microkernel,
and providing appropriate external servers to access its functionality. Consequently, all versions of the application share a common
functional and infrastructural core, but provide a tailored function
set and look-and-feel.
Clients, whether human or other software systems, access the microkernel¡¯s functionality solely via the interfaces or APIs provided by
the external servers, which forward all requests they receive to the
microkernel. If the microkernel implements the requested function
itself, it executes the function, otherwise it routes the request to the
corresponding internal server. Results are returned accordingly so
that the external servers can display or deliver them to the client.
A MICROKERNEL architecture ensures that every application version can
be tailored exactly for its purpose. Users or client systems only get
the functionality and look-and-feel that they require, but do not have
to incur the cost of anything they do not need. In general, evolving
a particular version towards new or different functions and aspects
¡®only¡¯ requires reconfiguring it with appropriate internal and external servers: the microkernel itself is unaffected by such upgrades.
Existing internal and external servers and other application versions
are similarly unaffected. In addition, a MICROKERNEL architecture minimizes development and maintenance efforts for all members of the
application family: each service, user interface, or API is implemented
only once.
The internal structure of the microkernel is typically based on LAYERS
(185). The bottommost layer abstracts from the underlying system
platform, thereby supporting the portability of all higher levels.
The second layer implements infrastructure functionality, such as
resource management, on which the microkernel depends. The layer
above hosts the domain functionality that is shared by all application
versions. The topmost layer includes the mechanisms for configuring
internal servers with the microkernel, as well as for routing requests
from external servers to their intended recipient.
Each specific and self-contained function and responsibility within
the microkernel can be realized as a DOMAIN OBJECT (208), which
supports its independent implementation and evolution. The routing
functionality of the microkernel is often implemented as a MEDIATOR
(410) that receives requests through a uniform interface and dispatches these requests onto corresponding domain functions in the
microkernel or the internal servers. To minimize resource consumption, particularly memory, the routing layer can use a COMPONENT
CONFIGURATOR (490) or an OBJECT MANAGER (492) to load internal servers
on demand, unload them after use, and control their lifecycle. This
design also supports the upgrade of a particular application version
with new, different, or modified functionality dynamically at runtime.
Internal servers follow a similar LAYERS design as the microkernel, but
do not usually provide a routing layer. In addition, if the functionality
of an internal server builds on system services and platform abstractions that are offered by the layers in the microkernel, they can
avoid implementing these services and abstractions themselves, and
instead call back the corresponding layers in the microkernel. This
keeps the server¡¯s footprint small, but at the expense of additional
runtime overhead to perform the callbacks. To minimize network
traffic in a distributed system, and to increase the performance of
internal servers, therefore, it may be beneficial to provide them with
all the system services and platform abstractions that they need.
The design of an external server strongly depends on its complexity
and purpose. It can range from a simple OBJECT ADAPTER (438) that
maps the application¡¯s published APIs onto its internal APIs, to a
complex user interface.
The application-specific data exchanged between external servers, the
microkernel, and its configured internal servers can be encapsulated
inside DATA TRANSFER OBJECTS (418).
##%%&&
When transforming a DOMAIN MODEL (182) into a technical software
architecture . . .
. . . we must sometimes provide a design that is prepared for evolution
and integration of unanticipated changes.
Support for variation is the key to sustainable architectures for
long-lived applications: over time they must respond to evolving and changing technologies, requirements, and platforms.
However, it is hard to forecast what can vary in an application
and when it must respond to a specific variation request.
To complicate matters, the need for variation can occur at any time,
specifically while the application is in productive use. Variations can
also be of any scale, ranging from local adjustments of an algorithm to
fundamental modifications of distribution infrastructure. Yet, while
the variation of the application should be possible at appropriate
times, the complexity associated with particular variations should be
hidden from maintainers, and there should be a uniform mechanism
for supporting different types of variation.
Therefore:
Objectify information about properties and variant aspects of
the application¡¯s structure, behavior, and state into a set of
metaobjects. Separate the metaobjects from the core application
logic via a two-layer architecture: a meta level contains the
metaobjects, a base level the application logic.
Provide the meta level with a metaobject protocol, which is a specialized interface that administrators, maintainers, or even other
systems can use to dynamically configure and modify all metaobjects
under the supervising control of the application. Connect the base
level with the meta level such that base-level objects first consult an
appropriate metaobject before they execute behavior or access state
that potentially can vary.
REFLECTION supports a high degree of runtime flexibility in a software architecture. Almost any information about a software system
can be made accessible; and any aspect that can change can be
made (ex)changeable. Some programming languages, therefore, support specific flavors of REFLECTION directly, such as Java with the
java.lang.reflect package and C# with the System.Reflection
namespace. Note, however, that the heavyweight measures of a
REFLECTION architecture only pay off if there are similarly heavyweight
flexibility requirements that justify these measures.
To realize a REFLECTION architecture, first specify a stable design for
the application that does not consider flexibility at all: stability is the
key to flexibility [Bus03]. Typically, each self-contained responsibility
of the application is encapsulated within a DOMAIN OBJECT (208), which
together form the base level of the REFLECTION architecture.
Using a suitable method, such as Open Implementation Analysis
and Design [KLLM95], Commonality/Variability Analysis [Cope98],
or Feature Modeling [CzEi02], identify all the structural and behavioral aspects of the application that can vary. Variant behavior often
includes algorithms for application functionality, lifecycle control of
domain objects, transaction protocols, IPC mechanisms, and policies
for security and failure handling. There may even be the need to add
completely new behavior to the system or to remove existing behavior.
Structural aspects that can vary include the application¡¯s thread or
process model, the deployment of domain objects to processes and
threads, or even the system¡¯s type structure. In addition, determine
all system-wide information, properties, and global state that can
influence the behavior of the application, such as runtime type information about what interfaces domain objects offer, what their inner
structure is, or whether they are persistent.
Realize each variant behavioral and structural aspect, system property, and state identified in the analysis as a separate metaobject, and
assign all metaobjects to the meta level of the REFLECTION architecture.
Such a strict encapsulation makes the aspects explicitly accessible, and thus (ex)changeable at any time. Changes to metaobjects
also cannot ripple through to the implementation of the application¡¯s
base level.
Open the implementation of each DOMAIN OBJECT at the base level
such that it consults an appropriate metaobject for each aspect
encapsulated in the meta level. Changes to the metaobjects thus
immediately impact the base level¡¯s subsequent behavior.
To support the creation, configuration, exchange, and disposal of
metaobjects at runtime, introduce a metaobject protocol that serves
as the sole interface to manage the meta level. The metaobject lifecycle infrastructure that is necessary for these activities can be realized
with help of ABSTRACT FACTORIES (525) and BUILDERS (527) to create and
dispose of metaobjects, and a COMPONENT CONFIGURATOR (490) or an
OBJECT MANAGER (492) to control the execution of specific metaobject
lifecycle steps. Such a design also enables the integration of metaobjects that were developed after the reflective application went live
with the meta level. An INTROSPECTIVE INTERFACE (286) and a DYNAMIC
INVOCATION INTERFACE (288) support application-external clients such as
test frameworks or object browsers, to obtain information about base level domain objects without becoming dependent on their internal
structure, as well as invoking methods on them without the need to
use their functional interfaces.
The metaobject protocol in conjunction with the two-layer structure
of a REFLECTION architecture is a prime example of how the open/close
principle [Mey97] can be realized. The metaobject protocol hides the
complexity of software evolution behind a ¡®simpler¡¯ interface, making
it easy, uniform, and dynamic, but allows the reflective application
to supervise its own evolution so that uncontrolled changes are
minimized. The separation of a REFLECTION architecture into a base
level and a meta level strictly separates variant from invariant aspects
in an application: metaobjects can be managed without implications
for the internal design and implementation of the domain objects at
the base level.
##%%&&
When transforming a DOMAIN MODEL (182) into a technical software
architecture . . .
. . . we must sometimes provide a design that is suitable for processing
data streams.
Some applications process streams of data: input data streams
are transformed stepwise into output data streams. However,
using common and familiar request/response semantics for
structuring such types of application is typically impractical.
Instead we must specify an appropriate data flow model for
them.
Modeling a data-flow-driven application raises some non-trivial developmental and operational challenges. First, the parts of the application should correspond to discrete and distinguishable actions on
the data flow. Second, some usage scenarios require explicit access
to intermediate yet meaningful results. Third, the chosen data flow
model should allow applications to read, process, and write data
streams incrementally rather than wholesale and sequentially so
that throughput is maximized. Last but not least, long-duration
processing activities must not become a performance bottleneck.
Therefore:
Divide the application¡¯s task into several self-contained data
processing steps and connect these steps to a data processing
pipeline via intermediate data buffers.
Implement each processing step as a separate filter component that
consumes and delivers data incrementally, and chain the filters such
that they model the application¡¯s main data flow. In the data processing pipeline, data that is produced by one filter is consumed by
its subsequent filters. Adjacent filters are decoupled using pipes that
buffer data exchanged between the filters.
A PIPES AND FILTERS architecture decouples different data processing
steps so that they can evolve independently of one another and
support an incremental data processing approach.
Within a PIPES AND FILTERS architecture, filters are the units of domain specific computation. Each filter can be implemented as a DOMAIN
OBJECT (208) that represents a specific, self-contained data processing step. Filters with a concurrent DOMAIN OBJECT implementation
enable incremental and concurrent data processing, which increases
the performance and throughput of a PIPES AND FILTERS arrangement.
If a filter performs a long-duration activity, consider integrating multiple parallel instances of the filter into the processing chain. Such a
configuration can further increase system performance and through put, as some filter instances can start processing new data streams
while others are processing previous data streams.
Pipes are the medium of data exchange and coordination within a
PIPES AND FILTERS architecture. Each pipe is a DOMAIN OBJECT that implements a policy for buffering and passing data along the filter chain:
data producing filters write data into a pipe, while data consuming filters receive their input from a pipe. The integration of pipes decouples
adjacent filters so that the filters can operate independently of one
another, which maximizes their individual operational performance.
In a single-process PIPES AND FILTERS arrangement, pipes are typically
implemented as queues. Pipes with a concurrent DOMAIN OBJECT implementation enable incremental and concurrent data processing, as do
concurrent filters. In a distributed arrangement, pipes are realized as
some form of MESSAGING (221) infrastructure that passes data streams
between remote filters. Pipes that are implemented as a DOMAIN OBJECT
shield filters from a knowledge of their specific implementation, which
also allows transparent swapping of implementation forms. Such a
design supports a flexible (re-)deployment of filters in a distributed
PIPES AND FILTERS arrangement. MESSAGES (420) help to encapsulate the
data streams that are passed along the pipes.
##%%&&
When transforming a DOMAIN MODEL (182) into a technical software
architecture . . .
. . . we must sometimes provide a design for applications whose parts
operate on, and coordinate their cooperation via, a set of shared data.
Some applications are inherently data-driven: interactions
between components do not follow a specific business process,
but depend on the data on which they operate. However, despite
the lack of a functional means to connect the components
of such applications, they must still interact in a controlled
manner.
One example of a data-driven system is a network management
and control application, such as a Telecommunication Management
Network (TMN) system. Such systems operate on massive amounts
of data provided by field devices. Core responsibilities like monitoring
and control, alarming, and reporting are largely independent of one
another, and it is the state of the data that determines the control
flow and collaboration of these tasks. Connecting the tasks directly
would hard-code a specific business process into the application,
which may be inappropriate if specific data is unavailable, not of the
required quality, or in a specific state. However, we need a coherent
computational state across the entire application.
Therefore:
Maintain all data in a central repository shared by all functional
components of the data-driven application and let the availability, quality, and state of that data trigger and coordinate the
control flow of the application logic.
Components work directly on the data maintained by the shared
repository, so that other components can react if this data changes.
If a component creates new data, or if the application receives new
data from its environment, is also stored in the shared repository, to
make it accessible to other components.
A SHARED REPOSITORY architecture allows integration of application
functionality with a data-driven control flow to form coherent software systems. It also supports coherent integration of applications
that operate on the same data, but neither share nor participate in
common business processes. Coordinating components via the state
of shared data can introduce performance and scalability bottlenecks,
however, if many concurrent components need access to the same
data exclusively and are thus serialized.
The shared repository is the central control coordination entity and
data access point of a data-driven application. It can be as simple as an in-memory data collection, or as complex as an external
data repository that is accessed via a DATABASE ACCESS LAYER (438).
If the shared repository is implemented as a DOMAIN OBJECT (208),
its concrete implementation is hidden from the application¡¯s components and can be swapped or modified transparently. DATA TRANSFER
OBJECTS (418) help to encapsulate the data passed between the shared
repository and the components of the application.
The data maintained by the shared repository is often encapsulated inside managed objects: DOMAIN OBJECTS that hide the details
of concrete data structures and offer meaningful operations for their
access and modification. Managed objects allow the application¡¯s
components to use specific data without becoming dependent on its
concrete representation, and support the modification of data representations without effects on the components that use the data.
Managed objects can also indicate the quality of the represented data
via a corresponding quality attribute, for example that the data is up-to-date, out-of-date, uncertain, or corrupted. Components can use
this information to control the specific treatment of that data.
In general, access to the shared repository and its managed
objects must be synchronized, because multiple components of the
application can access it concurrently. In most configurations, this
synchronization happens at the level of managed objects, which
maximizes the potential concurrency within the data-driven application. Providing a managed object with a THREAD-SAFE INTERFACE (406)
enforces synchronization at the interface of the managed object. If
only small portions of its methods are critical sections, synchronization via STRATEGIZED LOCKING (388) is a possible alternative. Realizing a
managed object as a MONITOR OBJECT (390) supports cooperative concurrency control of multiple components that access the managed
object simultaneously.
Many shared repositories offer a mechanism for notifying application
components about data changes within the repository. For example,
new data may have been inserted, existing data modified, or data may
have been dropped. Components can therefore react immediately to
changes to the data in the repository. In most cases, the change
notification mechanism is realized by an OBSERVER (405) arrangement:
the shared repository is the subject, the application¡¯s components are
its observers. Similarly, managed objects can also offer an OBSERVER based change propagation mechanism, which allows them to notify
components about specific value changes.
Which of the two options best suits a data-driven application depends
on its concrete responsibilities. The trade-off to consider is simplicity versus granularity: change notification at the level of the shared
repository is simple to implement, but could cause overhead due to
notification of components that are not interested in the changes
reported. Vice versa, a mechanism implemented at the level of managed objects avoids unnecessary notifications and data transfer, but
is of higher complexity. The more components of an application that
operate on the entire data maintained by the shared repository,
the more feasible a notification mechanism at the repository level
becomes, while the more selectively components access managed
objects, the more notification at the level of managed objects is the
best fit.
The data-driven service components of the application are typically
implemented as DOMAIN OBJECTS that realize a specific responsibility
by accessing and manipulating data in the shared repository. Cooperation between the components happens purely at the data level, by
notifying other components when a specific managed object changes
its state, or when data is inserted into, or deleted from, the shared
repository.
##%%&&
When transforming a DOMAIN MODEL (182) into a technical software
architecture . . .
. . . we must sometimes provide a design suitable for applications that
resolve tasks for which no deterministic solution strategy is known.
For some tasks no deterministic solution algorithms are known,
only approximate or uncertain knowledge is accessible. However,
despite this lack of proper algorithmic support, trial-and-error
techniques can be sufficiently successful and it is necessary to
develop productive applications for these types of task.
Examples of such systems include speech recognition, submarine
detection based on sonar signals, and the inference of protein
molecule structures from X-ray data. Such tasks must resolve several hard challenges: input data is often fuzzy or inaccurate, the
path towards a solution must be explored, every processing step
can generate alternative results, and often no optimal solution is
known. Nevertheless, it is important to compute valuable solutions
in a reasonable amount of time.
Therefore:
Use heuristic computation to resolve the task via multiple
smaller components with deterministic solution algorithms that
gradually improve an intermediate solution hypothesis.
Divide the overall task of the system into a set of smaller, self contained subtasks for which deterministic solution algorithms are
known, and assign the responsibility for each subtask to an independent knowledge source. To allow the knowledge sources to execute
independently of one another and in arbitrary order, let them cooperate via a non-deterministic data-driven approach. Using a shared
data repository, the blackboard, knowledge sources can evaluate
whether input data is available for them, process this input, and
deliver their results, which may then form the input for any other
knowledge source in the system.
Coordinate the computation with a control component that uses an
opportunistic heuristic to select and activate adequate knowledge
sources if the data on the blackboard does not yet represent a useful
final result, and finishes the computation if it does. Such a strategy
works towards a solution via incremental improvement of partial
results and evaluation of alternative hypothesis, instead of using a
deterministic algorithm.
A BLACKBOARD architecture helps in the construction of software
systems that must resolve tasks on the basis of uncertain, hypothetical, or incomplete knowledge and data. It also helps to discover
and optimize strongly deterministic solutions for tasks that lack
such solutions. On the other hand, there is no guarantee that a
BLACKBOARD-based system actually produces a useful result. In addition, a computational approach based on heuristics is often not
feasible for systems that demand a predictability in terms of result
quality and the time in which a result is produced.
To implement a BLACKBOARD system, first decompose the task that
it must resolve. What input does the system receive? What form of
output should it produce? What potential solution paths and intermediate results towards a solution are known? What are the well-known
algorithms that can contribute to the solution (path)? What input or
intermediate results can each algorithm process? What intermediate
or final results can each algorithm deliver? On the basis of this analysis, define self-contained and independently executable knowledge
sources for every algorithm that is involved in the tasks' solution.
Such independence allows the execution order of knowledge sources
to be arbitrary¡ªa necessary precondition for a heuristic solution
strategy. To allow a heuristic to determine a particular execution
order, split each knowledge source into two separate parts. A condition part examines whether the knowledge source can make a
contribution to the computation¡¯s progress by inspecting the data
written on the blackboard. An action part implements the knowledge
source¡¯s functionality: it reads one or more inputs from the blackboard, processes it, and writes one or more outputs back to the
blackboard. Alternatively, the action part could erase data from the
blackboard because it identifies the data as not contributing to the
overall task¡¯s solution. Typically, a knowledge source is implemented
as a DOMAIN OBJECT (208), which supports its independent evolution and optimization when more knowledge about the application¡¯s
overall task becomes available.
The blackboard is a data repository that maintains all partial and
final results that the knowledge sources produce. It can be designed
as an in-memory data collection, or as an external data repository
that is accessed via a DATABASE ACCESS LAYER (538). If the blackboard
is implemented as a DOMAIN OBJECT, its concrete implementation is
hidden from the knowledge sources and can be swapped or modified
transparently. DATA TRANSFER OBJECTS (418) help to encapsulate the
data passed between the blackboard and the knowledge sources.
A control component realizes the heuristic solution strategy of a
BLACKBOARD system. First it reads the system¡¯s input and stores it
on the blackboard, then it enters a loop that executes three steps.
The initial step calls the condition parts of all knowledge sources
to determine whether they can contribute in the current state of
computation. The second step uses a heuristic that analyzes the
results returned by the condition parts to determine the particular
knowledge source that can best contribute to the progress of the
computation. The final step invokes the action part of the selected
knowledge source, which then modifies the blackboard¡¯s content.
Once this knowledge source finishes its execution, the loop starts
over again.
The loop ends if the blackboard contains a valid final result, or if none
of the knowledge sources can improve the quality of any intermediate
solution hypothesis on the blackboard. Implementing the control
component as a DOMAIN OBJECT allows the chosen heuristics to be
modified and evolved transparently for the knowledge sources and
the blackboard of a concrete BLACKBOARD arrangement.
##%%&&
When realizing a DOMAIN MODEL (182), or its technical architecture
in terms of LAYERS (185), MODEL-VIEW-CONTROLLER (188), PRESENTATION ABSTRACTION-CONTROL (191), MICROKERNEL (194), REFLECTION (197), PIPES
AND FILTERS (200), SHARED REPOSITORY (202), or BLACKBOARD (205) . . .
. . . a key concern of all design work is to decouple self-contained and
coherent application responsibilities from one another.
The parts that make up a software system often expose manifold collaboration and containment relationships to one another.
However, implementing such interrelated functionality without
care can result in a design with a high structural complexity.
Separation of concerns is a key property of well-designed software.
The more decoupled are the different parts of a software system, the
better they can be developed and evolved independently. The fewer
relationships the parts have to one another, the smaller the structural complexity of the software architecture. The looser the parts
are coupled, the better they can be deployed in a computer network or composed into larger applications. In other words, a proper
partitioning of a software system avoids architectural fragmentation,
and developers can better maintain, evolve and reason about it. Yet
despite the need for clear separation of concerns, the implementation
of and collaboration between different parts in a software system
must be effective and efficient for key operational qualities, such as
performance, error handling, and security.
Therefore:
Encapsulate each distinct functionality of an application in a
self-contained building-block¡ªa domain object.
Provide all domain objects with an interface that is separate from
their implementation, and within each domain object program only
using these interfaces when accessing other domain objects.
DOMAIN OBJECT separates different functional responsibilities within
an application such that each functionality is well encapsulated and
can evolve independently. The specific partitioning of an application¡¯s
responsibilities into domain objects is based on one or more granularity criteria. An APPLICATION SERVICE [ACM01] is a domain object
that encapsulates a self-contained and complete business feature
or infrastructure aspect of an application, such as a banking, flight
booking, or logging service [Kaye03]. A COMPONENT [VSW02] is a domain
object that either encapsulates a functional building block such as an
income tax calculation or a currency conversion, or a domain entity
such as a bank account or a user. A VALUE OBJECT [PPR] [Fow03a],
a COPIED VALUE (394), and an IMMUTABLE VALUE (396) are small domain
objects whose identity is based on their state rather than their type,
such as a date, a currency exchange rate, or an amount of money. A
domain object can also aggregate other domain objects of the same
or smaller granularity. For example, services are often created from
components that use value objects.
Split each domain object into an EXPLICIT INTERFACE (281) that exports
its functionality and an ENCAPSULATED IMPLEMENTATION (313) that realizes
the functionality. This separation of interface and implementation
minimizes inter-domain-object coupling: each domain object only
depends on domain object interfaces, but not on domain object
implementations. It is thus possible to realize and evolve a domain
object implementation with minimal effect on other domain objects.
The explicit interface of a domain object defines a contract for key
operational properties, such as error behavior and security aspects,
on which other domain objects can rely.
There are several options for connecting the explicit interface of a
domain object with its encapsulated implementation. For example,
Java and C# support the concept of explicit interface in the core language, and classes (encapsulated implementations) can implement
them directly. In other statically typed languages such as C++, an
explicit interface can be expressed as an abstract base class from
which the explicit implementation derives.
A BRIDGE (436) or an OBJECT ADAPTER (438) explicitly decouples the
explicit interface of a domain object from its encapsulated implementation so that the two can vary independently. The degree of
decoupling between the explicit interface of a domain object and its
encapsulated implementation depends on its granularity and likelihood of change. The smaller the domain objects, for example when
realizing a VALUE OBJECT or an IMMUTABLE VALUE, the less beneficial
strict decoupling becomes. Similarly, the more often an encapsulated implementation evolves, the more strongly the explicit interface
should be decoupled.
Explicit interfaces also enable remote access to domain objects. Note,
however, that remoting is generally feasible only for ¡®larger¡¯ domain
objects such as services and coarse-grained components, but not
for ¡®small¡¯ domain objects like a value object. The smaller are the
domain objects, the more adverse is the ratio of networking overhead
versus computation time inside the domain object, with corresponding penalties on operational quality factors such as performance,
availability, and scalability.
Domain objects are often associated with an ABSTRACT FACTORY (525)
or BUILDER (527) that allows clients to obtain access to their explicit
interface and to manage their lifetime transparently. On platforms like
CCM [OMG02], EJB [MaHa99], and. NET [Ram02], domain objects
are controlled by a DECLARATIVE COMPONENT CONFIGURATION (461) that
specifies how their lifecycle, resource management, and other technical concerns like transactions and logging should be handled by
their hosting environment. A COMPONENT CONFIGURATOR (490) helps with
loading, replacing, (re)configuring, and unloading domain objects at
runtime.
##%%&&
When deploying a DOMAIN MODEL (182), or a PIPES AND FILTERS (200)
arrangement, to multiple processors or network nodes . . .
. . . we often need a communication infrastructure that integrates
independently developed services into a coherent system.
Some distributed systems are composed of services that were
developed independently. To form a coherent system, however,
these services must interact reliably, but without incurring
overly tight dependencies on one another.
Application integration is a key technique for composing solutions¡ª
often at the enterprise level¡ªfrom existing, self-contained, special purpose services. Each service provides its own business logic and
value, but together they can provide the business processes and value
chain of an entire enterprise. Integrating independent services into a
coherent application naturally requires reliable collaboration with one
another. Since services are developed independently, however, they
are generally unaware of each other¡¯s specific functional interfaces.
Each service may also participate in multiple integration contexts,
so using it in a specific context should not preclude its use in other
contexts.
Therefore:
Connect the services via a message bus that allows them to
transfer data messages asynchronously. Encode the messages
so that senders and receivers can communicate reliably without
having to know all the data type information statically.
The services that form the distributed system connect with the message bus to exchange data messages with other services. Clients can
initiate collaborations with remote services by sending them data messages asynchronously. The remote services process the received messages and return their responses¡ªif there are any¡ªasynchronously
to the clients via messages containing the processing results. The
messages are often self-describing: they contain both metadata that
describes the message schema, and the values corresponding to the
schema.
Middleware based on MESSAGING enables services in a distributed
application to interact without having to deal with remoting concerns by themselves, and without depending on statically defined
service interfaces and data structures. In addition, the asynchronous
nature of MESSAGING communication allows distributed application
services to handle multiple requests simultaneously without blocking, as well as participate in multiple application integration and
usage contexts. MESSAGING thus enables loose coupling, which is a
key to Enterprise Application Integration (EAI) [Lin03] and Service Oriented Architectures (SOA) [Kaye03]. The primary drawbacks of
MESSAGING are its lack of statically typed interfaces, which makes it
hard to validate system behavior prior to runtime, and the potential
for high time and space overhead necessary to process self-describing
messages [Bell06].
Data exchanged between application services are often encapsulated
inside MESSAGES (420). A message hides the concrete data format
of its contents from both the sender and the receiver, as well as
from the MESSAGING middleware itself, which enables the transparent
modification of its format. XML is a popular format for representing
both the metadata and data values of self-describing messages. It
enables clients to generate and send messages whose form and
content need not be fixed statically.
MESSAGING clients only know the endpoints of the services they use,
not their specific interfaces. Consequently the form and content of a
message cannot be checked statically on the client before sending it
to a specific service. Instead, the service that receives a message is
responsible for understanding the message¡¯s form and content. This
process typically involves parsing the message dynamically to validate
and extract its contents. If the service does not understand some of
the message fields, it can simply ignore them, thereby simplifying
the integration of services whose message formats were not originally
designed to work together.
A concrete MESSAGING arrangement typically consists of several specialized parts. MESSAGE CHANNELS (224) support point-to-point communication between interacting remote services and enable reliable
message exchange. MESSAGE ENDPOINTS (227) connect application services with the MESSAGING middleware: these can send and receive
messages without depending on concrete messaging APIs, which
enables the transparent exchange and evolution of the underlying
MESSAGING infrastructure.
If the sender and the receiver of a message do not share a common
message format, a MESSAGE TRANSLATOR (229) can convert messages
issued by the sender into a format understood by the receiver. If the
sender does not know where to address a message, a MESSAGE ROUTER
(231) can help to direct it to its intended receiver. A communication
failure that cannot be handled internally by the MESSAGING middleware
can be returned as a REMOTING ERROR [VKZ04] to the client that sent
the message.
##%%&&
When developing a MESSAGING (221) infrastructure or a CLIENT REQUEST
HANDLER (246) and SERVER REQUEST HANDLER (249) in a BROKER (237)
arrangement . . .
. . . we must provide a means to connect a set of clients and services
that communicate by sending and receiving messages.
Message-based communication supports loose coupling between
services in a distributed system. Messages only contain the data
to be exchanged between a set of clients and services, however,
so they do not know who is interested in them.
Loose coupling makes it easier to integrate diverse information systems, but somehow the loose ends must tie back together. It is not
sufficient for a client to send messages randomly while other services
randomly receive whatever messages they come across. A client that
sends out messages knows what sort of information these messages
contain and often also knows who it wants to receive the messages.
Similarly, services that receive messages look for particular messages they can process, and often for messages from specific senders.
In other words, clients and services need to exchange messages in
predictable and reliable ways.
Therefore:
Connect the collaborating clients and services using a message
channel that allows them to exchange messages.
When a client has a message to communicate, it writes that message
to the message channel. Services interested in the message can pick
it up from there and process it.
A message channel connects a set of interacting clients and services,
thereby allowing them to exchange messages in a well-defined and
reliable manner. Clients that write messages to the channel can be
sure that the services reading the messages from the channel are
interested in the information they contain, while services that read
messages are sure they have received information that they can use
and process.
A message channel is thus a logical address to which clients and
services can write messages and/or from which they can receive
messages. Several types of message channels are common. A POINT TO-POINT CHANNEL [HoWo03] connects exactly one client and one service
and ensures that only they can read the messages written to it. In
contrast, a PUBLISH-SUBSCRIBE CHANNEL [HoWo03] enables publishers
to broadcast messages to multiple subscribers using the PUBLISHER SUBSCRIBER pattern (234).
An INVALID MESSAGE CHANNEL [HoWo03] decouples the handling of erroneous messages separately from the rest of the application logic,
whereas a DEAD LETTER CHANNEL [HoWo03] handles messages that were
sent successfully but which could not be delivered. Finally, a DATATYPE
CHANNEL [HoWo03], ensures that all messages on a channel are of the
same type, which helps to reduce message validation overhead in the
intended receiver.
A message channel is shared by at least two concurrent entitles: a
client that sends messages to the channel, and a service that obtains
messages from the channel. Depending on a channel¡¯s implementation and use, therefore, it may require synchronization. A THREAD-SAFE
INTERFACE (384) enforces synchronization at the channel¡¯s interface,
and a realization as a MONITOR OBJECT (368) supports cooperative
concurrency control for simultaneous access to the channel.
In general, the operational requirements of a distributed application determine which specific message channel configuration is most
appropriate. For example, information assurance requirements may
dictate separate SECURE CHANNELS [SFHBS06] for selected security sensitive collaborations. Performance and scalability requirements
may equally dictate separate message channels for each type of
message, or even for each use case.
A message channel does not come without cost, however, since it
needs memory, networking resources, and persistent storage to support GUARANTEED DELIVERY [HoWo03]. Developers must therefore plan
and configure the number and types of message channels explicitly
and thoughtfully to ensure the desired quality of service in a given
system deployment. A well-designed set of message channels forms a
MESSAGE BUS [HoWo03] that acts like a messaging API for the clients
and services in the distributed system.
Clients and services that are not designed to use a message channel
or message bus can connect to it via a CHANNEL ADAPTER [HoWo03]. A
MESSAGING BRIDGE [HoWo03] helps to connect clients and services that
are designed to use different channel or bus implementations.
##%%&&
When developing a MESSAGING (221) infrastructure . . .
. . . we must enable clients and services in an application to send and
receive messages.
Clients and services in a stand-alone application usually collaborate by passing data to one another. When clients and services are
connected by a messaging infrastructure, however, such direct
collaboration is impossible: data must be transformed into messages and vice versa.
Performing the data-to-message transformation directly within the
applications would tightly couple them with the specific message
format required by the messaging middleware. It would therefore
be hard to use the services in other applications, and the mix of
domain-specific code with infrastructure code would complicate their
evolution and maintenance. Even if messaging is incorporated as a
fundamental part of the application, replacing the underlying messaging infrastructure is time-consuming, tedious, and error-prone.
Therefore:
Connect the clients and services of an application to the messaging infrastructure using specialized message endpoints that
allow clients and services to exchange messages.
When a client has data to communicate, it passes this data to its
associated message endpoint, which first converts the data into a
message understood by the messaging middleware, then sends that
message to an endpoint representing the message¡¯s receiver. This
endpoint converts the message into data that is understood by the
receiver service and passes the data to that service in an appropriate
format.
MESSAGE ENDPOINTS encapsulate the messaging middleware from the
application clients and services and customize the middleware¡¯s general messaging API for them. Modifications to the messaging API,
and even an exchange of the entire messaging infrastructure, can
therefore be transparent to applications. In addition, all necessary
changes are localized within the endpoints.
In general, a messaging endpoint should be designed as a MESSAGING
GATEWAY [HoWo03], to encapsulate the messaging-specific code and
expose a domain-specific interface to the service it represents. Internally, the endpoint can deploy a MESSAGING MAPPER [HoWo03] to
transfer data between the service and the messages. To provide
asynchronous access to a synchronous method, a message endpoint
can be structured as a SERVICE ACTIVATOR [HoWo03]. A TRANSACTIONAL
CLIENT [HoWo03] allows a message endpoint to control transactions
explicitly in the messaging middleware.
Message endpoints can select among several different approaches for
receiving messages. A POLLING CONSUMER [HoWo03] provides a proactive message reception strategy that reads messages only when the
represented service is ready to consume them. In contrast, an EVENT DRIVEN CONSUMER [HoWo03] supports a reactive message reception
strategy that processes a message immediately upon arrival. If a service implements stateless functionality, the message endpoint can be
a COMPETING CONSUMER [HoWo03], to allow multiple service instances
to process messages concurrently. A MESSAGE DISPATCHER [HoWo03]
helps to dispatch incoming messages to the ¡®right¡¯ recipient if several
services share the same message endpoint.
Designing a message endpoint as a SELECTIVE CONSUMER [HoWo03]
enables the filtering of incoming messages: a service only processes
messages that comply to the filter¡¯s criteria. A message endpoint can
also be a DURABLE SUBSCRIBER [HoWo03], so that messages received
while the represented service is unavailable are not lost. Finally, an
endpoint realized as an IDEMPOTENT RECEIVER [HoWo03] can handle
messages that were accidentally received multiple times.
The type and functionality of the represented service generally dictates which of the message reception strategies outlined above are
most appropriate for a specific endpoint. The development of the
message endpoint can be customized for ¡®its¡¯ service.
##%%&&
When developing a MESSAGING (221) infrastructure . . .
. . . we often must transform messages from the format delivered by
the client to the format understood by the service that receives them.
Messages enable a loosely coupled style of communication
between an application¡¯s clients and services. As a consequence
of this decoupling, however, the client that sends a message
cannot assume that the services that receive it understand the
same message format.
In complex integration scenarios in which existing and independently
developed components are composed into new applications, it is
likely that many services will require a specific message format.
Resolving such a ¡®Tower of Babel¡¯ confusion of ¡®languages¡¯ inside the
services would introduce explicit and mutual dependencies between
them, which contradicts the idea of loose coupling and degrades
the benefits of message-based communication. Unifying the message
formats across all services is often infeasible, however, because it
can degrade their usability in other applications and integration
scenarios.
Therefore:
Introduce message translators between clients and services of an
application that convert messages from one format into another.
A message translator provides a bidirectional translation of message
formats. In a specific collaboration, clients can send messages in any
format they use. The message translator ensures that services get
these messages in the formats they understand.
A MESSAGE TRANSLATOR maintains the loosely coupled style of communication introduced by MESSAGING even if the clients and services of an
application do not share a common message format. In addition, all
message transformation code is localized within a dedicated entity.
This design supports evolution that is independent of, and transparent to, the clients and services that exchange messages via the
translator.
In many integration scenarios, message exchange can be supported
by placing specific requirements on the format and contents of a
message header. An ENVELOPE WRAPPER [HoWo03] helps encapsulate
the message payload so that it complies with the format required by
the messaging infrastructure. When the message arrives at its destination the payload can be unwrapped. A CONTENT ENRICHER [HoWo03]
is needed if the target service requires data fields in a message that
the originating client cannot supply: it has the ability to locate or
compute the missing information from the available data. The opposite action¡ªremoving unneeded data from a message¡ªis supported
by a CONTENT FILTER [HoWo03]. A CLAIM CHECK [HoWo03] is similar to
a CONTENT FILTER, but stores the removed data for later retrieval. A
NORMALIZER [HoWo03] helps convert multiple different message format
into one common format, and a CANONICAL DATA FORMAT [HoWo03] that
is independent of any specific service can be used inside the messaging middleware to minimize the message transformations within an
application.
##%%&&
When developing a MESSAGING (221) infrastructure . . .
. . . we must select a route to propagate messages through a system
from their source to their destination.
Messages exchanged between collaborating clients and services
must be routed through the messaging infrastructure. None of
these entities, however, should have knowledge about the routing path to choose.
Making application clients and services responsible for determining
the paths that messages should take through the system is not an
effective solution to the routing problem, nor should they have to
redirect messages they receive that were not intended for them. Such
designs would tightly couple application code with infrastructure
code, with clients and services depending on the internal structure
and configuration of the messaging infrastructure, thereby causing
maintenance problems when changes occurred. Similarly, making
messages responsible for their own routing introduces the same
problems for the data exchanged between collaborating components.
Therefore:
Provide message routers that consume messages from one message channel and reinsert them into different message channels,
depending on a set of conditions.
A message router connects a set of message channels to a message
channel network. Messages it reads from one channel are routed to
a different channel that directs them to their intended receiver.
The key benefit of a MESSAGE ROUTER is that it maintains the decision
criteria for the destination of messages in a designated location, separate from application clients, services, and the data they exchange.
If new message types are defined, the routing criteria within the
message router can be modified easily and locally. If necessary, new
message routers can be inserted into the messaging middleware. A
message router thus increases the options developers have to send
messages between application clients and services, as well as to
change routing strategies independently of, and transparently to,
applications.
The downside of MESSAGE ROUTER is that it adds extra processing steps
to an application, which may degrade its performance. A message
router also must know all the message channels it connects, which
may become a maintenance problem if configurations change frequently. Moreover, the more message routers a system contains, the
harder it is to analyze and understand the overall flow of messages
through the system without additional tools.
Developers must therefore plan and configure the number and types
of message routers carefully to meet quality of service requirements
in a given system deployment. The more message routers a system
configuration contains, the more flexibly messages can be routed
between the components of the application, but the less efficient the
message exchange becomes. In general, therefore, select the minimal
set of message routers that meet application requirements to balance
the needs for simplicity, flexibility, and quality of service.
There are many types of message routers. A CONTEXT-BASED ROUTER
[HoWo03] bases its routing decisions on environmental conditions,
such as system load, failover scenarios, or the need for a system
monitoring DETOUR [HoWo03]. A CONTENT-BASED ROUTER [HoWo03], in
contrast, determines a message¡¯s destination using specific message
properties such as their type or content. A MESSAGE FILTER [HoWo03]
assists a CONTENT-BASED ROUTER by discarding messages that do not
match the routing criteria, and a RECIPIENT LIST [HoWo03] determines
a list of recipients from the messages it receives. A PROCESS MANAGER
[HoWo03] routes messages based on intermediate results it receives in
response to previously routed messages. A MESSAGE BROKER [HoWo03]
provides a central hub-and-spoke architecture for routing messages
throughout an application.
Additional message routers help in managing messages exchanged
between components. A SPLITTER [HoWo03] converts a single large
message into several smaller messages that can be routed individually. An AGGREGATOR [HoWo03] provides the opposite functionality, by
integrating multiple messages into a single message. A RESEQUENCER
[HoWo03] helps to collect and reorder out-of-sequence messages so
that they can be republished in the correct order. A ROUTING SLIP
[HoWo03] adds explicit routing information to a message before it is
sent to its receiver. A COMPOSED MESSAGE PROCESSOR [HoWo03] splits a
message into multiple parts using SPLITTER, performs some processing
on each message part, and reassembles the parts into a single message via AGGREGATOR before directing it to an output channel. Finally, a
SCATTER-GATHER [HoWo03] broadcasts a message to multiple recipients
and creates a single aggregated response message from the individual
responses of each recipient.
There are two general options for implementing the control logic
of a message router: it may be either statically or dynamically
configurable. Statically configured message routers have less runtime overhead but are less flexible; dynamically configured message
routers have the inverse properties. Dynamic configuration can be
realized with help of a central CONTROL BUS [HoWo03], or by implementing the router as a DYNAMIC ROUTER [HoWo03] that configures
itself based on control messages from potential message recipients.
A message router that receives messages from multiple input channels must be synchronized. A THREAD-SAFE INTERFACE (384) enforces
synchronization at the router¡¯s interface, while realization as a
MONITOR OBJECT (368) supports cooperative concurrency control of
all message channels, enabling the router to receive messages simultaneously.
##%%&&
When deploying a DOMAIN MODEL (182) to multiple processors or network nodes . . .
. . . we often need an infrastructure that allows application components to notify each other about events of interest.
Components in some distributed applications are loosely coupled and operate largely independently. If such applications
need to propagate information to some or all of their components, however, a notification mechanism is needed to inform
the components about state changes or other interesting events
that affect or coordinate their own computation.
Nevertheless, this notification mechanism should not couple application components too tightly, or they will lose their independence. Such
application components only want to know that another component
in the system is in a specific state, not which specific component
is involved. Similarly, components that disseminate events often
do not care which other components want to receive the information. In addition, components should not depend on how other
components can be reached, or on their specific location in the
system.
Therefore:
Define a change propagation infrastructure that allows publishers
in a distributed application to disseminate events that convey
information that may be of interest to others. Notify subscribers
interested in those events whenever such information is published.
Publishers register with the change propagation infrastructure to
inform it about what types of events they can publish. Similarly,
subscribers register with the infrastructure to inform it about what
types of events they want to receive. The infrastructure uses this
registration information to route events from their publishers through
the network to interested subscribers. Subscribers receiving events
from the infrastructure can use information in the events to guide or
coordinate their own computation.
Like MESSAGING, PUBLISHER-SUBSCRIBER supports asynchronous communication, in which publishers transmit events to subscribers without
blocking to wait for a response. Asynchrony decouples publishers
and subscribers so that they can be active and available at different points in time, and also leverages the parallelism inherent in a
distributed system. In addition, PUBLISHER-SUBSCRIBER allows components in an application to coordinate their computation anonymously
without introducing explicit dependencies to one another: they are
unaware and independent of each other¡¯s location and identity, since
they only send and receive events about changes of their state and/or
the changed state itself.
Only the infrastructure has the knowledge of how the components
connect, where they are located, and how events is routed through the
system. PUBLISHER-SUBSCRIBER also supports group communication, in
which publishers of events need not inform each subscriber explicitly, and the infrastructure forwards the events to all interested
subscribers.
A drawback of anonymous communication is that it can cause unnecessary overhead if subscribers are interested in a specific type of
event, and will only react if the event¡¯s content meets specific criteria.
One way to address this problem is by filtering based on the type
or content of events. Filtering can incur other costs, however. For
example, filtering inside PUBLISHER-SUBSCRIBER middleware decreases
its throughput, filtering within the subscribers can result in unnecessary notifications, and filtering inside publishers can break the
anonymous communication model.
The information exchanged between the components connected by
PUBLISHER-SUBSCRIBER middleware is encapsulated inside events, which
are realized as MESSAGES (420). An event hides its concrete message
format from both the publisher and subscriber(s), as well as from
the PUBLISHER-SUBSCRIBER middleware itself, which enables transparent
modification of the message¡¯s format.
PUBLISHER-SUBSCRIBER middleware can be implemented in various ways.
One approach involves the reuse of MESSAGING and BROKER middleware.
For example, PUBLISHER-SUBSCRIBER middleware has been implemented
on top of MESSAGING and BROKER middleware, as is the case with many
WS-NOTIFICATION [OASIS06c] [OASIS06c] and CORBA Notification
Service [OMG04c] products, respectively. Another approach is to
implement PUBLISHER-SUBSCRIBER using fundamental concurrency and
network programming patterns [POSA2], as is the case with many
DDS [OMG05b] products. In general, the former approach simplifies
the efforts of the middleware developers, whereas the latter approach
yields better performance.
To support anonymous and asynchronous group communication, a
set of PUBLISH-SUBSCRIBE CHANNELS [HoWo03] or Event Channels [HV99]
can broadcast or multicast event messages from publishers to subscribers. Components can inform a specific channel about which
events they publish and which events they would like to receive.
EVENT-DRIVEN CONSUMERS [HoWo03] support the transparent adaptation
of a consumer to a specific notification publish/subscribe APIs. Such
a design enables transparent exchange and evolution of the underlying PUBLISHER-SUBSCRIBER infrastructure. Designing subscribers as
SELECTIVE CONSUMERS [HoWo03] enables the filtering of incoming event
messages: a subscriber only processes events whose content complies
with the filter¡¯s criteria.
If the publisher and subscriber of an event do not share a common message format, a MESSAGE TRANSLATOR (229) can convert events
issued by a publisher into the format understood by its subscribers.
MESSAGE ROUTERS (231) help maintain information about how to route
events through the middleware to their registered subscribers. A
communication failure that cannot be handled internally by the
PUBLISHER-SUBSCRIBER middleware can be returned as a REMOTING ERROR
[VKZ04] to the publisher that sent the event.
##%%&&
When deploying a DOMAIN MODEL (182) to multiple processors or network nodes . . .
. . . we often need a communication infrastructure that shields applications from the complexities of component location and IPC.
Distributed systems face many challenges that do not arise in
single-process systems. Application code, however, should not
need to address these challenges directly. Moreover, applications
should be simplified by using a modular programming model that
shields them from the details of networking and location.
Sending requests to services in distributed systems is hard. One
source of complexity arises when porting services written in different
languages onto different operating system platforms. If services are
tightly coupled to a particular context, it is time-consuming and
costly to port them to another distribution environment or reuse
them in other distributed applications. Another source of complexity
arises from the effort required to determine where and how to deploy
service implementations in a distributed system. Ideally, services
should interact by calling methods on one another in a common,
location-independent manner, regardless of whether the services are
local or remote.
Therefore:
Use a federation of brokers to separate and encapsulate the
details of the communication infrastructure in a distributed
system from its application functionality. Define a component based programming model so that clients can invoke methods
on remote services as if they were local.
At least one broker instance is defined per participating network
node. Component interfaces and locations are registered with their
local broker to gain visibility within the distributed system. To invoke
functionality on a component, clients ask their local broker for a
proxy, which acts as the remote component¡¯s surrogate. A client calls
a method on the proxy to initiate a request to a component. The
proxy collaborates with the client and server-side brokers to deliver
the request to the component and receive any results.
A BROKER enables components of a distributed application to inter act without handling remoting concerns by themselves. It can also
optimize communication mechanisms, such as using remote method
invocation versus collocated method calls, depending on the location
of client and server components. For example, if the client and component are in the same address space, the broker can optimize the
communication path to alleviate unnecessary overhead.
Most BROKER realizations are based on a LAYERS (185) architecture
to manage complexity, such as CORBA [OMG04a] and Microsoft¡¯s
.NET Remoting [Ram02]. These layers are further decomposed into
¡®special-purpose¡¯ components for specific networking and communication tasks. We illustrate this partitioning using the CORBA layering
[SC99]¡ªother layering schemes and middleware may involve different assignments [VKZ04].
An OS adaptation layer shields a broker from its underlying execution
platform. In languages that use virtual machines, such as Java, this
layer is the virtual machine. In other languages, such as C++, it
usually contains a set of WRAPPER FACADES (459) that provide a uniform
interface to specific OS APIs.
An ORB core layer forms the heart of a BROKER arrangement. In general it is a messaging infrastructure consisting of two components.
A REQUESTOR (242) forwards request MESSAGES (420) from a client to
the local broker of the invoked remote component, while an INVOKER
(244) encapsulates the functionality for receiving request messages
sent by a client-side broker and dispatching these requests to the
addressed remote components. If a communication failure cannot
be resolved internally by a BROKER infrastructure, a REMOTING ERROR
[VKZ04] is signaled to the client that issued the failed request. A
COMPONENT CONFIGURATOR (490) can configure REQUESTOR and INVOKER
implementations with specific communication strategies and protocols. This supports the transparent exchange and evolution of
functionality within a BROKER, as well as a protocol-level integration
of heterogeneous or legacy components into a distributed system.
Additional LOOKUP (495) functionality allows components to register
their interfaces and location with a BROKER infrastructure. Clients
can similarly use LOOKUP to find these components and the access
to them. Using LOOKUP, clients need not know the concrete location
of components, but can connect to them at runtime. LOOKUP also
enables a flexible deployment of components, which supports both
an optimal utilization of network resources and different application deployment scenarios. To complement remote procedure call
invocation, a BROKER may interact with an event channel to support
event-based notifications, which is in essence a PUBLISHER-SUBSCRIBER
(234) service.
An ORB adapter typically provides a CONTAINER (488) that manages
the technical environment of remote components. It interacts with a
set of skeletons, which are OBJECT ADAPTERS (438) that map between
the generic messaging infrastructure of the broker and the specific
interfaces of remote components. A FACADE (294) presents a simple
interface that components can use to access their local broker.
A CLIENT PROXY (240) represents a component in the client¡¯s address
space. The proxy offers an identical interface that maps specific
method invocations on the component onto the broker¡¯s message oriented communication functionality. Proxies allow clients to access
remote component functionality as if they were collocated, and can be
used to implement collocation optimizations transparently [ScVi99].
Client proxies increase location-independent communication in a distributed system, but do not achieve full transparency. Before clients
can use a client proxy they must obtain it from their local broker,
for example via a LOOKUP. This activity is not necessary for local
components unless they use a FACTORY METHOD (529) to access the
components they use. In addition, client proxies may be unable to
handle all REMOTING ERRORS transparently to their clients. A BUSINESS
DELEGATE (292) can encapsulate such ¡®infrastructural concerns,¡¯ and
thus help to provide more complete location-independent communication among the components of a distributed system.
##%%&&
When constructing a client-side BROKER (237) infrastructure, realizing PROXY-based (290) interfaces for distributed components, or
implementing a BUSINESS DELEGATE (292) for a remote component . . .
. . . we must provide an abstraction that allows clients to access
remote components using remote method invocation.
Accessing the services of a remote component requires the
client side to use a specific data format and networking protocol. Hard-coding the format and protocol directly into the
client application, however, makes it dependent on the remoteness of its collaboration partner, because invocations on remote
components will differ from invocations on local components.
Ideally, access to a component should be location-independent. There
should be no functional difference between a method invocation on a
local component and a method invocation on a remote component.
Therefore:
Provide a client proxy in the client¡¯s address space that is a surrogate for the remote component. The proxy provides the same
interface as the remote component, and maps client invocations
to the specific message format and protocol used to send these
invocations across the network.
Ensure client applications issue requests to the remote component
only via its client proxy. The proxy then transforms the concrete
method invocation and its parameters into the data format under stood by the network, and uses an IPC mechanism to send the data
to the remote component. The client proxy also transforms results
returned from the component represented by the proxy back into the
format understood by its clients.
Client proxies support a remote method invocation style of IPC. As
a result there is no API difference between a call to a local or a
remote component, which enhances location-independent communication within a distributed application. In addition, a client proxy can
shield its clients from changes in the represented component¡¯s ¡®real¡¯
interfaces, which avoids rippling effects in the case of component
evolution. If the proxy interface is designed in a network-unaware
style, however, it may incur excessive overhead. For example, it could
offer many fine-grained methods, such as accessors for each visible attribute of the represented component. Accessing all attributes
would require a client to invoke many proxy calls, each of which
incurs network overhead.
In addition, client proxies can only support, but not fully achieve,
location-independent communication. Before clients can use a client
proxy they must obtain it from their local broker¡ªthus clients are
aware of the potential remoteness of the represented component.
Moreover, a client proxy may not be able to handle all errors returned
by the network transparently for its clients.
A client proxy can use a RESOURCE CACHE (505) to maintain immutable
data and state of the represented remote component, once this data
and state is first accessed and transferred. Caching avoids unnecessary performance penalties and network traffic for subsequent
accesses to the data and state. If the immutable state and data
is encapsulated within IMMUTABLE VALUES (396), the client proxy can
pass it directly to clients. A client proxy can also use AUTHORIZATION
(351) to enforce access rights to the remote component on the client
side, which helps minimize unnecessary performance penalties and
network traffic if access is denied.
Designing the client proxy as a REMOTE FACADE [Fow03a] helps to
address performance problems by coalescing related fine-grained
methods into a single coarser-grained method, such as a method
that returns all visible attributes of the represented component in
response to a single call.
##%%&&
When constructing a client-side BROKER (237) infrastructure . . .
. . . we must provide a means for sending method invocations over the
network to remote components.
The client-side invocation of a method on a remote component
involves many administrative and infrastructure tasks. Implementing these tasks repeatedly within each client is tedious,
error-prone, and pollutes application code with infrastructure
code that may be non-portable.
Invoking a method on a remote component requires the client side
to marshal invocation information, manage network connections,
transmit the invocation over the network, and handle invocation
results and errors. These activities are unnecessary for local method
invocations. If client applications handle these issues directly, they
can become dependent on specific networking protocols and IPC
mechanisms, thereby decreasing their portability and reusability in
other deployment scenarios and applications. Moreover, client developers would be distracted from their primary tasks: implementing
application functionality correctly and efficiently.
Therefore:
Create a requestor that encapsulates the creation, handling, and
sending of request messages to remote components.
Clients that want to access a remote component supply the requestor
with information about the component¡¯s location, a reference to the
component, the operation to be invoked, and its arguments. The
requestor uses the information to construct a corresponding request
message and send it over the network to the remote component.
A requestor shields application logic in a distributed system from the
details of client-side networking and IPC activities and tasks.
A requestor can delegate some of its sub-activities to other components. A MARSHALER [VKZ04] serializes concrete service requests
into request MESSAGES (420) and de-serializes corresponding result
messages into concrete responses. A CLIENT REQUEST HANDLER (246)
manages connections and encapsulates specific IPC mechanisms,
thereby simplifying sending request messages across a network and
receiving result messages.
Some applications require additional requestor activities, such as
adding a security token, which an INTERCEPTOR (467) can encapsulate via a uniform interface. In general, a MARSHALER, CLIENT REQUEST
HANDLER, and INTERCEPTOR manipulate client-side request processing
aspects without affecting the requestor¡¯s core algorithm for creating, handling, and sending requests. An ABSOLUTE OBJECT REFERENCE
[VKZ04] can encapsulate information about the location and identity
of the remote component that is the target of the request. Similarly,
a REMOTING ERROR [VKZ04] is returned to the client when a failure
cannot be handled transparently by the requestor.
In general there are three deployment options for a requestor
[SMFG00]. The simplest option is to deploy one requestor for all
client threads or processes on a node. The more clients access the
requestor, however, the more it becomes a throughput and scalability bottleneck. To alleviate this drawback, there could be a separate
requestor per client, or several clients could share a requestor. A
requestor that is shared by multiple concurrent clients must be
synchronized. Providing the requestor with a THREAD-SAFE INTERFACE
(384) is a simple, coarse-grained synchronization option, because it
enforces synchronization at the interface of the requestor, even if only
small portions of its methods are critical sections. In this case, synchronization via STRATEGIZED LOCKING (388) is an alternative. Realizing a
requestor as a MONITOR OBJECT (368) supports cooperative concurrency
control of multiple clients that access the requestor simultaneously.
If there are multiple requestors per client application, they must synchronize their internal use of shared resources, such as connections
or cached request objects.
##%%&&
When constructing a server-side BROKER (237) infrastructure . . .
. . . we must provide a means for receiving method invocations from
the network and dispatching them to remote components.
The server side must perform many administrative and infrastructure tasks to transform data received from clients into
an invocation on a specific method of a remote component.
Implementing these tasks repeatedly within each component
implementation is tedious, error-prone, and pollutes application
code with infrastructure code that is often non-portable.
Invoking a specific method of a component implementation in
response to a client request requires a server to manage network
connections, receive data on the connections, demarshal that data to
receive the associated invocation information, identify the intended
component implementation, invoke the appropriate method on that
component, and return its results or errors. If remote components
handled these concerns directly, they would be tightly coupled to specific networking protocols and IPC mechanisms, thereby decreasing
their portability and reusability in other deployment scenarios and
applications. Moreover, server developers would be distracted from
their primary tasks: implementing application functionality correctly
and efficiently.
Therefore:
Create an invoker that encapsulates the reception and dispatch
of request messages from remote clients in a specific method of
a component implementation.
An invoker listens to network connections for request messages
to arrive from remote clients, receives the request messages when
they arrive, demarshals the received information to determine which
method and parameters to invoke on which component implementation, and dispatches that method on the identified component.
An invoker shields the application logic of a distributed system from
the details of server-side networking and IPC tasks and activities.
An invoker can delegate several sub-activities of its responsibility
to other components. A MARSHALER [VKZ04] can de-serialize request
MESSAGES (420) into a concrete service and serialize corresponding
services results into result messages. A SERVER REQUEST HANDLER (249)
manages connections and encapsulates a specific IPC mechanism,
thereby simplifying the process of receiving requests and sending
results across a network.
Some applications also require additional invocation activities, such
as interpreting an embedded security token, which an INTERCEPTOR
(444) can encapsulate via a uniform interface. In general, a MARSHALER,
SERVER REQUEST HANDLER, and INTERCEPTOR manipulate server-side request processing aspects without affecting the invoker¡¯s core algorithm for the reception and dispatch of client requests on component
implementations.
An ABSOLUTE OBJECT REFERENCE [VKZ04] encapsulates the identity of
a specific component implementation. A REMOTING ERROR [VKZ04] is
returned to the client that issued a request if failures occur that
cannot be handled by the invoker. The invoker can also delegate its
invocation to a LOCATION FORWARDER [VKZ04] if it cannot find the component implementation, for example if the component was redeployed,
or is temporarily unavailable due to high machine load.
An invoker has several deployment options, the simplest of which is
to deploy one invoker for all components implementations in a server
application. The more components are accessed via one invoker, however, the more it becomes a throughput and scalability bottleneck. To
alleviate this drawback, several remote components could share an
invoker, or each component could have its own invoker.
##%%&&
When developing a REQUESTOR (242) . . .
. . . we must send requests to, and receive replies from, the network.
Sending client requests to and receiving replies from the net work involves various low-level IPC tasks, such as connection
management, time-out handling, and error detection. Writing
and performing these tasks separately for each client uses networking and end system resources ineffectively.
The more clients access the network, and the more requests and
replies must be handled simultaneously, the more efficiently network
resources must be managed to achieve appropriate quality of service
in a distributed application. Network connections and bandwidth,
for example, are limited resources and must be shared and used
judiciously by all clients to ensure acceptable latency and jitter. In
addition, writing error detection and time-out handling tasks separately for each client duplicates code and pollutes the application
with non-portable networking code.
Therefore:
Provide a specialized client request handler that encapsulates
and performs all IPC tasks on behalf of client components that
send requests to and receive replies from the network.
The functional responsibilities of a client request handler include
connection establishment, request sending and result dispatching,
and time-out and error handling, which it performs with help of
specific IPC mechanisms. In addition, the client request handler is
responsible for efficient management and utilization of networking
and computing resources, such as network connections, memory,
and threads.
The centralized execution and management of all client-side networking activities within a CLIENT REQUEST HANDLER can improve distributed
application quality of service, such as latency, throughput, scalability, and resource utilization. The encapsulation of specific IPC
mechanisms makes communication transparent for clients that issue
requests to remote components.
To establish concrete connections to remote components, the client
request handler implements the connector role of ACCEPTOR-CONNECTOR
(265), which supports the evolution of network connection establishment strategies independently of other client request handler
responsibilities. If the client request handler is shared by multiple
concurrent clients, the connector must be synchronized. Providing
the connector with a THREAD-SAFE INTERFACE (384) is a simple but
coarse-grained synchronization option, because it enforces synchronization at the interface of the connector, even if only small portions
of its methods are critical sections. In this case, consider using
STRATEGIZED LOCKING (388) to parameterize the synchronization mechanisms. Realizing a connector as a MONITOR OBJECT (368) supports
cooperative concurrency control when multiple clients access the
connector simultaneously.
A connection created by a connector can be encapsulated within a
connection handler that plays the service handler role in ACCEPTOR CONNECTOR. This design treats a connection as a first-class entity,
which supports efficient maintenance of connection-specific state,
as well as the handling of REMOTING ERRORS [VKZ04] that occur on
the connection. Scalability is also supported, since each connection
handler can run in its own thread, thereby processing requests from
and replies to multiple clients simultaneously.
A connection handler can implement a synchronous or asynchronous
communication strategy. Synchronous communication can simplify
the client programming model, but reduces performance and through put, whereas asynchronous communication has the inverse proper ties. Four patterns¡ªFIRE AND FORGET, SYNC WITH SERVER, POLL OBJECT,
and RESULT CALLBACK [VKZ04]¡ªhelp realize an asynchronous communication model. These four patterns provide different strategies for
addressing the following three aspects: whether or not a result is
sent to the client, whether or not the client receives an acknowledgement, and, if a result is sent to the client, whether it is the client¡¯s
responsibility to obtain the result or whether it is informed using
a callback.
If a client expects a result or an acknowledgement, a connection handler can use time-outs to detect potential failures of asynchronous
communication. Depending on whether a connection handler processes data serially or is interrupt-driven, it can register with a
REACTOR (259) or PROACTOR (262), respectively, which will notify it when
a specific result arrives.
The specific IPC mechanism used by the connector and the connection handlers of a client request handler can be encapsulated
by a PROTOCOL PLUG-IN [VKZ04] or a set of WRAPPER FACADES (459).
Both patterns hide IPC mechanism details behind uniform, platform independent interfaces. A PROTOCOL PLUG-IN allows runtime (re)con figuration of IPC mechanisms, but incurs some runtime overhead.
WRAPPER FACADE, in contrast, avoids runtime overhead, but only sup ports compile-time configuration. The requests are encapsulated
within MESSAGES (420) and sent over the network via a MESSAGE CHANNEL
(224). If security is required, the IPC mechanism should use a SECURE
CHANNEL [SFHBS06] to transmit requests.
An OBJECT MANAGER (492) can enhance client request handler performance via caching. For example, if a specific connection is no longer
needed, it need not be destroyed, but can instead be kept ¡®alive¡¯ for
a predetermined time and reused for another collaboration between
the client and server it connects.
Results of specific invocations, as well as any REMOTING ERRORS [VKZ04]
that cannot be resolved by the client request handler, are returned to
the client that issued the corresponding request.
##%%&&
When developing an INVOKER (244) . . .
. . . we must receive requests send replies across the network.
Receiving client requests from and sending replies to the net work involves several low-level IPC tasks, including connection
management, time-out handling, and error detection. Writing
and performing these tasks separately for each client uses net working and end system resources ineffectively.
The more remote requests and replies must be handled simultaneously by a server part of a distributed application, the more efficiently
network resources must be managed and utilized to achieve an
appropriate quality of service. Network connections and bandwidth,
for example, are limited resources, and must be shared and used
effectively by all remote components to provide an appropriate performance, throughput, and scalability on the server-side of a distributed
system. In addition, writing error detection and time-out handling
tasks separately for each component duplicates code and pollutes
the application with non-portable networking code.
Therefore:
Provide a specialized server request handler that encapsulates
and performs all IPC tasks on behalf of remote components that
receive requests from and send replies to the network.
The functional responsibilities of a server request handler include
connection establishment, request reception and dispatching, result
sending, and error handling, which it performs with the help of
specific IPC mechanisms. In addition, the server request handler is
responsible for efficient management and utilization of networking
and computing resources such as network connections, memory,
and threads.
The centralized execution and management of all server-side networking activities within a SERVER REQUEST HANDLER can improve the
distributed application¡¯s quality of service, in particular, latency,
throughput, scalability, and resource utilization. The encapsulation of specific IPC mechanisms makes communication transparent
for application components that receive requests from and send
responses back to remote clients.
The client request handler needs an event-handling infrastructure
that listens on the network for connection requests to arrive, establishes the requested connections, and dispatches service requests
to methods on the appropriate application components. This infrastructure must allow multiple connection and service requests to
be received and processed simultaneously to achieve appropriate
latency, throughput, and scalability. Its core can be realized by a
REACTOR (259) or PROACTOR (262), depending on whether the processing of received events is handled serially, or driven by interrupts,
respectively.
The event handlers of a REACTOR or PROACTOR can be realized as an
ACCEPTOR-CONNECTOR (265) to separate connection establishment from
request and reply handling. A dedicated acceptor listens on the network for connection requests to occur, accepts these requests, and
creates a connection handler that encapsulates the newly established connection. The connection handler then performs the IPC
on behalf of the application component accessed via that connection. This design enables connection establishment and data transfer
strategies to evolve independently of each other. Connections are also
treated as first class entities, which supports efficient maintenance
of connection-specific state and handling of any REMOTING ERRORS
[VKZ04] that occur on the connection. In addition, scalability is supported: each connection handler can run in its own thread, which
allows a server request handler to handle requests from, and replies
for, multiple clients concurrently.
A connection handler can implement a synchronous or asynchronous
communication strategy. Synchronous communication can simplify
the client programming model but reduces performance, whereas
asynchronous communication has the inverse properties. The SYNC
WITH SERVER pattern [VKZ04] can help to return acknowledgements
for an asynchronous communication model.
The specific IPC mechanism used by the acceptor and the connection handlers of a server request handler can be encapsulated
by a PROTOCOL PLUG-IN [VKZ04] or a set of WRAPPER FACADES (459).
Both patterns hide IPC mechanism details behind uniform and
platform-independent interfaces. A PROTOCOL PLUG-IN allows runtime
(re)configuration of IPC mechanisms, but incurs some runtime overhead. WRAPPER FACADE, in contrast, enhances performance, but only
supports compile-time configuration. The requests are encapsulated
within MESSAGES (420) and sent over the network via a MESSAGE CHANNEL
(224). If security is required, the IPC mechanism should use a SECURE
CHANNEL [SFHBS06] to transmit requests.
An OBJECT MANAGER (492) can enhance client request handler performance via caching. For example, if a specific connection is no longer
needed, it need not be destroyed, but can instead be kept ¡®alive¡¯ for
a predetermined time and reused for another collaboration between
the client and server it connects.
##%%&&
When developing event-driven software, or designing a CLIENT REQUEST
HANDLER (246) or a SERVER REQUEST HANDLER (249) . . .
. . . we must decouple infrastructure behavior associated with detecting, demultiplexing, and dispatching events from short-duration
components that service the events.
Event-driven software often receives service request events from
multiple event sources, which it demultiplexes and dispatches to
event handlers that perform further service processing. Events
can also arrive simultaneously at the event-driven application.
However, to simplify software development, events should be
processed sequentially and synchronously.
Efficiently and flexibly processing events that arrive concurrently from
multiple sources is hard. For example, using multi-threading to wait
for events to occur in a set of event sources can introduce overheads
due to synchronization, context switching, and data movement. In
contrast, blocking indefinitely on a single event source can prevent
the servicing of other event sources, degrading the quality of service
to clients. In addition, it should be easy to integrate new or improved
event handlers into the event-handling infrastructure.
Therefore:
Provide an event handling infrastructure that waits on multiple
event sources simultaneously for service request events to occur,
but only demultiplexes and dispatches one event at a time to a
corresponding event handler that performs the service.
A reactor component coordinates the processing of events within the
event-driven application. It defines an event loop that uses an operating system event demultiplexer to wait synchronously for service
request events to occur on a set of event sources. By delegating the
demultiplexing of events to the operating system, the reactor can
wait for multiple event sources simultaneously without a need to
multi-thread the application code. When events arrive, the event loop
dispatches them one at a time to event handlers that implement the
requested application functionality. Each event handler then reacts
and services ¡®its¡¯ event synchronously.
There are several benefits to a REACTOR design. First, operating system event demultiplexing mechanisms can wait on a set of event
sources while avoiding the performance overhead and programming
complexity associated with multi-threading. Second, encapsulating
the software event loop within the reactor shields service event handlers from complexities in the synchronous event demultiplexing
and dispatching infrastructure. Third, event serialization becomes
transparent for an application¡¯s components, which can execute
sequentially and synchronously without the need for explicit locking.
A reactor component forms the heart of event-driven software: it
encapsulates a reusable event demultiplexing and dispatching infrastructure. In particular, the reactor defines an event loop that uses
an event demultiplexer provided by the underlying operating system, such as select or WaitForMultipleObjects, to wait for service
request events to occur on a set of event sources that are identified
by handles. Calling the event demultiplexer blocks the reactor until
one or more events arrive on the event sources and it is possible
to process these events without blocking. The event demultiplexer
returns the handles of all event sources on which service request
events occurred to the reactor, which then dispatches these events,
one at a time, as COPIED VALUES (394) to the handlers that react and
process the events synchronously.
Different reactor implementations are often required when platforms
offer different event demultiplexers. In such a situation, an EXPLICIT
INTERFACE (281) may be needed to separate the reactor interface
from its implementations. Event handlers are often arranged in an
ACCEPTOR-CONNECTOR (265) configuration, in which service handlers
provide domain-specific functionality and acceptors and connectors
establish connections on behalf of service event handlers. An extensible event handling infrastructure can be supported by defining a
common EXPLICIT INTERFACE for all event handlers that specifies the
set of operations available for processing service request events. This
design minimizes the coupling between the reactor and the signatures
and logic of specific services, which all use a generic event handler
interface.
Processing a service request event often requires an event handler to
perform additional I/O on the event source on which the event arrived,
for example to read parameter values associated with a client request,
or to return results to the client that issued the event. WRAPPER FACADES
(459) can be used to simplify communication between event handlers
and event sources and to remove dependencies on platform-specific
I/O functions. The I/O handles and event handlers within a reactor
are typically stored and retrieved using an OBJECT MANAGER (492).
An event handler does not return control to the reactor until it is done
processing a service request event. If an event handler blocks for an
extended period, therefore, no other event handlers can be dispatched
to service events. As a result, a single-threaded REACTOR configuration
is best suited for event handlers that perform short-duration services
that do not block on I/O handles or locks, but is infeasible for event
handlers that perform long-duration actions.
To alleviate this drawback, event-driven software can implement con current event handlers, which allow the event-driven application to
process multiple events simultaneously. The HALF-SYNC/HALF-ASYNC
(359) pattern can work in conjunction with REACTOR to process long duration client requests and replies concurrently in separate threads
of control. Similarly, the LEADER/FOLLOWERS (362) pattern is suitable
for event-driven software that uses a thread pool to process a high
volume of short-duration, repetitive, and atomic actions.
##%%&&
When developing event-driven software, or designing a CLIENT REQUEST
HANDLER (246) or a SERVER REQUEST HANDLER (249) . . .
. . . we must decouple infrastructure behavior associated with the
detection, demultiplexing, and dispatch of events from long-duration
components that service the events.
To achieve the required performance and throughput, event driven applications must often be able to process multiple events
simultaneously. Resolving this problem via multi-threading,
however, may be undesirable, due to the overhead of synchronization, context switching, and data movement.
Nevertheless, service processing should not be delayed unduly by
long-duration activities on event sources, such as awaiting service
request events from remote clients or performing I/O with clients or
other components such as a database. Performance and throughput
should also be maximized. In addition, it should be easy to integrate new or improved components into the existing event-handling
infrastructure.
Therefore:
Split an application¡¯s functionality into asynchronous operations
that perform activities on event sources and completion handlers
that use the results of asynchronous operations to implement
application service logic. Let the operating system execute the
asynchronous operations, but execute the completion handlers
in the application¡¯s thread of control.
A proactor component coordinates the collaboration between completion handlers and the operating system. It defines an event loop that
uses an operating system event demultiplexer to wait synchronously
for events that indicate the completion of asynchronous operations
to occur. Initially all completion handlers ¡®proactively¡¯ call an asynchronous operation to wait for service request events to arrive, and
then run the event loop on the proactor. When such an event arrives,
the proactor dispatches the result of the completed asynchronous
operation to the corresponding completion handler. This handler then
continues its execution, which may invoke another asynchronous
operation.
A PROACTOR event-handling infrastructure allows multiple long duration services to be executed simultaneously within a single
application thread, which enhances the performance and through put of event-driven software by avoiding multi-threading overhead,
such as synchronization, context switching, and data movement¡ªas
long as completion handlers do not operate on the same resources
at the same time. Encapsulating the software event loop within the
proactor also shields completion handlers from complexities in the
asynchronous event demultiplexing and dispatching infrastructure.
To implement a PROACTOR arrangement, most modern operating systems offer asynchronous operations, such as the aio * API in
Real-time POSIX [POSIX95], and Overlapped I/O in Windows [Sol98].
Without blocking their callers, the operating system executes these
asynchronous operations on event sources identified via handles.
Completion handlers can use the operations to delegate the execution of long-duration I/O activities to the operating system, which
enables the handlers to process other requests until the operations
complete. A downside of the Proactor is its reliance on operating
system support for asynchronous I/O to run efficiently.
For efficiency reasons, the collaboration between completion handlers and asynchronous operations is often based on ASYNCHRONOUS
COMPLETION TOKENS (ACTs) (268). When a completion handler invokes
an asynchronous operation, it also passes an ACT that contains
unambiguous identification of the calling handler, as well as the
handle of the event source on which the operation should execute.
When the asynchronous operation completes, it fills the ACT with its
results, and the operating system generates a completion event on the
respective event source to indicate that the operation has finished.
This completion event also contains the corresponding ACT.
Completion events are returned to completion handlers via the proactor, whose event loop uses an event demultiplexer provided by the
operating system, such as GetQueuedCompletionStatus in the Windows API [Sol98]. This event demultiplexer waits for completion events
to occur on event sources that can be identified via handles. When
such events occur, the event demultiplexer returns them to the proactor, which uses their ACTs to dispatch each event to the associated
completion handler as a COPIED VALUE (394), to process the results of
the asynchronous operation. Once a completion handler calls another
asynchronous operation, control returns to the proactor, so that it
can wait for and dispatch the next completion event.
Different platforms often require different proactor implementations,
so the proactor interface should be separated from its realization via
an EXPLICIT INTERFACE (281). Completion handlers are often designed as
an ACCEPTOR-CONNECTOR (265) configuration, in which service handlers
provide domain-specific functionality and acceptors and connectors
establish connections asynchronously on behalf of service handlers.
To support an extensible event-handling infrastructure, define a common EXPLICIT INTERFACE for all completion handlers that specifies the
set of operations available for processing service request events. This
design minimizes the coupling between the proactor and the signatures and logic of specific services, which all use a generic completion
handler interface.
Processing a service request event often requires a completion handler
to perform additional I/O on the event source on which the event
arrived, for example to read parameter values asynchronously that
are associated with a client request, or to return results to the client
that issued the event. WRAPPER FACADES (459) can be used to simplify
the communication between completion handlers and event sources
and to remove dependencies on platform-specific I/O functions.
The HALF-SYNC/HALF-ASYNC (359) pattern can work in conjunction
with PROACTOR to process long-duration clients requests and replies
synchronously and sequentially, which simplifies application programming without degrading the performance of the PROACTOR event handling infrastructure.
##%%&&
When implementing event handlers in a connection-oriented net worked system, such as event handlers in a REACTOR (259) architecture
or completion handlers in a PROACTOR (262) architecture, or when
designing a CLIENT REQUEST HANDLER (246) or a SERVER REQUEST HANDLER
(249) . . .
. . . we want to decouple infrastructure behavior associated with
establishing connections and initializing event handlers from the
application-specific processing within these handlers.
Before peer event handlers in a networked system can execute
their functionality with other peer event handlers they must first
be connected and initialized. The connection establishment and
initialization code of a peer event handler, however, is largely
independent of the functionality that it performs.
To complicate matters, the application functionality of an event
handler usually changes more frequently than its connection and
initialization strategies. In addition, an event handler may change its
connection role dynamically: in one scenario it initiates a connection to a remote peer actively, while in another scenario it accepts a
connection request passively from a remote peer.
Therefore:
Decouple the connection and initialization of peer event handlers
in a networked system from the processing that these peers
subsequently perform.
A client service handler can initiate a connection to a remote service
handler by calling its local connector, a factory that actively initiates
new connections to remote peers. The connector sends the request
to a corresponding acceptor, a factory on the server side that accepts
connection requests passively from remote peers. After the factories
establish a connection between two peers, they initialize the associated service handlers and pass the connection to the handlers. The
service handlers then use the connection to execute their application
functionality cooperatively.
An ACCEPTOR-CONNECTOR arrangement encapsulates connection establishment within separate acceptor and connector components, which
shields service handlers from complexities in the underlying network
programming infrastructure. In addition, connection establishment
and initialization behavior can vary independently of service handler
functionality. Note, however, that an ACCEPTOR-CONNECTOR arrangement may add unnecessary complexity for simple client applications
that connect with only one server and perform one service using a
single network programming interface.
Design acceptors to use an event demultiplexer to listen for the
arrival of connection requests passively. If an ACCEPTOR-CONNECTOR
arrangement is built on top of a REACTOR or PROACTOR, the acceptors
can use the event demultiplexer that is provided by these event handling infrastructures. When a connection request arrives, the
event demultiplexer dispatches the request to the acceptor, which
performs three steps to establish a connection with the remote peer
that issued the request. First, a FACTORY METHOD (529) creates the
service handler instance that should be connected to the remote peer.
Second, the acceptor establishes the connection with the remote peer,
and third, it passes the connection to the associated service handler
and finishes initializing it.
In general, a connector factory can support a synchronous or an
asynchronous connection establishment strategy to service handlers.
Synchronous connection establishment is most useful if the connection establishment latency is low, if service handlers must be
initialized in a fixed order, and if service handlers can deploy a thread per-connection model to connect to remote peers. Otherwise, an
asynchronous connection establishment strategy is more beneficial.
Both synchronous and asynchronous connection establishment can
be supported transparently to service handlers by dividing connectors into a connect method that is called by a service handler to
establish a new connection actively, and a complete method that
passes this connection to the service handler that requested it after
the connection is established [POSA2].
Each activity in the connector and acceptor factories can be realized with help of TEMPLATE METHODS (453) or STRATEGIES (455). Such
a design supports flexible evolution and exchange of connection
establishment and initialization policies transparently to service handlers. TEMPLATE METHODS are most appropriate if the flexibility is
needed at compile time, whereas typical STRATEGIES support a run time configuration and reconfiguration of acceptors and connectors.
In C++, a STRATEGY can also be expressed as a compile-time policy,
in which case the choice is simply between using an inheritance versus a delegation-based approach¡ªbinding time need not be a
consideration.
The I/O handles and event handlers used by a connector component to support asynchronous connection establishment are typically
stored and retrieved using an OBJECT MANAGER (492).
The acceptor and connector factories perform their connection establishment functionality, and the service handlers their application
functionality, by exchanging messages with their peers via an IPC
mechanism. Encapsulating the IPC mechanism inside a set of WRAPPER
FACADES (459) ensures its correct and portable use within the acceptor
and connector factories and service handlers.
Concurrent service handlers can improve the throughput of an
event-driven application because multiple events can be processed
simultaneously. A concurrent service handler can be implemented as
an ACTIVE OBJECT (365) if it represents a coarse-grained component or
service, or as a MONITOR OBJECT (368) if it is a fine-grained (distributed)
object.
##%%&&
When designing a LAYERS (185), DOMAIN OBJECT (208), REACTOR (259),
PROACTOR (262), ITERATOR (298), COMPOSITE (319), ACTIVE OBJECT (365),
COMMAND (412), INTERCEPTOR (444), CHAIN OF RESPONSIBILITY (440), BRIDGE
(436), OBJECT ADAPTER (438), VISITOR (447), DECORATOR (453), STRATEGY
(455), WRAPPER FACADE (459), OBSERVER (405), or CONTAINER (488)
arrangement . . .
. . . a major concern of all software architecture work is the effective
and appropriate expression of component interfaces.
A component represents a self-contained unit of functionality
and deployment with a published usage protocol. Clients can
use it as a building block in providing their own functionality.
Direct access to the full component implementation, however,
would make clients dependent on component internals, which
ultimately increases application internal software coupling.
Ideally a client should only depend on a component¡¯s published
interface. If this interface remains stable, modifications to the component¡¯s implementation should not affect its clients. Encapsulating
components within ordinary concrete classes is impractical: these
class interfaces are always bound to their implementations. Even
with abstract classes, the typical inclusion of a partial implementation is more binding than is appropriate for loose coupling and
stability. Location independence is a further concern: clients of a
component may reside in remote address spaces, and a component¡¯s
location may change at runtime, so client dependencies on a component¡¯s location should therefore be avoided. Finally, the methods
offered by a component should be meaningful for clients and support
its effective and correct usage, especially in distributed or concurrent
deployments.
Therefore:
Separate the declared interface of a component from its implementation. Export the interface to the clients of the component,
but keep its implementation private and location-transparent to
the client.
A call from the client through this explicit interface will be forwarded
to the component, but the client code will depend only on the interface
and not on the implementation. An EXPLICIT INTERFACE is associated with
a contract [Mey97] that clients must follow to use such a component
correctly. This contract includes operations offered by the component,
the protocol for calling the operations, and any other constraints and
information that clients must know to use the component correctly
and effectively.
An EXPLICIT INTERFACE enforces a strict separation of the component¡¯s
interface from its concrete implementation, which separates component usage issues from concrete realization and location details. This
separation also enables the transparent modification of component
implementations independently of the clients using it, as long as the
contract defined by the interfaces remains stable.
Several patterns help in structuring a component¡¯s explicit interface.
An EXTENSION INTERFACE (284) supports the partitioning of an explicit
interface into multiple smaller interfaces, one for each role of the
component. EXTENSION INTERFACE also enables the extension of the
component with new role-specific interfaces. In general, an EXTENSION
INTERFACE supports interface evolution while minimizing the effect of
this evolution on the component¡¯s clients.
A PROXY (290) helps to encapsulate specific house-keeping tasks associated with invoking a component. For example, it can transform a
method call into a message that can be sent across the network to
the component implementation, load it from the database on the first
access, or cache immutable state for efficient client access. A BUSINESS
DELEGATE (292), in contrast, is most useful in dynamic distributed
environments, which typically require a range of infrastructure tasks
to be performed when accessing the component. For example, before
a component can be invoked, its implementation must first be located
and a connection to its implementation must be established. A
BUSINESS DELEGATE can execute these tasks transparently for clients
when they invoke a method on the component. A FACADE (294) shields
clients from the internal structure of the component, which can consist of even smaller parts. It provides a single, defined entry point into
the component, which allows the component¡¯s structure to be varied
without effects onto its clients.
A key issue when specifying an explicit interface is operational
quality: clients must be able to use the component effectively and
correctly. Designing an explicit interface as a THREAD-SAFE INTERFACE
(384) serializes the access to a component in concurrent usage scenarios, keeping locking overhead to a minimum and, in the case
of non-recursive locks, avoiding self-deadlock should a component
invoke methods on itself. AUTHORIZATION (351) ensures secure access
to the component¡¯s functionality. A COMBINED METHOD (296) represents
a series of method invocations on the component that are always
called together and in a specific order, which makes an explicit interface more expressive, because it reflects common component usage.
FACTORY METHODS (529) and DISPOSAL METHODS (531) allow clients to create and dispose of a component without being dependent on either its
internal structure or the processes for constructing and destroying it.
If an interface represents an aggregate structure such as a collection,
clients may need access to the elements or want to execute actions
on them. An ITERATOR (298) allows clients to traverse these elements
one at a time without breaking the component¡¯s encapsulation. A
BATCH METHOD (302) is similar in intent to an ITERATOR, but sends or
returns multiple elements on each invocation, which is beneficial for
distributed and concurrent systems, because networking and synchronization overhead is minimized. An ENUMERATION METHOD (300)
helps to execute a specific action on each element without requiring the caller to manage the traversal explicitly, which minimizes
synchronization overhead in concurrent usage scenarios.
The parameters and results of an invocation on an explicit interface
can be encapsulated into DATA TRANSFER OBJECTS (418), which avoids
clients or the component having to depend on concrete data representations. If clients need access to the component¡¯s internal state, it
can be returned as a MEMENTO (414) to maintain encapsulation. If the
component needs client-specific information to execute its services,
it can be passed as a CONTEXT OBJECT (416) to the component.
##%%&&
When specifying an EXPLICIT INTERFACE (281) . . .
. . . we may want to ensure client stability and type-safety in the face
of interface evolution.
Clients can use a component effectively only if it provides a
stable and coherent interface. The interface of a component is
often affected, however, when its functionality is modified or
extended, which can break the client code¡ªin some cases even
if the new functionality is not used.
Ideally, clients of a component should not break when parts of the
component¡¯s interface that they do not use change, or if they are
not interested in new services added to the component. Even when
interface parts that they actually do use change, clients should not
break if they do not use the changes. Similarly, the existing interface
of a component should remain stable when its implementation is
extended with new services, or when existing service signatures are
updated.
Therefore:
Let clients access a component only via specialized extension
interfaces, and introduce one such interface for each role that
the component provides. Introduce new extension interfaces
whenever the component evolves to include new functionality or
updated signatures within existing extension interfaces.
Clients interested in a specific role of the component issue requests
through the corresponding extension interface, which forwards the
request to the associated component implementation. When implementing a component¡¯s extension interfaces, first identify the different
roles that it can play within its envisioned usage scenarios. Encapsulate each role into a separate extension interface and allow clients to
access only those extension interfaces that they actually need to do
their job. Avoid modifying existing extension interfaces, and instead
create new extension interfaces when extending the component with
new functionality or when updating existing service signatures.
An EXTENSION INTERFACE design offers several benefits. First, it minimizes client coupling to the component: clients only depend on
the interfaces of those roles they actually use, which ensures that
they do not break when signatures change or new services are
added to the component. Second, clients can still access component functionality via concise and strongly typed interfaces, without
resorting to accessing bloated ¡®one-size-fits-all¡¯ interfaces or inefficient
dynamically typed message-oriented interfaces. In particular, each
role-specific extension interface of an EXTENSION INTERFACE arrangement can be an EXPLICIT INTERFACE that specifies the methods¡ªand
only those methods¡ªnecessary to fulfill its respective role.
To manage a component¡¯s extension interfaces, introduce a special
root interface that offers functionality to retrieve and access each
extension interface of the component. Optionally, the root interface
can offer an INTROSPECTIVE INTERFACE (286) that allows clients to obtain
information about the component, and a DYNAMIC INVOCATION INTERFACE
(288) through which client can issue requests to the component without the need to use one of its role-specific extension interface. Both
types of interface are especially useful if the component is monitored
or accessed by external tools, such as testing tools or system monitors, or if it must be integrated dynamically into applications that
originally were not designed to use the component.
LIFECYCLE CALLBACKS (499) and configuration functionality are optional
functionality for the root interface, which an application can use
to initialize the component and control its lifecycle actively. Make
sure that the root interface functionality is accessible through all
extension interfaces of the component, for example by deriving all
extension interfaces from the root interface, or by implementing its
functionary within all extension interfaces.
##%%&&
When specifying a REFLECTION (197) architecture or the root functionality of an EXTENSION INTERFACE (284) . . .
. . . we must sometimes allow clients to access information, also known
as metadata, about the components they are using.
Using a component correctly may require clients to access information about it, such as its type, identity, supported interfaces,
or current state. However, allowing clients to access such mechanistic details directly could break component encapsulation and
reduce dependency stability.
In addition, clients would become dependent on the component¡¯s
structure, which increases an application¡¯s complexity and complicates its maintenance and evolution. Subsequent changes to the
component¡¯s implementation and interface could potentially ripple
through to all its clients. Avoiding such ripple effects is particularly important for application-external tools such as debuggers and
system monitors, which are unaware of the specific properties of
the components they control, but need to access their mechanisms
without introducing dependencies on their implementation details. A
component should therefore be a self-contained, well-encapsulated,
building block¡ªregardless of how clients want or need to use it.
Therefore:
Introduce a special introspective interface for the component
that allows clients to access information about its mechanisms
and structure. Keep the introspective interface separate from
the component¡¯s ¡®operational¡¯ interfaces.
By calling the introspective interface, clients can access information that helps them to control how they use the component and/or
monitor and reason about it. General metadata provided by an introspective interface could include the component¡¯s type, name, location,
version, interfaces, configuration and deployment parameters, as well
as its dependencies to other components. An introspective interface
could also provide metadata useful for managing the component,
such as the number of clients, load, and resource usage. An introspective interface supports the calling a component through a DYNAMIC
INVOCATION INTERFACE.
An INTROSPECTIVE INTERFACE provides controlled access to a component¡¯s
details without breaking its encapsulation. In addition, it separates
component usage from the process of obtaining information about it,
which makes it ideal for tools to obtain information without becoming
dependent on their specific interfaces, internal design, and implementation details. However, most component usage scenarios actually do
not need introspective access to component metadata, so providing an INTROSPECTIVE INTERFACE should be considered an option, not a
mandatory requirement, for a component.
Some languages offer standard introspective interfaces for obtaining
metadata about objects and classes, such as the java.lang.reflect
package in Java and the System.Reflection namespace in C#. Other
languages, such as C and C++, require framework support for introspective interface, as is the case in CORBA and COM. AUTHORIZATION
(351) ensures secure access to the component¡¯s details: in general,
not every client is allowed to obtain this information.
The parameters and results of an invocation on an introspective
interface can be encapsulated into DATA TRANSFER OBJECTS (418),
which avoids dependencies of clients or the component on concrete input/output data representations. If clients need to access the
component¡¯s internal state, it can be returned as a MEMENTO (414) to
maintain component encapsulation. Similarly, if client-specific information is needed to obtain the ¡®right¡¯ information, for example if it
maintains sessions with its clients, this information can be passed to
the component as a CONTEXT OBJECT (416).
##%%&&
When specifying a REFLECTION (197) architecture, the root functionality
of an EXTENSION INTERFACE (284), a METHODS FOR STATES (469) arrangement, or a CONTAINER (488) . . .
. . . we may need to allow access to the functionality of a component
without knowing or using any of its statically typed interfaces.
A component may need to support calls of its methods outside
the declared and enumerated protocol of an explicit interface.
Such openness is needed when clients have to invoke additional
capabilities on the component that cannot necessarily be known
by the component¡¯s client in advance.
A component may be loaded dynamically into a framework environment, such as a user-interface component in a client, a business-logic
component on a server, or a set of test cases in an automated testing tool. The loaded component can support a variety of methods
suitable for its task, but not specifically relevant to its calling framework¡ªexcept that the framework must be aware of the methods and
be able to forward events or translate calls to them.
In principle, the features exported by the component may be un bounded, such as the many possible names of test cases a component may offer. Alternatively, features may form a coherent model,
but not one for which a generic framework would be specialized.
For example, user-interface controls can choose to support only a
handful of the events and properties picked up by a GUI framework. An EXPLICIT INTERFACE (281) can capture the specific detail of
what is offered, but not necessarily in a way that such dynamic
clients can use. An INTROSPECTIVE INTERFACE supports querying of methods, but not necessarily in a way that allows calling by dynamic
clients.
Therefore:
Introduce an invocation interface for the component that allows
clients to compose calls on the component dynamically. Methods
are identified at runtime by strings, and arguments are passed
as generally typed collections. Keep the dynamic invocation
interface separate from the component¡¯s ¡®operational¡¯ interfaces.
Calls on the component through a dynamic invocation interface are
dispatched to specific methods on the component¡¯s declared interface.
A DYNAMIC INVOCATION INTERFACE opens up the options available to a
client when calling a component. Interface usage based on a dynamic
protocol, rather than on static explicit interfaces, is open and non intrusive, which enhances client application flexibility.
In addition to such benefits, however, with any form of late binding
there is also the liability of late error detection for incorrect implementation or incorrect use of a protocol. It is important to establish
a protocol for naming or annotating methods that components can
follow, but this still offers no guarantee of correctness. It is possible
for some checking tools to detect incorrect usage of dynamic interface
invocation, but the options are more constrained than for statically checked invocations. Similarly, automated refactoring through
dynamic invocations does not have the same guarantee of correctness
as automated refactoring through statically checked interfaces.
The evolvability and flexibility of a dynamic invocation interface
may also be associated with a performance overhead. Heavy use
of reflection-based inspection and invocation in languages with statically checked type systems contrasts with the cheap execution of
methods through declared interfaces. This overhead is less problematic in interpreted languages such as Smalltalk, Ruby, and Python,
whose type systems and normal execution model are already based
on a dynamic model.
##%%&&
When specifying an EXPLICIT INTERFACE (281) . . .
. . . we often want to avoid accessing services of a component implementation directly.
Software systems consist of cooperating components: client
components access and use the services provided by other components. It is often impractical, or even impossible, to access
the services of a component directly, for example because we
must first check the access rights of its clients, or because its
implementation resides on a remote server.
Including ¡®housekeeping¡¯ functionality such as authorization within a
component is undesirable, for two reasons. First, we might not need
such functionality for every use of the component. Second, it mixes
multiple orthogonal concerns within a single implementation, thereby
making it hard to modify each concern separately and independently.
A component¡¯s functionality should always be independent of any
housekeeping activities. For similar reasons it is undesirable for
component clients to perform this housekeeping functionality, since
it would couple them tightly with the component¡¯s implementation.
For example, if clients are to access a remote component directly,
they become dependent on the component¡¯s location, as well as on
the networking protocols that are used to access its functionality,
which should be transparent to a component¡¯s clients.
Therefore:
Encapsulate all component housekeeping functionality within a
separate surrogate of the component¡ªthe proxy¡ªand let clients
communicate only through the proxy rather than with the component itself.
Design the proxy so it offers the same public interface as the component. When a client calls a method on the proxy, first perform
all housekeeping preprocessing that must be done before forwarding the client¡¯s request to the ¡®real¡¯ component, then let the proxy
call the corresponding method on the component. When control
flow returns, first perform all necessary housekeeping post processing
before returning any results of the method to the client.
A PROXY frees both the client and the component from implementing
component-specific housekeeping functionality. It is also transparent
to clients whether they are connected with the component or its proxy,
because both publish an identical interface. The primary liabilities of
a proxy are the hidden costs it can introduce for clients, although for
many uses these costs are negligible compared to the execution time
of the component¡¯s services.
There are many types of proxy [POSA1]. A CLIENT PROXY (240) shields
the clients of a remote component from network addresses and IPC
protocols to enable location independence within a distributed system: clients can use the client proxy as if it were a local component.
A BUSINESS DELEGATE (292) goes one step further: it shields clients
from all IPC, as well as locating remote components, load balancing
when multiple component instances are available in a distributed
application, and handling of specific networking errors. A THREAD SAFE INTERFACE (384) is a proxy that serializes access to concurrent
components transparently for both the client and the components. A
COUNTING HANDLE (522) is normally expressed as a proxy that helps to
access the functionality of shared heap objects whose lifetime must
be managed explicitly by an application, to avoid memory leaks when
the object is no longer used. A VIRTUAL PROXY (497) loads or creates
an expensive component on demand and may delete it from memory
after use. Finally, a FIREWALL PROXY (349) protects a software system
from specific types of external attack.
If it is impossible or impractical to provide the proxy with an interface
identical to that of the represented component, implement an OBJECT
ADAPTER (438) that maps between the two interfaces.
##%%&&
When specifying a BROKER-based (237) distribution infrastructure, an
EXPLICIT INTERFACE (281) or PROXY (290), or a REPLICATED COMPONENT GROUP
(326) . . .
. . . we must often consider that a component is, or can be, accessed
from another address space.
Due to performance and reliability properties of networks,
accessing remote components differs significantly from accessing local components. Ideally, however, clients should not need
to care whether the components they use are collocated or
remote.
An invocation across a network involves specific infrastructure tasks,
such as retrieving the location of the remote component, error handling, and load balancing. Many of these tasks are unnecessary for
collocated components. If clients depended on specific locations of
components, or even on their remoteness, they would have to perform these tasks themselves, which would mix networking code with
domain-specific code and increase application complexity. Moreover,
clients would need to be modified if the location of components they
used changed, for example due to fault recovery or load balancing.
Consequently, client code should be as independent of the location
and plumbing support of invoked components as possible.
Therefore:
Introduce a business delegate for each remote component that
can be created, used, and disposed of like a collocated component, and whose interface is identical to that of the component
it represents. Let the business delegate perform all networking
tasks transparently for clients using the component.
On creation or access by a client, the business delegate locates
the remote component it represents. Subsequent method calls on
the business delegate are forwarded to the remote component using
that location information. In addition, the business delegate handles
errors that can occur when communicating with the remote component. If multiple instances of the component are deployed, the
business delegate can also perform load balancing before issuing a
request to a specific component instance. A business delegate is also
an ideal access point for system management functionality, to monitor and control all client-side communication and interaction with
the remote component.
A BUSINESS DELEGATES supports location-independent component invocation in a distributed system: networking tasks and issues are
hidden from the delegate¡¯s clients. In addition, each business delegate follows the same lifetime and usage protocol as components
collocated with its clients: it can be created and disposed of via constructors and destructors or garbage collection, and it can be called
via ¡®ordinary¡¯ method invocation. The primary liability of a business
delegate is the hidden cost it can introduce for clients, though for
most usage scenarios this cost is negligible compared to the execution
time of the business delegate¡¯s primary responsibilities.
Typically, a business delegate uses a LOOKUP (495) service to retrieve
the location of one or more instances of the remote component it
represents. The access to a specific remote component instance is
often encapsulated inside a CLIENT PROXY (240), which shields the
business delegate from the details of the networking protocol used to
access the instance. LAZY ACQUISITION (507) helps to defer connection
to the represented component to the time when it is first accessed by
a client calling the business delegate.
If calling the remote component returns a REMOTING ERROR [VKZ04],
the business delegate can take appropriate action to resolve the error
before it signals a failure to clients. For example, if the connection to
the component breaks, the business delegate can try to re-establish
the connection and try to issue the call again. Similarly, if a specific
component instance is overloaded, the business delegate can try
calling a more lightly loaded instance of the same component.
##%%&&
When developing a BROKER (237), an EXPLICIT INTERFACE (281), a WHOLE PART (317) structure, or a COMPONENT CONFIGURATOR (490) . . .
. . . we must sometimes access a group of components that together
provide a broad service to their clients.
Complex services are often provided by a group of components,
each of which can offer its own self-contained services to clients.
If clients that want to invoke a complex service must maintain
explicit relationships to each component in the group, however,
they become dependent on the group¡¯s internal structure.
If this structure changes, all clients are affected. In addition, the
more such dependencies exist, the greater the physical and logical
complexity of the software system [Lak95]. Ideally, clients should
have a single entry point to a group of related components that
simplifies the invocation and execution of common tasks from the
client¡¯s perspective. On the other hand, there could be clients that
use only one specific component in the group. Forcing these clients
to invoke methods on that component through a separate component
entry point would introduce an unnecessary level of indirection, with
corresponding performance penalties. In addition, there could be
clients that want to execute specific, more sophisticated tasks on the
component group, which require the means to integrate and invoke
its components differently than in the common usage scenarios.
Therefore:
Specify a single point of access for the component group¡ªthe
facade¡ªthat mediates client requests to the appropriate components in the group for common usage scenarios, but can be
bypassed for specific, more sophisticated scenarios.
In addition to routing requests, the facade performs all necessary
adaptation between the signatures of its published services and the
interfaces of the ¡®protected¡¯ components. A facade can also aggregate
features of different components to new, ¡®higher-level¡¯ services. Nevertheless, clients are not forced to call the components through the
facade: direct access to each specific component is still possible.
Clients that call the component group through its FACADE are independent of the group¡¯s internal structure and relationships, so they
are not affected when this structure changes. In addition, clients that
want to access a specific component in the group can bypass the
facade to call that part directly, which avoids performance penalties
for such clients.
In a remote application setting, a facade can be deployed as a SERVICE
GATEWAY [MS03] to the address spaces of its clients, or as a SESSION
FACADE [ACM01] to the (server-side) address space of its component
group. A SERVICE GATEWAY supports throughput and scalability. All
overhead of deciding which component of the group to invoke, as
well as all adaptation of requests on the facade to the interfaces
of the corresponding components, is located on the client side. The
downside of a SERVICE GATEWAY is that each component in the group is
accessed remotely, which can increase the networking overhead, as
well as the structural complexity of a distributed application.
A SESSION FACADE avoids these penalties: remote clients can access
the component group only via its facade. The more frequently clients
access the component, however, and the more routing and adaptation overhead is necessary to mediate requests appropriately to the
corresponding components in the group, the more a SESSION FACADE
can become a performance and scalability bottleneck.
A GATEWAY [Fow03a] is another form of FACADE, which represents
an access point to an external system used by an application. The
application thus becomes independent of the specific interfaces of the
external system and also of its internal structure. A TRANSFER OBJECT
ASSEMBLER [ACM01] is a FACADE that combines results received from
several components into an aggregated result that can be returned
wholesale to the clients of the components.
##%%&&
When specifying an EXPLICIT INTERFACE (281) or an ITERATOR (298) . . .
. . . we often need to invoke multiple methods on a component together
and in the same order.
Clients often must invoke multiple methods on a component
in the same order to perform a specific task. From a client¡¯s
perspective, however, it is tedious and error-prone to call the
method sequence explicitly each time it wants to execute the
task on the component.
Repeating the same method call sequence across an entire application
makes it harder to develop, understand, and maintain. It is easy to
call the methods in the wrong order, forget to call a specific method,
or pass incorrect parameters to them. Changes to the calling order,
or changes to the signatures of the called methods, affects all the
code executing the method sequence.
Additional problems arise in distributed and concurrent systems in
which each remote call incurs networking overhead, and a sequence
of calls on a mutable component may not have the desired outcome
if the component is shared across threads, even if each individual
method is safe against race conditions. Moreover, if any of the methods in the sequence fails, it is the responsibility of the calling client
to handle this failure and to undo or roll-back the changes on the
component caused by method calls prior to the one that failed.
Therefore:
Combine methods that must be, or commonly are, executed
together on a component into a single method.
Clients that want to execute the method sequence issue a request
to the combined method, which then executes that sequence on
behalf of the clients. In addition, the implementation of the combined
method handles all necessary distribution, concurrency, and failure
management aspects associated with invoking the method sequence.
Using a COMBINED METHOD makes the interface of a component or an
object more expressive and cohesive, because it reflects common use.
Similarly, the robustness of the application improves, because there
is only one place, rather than many, where the method sequence is
programmed and failures are handled. Consequently, from a client¡¯s
perspective, the component or object becomes easier to use.
Distribution overhead also occurs only once for calls to a combined
method, as it replaces many remote calls by a single one. Similarly, concurrency hazards that can result from non-determinism
are eliminated, because the entire calling sequence is synchronized,
rather than each individual method call. Finally, failure handling and
recovery strategies are encapsulated within the combined method,
leading to a more transaction-like style of method design: either the
entire sequence is executed successfully, with corresponding effects
on the component or object, or, if any method in the sequence fails,
it appears to clients as if the entire sequence was not executed at all,
leaving the component or object unchanged.
A combined method that is based on querying or setting a group of
values at once can be expressed with a DATA TRANSFER OBJECT (418).
If the access is to traverse all the elements of an aggregate, such as
a collection, a BATCH METHOD (302) can be seen as a generalization
of COMBINED METHOD, in which, instead of combining calls to different
methods on an object, it is the whole loop and access of successive
elements of the target object that is folded into a single call.
##%%&&
When specifying EXPLICIT INTERFACE (281), ENUMERATION METHOD (300),
COMPOSITE (319), or OBJECT MANAGER (492) . . .
. . . we often need to access elements of an aggregate object sequentially without exposing its underlying structure.
Clients often want to traverse elements that are encapsulated
within an aggregate, such as the elements maintained by a
collection. Clients may not wish, however, to depend on the
aggregate¡¯s internal structure to access components of interest,
nor do aggregates want to expose their internal structure to
clients.
To complicate matters, clients often need to traverse the components
in a specific order that best fits their needs. Multiple clients may
also want to access the aggregate simultaneously, and it is even
possible that a single (multi-threaded) client needs to run multiple
simultaneous traversals. Supporting multiple traversal strategies as
well as simultaneous traversal directly within the aggregate, however,
would complicate its internal structure. For example, the aggregate
must maintain the concrete state of each active traversal within a
separate session. Developers would also be distracted from realizing
the aggregate¡¯s domain responsibility.
Therefore:
Objectify the strategy to access and traverse the components
maintained by the aggregate into a separate iterator component.
Let this iterator be the only means for clients to access the
component, and allow the iterator access to the representation
of the aggregate necessary for it to carry out the traversal.
Clients that want to traverse the aggregate¡¯s contained elements must
first obtain an iterator from the aggregate. Clients can then use this
iterator to access the elements in sequence.
An ITERATOR preserves the encapsulation of the aggregate and keeps
clients independent of its internal structure. In addition, the reification of access and traversal strategies allows multiple clients to
maintain their own traversal over the aggregate¡¯s content, and single clients to run several independent traversals simultaneously.
An ITERATOR arrangement is however most suitable in single-process,
single-threaded component deployments.
A common ITERATOR arrangement is based on an abstract iterator that
is declared as an EXPLICIT INTERFACE (281) for accessing and traversing
components maintained by the aggregate. Concrete iterators derive
from or implement this interface to realize particular access and
traversal strategies, such as breadth-first or depth-first for tree-like
hierarchies. Such a design helps to encapsulate different traversal
strategies behind a uniform interface, as well as integrating new or
evolving existing iterator types without modification to the existing
iterator management infrastructure within the aggregate.
A COMBINED METHOD (296) on the iterator¡¯s interface avoids subtle
race conditions when accessing the aggregate via iterators running
in separate threads. Similarly, a BATCH METHOD (302) on the iterator¡¯s
interface that supports ¡®chunky¡¯ access to the aggregate¡¯s elements
avoids performance penalties and unnecessary network load when
the iterator is remote to the aggregate.
Provide the aggregate¡¯s interface with a FACTORY METHOD (529) and
a DISPOSAL METHOD (531) to create and dispose of concrete iterators
on client request. Both methods separate and encapsulate the lifetime control of iterators behind a common interface, and separate it
from the aggregate¡¯s domain logic. If the aggregate¡¯s internal structure can be modified during a traversal, there may be a need for
robust iterators. Robustness can be achieved with an OBSERVER (405)
arrangement: the aggregate plays the role of a subject that notifies all
active iterators whenever its internal structure changes, such as on
the deletion of an aggregated component [Kof04].
##%%&&
Within an EXPLICIT INTERFACE (281), BATCH METHOD (302), or OBJECT
MANAGER (492) . . .
. . . we may want to iterate over the elements of an aggregate component, invoking an action on each element.
Some types of aggregate components, such as graphs or trees,
have representations that do not conveniently support ITERATOR based traversal. Similarly, using an ITERATOR approach to access
the elements of an aggregate that is shared between threads
can incur unnecessary overhead from repeated locking. Remote
aggregate access incurs an even greater overhead. It must be possible, however, to access the elements of the aggregate efficiently
to execute actions on them.
To further complicate matters, there are times when an aggregate
requires pre- and post-iteration code to be executed before and after
the traversal. The most obvious and common case is synchronization
against threaded interruption. Expecting the clients of the aggregate
to write this code themselves is tedious and error-prone. In the
specific case of Java thread synchronization in distributed systems,
an external synchronized block is problematic, because it can give
you the illusion of safety without any of the actual safety [Hen01c].
Therefore:
Bring the iteration inside the aggregate and encapsulate it in
a single enumeration method that is responsible for complete
traversal. Pass the task of the loop¡ªthe action to be executed
on each element of the aggregate¡ªas an argument to the enumeration method, and apply it to each element in turn.
In contrast to an ITERATOR approach, ENUMERATION METHOD performs
a complete traversal, including the invocation of an action on each
element of the traversed aggregate, wholesale, rather than in many
separate bits and pieces.
In distributed and networked environment, reducing many remote
calls to a single ENUMERATION METHOD improves performance, incurs
less network errors, and saves precious bandwidth. The key to these
benefits is that an ENUMERATION METHOD realizes the principle of inversion of control¡ªit is not the client that controls the iteration, but the
aggregate itself. In addition, an ENUMERATION METHOD applies any of the
relevant pre- and post-iteration activities on the aggregate itself, such
as synchronization, performing the loop in between. This approach
allows the aggregate to keep control of how it behaves: its design
becomes more complete, explicit, encapsulated, and self-contained.
The action passed to an enumeration method is a COMMAND object
(358), or some kind of method reference such as a function pointer
in C++ or a delegate in C#. Such a design allows the provision of
a ¡®generic¡¯ enumeration method for the aggregate, to which clients
can pass arbitrary actions. However, one liability can work against
it for remote access. For the inversion of control to be efficient, calls
to the COMMAND must be local rather than remote. This means that
the COMMAND object must be copied from the client to the server, and
should not be accessed indirectly as a remote object. This constraint
also has implications for the COMMAND¡¯s code, which must also be
local at the point of call, either already present or transferred on
the first call. Such code transfer implies that this pattern does not
apply in heterogeneous systems. Enumeration methods are, however,
particularly effective in multi-threaded situations. A BATCH METHOD
may be more suitable in remote cases.
A VISITOR (447) can help an enumeration method to simplify the
traversal of non-linear aggregate structures such as graphs without
becoming dependent on the structure. Alternatively, if the aggregate¡¯s
structure is linear, such as in a double-linked list, an ITERATOR (298)
can help to achieve this independence.
##%%&&
Within an EXPLICIT INTERFACE (281), ITERATOR (298), or OBJECT MANAGER
(492) . . .
. . . we may need to perform bulk accesses on an aggregate component.
Clients sometimes perform bulk accesses on an aggregate component, for example to retrieve all elements in a collection that
meet certain properties. If access to the aggregate is expensive,
for example because it is remote or concurrent, accessing it
separately for each element can incur significant performance
penalties and concurrency overhead.
If the aggregate is remote, each access incurs latency and jitter,
decreases the available network bandwidth, and introduces additional points of failure. If the aggregate is a concurrent component,
synchronization and thread management overhead must be added
to the cost of each access. Similarly, any other per-call housekeeping code, such as for authorization, further decreases performance.
Nevertheless, it must be possible to perform bulk accesses to an
aggregate efficiently and without interruption.
Therefore:
Define a single batch method that performs the action on the
aggregate repeatedly. The method is declared to take all the
arguments for each execution of the action, for example via an
array or a collection, and to return results by similar means.
A batch method folds repetition into a data structure, rather than a
loop within the client, so that looping is performed before or after the
method call, in preparation or follow-up respectively.
A BATCH METHOD reduces the cost of accessing the aggregate to a
single access or a few ¡®chunked¡¯ accesses. In distributed systems this
can significantly improve performance, incur less network errors,
and save precious bandwidth. Although by using a BATCH METHOD
each access to the aggregate becomes more expensive, the overall
cost for bulk accesses has been reduced. Bulk accesses can also be
synchronized as appropriate within the method call.
The trade-off in complexity is that a batch method performs significantly more housekeeping to set up and work with the results of the
call, and requires more intermediate data structures to pass arguments and receive results. The higher are the costs for networking,
concurrency, and other per-call housekeeping, however, the more
affordable this overhead becomes. BATCH METHOD can be seen as a
generalization of COMBINED METHOD, in which a whole loop traversal
is folded into the target, not simply a short sequence of different
methods.
Simplistically, the parameters of a batch method are either in or
out, and these may be expressed as simple collections, such as
arrays of values or name¨Cvalue pairs, or even variable argument
lists, assuming appropriate type-safe language support exists. In
parameters list elements that are to be sent from the caller to the
aggregate, such as elements to add or keys to query. Out parameters,
possibly expressed as return values, list results returned from the
aggregate, such as found values.
Typically, batch methods are specific rather than general. That is,
they are named after a particular action and their arguments reflect
the inputs and results directly. For example, finding values that
match keys can be expressed as a single method. However, if more
generalization is required, further parameters that control the encapsulated loop are needed. Examples finding all entries that are older
than a particular date, or greater than a particular value. The most
generalization that is possible is to pass in a predicate or some
control code in the form of a COMMAND object (412), which makes a
BATCH METHOD more like an ENUMERATION METHOD (300), and with similar
liabilities in a distributed environment.
##%%&&
When developing a LAYERS (185) architecture, an application¡¯s DOMAIN
OBJECTS (208), a BRIDGE (436) arrangement, or generally when designing components for a component-based system . . .
. . . a significant decision involves the realization of component implementations against component interfaces.
A component offers its services through interfaces that define its
usage protocols, published functionality, and quality of service
properties. However, an interface is only a promise: a component
must provide fulfillment. Thus a component implementation is
often subjected to assumptions, considerations, and constraints
that cannot be exposed through its interface.
Regardless of the contract defined by a component¡¯s interface, its
realization is exposed to constraints and requirements that are of
little interest for component users, but are crucial in fulfilling the
contract. For example, the component may need to be prepared
for distributed deployment without decreasing its performance and
throughput properties. Or, depending on its concrete usage and
deployment scenarios, different algorithms, additional functionality,
or additional quality properties may be required, such as different tax
calculation algorithms, a user-specific business activity, or stronger
security measures. Finally, almost all components are subject to
evolution; their implementation can change over time. Nevertheless,
clients should be shielded from all these aspects¡ªthey are only
interested in the contract, not in how it is fulfilled.
Therefore:
Ensure that all component implementation details remain hid den behind its interfaces to shield clients from representation
choices that may change during the lifetime of an application,
or are dependent on the component¡¯s specific deployment.
A component¡¯s client cannot programmatically assume more than is
exposed through its official interface, which supports the component
implementor¡¯s ability to change the implementation without breaking
clients.
A component implementation that respects the boundary defined
by its interfaces ensures that the dependency of its clients is on
its interface, its whole interface, and nothing but its interface. The
ENCAPSULATED IMPLEMENTATION is free to evolve, while preserving the
stability of its clients. Developers of client code are still presented
with a simple and stable interface to use.
The interface boundary seals the component implementation from
its environment, and vice versa, but it is possible that the component has a dependency on some features of its calling environment. To avoid introducing a reverse dependency, pass CONTEXT
OBJECTS (416) from the caller to the component when such information or behavior is needed. A component may also need to
pass out implementation-dependent state to the client for later
use with respect to the component, for example a position in a
traversal or a callback handler for a registered event of interest. In
this case, preserve the component¡¯s encapsulation by returning a
MEMENTO (414).
In general, an ENCAPSULATED IMPLEMENTATION should provide a well encapsulated software representation of the component¡¯s specific
functional responsibilities. Domain-specific patterns can support
the creation of this representation, such as those for health care,
corporate finance, telecommunication, and public transportation
applications [Fow97] [Ris01] [PLoPD1] [PLoPD2] [PLoPD3] [PLoPD4]
[PLoPD5].
Sometimes the representation of the component¡¯s functionality can
also be defined on the basis of more general patterns that are not
bound to a specific (application) domain. For example, components
that represent a hierarchy of elements can be realized using a WHOLE PART (317) or COMPOSITE (319) design, dependent on how distinct
or uniform the elements of the hierarchy are. If the functionality
of the component is centered on a state machine, its realization
can be based on an OBJECTS FOR STATES (467) or METHODS FOR STATES
(469) design, dependent on the size of the state machine and the
amount of data and context information shared between states. If the
component¡¯s responsibility is to interpret structured files or sentences
of a given language, such as in a parser, an INTERPRETER (442) design
could be considered as its fundamental structure.
A key concern of all ENCAPSULATED IMPLEMENTATIONS is fulfilling the operational qualities specified in the component¡¯s contract: performance,
scalability, and so forth. Providing the ¡®right¡¯ structure and behavior
is simply not enough to ensure a component¡¯s usability and acceptance. Two techniques that help in achieving key operational qualities
are distribution and concurrency.
Deploying an ENCAPSULATED IMPLEMENTATION to multiple hosts in a
distributed system allows a component to take advantage of the
resources available in the entire network, rather than only of those
available on a single network node. The more resources are available,
the better the component¡¯s operational quality. Which particular
qualities can be supported by distributed deployment depends on
the chosen component partitioning. MASTER-SLAVE (321), HALF-OBJECT
PLUS PROTOCOL (324), and REPLICATED COMPONENT GROUP (326) offer different trade-offs for the support of performance, availability, scalability,
fault tolerance, and computational accuracy.
A concurrent ENCAPSULATED IMPLEMENTATION impacts the operational
qualities of a component¡ªspecifically its performance and through put¡ªbecause multiple client requests can be handled and processed
simultaneously. An ACTIVE OBJECT (365) arrangement supports the
implementation of components in their own set of threads, whereas
a MONITOR OBJECT (368) helps in realizing components that are collocated within their client threads. HALF-SYNC/HALF-ASYNC (359) and
LEADER/FOLLOWERS (362) configurations are most suitable for components that process network I/O.
However, all the above deployment and concurrency models come
with certain costs, which are mainly due to duplicated functionality, increased resource usage, more intricate implementation, and
the coordination between the concurrent parts of the ENCAPSULATED
IMPLEMENTATION or other distributed components. Before introducing
any of these models, it is important to assess whether their costs
outweigh their benefits.
Another key concern for almost all ENCAPSULATED IMPLEMENTATIONS is
support for evolution, extension, and adaptation. These developmental qualities enable the effective use of components within different
application deployments and variants, and also their reuse within
completely different applications.
STRATEGIES (455) and TEMPLATE METHODS (453), for example, support the
separation of variant from invariant behavior, STRATEGY by using delegation, and TEMPLATE METHOD by inheritance. In contrast, a VISITOR (447)
allows functionality to be added to an ENCAPSULATED IMPLEMENTATION that
was not envisioned during its original development. The control flow
inside a component can be extended by INTERCEPTORS (444), DECORATORS
(472) and, if C++ is used to realize the ENCAPSULATED IMPLEMENTATION,
EXECUTE-AROUND OBJECTS (451). INTERCEPTORS can inject out-of-band
behavior into a function¡¯s control flow, whereas DECORATORS help in
wrapping a function with specialized behavior that is to be executed
before or after a function. An EXECUTE-AROUND OBJECT is similar in
this respect to a DECORATOR: it allows additional functionality to be
executed before or after a sequence of statements in an exception safe manner, which makes it a preferred C++ idiom for resource
acquisition and release. Finally, OBJECT ADAPTERS (438) and WRAPPER
FACADES (459) support the integration of an ENCAPSULATED IMPLEMENTATION
with its environment by adapting the provided interfaces of components, libraries, and operating systems to those expected or required
by the ENCAPSULATED IMPLEMENTATION. The difference between the two
design options is that OBJECT ADAPTER does not hide the adapted interfaces, which are still accessible by the ENCAPSULATED IMPLEMENTATION,
whereas in a WRAPPER FACADE arrangement these interfaces are fully
sealed.
While a certain degree of flexibility is necessary to use an ENCAPSULATED
IMPLEMENTATION effectively in concrete applications, too much flexibility
could result in exactly the opposite: a component that is so flexible
that it is of no use at all [Bus03]. It is thus important that only
mandatory variability is supported by an ENCAPSULATED IMPLEMENTATION,
not all ¡®nice-to-have¡¯ variability. The mandatory variability for a component can be identified with the help of appropriate methods, such
as Open Implementation Analysis and Design [KLLM95], Commonality/Variability Analysis [Cope98], or Feature Modeling [CzEi02].
##%%&&
When partitioning an ENCAPSULATED IMPLEMENTATION (313) . . .
. . . at times we need to, or are able to, decompose complex component
objects into several smaller parts.
Some component objects aggregate so much functionality that it
is impractical to realize them as monolithic units. Instead, they
should be partitioned into smaller parts with defined responsibilities. However, despite the need for partitioning, clients typically
do not want to deal with such a community of smaller parts.
In particular, the component should not allow direct access to its
constituent parts, because in general they do not provide services
that are meaningful for its clients. In addition, clients should not be
affected if the partitioning of these parts is changed. On the other
hand, it is often necessary to design the parts so that they are also
usable elsewhere, which requires avoiding tight dependencies of the
parts on specific components, because each component will probably
combine the parts differently, or with other parts.
Therefore:
Partition the component object into a whole that encapsulates
and orchestrates multiple independent parts, and define an
interface for the whole that is the only means to access the
component¡¯s functionality.
The parts implement self-contained entities or mechanisms from
which higher-level functionality can be composed. The whole, in
contrast, represents an aggregate that implements a policy that uses
the functionality offered by the parts to provide the required component behavior. When a client calls a method on the whole, the
method executes its policy, using one or more parts encapsulated by
the whole to realize its behavior.
A WHOLE-PART design prevents clients from accessing any of the aggregate¡¯s constituent parts directly, thus allowing it to appear as a
self-contained semantic unit. The strict separation of policies from
mechanisms within a WHOLE-PART arrangement enables the aggregate
to combine and use its constituent parts transparently for both its
clients and parts, as well as to reuse the parts within other aggregate
components.
There are many ways to implement a particular WHOLE-PART structure.
The whole is often a MEDIATOR (410) or a FACADE (294), and the parts are
independent components by themselves. Functionality that traverses
multiple parts in a specific order can be implemented as a VISITOR
(447), which avoids this functionality being scattered across the
parts. Finally, a part can also be a WHOLE-PART arrangement by itself,
which leads to recursive WHOLE-PART structures. The data exchanged
between the whole and its parts is often encapsulated within DATA
TRANSFER OBJECTS (418), which avoids dependencies of the whole on
specific data representations, and increases the reusability of parts
in other WHOLE-PART arrangements.
Two important issues to consider when realizing a WHOLE-PART structure are the sharing and lifecycle management of parts. In many
arrangements, such as a representation of a drive in a factory process control system, parts are neither shared with other wholes, nor
are they visible from outside the arrangement. The lifecycle of the
parts is thus bound to the lifecycle of their aggregating whole¡ªthey
come into existence when the whole is created, and cease to exist
when it is destroyed. In other arrangements, parts can also live outside the whole or can be shared among several wholes, such as
attachments to an e-mail. Such parts must be able to manage their
lifecycle themselves, including all constraints that apply when being
bound to a whole or set of wholes. In environments that do not support AUTOMATED GARBAGE COLLECTION (517), COUNTING HANDLES (522) can
help in managing the lifetime of independent and shared parts.
##%%&&
When implementing an ENCAPSULATED IMPLEMENTATION (313), or when
realizing an INTERPRETER (442) configuration . . .
. . . we sometimes need to treat atomic elements and aggregate elements of a whole-part hierarchy uniformly.
Sometimes whole-part hierarchies are recursively composed of
objects that all support the same interface. However, clients are
often uninterested in either the concrete arrangement or the
recursive nature of the structure¡ªinstead they want to use, and
act on, the whole-part hierarchy as if it were a single entity.
Nevertheless, within the whole-part arrangement the hierarchical
structure must often be preserved. For example, it may represent a
corresponding hierarchy in an application domain, such as a network
or warehouse topology, or a hierarchical file system. To complicate
matters, the transparency of the hierarchy to clients should not break
if its constituent objects are rearranged, for example when moving
a directory or file in a file system structure to another directory or
volume. In addition, an extension of the hierarchy with new object
types that implement the common interface should have minimal
effect on other types in the hierarchy. For example, the hierarchy
should remain stable when extending a file system structure with a
recycle bin or a new file type.
Therefore:
Declare a component interface that specifies the functionality
common to all objects in the whole-part hierarchy, and subclass
from this interface to realize the hierarchy¡¯s specific objects and
their recursive composition.
There are two types of object in a COMPOSITE hierarchy: leaves and
composites. Leaf objects realize behavior for atomic entities that
cannot be decomposed, such as concrete file types. Composite objects
define behavior for aggregates in the hierarchy and the functionality
to maintain multiple objects that support the shared interface, such
as volumes and directories in a file system.
Clients refer to and manipulate the aggregate through the component
interface. If the interface represents a leaf, the request is executed
by the leaf¡¯s implementation. In contrast, if the interface represents
a composite, the request is forwarded to one or more of its children,
which execute the request if they are leaves, or forward the request
recursively to their own children if they are composites. In addition, the composite can perform its own activities before and after
forwarding the request to any aggregated component. Results of an
invocation on the component interface are returned back along the
recursive call chain of composites and leafs.
A COMPOSITE design supports the representation of arbitrary whole part hierarchies of objects that implement the component interface.
This hierarchy is transparent to clients: they only see objects that
support a uniform contract. In addition, the hierarchy is easy to
evolve: an extension or rearrangement of an existing COMPOSITE configuration does not require modification of existing leaves and composites. On the other hand, a COMPOSITE design is only useful if all
objects in the whole-part hierarchy implement the same functionality.
To hide the hierarchical nature of the COMPOSITE arrangement from
clients, its component interface must accumulate all methods offered
by its leaf and composite objects. The more diverse these functions
are, the more the component interface becomes bloated with functions implemented only by few leaf and composite objects, making
the interface useless for clients.
A composite typically accesses its children via an ITERATOR (298) or
an ENUMERATION METHOD, which avoids dependencies on their concrete
arrangement in the COMPOSITE structure. A VISITOR (447) can help to
implement behavior that operates on the entire COMPOSITE hierarchy,
which avoids tight coupling of the functionality to be implemented
with the strategy for traversing the object hierarchy.
##%%&&
When implementing an ENCAPSULATED IMPLEMENTATION (313) . . .
. . . we must sometimes provide increased performance, fault tolerance, or result accuracy for a component implementation.
Some components must meet high performance, fault-tolerance,
or accuracy requirements, in particular if they are responsible for
critical or complex activities, such as a monitoring service in a
nuclear power plant control system, or a service for DNA analysis
in a medical system. Addressing these requirements using more
powerful computing resources can help, but such solutions may
be too expensive, or simply insufficient.
For example, performance can be improved by optimized algorithms
and faster hardware, but do not really help for NP-hard or NP complete problems that operate on large amount of data, such as DNA
analysis. Accuracy can be improved by algorithm and code verifications, but in the case of complex algorithms, such verifications are
often barely possible, or offer the possibility of errors. Fault-tolerance
can be improved by increasing the stability and robustness of code,
but does not help in the case of failures in the component¡¯s hosting environment. Yet components are only useful if they meet their
requirements and resolve these with reasonable cost.
Therefore:
Meet the performance, fault-tolerance, or accuracy requirements
of the component via a ¡®divide and conquer¡¯ strategy. Split its
services into independent subtasks that can be executed in parallel, and combine the partial results returned by these subtasks
to provide the service¡¯s final result.
The component is partitioned into a master that implements a specific
¡®divide and conquer¡¯ strategy and serves as its access point for clients,
and at least two slaves that realize the subtasks to be executed in
parallel. When the master receives a client request, it splits the work
to be done into as many parts as slaves are available, delegates the
processing of each part to a separate slave, waits until all slaves
finish their execution, and computes a final result from the partial
results that the slaves return.
A MASTER-SLAVE design can significantly improve the performance of
long-duration services due to the parallel execution of independent
subtasks. For this reason, MASTER-SLAVE arrangements are specifically
popular and beneficial for components deployed on multi-core processors [IBM06]. Other operational qualities supported by a MASTER-SLAVE
configuration are fault tolerance, availability, and computational
accuracy. Fault tolerance and availability is supported by enabling
slaves to run as replicas, so that if one slave fails, others can continue
to process requests and return results. Computational accuracy is
supported by voting on the results of multiple slaves that all realize
the same service, often using different algorithms.
As its downside, a MASTER-SLAVE configuration is only feasible for services that can be processed using a ¡®divide and conquer¡¯ strategy with
parallel execution of subtasks. For example, if there is a high degree of
independence between the data to be processed, it can be partitioned
into multiple chunks of equal volume, and a set of slaves replicating
the service¡¯s functionality then process these chunks. Operations on
large or clustered databases fall into this category [DeGe04]. If the
data cannot be divided, it may be possible to partition the service
itself into multiple independent steps that can be executed in parallel
by a set of slaves. Many algorithms on graphs belong to this category, in particular those for routing or collecting status information.
If any form of meaningful service partitioning requires coordination
and synchronization activities between the subtasks, a MASTER-SLAVE
arrangement is probably inappropriate for addressing the component¡¯s performance, fault-tolerance, or accuracy requirements.
In a MASTER-SLAVE arrangement, the master acts as the central access
point for clients and encapsulates the component¡¯s specific ¡®divide
and conquer¡¯ strategy. This strategy is primarily determined by the
intent of the MASTER-SLAVE arrangement. If, for example, the intent is
performance, the master partitions the workload or data to process
it into as many parts of identical volume as slaves are available,
executes all slaves in parallel, and assembles the final result from
the partial results that the slaves return. If the component¡¯s intent
is to provide computational accuracy or fault tolerance, all slaves
process the entire data. A voting strategy like ¡®n of m slaves must
return the same result¡¯ helps the master to determine the final result.
A TEMPLATE METHOD (453) or STRATEGY (455) provide design support for
configuring and varying the ¡®divide and conquer¡¯ strategy without
affecting component clients, slaves, and the master¡¯s invariant core
algorithm for slave coordination.
In most MASTER-SLAVE arrangements there is only one master. However, such a design introduces a single point of failure, which cannot
always be tolerated. An alternative implementation is, therefore, to
merge the roles of master and slave into a single object and to
define a particular MASTER-SLAVE arrangement by specifying which
role a particular instance in this arrangement embodies. Using a
suitable mechanism such as heartbeats, the master regularly notifies all slaves that it is alive. If this notification is not received by
a slave, it assumes that the master failed, claims the master role,
and notifies the remaining slaves about this take-over. In cooperation with the underlying communication infrastructure, subsequent
client requests are then redirected to the new master. Alternatively,
a watchdog can relaunch the master in the case of failure.
The number of slaves in a MASTER-SLAVE configuration is largely
dependent on the amount of available computing resources such
as memory, threads, and CPUs. The more CPUs are available, the
greater the potential parallelism, and the more slaves can be assigned
to a MASTER-SLAVE arrangement.
The data exchanged between master and slaves is often encapsulated
within DATA TRANSFER OBJECTS (418) to avoid dependencies to specific
data representations. Access to slave results can be coordinated
conveniently via FUTURES (382). When a slave is invoked, it returns a
future that represents the result the slave will compute. When the
slave finishes execution, it fills the future with the corresponding
data. If the master accesses a future before the corresponding slave
has filled it, it will block until the slave¡¯s result is available.
##%%&&
When realizing an ENCAPSULATED IMPLEMENTATION (313) or a LOOKUP (495)
service . . .
. . . at times we need to ensure reduced response time when accessing
a single object from multiple address spaces.
Distributed systems often use designs in which clients access
objects of components that reside in other address spaces. However, due to the latency and jitter incurred when exchanging
requests and responses across the network, this design can be
impractical when requirements demand rapid response.
Resolving the problem via replication is not always feasible. For
example, a component may need access to information from multiple
address spaces to carry out its behavior. Obtaining this information
from each replicated component would result in significant network
load and traffic, which can decrease or even eliminate the performance advantages of the component¡¯s replication. The same effect
occurs for replicated components that represent a single stateful
entity within the distributed system. If the state of any of the replicas is modified, the state of the other replicas must be updated
accordingly across the network.
Therefore:
Divide the objects into multiple ¡®half objects,¡¯ one for each
address space in which they is used. Each half object implements
the functionality and data required by the clients that reside in
¡®its¡¯ address space. A protocol between the half objects helps to
coordinate their activities and keep their state consistent.
When a client invokes a service on the component that does not
involve information maintained in other address spaces, the corresponding half object executes this service locally in the client¡¯s
address space without involving the network at all. Otherwise the
client-local half object obtains the necessary information from the
other address spaces of the system via the protocol that connects
the distributed half objects.
A HALF-OBJECT PLUS PROTOCOL arrangement optimizes performance by
minimizing the use of the network, but at the expense of duplicating
functionality and perhaps data. All the services of the logical object
that can be executed locally within the address spaces of its clients
are executed locally by the corresponding half object. Only if multiple
address spaces need to be involved in a service execution is the
protocol that connects the half objects necessary to coordinate the
distributed computation. In general, the more functionality that can
be executed locally in the address space of clients, and the less
data need to be exchanged between the half objects to keep them
consistent, the better the performance of an HALF-OBJECT PLUS PROTOCOL
arrangement. The greater the need for distributed computation, and
the more data that needs to be exchanged via the protocol, the less
beneficial a HALF-OBJECT PLUS PROTOCOL design becomes. As a general
rule of thumb, duplication of internal state should be reduced to
minimize the need for data exchange and synchronization via the
protocol.
A side-benefit of a HALF-OBJECT PLUS PROTOCOL design is scalability. A
new half object is added to the arrangement with every new client
address space, which avoids the use of computing resources in other
address spaces.
The concrete design of the protocol between the half objects depends
on what particular coordination they need. Simple data exchange protocols can be based on an OBSERVER (405) design to avoid unnecessary
updates and coordination activities. Actions that the half objects in
the arrangement can invoke on one another can be encapsulated into
COMMANDS (412) or MESSAGES (420), to keep the protocol independent
of a specific action set.
##%%&&
When realizing an ENCAPSULATED IMPLEMENTATION (313) . . .
. . . we must sometimes support fault tolerance and high availability
for component implementations.
Some components in a system must meet high availability and
fault tolerance requirements, in particular if they execute or
coordinate central activities, such as a directory service in a
telecommunication system. Brute force solutions to this problem, however, such as complete hot or cold stand-by system
replication, are often too expensive for many applications due to
their high total cost of ownership.
In most systems only a few components are exposed to extreme
availability and fault tolerance requirements, which hardly justifies expensive hardware-based system-level solutions. Providing high
availability and fault tolerance within a low cost environment, however, is a challenge. Commodity hardware and standard networks are
sources of failure¡ªand thus unavailability¡ªin themselves, which
components with high availability and fault-tolerance requirements
must take into account explicitly: network connections and inter process communication can fail, hosts can be down. Figuratively
speaking, the components must be strong in a weak environment.
Therefore:
Provide a group of component implementations instead of a single implementation, and replicate these implementations across
different network nodes. Forward client requests on the component interface to all implementation instances, and wait until
one of the instances returns a result.
The first result returned by any of the component instances in the
group is returned to the client.
The key benefit of a REPLICATED COMPONENT GROUP is that it enhances
the fault-tolerance of a component: as long as at least one of the
component instances within the group is accessible, client requests
can be serviced. A REPLICATED COMPONENT GROUP also supports availability, because if a component instance in the group cannot execute
a request immediately due to high workload, other, less overloaded
object implementations can service the request instead.
One downside of a REPLICATED COMPONENT GROUP configuration is that it
requires all component instances to maintain consistent state, which
can lead to ¡®chatty¡¯ communication, resulting in heavy network traffic.
The more state there is to maintain, and the higher its rate of change,
the more networking overhead is necessary to keep the group state
consistent. In addition, a request to a REPLICATED COMPONENT GROUP is
executed by all its constituent component implementations, which
can consume a significant amount of computing resources. As a
result a REPLICATED COMPONENT GROUP design should only be considered
for components that play a central role in a distributed system and
whose availability and fault tolerance is crucial for the system¡¯s
operability.
A BUSINESS DELEGATE (292) that serves as the component¡¯s interface
helps to keep a concrete REPLICATED COMPONENT GROUP transparent to its
clients. Additional LOOKUP (495) functionality allows component implementations to join and leave the group dynamically, which enables
runtime redeployment of group members without compromising the
component¡¯s availability.
The protocol for keeping the state of instances in the group consistent
is often based on an OBSERVER (405) design, which avoids unnecessary
updates and coordination activities. Requests that the arrangement
should execute can be encapsulated into COMMANDS (412) or MESSAGES
(420), to keep the communication within the group independent of a
specific request set.
##%%&&
When developing a MODEL-VIEW-CONTROLLER (188) architecture where
the view is remote from the model . . .
. . . we may need a mechanism to consolidate the handling and execution of service requests issued by a specific form in a form-based
user interface.
Some applications offer form-based user interfaces, with each
form invoking a coherent, interrelated set of application functions. Encapsulating the handling of each type of client request
in a separate controller, however, could complicate controller
implementations and duplicate functionality that is common to
handling the requests issued from a specific form.
For example, in Web applications with static HTML pages, some pages
handle the input and processing of specific data records by taking
the input, loading data from the database if the record already exists,
and checking the integrity of the record¡¯s data before processing
it in the application. Similarly, results of data processing may be
displayed to users. There could also be common constraints on the
execution of specific functions, such as security, logging, and other
housekeeping functionality. A strict encapsulation of each type of
function offered by a page into a separate controller, however, would
ignore their interdependencies and common execution constraints,
yielding complicated and redundant code that is hard to maintain
and evolve.
Therefore:
For each form offered by an application¡¯s user interface, intro duce a page controller to control the execution of all requests
issued from that form.
All client requests from a specific form are channelled through
their associated page controller. The page controller transforms each
request into a more specific request on the application¡¯s components, for example by extracting information from the client request
parameters. The page controller also performs any housekeeping
functionality associated with requests issued from the form. In addition, if the form includes a small and form-local workflow, the page
controller can maintain the necessary session state information.
A PAGE CONTROLLER centralizes access to an application¡¯s domain services from a specific user interface form, which avoids duplication
of functionality associated with handling an individual client request
issued through that form. As a result, common request-handling
code becomes easier to maintain and evolve. In addition, the request
handling of different forms is separated, which supports form-based
request handling strategies and rearranging the forms within another
user-level workflow.
Request handling functionality that follows a common core algorithm with variant policies can be realized with TEMPLATE METHOD (453)
or STRATEGY (455). The functionality performed by a page controller
results in a service request on the application¡¯s components. These
requests are often realized as COMMANDS (412) that support the managed execution of the requests on the application logic, as well as
other features such as scheduling, logging, and undo/redo.
There are two general deployment options for a page controller: per client and per-application. Per-client deployment is more scalable,
but requires additional resources on the client side. Per-application
deployment, in contrast, helps to minimize client footprint. A page
controller shared by multiple clients must usually be synchronized.
Providing it with a THREAD-SAFE INTERFACE (384) is a simple yet coarse grained synchronization option, because it enforces synchronization
at the page controller¡¯s interface, even if only small portions of
its methods are critical sections. Synchronization via STRATEGIZED
LOCKING (388) may be an alternative if finer levels of serialization are
required.
##%%&&
When developing a MODEL-VIEW-CONTROLLER (188) architecture where
the view is remote from the model . . .
. . . we often need a mechanism to consolidate the handling and
execution of service requests issued to an application.
When handling a request, networked applications often perform
similar actions, including pre- and post-processing actions such
as authorization and logging, and context-specific behavior such
as providing particular views for specific users. Implementing
this transformation functionality within each controller of an
application, however, can duplicate code and thus complicate
maintenance and evolution.
As the number of controllers grow, it also becomes hard to identify
all the sites where code is duplicated. As a result, the complexity of
modifying each duplicate code fragment correctly opens a floodgate
to incorrect and inconsistent application behavior. In addition, the
memory footprint of the application, especially its controllers, can
increase significantly as the transformation from general to specific
requests becomes more complicated. This problem becomes even
worse if common infrastructure functionality such as authorization
and logging must be performed with each request.
Therefore:
Introduce a front controller that publishes the application¡¯s functionality and transforms client service requests into specific
requests that can be invoked on the application¡¯s components.
All client requests are channelled through the front controller. This
controller transforms each request into a more specific request to
the application¡¯s components, for example by extracting information
from the client request parameters.
A FRONT CONTROLLER centralizes access to an application¡¯s domain services, which avoids duplicated implementations of functionality associated with handling individual client requests. As a result, not only
does the common request handling code become easier to maintain
and evolve, but the application memory footprint is also minimized. In
addition, a FRONT CONTROLLER can provide common housekeeping functionality, such as authorization and logging. One downside of a FRONT
CONTROLLER is that its centralized design can create a performance and
scalability bottleneck if many clients requests arrive simultaneously.
Similarly, it can be a single point of failure in an application.
Common request transformation and housekeeping functionality can
be implemented via TEMPLATE METHOD (453) or STRATEGY (455). Alternatively, if a specific housekeeping or transformation feature is completely optional, it can be added to the core request transformation
functionality via a DECORATOR (449), also known as INTERCEPTING FILTER
[ACM01]. The transformation performed by a front controller results
in a concrete service request to the application¡¯s components. These
requests are often encapsulated in COMMAND (412) objects that support the managed execution of the requests on the application logic,
as well as other features such as scheduling, logging, and undo/redo.
There are two general deployment options for a front controller: per client and per-application. Per-client deployment alleviates the performance, scalability, and failure penalties of a front controller, but
requires additional client resources, such as memory and CPU time.
Per-application deployment, in contrast, helps minimize the footprint
of clients, but at the expense of the drawbacks outlined above.
A front controller shared by multiple clients must often be synchronized. Providing it with a THREAD-SAFE INTERFACE (384) is a coarse grained synchronization option that enforces synchronization at the
front controller¡¯s interface. If only small portions of the front controller¡¯s methods are critical sections, synchronization via STRATEGIZED
LOCKING (388) provides a finer level of serialization. The COMMAND (412)
objects, however, are created per request and are not shared, so they
need not offer thread-safe interfaces.
##%%&&
When developing a MODEL-VIEW-CONTROLLER (188) architecture where
the view is remote from the model . . .
. . . we must often provide an access point for handling user interface
navigation and the workflow of an application.
Some applications lead their users through a series of screens or
forms following a specific workflow, or present specific screens
or forms only under certain conditions. Placing such logic in
the application¡¯s controllers, however, mixes user-interface code
with application-specific workflow logic.
Moreover, different controllers could instigate the same workflow,
which would lead to duplicated logic that is hard to maintain and
evolve. Another approach is to implement the logic of the screen
or form to display next in response to a specific action directly
within the application logic. This approach is not practical, however,
since application components, which are generally independent of
presentation aspects, would become dependent on the partitioning
and screen ordering of a specific user interface.
Therefore:
Encapsulate the application¡¯s workflow within a separate application controller. User-interface controllers use the application
controller to determine the appropriate actions to invoke on
application logic, as well as the correct view to display after the
action has been executed.
The application controller acts as a central access point for user interface elements, unifying access to the functionality of an
application with its workflow. When invoked by an user-interface
controller, the application controller identifies the correct function to
execute on the application. The function selected depends both on the
input received from the controller and on the application controller¡¯s
current workflow state. Similarly, after the invoked function has been
executed, the workflow transitions into a new state, which allows the
application controller to determine the specific view to display in the
user interface in response to the executed function.
An APPLICATION CONTROLLER helps to encapsulate parts of an application
where user-interface aspects and domain-logic aspects are interwoven. As a result, the core domain logic of the application remains
independent of any user-interface considerations, such as its structure, and user-interface controllers and views are independent of
the workflow and state of the application logic. If the application¡¯s
workflow or the user-interface structure changes, the corresponding modifications to the application¡¯s code are limited largely to the
application controller. In a distributed application, the liabilities of
an APPLICATION CONTROLLER design include potential performance and
scalability bottlenecks, due to its central role within an application.
It can also be a single point of failure.
An application controller typically transforms requests received from
a user-interface controller into a concrete COMMAND (412) object.
COMMAND supports the managed execution of the requests to the application logic, as well as other features, such as scheduling, logging,
and undo/redo.
There are two deployment options for an application controller. Per client deployment reduces the performance, scalability, and failure
penalties discussed above, but requires additional client resources.
Per-application deployment, in contrast, helps minimize the footprint
of clients, but at the expense of the drawbacks described above.
An application controller shared by multiple clients must often be
synchronized. Providing it with a THREAD-SAFE INTERFACE (384) is a
coarse-grained synchronization option that enforces synchronization
at the application controller¡¯s interface. If only small portions of its
methods are critical sections, synchronization via STRATEGIZED LOCKING
(388) provides a finer level of serialization.
##%%&&
When developing a MODEL-VIEW-CONTROLLER (188) architecture or an
ACTIVE OBJECT (365), or when encapsulating service requests for an
application into COMMANDS (412) or MESSAGES (420) . . .
. . . we need a mechanism for executing service requests.
If an application can receive requests from multiple clients,
they may need to manage the execution of these requests, for
example to handle request scheduling, logging, and undo/redo.
An individual client, however, generally has no knowledge about
when and under what conditions its requests execute.
Allowing a client to obtain this information from an application would
increase its logical and physical complexity [Lak95], with corresponding degradation in modularity, extensibility, and understandability.
A client should therefore be able to issue requests without needing to know the conditions under which the requests execute. To
complicate matters, many distributed applications must also support
additional housekeeping and system management functionality, such
as logging, authorization, and multiple undo/redo. Neither application clients nor application components should be responsible for
these tasks. Making the clients responsible would increase their
coupling to the component and reduce client cohesion., while making
the components responsible would clutter their intent with additional
complexity, thereby complicating future development.
Therefore:
Introduce a command processor to execute requests to the application. The command processor acts on behalf of the clients and
within the constraints of the application.
The command processor is the only means for clients to issue requests
to the application. In addition, the command processor can access
information about the computational state of its components, so it
can schedule the execution sequence of pending service requests
in terms of specific criteria such as priorities and throughput. The
command processor can also offer useful housekeeping and system
management functionality, such as history-based functionality and
persistence.
Since a COMMAND PROCESSOR manages the functionality of a component,
the component¡¯s clients and the component itself are freed from
organizing the execution of concrete service requests. As a result the
degree of coupling between the two parties is minimized.
Removing the responsibility of invoking services on the component
from its clients implies that the clients cannot call the component¡¯s
functionality directly. Instead, they send ¡®objectified¡¯ requests, such
as COMMANDS (412) or MESSAGES (420), to the command processor,
which then executes the requests on the component. This request
reification is the key to handling requests explicitly and centrally
within the command processor.
Internally, the command processor can manage the requests it
receives via multiple COLLECTIONS FOR STATES (471). For example, a
do collection can hold all requests that have yet to execute, as well
as all requests that have been undone and can now be redone. An
undo collection can maintain all requests that have been executed
on the component that can be rolled back. If the command processor receives requests as COMMAND objects, it can simply invoke these
objects to execute the encapsulated requests on the component. If
requests are received as MESSAGES, an INTERPRETER (442) can help to
transform their content into COMMANDS.
Scheduling and other housekeeping functionality, such as logging
and authorization, can be realized and configured via TEMPLATE METHOD
(453), STRATEGY (455), or INTERCEPTOR (442). These patterns allow the
configuration of a command processor with different request execution policies, considering different trade-off options in terms of
binding time and looseness of coupling. If the command processor is
shared by multiple clients it should be implemented using MONITOR
OBJECT (368). STRATEGIZED LOCKING (388) offers another dimension of
configuration for synchronization.
##%%&&
When specifying a MODEL-VIEW-CONTROLLER (188) architecture . . .
. . . we often need to render application data or other information into
a predefined view format.
Many views onto an application present dynamic content, such
as the results of database queries, so their appearance can change
with each display and update. Providing separate view implementations for each possible appearance of the view, however,
inflates the number of views and duplicates code.
Ideally a view whose content can vary should be designed and
implemented similarly to static views¡ªthat is, views whose content,
structure, and appearance is fixed and therefore simple to develop.
Variations in a view¡¯s appearance, however, suggest multiple implementations. In contrast, the potential variations are not arbitrary, so
there should be a way to handle such bounded variation within a
single implementation.
Therefore:
Introduce a template view that predefines the view¡¯s structure
and which contains placeholders for dynamic application data.
When the view is invoked to display or update itself, the placeholders
for application data are filled in, for example as a result of some
computation or of database queries. The view is displayed to the user
after the application data to display has been determined.
A TEMPLATE VIEW simplifies the development of views that follow a
common structure and format but whose content is dynamic. A
single class encapsulates the structure of the view, the code to access
and compute the content, and the code to display that content at the
appropriate places in the view.
Popular forms of TEMPLATE VIEW are server pages, such as ASP.NET,
JSP, and PHP-based page generation. These technologies actually go
further than a pure TEMPLATE VIEW, because they allow embedding of
arbitrary active content, i.e. programming logic. Using that capability
carelessly, however, can yield template views that contain excessive
application logic, which contradicts the goal of having views that
only display information. This problem can be resolved by inserting
a helper class between the view and the application¡¯s components
that contains the necessary programming logic [Fow03a]. The same
problem can occur if the template view contains conditional display
and iterations. Although such constructs cannot always be avoided,
their use should be minimized, and if possible factored out to the
helper class associated with the view [Fow03a].
##%%&&
When specifying a MODEL-VIEW-CONTROLLER (188) architecture . . .
. . . we often need to transform the data received from the application
in response to user requests into specific views onto the data.
Many views onto an application present content collected from
different and often complex data structure returned by application services. The data structures, however, generally contain no
knowledge about what specific data fields should be presented
and how that data should be formatted.
Implementing the inspection of the data and its transformation into
a specific output format within the views themselves is a clumsy
solution to the problem, because it mixes application logic with user interface code. The clear separation of application logic and user
interface would be blurred. In addition, the larger and more complex the data inspection and transformation code, the less suitable
the views become for low-footprint thin clients, which is often a
requirement for Web applications.
Therefore:
Introduce a transform view that walks the structure of the data
received from the application, recognizes the data to display,
and transforms it into a specific output format.
A transform view is an adapter that converts a specific type of input
into a specific type of output. A controller can pass the data it receives
in response to a service request for the application to the transform
view and receive the concrete view to display as a result.
A TRANSFORM VIEW helps to simplify the development of views that
must be assembled from complex data sets. Application logic for data
inspection is clearly separated from specific user-interface technologies and view-specific data-rendering code, which also allows these
two aspects to vary independently. A TRANSFORM VIEW is also a good
candidate for being a pluggable component, thereby affording a high
degree of flexibility in application configuration.
The application data converted by a TRANSFORM VIEW is typically encapsulated inside DATA TRANSFER OBJECTS (418) to keep the transform view
independent of concrete data structures. To simplify the programming of a specific transformation, the data transfer objects could
serialize their content into a standard representation, such as XML.
This design also supports the use of popular data transformation
technologies such as XSLT.
The downside, however, is that transformation logic is code in its
own right, and metacode at that, and does not necessarily have the
tool support of more direct approaches such as TEMPLATE VIEW. It
can therefore be more general than is strictly necessary, and the
subtlety of testing or debugging a transformation may dissuade some
developers from using it.
##%%&&
When designing the PROXY-based (290) interfaces of an application
that is accessible via the Internet or other public networks . . .
. . . we must protect components from external attacks.
An application that is accessible via a public network such as the
Internet has little reliable knowledge about who is using it, since
virtually anybody can invoke its functionality. There is a need,
however, to protect the application from potential cyber attacks
embedded in service requests from clients.
Protecting publicly accessible applications against cyber attacks is
hard. On one hand such applications should be available for public
access, so we cannot require that all users be known by the application before they can access it. At least some services¡ªa home
page, a registration page, or basic query functionality¡ªshould be
available without already being a registered user. On the other hand,
if application services are publicly accessible, cyber attacks can be
embedded within the payload of service requests. Depending on the
security policies for the application, potential cyber attacks should
be identified and access to the application denied.
Therefore:
Introduce a firewall proxy for the publicly accessible functionality of the application. This proxy enforces security policies on
each client request to protect the components that implement
this functionality from cyber attacks.
An external client can access application functionality only via the
firewall proxy. When a client issues a request to that proxy, the
proxy checks the request against applicable access rules, for example
by inspecting the payload of the request message. If the request is
accepted, the firewall proxy forwards the request to the protected
component, otherwise it denies access to the client¡¯s request.
A FIREWALL PROXY supports the detailed filtering of service requests for
components on a packet and payload level, which helps to identify
cyber attacks at a higher level than that of networking protocols.
In addition, security checks are separated from the components,
which allows clients internal to the application or behind the firewall
to access components directly, without incurring the performance
overhead of security checks.
The security policy applied by a firewall proxy is often captured
in a rule base that is separate from the proxy itself. This design
supports variation in security policies without modifying the proxy
implementation. As with any security mechanism based on denial of
service for access identified as malicious, however, there is the risk of
misidentification. The perceived effectiveness of a firewall proxy thus
depends heavily on the quality of the rule base.
Implementing the firewall proxy as a PACKET FILTER FIREWALL [SFHBS06]
supports request forwarding from trusted clients directly to the components, without inspecting request payloads. An implementation as
a STATEFUL FIREWALL [SFHBS06] avoids payload inspections for requests
received from already established and checked connections. Both
designs can improve external client quality of service. Configuring
the firewall proxy as a REVERSE PROTECTION PROXY [SFHBS06] adds
another security barrier for external clients that limits the accessible functionality of protected components regardless of who sends
requests, over which connection they are received, or the cleanliness
of a request payload.
Finally, the firewall proxy can also act as an INTEGRATION REVERSE
PROXY [SFHBS06] if its published functionality is realized by multiple
components deployed to multiple servers. This design helps hide the
application deployment model and the network topology from external
clients and potential attackers.
##%%&&
When realizing a CLIENT PROXY (240) or an EXPLICIT INTERFACE (281) . . .
. . . we must ensure that only specific clients can access the functionality of a subsystem.
A subsystem must provide well-defined and meaningful functionality to its clients, otherwise it is of little value to them. Clients
can invoke this functionality by sending service requests. Not
all clients that potentially can send requests to the subsystem,
however, may be entitled to invoke its functionality.
Subsystems that maintain sensitive information, such as medical or
financial data, must be protected from unauthorized access. Not all
users of an application or internal clients of a subsystem should be
allowed to access or manipulate this information, otherwise the confidentiality, integrity, or availability of the data could be endangered.
The same situation holds for components that offer ¡®manipulation¡¯
functionality, such as administration and configuration tasks. Hard coding information about the ¡®trusted¡¯ clients directly within the
components is a clumsy and high-maintenance solution, however,
because significant effort and cost would be required to redeploy the
components in multiple applications with different clients, change
the set of authorized clients, or evolve the authorization rights of
clients.
Therefore:
Assign access rights to each client that can send service requests
to the security-sensitive subsystem and check these rights before
executing any requests on the subsystem.
Access rights specify the operations or categories of operations clients
are allowed to invoke on the subsystem. If a client issues a service
request to the subsystem, its access rights are checked. If they have
sufficient rights, the requested service is executed, otherwise their
access to the subsystem is denied.
The key benefit of AUTHORIZATION is that subsystems can only be
accessed by authorized clients, which is important for security sensitive applications. A secondary benefit is that specific access
rights, as well the policies for checking them, can evolve transparently
for both clients and components of an application.
A common form of implementation for an AUTHORIZATION infrastructure is ROLE-BASED ACCESS CONTROL [SFHBS06]. Instead of assigning
individual access rights to each client, there is a defined set of user
roles in an application, such as administrator, power user, user, and
guest, with specific access rights associated with each role. Users and
client components must be assigned to, or act under the perspective
of, one or more roles. Depending on the access rights of these roles
they can¡ªor cannot¡ªaccess the application¡¯s components and their
functionality. ROLE-BASED ACCESS CONTROL simplifies the realization and
evolution of an AUTHORIZATION infrastructure by limiting the number of
specified access rights specified to a small set of roles. In addition, a
ROLE-BASED ACCESS CONTROL design can enforce structures and policies
of the organization that owns or hosts the application.
A REFERENCE MONITOR [SFHBS06] complements an AUTHORIZATION structure with a defined access right decision and enforcement point.
All client requests are intercepted by a central reference monitor to
check their access rights for compliance with the application¡¯s authorization rules. This design strictly enforces an application¡¯s security
policies by checking all client service requests. A reference monitor
avoids code duplication and simplifies maintenance by centralizing
the checking of access rights for each component that requires authorized access. Multiple reference monitor instances can be deployed in
a system to minimize performance or scalability bottlenecks or single
points of failure.
##%%&&
When developing concurrent software, specifically a concurrent
ENCAPSULATED IMPLEMENTATION (313) or a network server that employs
a REACTOR (259) or PROACTOR (262) event handling infrastructure . . .
. . . we need to make performance efficient and scalable while ensuring
that any use of concurrency simplifies programming.
Concurrent software often performs both asynchronous and synchronous service processing. Asynchrony is used to process
low-level system services efficiently, synchrony to simplify application service processing. To benefit from both programming
models, however, it is essential to coordinate asynchronous and
synchronous service processing efficiently.
Asynchronous and synchronous service processing is usually interrelated. For example, the I/O layer of Web servers often uses asynchronous read operations to obtain HTTP GET requests [HPS97].
Conversely, the processing of the GET requests at the CGI layer often
runs synchronously in separate threads of control. The asynchronous
arrival of requests at the I/O layer must therefore be integrated
with synchronous processing of the requests at the CGI layer. From
a different point of view, similar observations can be made about
AJAX¡ªthe use of asynchronous JavaScript and XML to increase the
perceived responsiveness of Web clients [Gar05]. In general, asynchronous and synchronous services should cooperate effectively and
be encapsulated against either other¡¯s deficiencies.
Therefore:
Decompose the services of concurrent software into two separated layers¡ªsynchronous and asynchronous¡ªand add a queueing layer to mediate communication between them.
Process higher-level services, such as domain functionality, database
queries, or file transfers, synchronously in separate threads or processes. Conversely, process lower-level system services, such as
short-lived protocol handlers driven by interrupts from network hard ware, asynchronously. If services in the synchronous layer must
communicate with services in the asynchronous layer, have them
exchange messages via a queueing layer.
A HALF-SYNC/HALF-ASYNC design enforces a strict separation of concerns between the three layers, which makes concurrent software
easier to understand, debug, and evolve. In addition, asynchronous
and synchronous services do not suffer from each other¡¯s liabilities: asynchronous service performance does not degrade due to
blocking synchronous services, and the simplicity of programming
synchronous services is unaffected by asynchronous complexities
such as explicit state management. Finally, using a queueing layer
avoids hard-coded dependencies between the asynchronous and synchronous service layers, as well as making it easy to reprioritize the
order in which messages are processed. The strict decoupling of the
asynchronous layer from the synchronous layer, however, requires
that data exchanged between the two layers must be either communicated as COPIED VALUES, which can introduce performance penalties
and resource management overhead the more data there is to pass,
or represented as IMMUTABLE VALUES, which are lighter in weight but
perhaps more intricate to construct.
In general, a HALF-SYNC/HALF-ASYNC arrangement employs LAYERS (185)
to keep its three distinct execution and communication models independent and encapsulated.
Services in the synchronous layer, such as database queries, file
transfers, or domain functionality, generally run in their own threads,
which allows concurrent execution of multiple services. If realized as
an ACTIVE OBJECT (365) a service can also handle multiple service
requests simultaneously, which in turn can improve the performance
and throughput of an application.
Services in the asynchronous layer can be realized with the help
of asynchronous interrupts or operating system APIs that support asynchronous I/O, for example Windows overlapped I/O and
I/O completion ports [Sol98], or the POSIX aio * family of asynchronous I/O system calls [POSIX95]. WRAPPER FACADES (459) help to
encapsulate platform-specific asynchronous I/O functions behind
a uniform interface, which simplifies their correct use and the
portability of the asynchronous layer to another operating system.
Alternatively, if a HALF-SYNC/HALF-ASYNC arrangement is designed in
conjunction with a PROACTOR or REACTOR event-handling infrastructure, this event-handling infrastructure is the asynchronous layer.
Although a REACTOR is not truly asynchronous, it shares key properties
with asynchrony if its services implement short-duration operations
that do not block for long periods of time.
The queueing layer often consists of a message queue shared by
all services in the synchronous and asynchronous layers. Sophisticated queueing layers can provide multiple message queues, for
example one message queue for every message priority or communication endpoint. Message queues can be implemented as MONITOR
OBJECTS (368) to serialize access to the message queues transparently
for asynchronous and synchronous services. TEMPLATE METHODS (453)
and STRATEGIES (455) support the setting of various aspects of the
message queues, for example configuring their behavior for ordering,
serialization, notification, and flow control. STRATEGIES are the more
flexible option, offering loose coupling and runtime configuration and
re-configuration of the message queues. TEMPLATE METHODS can be
appropriate if only compile-time flexibility is needed. The information
routed by the queueing layer is encapsulated within MESSAGES (420).
##%%&&
When developing concurrent software, specifically a concurrent
ENCAPSULATED IMPLEMENTATION (313) or a network server that employs
a REACTOR (259) event-handling infrastructure . . .
. . . we must often react on and process multiple events from multiple
event sources both concurrently and efficiently.
Most event-driven software uses multi-threading to process multiple events concurrently. It is surprisingly hard, however, to
allocate work to threads in an efficient, predictable, and simple
manner.
In event-driven software, particularly server software, it is often necessary to define efficient demultiplexing associations between threads
and event sources. It is also necessary to prevent race conditions if
multiple threads demultiplex events on a shared set of event sources.
For example, a Web server may use multiple threads to service multiple GET requests scalably on multiple I/O handles. Some methods
of associating threads and event sources are inefficient because they
incur high levels of overhead. For example, creating a thread for each
request, or dedicating a separate thread for each event source, can
be inefficient due to scalability limitations of operating systems and
hardware. What is needed is an architecture for concurrent reactive
software that is both easy to use and efficient.
Therefore:
Use a pre-allocated pool of threads to coordinate the detection,
demultiplexing, dispatching, and processing of events. In this
pool only one thread at a time¡ªthe leader¡ªmay wait for an
event on a set of shared event sources. When an event arrives,
the leader promotes another thread in the pool to become the
new leader and then processes the event concurrently.
While the leader is listening on the event sources for an event to
occur, other threads¡ªthe followers¡ªcan queue up and sleep until
they are promoted to be the leader. When the current leader thread
detects an event from the event sources it does two things. It first
promotes a follower thread to become the new leader, then it morphs
itself into a processor thread that demultiplexes and dispatches the
event to a designated event handler that runs in the same thread that
received the event. Multiple processing threads can handle events
concurrently while the current leader thread waits for new events
to occur on the shared event sources. After handling its event, a
processing thread reverts to the follower role and sleeps until it
becomes the leader again.
By pre-allocating a pool of threads, a LEADER/FOLLOWERS design avoids
the overhead of dynamic thread creation and deletion. Having threads
in the pool self-organize and not exchange data between themselves
also minimizes the overhead of context switching, synchronization,
data movement, and dynamic memory management. Moreover, letting
the leader thread perform the promotion of the next follower prevents
performance bottlenecks arising from having a centralized manager
make the promotion decisions.
The price to pay for such performance optimizations is limited
applicability. A LEADER/FOLLOWERS configuration only pays off for
short-duration, atomic, repetitive, and event-based actions, such
as receiving and dispatching network events or storing high-volume
data records in a database. The more services the event handlers
offer, the larger they are in size, while the longer they need to
execute a request, the more resources a thread in the pool occupies and the more threads are needed in the pool. Correspondingly
fewer resources are available for other functionality in the application, which can have a negative impact on the application¡¯s overall
performance, throughput, scalability, and availability.
In most LEADER/FOLLOWERS designs the shared event sources are
encapsulated within a dispatcher component. If a LEADER/FOLLOWERS
arrangement is designed in conjunction with a REACTOR event-handling
infrastructure, its reactor component is the dispatcher. Encapsulating the event sources separates the event demultiplexing and
dispatching mechanism from the event handlers. Providing the dispatcher with methods for deactivating and reactivating a specific
event source avoids race conditions if a new leader thread is selected
simultaneously with completion of processing of the most recent
event.
Specify the threads as a RESOURCE POOL (503), and use a MONITOR
OBJECT (368) to maintain the dispatcher and synchronize access to the
shared event sources. This design enhances performance by using
a self-organizing concurrency model that avoids the overhead of a
separate queueing layer between event sources and event handlers.
Inside the thread pool the monitor object offer two methods to its
threads. A join method allows newly initialized threads to join the
pool. The joining thread first waits to become the new leader by
suspending its own execution on the thread pool¡¯s monitor condition.
After it becomes the leader, it accesses the shared event sources
to wait for and process an incoming event. A promote new leader
method allows the current leader thread to promote a new leader by
notifying a sleeping follower via the thread pool¡¯s monitor condition.
The notified follower resumes execution of the thread pool¡¯s join
method and accesses the shared event sources to wait for the next
event to occur.
Multiple promotion protocols, such as last-in/first-out, first-in/first out, and highest priority, can be supported via TEMPLATE METHODS (453)
and STRATEGIES (455).
##%%&&
When developing an ENCAPSULATED IMPLEMENTATION (313), the synchronous service layer in a HALF-SYNC/HALF-ASYNC (359) architecture,
or service handlers in an ACCEPTOR-CONNECTOR (265) configuration . . .
. . . we must often ensure that the operations of components can run
concurrently within their own threads of control.
Concurrency can improve software quality of service, for example
by allowing components to process multiple client requests
simultaneously without blocking. Developers, however, must
decide how to express the units of concurrency in their software
and how to interact with them as they run.
In particular, clients should be able to issue requests on components without blocking until the requests execute. It should also be
possible to schedule the execution of client requests according to specific criteria, such as request priorities or deadlines. To keep service
requests independent, they should be serialized and scheduled transparently to the component and its clients, thereby enabling the reuse
of software implementations that require different synchronization
strategies.
Therefore:
Define the units of concurrency to be service requests on components, and run service requests on a component in a different
thread from the requesting client thread. Enable the client and
component to interact asynchronously to produce and consume
service results.
Clients can initiate a service request on the component by calling a
method on its interface, which is exposed to the clients¡¯ thread(s).
Design the component¡¯s interface without subjecting it to synchronization constraints and return control immediately to clients after
they issue their requests. In addition, objectify the requests, pass
them to the component implementation running in one or more separate threads, and let the implementation schedule the execution of
the requests independently of the point in time at which they were
initiated. Provide a way for the component to return results to the
client when the service is complete.
An ACTIVE OBJECT design enhances concurrency in an application by
allowing clients threads and the execution of service requests to run
simultaneously: clients are not blocked while their service requests
are executed. In addition, synchronization complexity is reduced by
using a scheduler that evaluates synchronization constraints to serialize access to the component¡¯s implementation. The separation of
request scheduling from the actual component implementation also
supports the reuse of the component in scenarios that do not require
synchronization, as well as enhancing legacy components that are
not designed for concurrent access to be adapted for use in a concur rent application. Finally, the order of service request execution can
differ from service request invocation, which can better account for
priorities, deadlines, and other synchronization constraints¡ªbut at
the cost of complicated debugging. An ACTIVE OBJECT arrangement also
introduces a heavyweight request handling and execution infrastructure, which can cause performance penalties for components that
only implement short-duration methods.
Use an EXPLICIT INTERFACE (281) to expose the component¡¯s interface to
client threads. This design allows clients to access the component is
if it were collocated in their own thread. Design the interface so that
its method signatures do not include synchronization parameters.
Clients therefore appear to have an exclusive access to the concurrent
component even if it shared by multiple client threads.
At runtime, let the component interface objectify all method invocations into service requests, which are typically realized as COMMANDS
(435) that convey the necessary synchronization constraints of
their corresponding method invocations. This request objectification
decouples service requests from service execution in time and space,
so that each client can invoke services on the component without
blocking itself or other clients. Store the created service requests into
a shared activation list that maintains all pending service requests
on the concurrent component. Implementing the activation list as a
MONITOR OBJECT (368) helps to ensure thread-safe concurrent access.
One or more threads host the component¡¯s implementation. Within
each thread, a servant implements the component¡¯s functionality. A
scheduler dequeues pending service request objects from the shared
activation list and executes them on the servants. Such a design
allows service requests and executions to run concurrently, that is,
service requests are invoked in client threads, while service executions run in different threads. In addition, the scheduler separates
component functionality from scheduling and synchronization mechanisms, which supports independent realization and evolution of
both concerns.
Design the scheduler as a COMMAND PROCESSOR (343) that implements
the component¡¯s event loop. This monitors the activation list to identify service requests that become executable, removes these requests
from the activation list, and executes them on their servant. TEMPLATE
METHODS (453) and STRATEGIES (455) can support multiple scheduling
policies within the scheduler. TEMPLATE METHODS are most appropriate if the configuration of the scheduler is possible at compile time.
STRATEGIES, in contrast, support runtime configuration and reconfiguration of scheduling policies.
Clients can obtain the result of a service request on the concurrent
component via a FUTURE (382). The interface of the concurrent component returns the future to the client after the service¡¯s invocation,
while the associated service request fills the future after the servant
has finished with the service¡¯s execution. If the client accesses the
future before it contains the service¡¯s result, the client can block or
poll until the result is available. When futures are no longer needed
they can be reclaimed safely via AUTOMATED GARBAGE COLLECTION (517)
if this strategy is supported by the programming language, or via a
COUNTING HANDLE (522) if the reclaim must be coded manually.
##%%&&
When developing a SHARED REPOSITORY (202) architecture, a REQUESTOR
(242), CLIENT REQUEST HANDLER (246), MESSAGE CHANNEL (224), MESSAGE
ROUTER (231) distribution infrastructure, ENCAPSULATED IMPLEMENTATION
(313), ACCEPTOR-CONNECTOR (265) arrangement, HALF-SYNC/HALF-ASYNC
(359), LEADER/FOLLOWERS (362), or ACTIVE OBJECT (368) concurrency
model . . .
. . . we must consider that objects can be shared between threads.
Concurrent software often contains objects whose methods are
invoked by multiple client threads. To protect the internal state
of shared objects, it is necessary to synchronize and schedule client access to them. To simplify programming, however,
clients should not need to distinguish programmatically between
accessing shared and non-shared components.
Instead, each object accessed by multiple client threads should
ensure that its methods are serialized transparently without requiring explicit client intervention. To ensure the quality of service of its
clients, a shared object should also relinquish its thread of control
voluntarily if any of its methods must block during execution, leaving the component in a stable state so that other client threads can
access it safely.
Therefore:
Execute a shared object in each of its client threads, and let it
self-coordinate a serialized, yet interleaved, execution sequence.
Access the shared object only through synchronized methods
that allow execution of only one method at a time.
Each monitor object contains a monitor lock that it uses to serialize
access to the object¡¯s state. Within a synchronized method, first
acquire the monitor lock to ensure no other synchronized methods
can execute. Once the lock is held, evaluate whether the shared
object¡¯s current state allows the synchronized method to run. If it
does, execute it, otherwise suspend the execution of the synchronized
method on a condition. If called, a monitor condition should suspend
the thread of its caller until it is notified to wake it. When suspending
a thread, the monitor condition should also release the monitor lock,
and when resuming this thread, re-acquire the monitor lock.
Suspending a synchronized method allows other client threads to
access the shared object via its synchronized methods. Any synchronized method that executes, completing execution, may affect the
validity of monitor conditions, in which case it should notify the corresponding monitor condition so that suspended method invocations
can resume execution. Before terminating a synchronized method,
release the monitor lock so that other synchronized methods called
by other threads can execute.
   
Designing a shared object type as a MONITOR OBJECT simplifies concurrency control by sharing the object among cooperating threads and
combining state synchronization with method invocation. A MONITOR
OBJECT also helps in implementing a cooperative method execution
sequence that ensures the availability of the shared object to its
clients, and maximizes its availability within the constraints of serialization, to ensure that state changes are complete and free of race
conditions. A liability of this pattern, however, is that it couples
domain functionality tightly with synchronization aspects. It can be
hard to compose or nest monitor objects without risking deadlock,
for example, if one monitor object makes callbacks into objects that
in turn use other monitor objects.
A monitor object can use a THREAD-SAFE INTERFACE (384) to decouple
synchronization from its functionality. Such a design also allows both
concerns to vary independently. GUARDED SUSPENSION (380) can be used
to coordinate threads running in the object. The execution of methods
is scheduled via monitor conditions and monitor locks that determine
the circumstances under which they should suspend or resume their
execution and that of collaborating components.
##%%&&
When implementing a MONITOR OBJECT (368), a FUTURE (382), or another
component that is shared between multiple threads . . .
. . . we must schedule the execution sequence of two or more threads
cooperatively.
In concurrent programs we can often only execute a method
invoked on component if specific conditions¡ªcalled guards¡ª
apply. As the state of a method¡¯s guard conditions may eventually become true as a result of another concurrent action on the
same component, it is not always feasible to abort the method if
it cannot be executed immediately.
For example, we can remove a message from a synchronized message
queue only when the queue is not empty, which is the guard condition. If another client thread inserts a message into the queue, it is
not longer empty. A previously invoked removal method that could
not execute because its guard condition is ¡®queue empty¡¯ can now
proceed and finish successfully.
Blocking indefinitely is not really an option, because it prevents
other client threads from performing potentially useful work on the
shared component. Unconditional blocking can also create a potential
deadlock situation by locking out all client threads, including ones
that could change the state of the guard condition to true.
Aborting the attempt to access the critical section has the benefit
of failing fast, signaling failure to the calling thread with a status
result value or exception. It is intrusive for code in the client thread,
however, which must implement some kind of retry strategy that
will clutter the client code with mechanistic detail that is better
encapsulated elsewhere.
Therefore:
Instead of aborting the method, suspend its client thread so that
other client threads can access the shared component safely and
change the state of the method¡¯s guard condition. If this state
changes, resume the suspended thread so that the thread can
try to continue the execution of the interrupted method.
Guarded suspension allows a shared component to control and
schedule the execution sequence of its client threads transparently
to these threads.
The key benefit of a GUARDED SUSPENSION design is that it is nonintrusive for the client. The called method encapsulates the policy
and mechanism for access, leaving the calling code free of clutter and
potentially duplicated mechanisms. An unexceptional and common
situation is no longer flagged as a failure. The blocking behavior is
such that the shared component continues to be available. If the
suspension is at OS level, there is also no CPU wastage. As a result,
GUARDED SUSPENSION minimizes concurrency overhead in the client
threads and increases the shared component¡¯s availability.
Guarded suspension implementation primitives are offered in many
OS APIs, but should be used via a WRAPPER FACADE (459) rather than
directly. There are multiple ways to realize a guarded suspension
[Lea99]: waits and notifications via condition variables and associated
mutexes, busy-waits via spin-loops, and suspending and resuming
client threads directly.
Although non-intrusive and non-failing blocking can simplify client
code, a client may want the choice between blocking or doing something else, particularly if the suspension is long. This can be resolved
by also providing non-blocking and time-out variants of the method.
For example, a queue could provide a blocking get method and a
non-blocking try get method.
##%%&&
When implementing a MASTER-SLAVE (321) arrangement, an ACTIVE
OBJECT (365), PARTIAL ACQUISITION (511), or software in which client and
server run concurrently and communicate via method calls . . .
. . . at times we must access data that is computed concurrently with
the control flow of a client.
Services that are invoked concurrently on a component may
need to return a result to the calling client. However, if the client
does not block after calling the service, continuing instead with
its own computation, the service¡¯s result may not be available
when the client needs to use it.
A common use of concurrency is to optimize performance by overlapping computation and communication. This optimization can be
simple for one-way calls that return no results: invocation can be as
simple as ¡®fire and forget.¡¯ A client, however, may want to invoke one
or more two-way methods on one or more servers without having to
wait synchronously for the server response(s). A call¨Creturn procedural model is simple to use but cannot, in this case, be used to return
a result. Nevertheless, when the client needs a result to continue its
processing, there must be a straightforward way to obtain it.
Therefore:
Immediately return a ¡®virtual¡¯ data object¡ªcalled a future¡ªto
the client when it invokes a service. This future keeps track
of the state of the service¡¯s concurrent computation and only
provides a value to clients when the computation is complete.
If the client accesses the service¡¯s result before it is available, the
future suspends the client thread. After the result is available and
stored in the future, the client¡¯s thread will be resumed automatically
so that the client can continue executing and use the result of the
service. A client can also poll a future through a non-blocking or
timed accessor, as well as a completion query, which enables it to
check whether the result is available without indefinitely suspending
the client thread.
Using a FUTURE can enhance parallelism by allowing clients to synchronize with services that they invoked concurrently at the latest
possible point in time. A FUTURE also enables method results to be
processed in a different order than that in which the methods were
invoked, which can enhance flexibility and performance.
To enhance parallelism, do not access the future immediately after
receiving it, since this is an expensive way of achieving a synchronous
two-way service call. Instead, let the client execute as many different
operations or instructions as possible after the service invocation,
and access the future only when the client cannot make any further
progress without the service¡¯s result. The more time that passes
between the service invocation and the access to the future, the less
likely it is that the client will block, which increases concurrency and
parallelism.
A FUTURE acts as a VIRTUAL PROXY (497) for data that is not yet computed.
It may be implemented directly in terms of locking primitives that
support GUARDED SUSPENSION (380), or it may be layered on top of an
event-handling scheme expressed in terms of ASYNCHRONOUS COMPLETION
TOKENS (268).
##%%&&
In a REQUESTOR (242), CLIENT REQUEST HANDLER (246), MESSAGE CHANNEL
(224), MESSAGE ROUTER (231), SHARED REPOSITORY (202), EXPLICIT INTERFACE
(281), PROXY (290), PAGE CONTROLLER (337), FRONT CONTROLLER (339),
APPLICATION CONTROLLER (341), MONITOR OBJECT (368), OBJECT MANAGER
(492), or BUILDER (527) arrangement . . .
. . . we must often ensure thread-safe access to components in a
concurrent program.
Components in a concurrent program must be thread-safe. Often
their methods acquire locks to protect critical sections from concurrent access. Self-deadlock can occur, however, if an acquired
lock is non-recursive and the method calls another method in
the component that tries to acquire the same lock.
Although a re-entrant lock prevents self-deadlock, for some platforms it incurs unnecessary overhead when acquiring and releasing
the lock multiple times across intra-component method calls. Ideally,
a design that avoids self-deadlock, or does not take advantage of
a lock¡¯s support for re-entrancy, should also incur minimal locking
overhead. Assigning the responsibility for synchronizing the shared
component with its clients is undesirable, however, because this
design couples these clients tightly with the component. Such coupling increases both the complexity of usage and the probability of
incorrect usage.
Therefore:
Split a component¡¯s methods into publicly accessible interface
methods and corresponding private implementation methods. An
interface method acquires a lock, calls its corresponding implementation method, and releases the lock. An implementation
method assumes the necessary lock is held, does its work, and
only invokes other implementation methods.
To ensure proper synchronization, clients of a component designed
using THREAD-SAFE INTERFACE can only invoke its interface methods.
Once an interface method obtains the necessary lock, it forwards
control to the corresponding implementation method, which then
processes the client¡¯s request. Self-deadlock and locking overhead are
thus avoided, because the code in the implementation method does
not acquire the component¡¯s lock, nor do any other implementation
methods it may call on this component. When control returns from
the implementation method to its interface method, the lock held by
the interface method is released and any results are returned to the
client that invoked the method.
A THREAD-SAFE INTERFACE ensures that self-deadlock does not occur as
a result of intra-component method calls. In addition, locks are not
acquired or released unnecessarily. Finally, a THREAD-SAFE INTERFACE
separates locking and functionality issues, which helps to simplify
both aspects.
Note, however, that the self-deadlock problem is not resolved if control
leaves the component¡¯s scope temporarily and the underlying locking
mechanism is strict. This situation can occur if an implementation
method delegates control to a different component which then tries to
re-enter the first component by calling one of its interface methods.
In this case, the interface method will try to reacquire the locks the
component already holds. A common way to resolve this problem is
to realize a THREAD-SAFE INTERFACE using a re-entrant lock.
Often a component with a THREAD-SAFE INTERFACE is designed as a
class with public synchronized interface methods and private non synchronized implementation methods. Alternatively, a THREAD-SAFE
INTERFACE arrangement can be designed as a TEMPLATE METHOD (453),
with template methods corresponding to interface methods and hook
methods corresponding to implementation methods. Components can
use STRATEGIZED LOCKING (388) to vary their configuration with the
optimal lock type.
##%%&&
When performing a LAZY ACQUISITION (507) in a concurrent environment . . .
. . . at times it may be necessary to perform thread-safe, one-time
initialization in a method without paying the cost of synchronization
for all subsequent calls of the method.
A common way of avoiding race conditions within a component shared between threads is to serialize access to its critical
sections. A thread that wants to enter a critical section must first
acquire a lock. This design, however, can incur excessive locking
overhead if the object¡¯s critical section is entered frequently but
is executed conditionally only once.
For example, consider an object whose representation is initialized
by LAZY ACQUISITION. In a multi-threaded environment the just-in-time
initialization code¡ªthe part that checks whether the representation
already exists, creating it if it does not¡ªis a critical section that runs
just once during the object¡¯s lifetime. Guarding this code with a lock,
however, incurs an overhead for all calls to the accessor method, not
just the one call when initialization occurs. The lock is then acquired
and released unnecessarily, because the object¡¯s existence check
prevents control flow from executing the critical initialization code.
Therefore:
Provide the shared object with a ¡®hint¡¯ as to whether execution of
a particular critical section is necessary. Check this hint before
and after acquiring the lock that guards this critical section.
If the critical code has already been executed, the hint¡¯s first check
lets threads skip this code and its associated lock acquisition: no
locking overhead is incurred. The hint¡¯s second check prevents race
conditions if two or more threads passed the first check in parallel.
Only one of these threads will successfully acquire the lock, pass the
hint¡¯s second check, execute the critical section, and change the hint.
Once the lock is released, any waiting threads will bypass the critical
section, because the hint¡¯s second check indicates that it has already
been executed.
Within an object, DOUBLE-CHECKED LOCKING ensures that locks are only
acquired and released if a critical section guarded by a conditional
statement must really be executed, which is often an ¡®exceptional¡¯
situation. The common case of not executing the critical situation
requires no locking, and thus executes quickly and efficiently.
The hint used for DOUBLE-CHECKED LOCKING should initially indicate
that the critical section has yet to be executed. If this hint also has
an application-specific purpose, such as a pointer to an internal
representation object, ensure that it has an atomic type. Guard
the critical section by checking the hint, only letting threads enter
if the critical section has yet to be executed. Inside the critical
section acquire the lock, check the hint again, and¡ªdepending on its
value¡ªexecute the critical section. Before releasing the lock, change
the hint so that subsequent threads do not enter the critical section.
STRATEGIZED LOCKING (388) supports configuration of the lock with a
type appropriate for the application.
In the absence of a platform supporting a memory model that guarantees a coherent view of memory updates, CPU-specific instructions
such as memory barriers are needed to access the hint safely. There
is currently no portable memory model for C++, so platform-specific
approaches must be adopted. Current versions of Java support an
appropriate memory model, but older versions do not. As with any
lock-free technique, the double-checked aspect of DOUBLE-CHECKED
LOCKING can be remarkably subtle and error prone. Developers need
to be keenly aware of its subtleties [MeAl04a] [MeAl04b]. If access to
this level of mechanism is not possible, or is otherwise impractical,
an alternative design should be considered, in particular one that
avoids the need for any locking.
##%%&&
When realizing a REQUESTOR (242), CLIENT REQUEST HANDLER (246),
SHARED REPOSITORY (202), PAGE CONTROLLER (337), FRONT CONTROLLER (339),
APPLICATION CONTROLLER (341), COMMAND PROCESSOR (343), THREAD-SAFE
INTERFACE (384), or DOUBLE-CHECKED LOCKING (386) . . .
. . . a key issue in multi-threaded programming is the selection of the
appropriate locking strategy for a particular environment.
Components that are shared across threads in multi-threaded
environments must protect their critical sections from concurrent access. However, different software configurations may
require different locking strategies, such as mutexes, readers¨C
writer locks, or semaphores.
Hard-coding the locking strategy into the components and adapting it
to specific environments is a straightforward solution to this problem,
but infeasible for most applications. Components would depend on
their environment, and whenever that environment changes and suggests a different locking strategy, all components must be updated,
with corresponding maintenance overhead. Providing different component versions for each environment is also infeasible¡ªit causes
similar maintenance overhead. Ideally, it should be possible to
customize a component¡¯s locking strategy without making its implementation dependent on a specific environment.
Therefore:
Define locks in terms of ¡®pluggable¡¯ types, with each type objectifying a particular synchronization strategy. Provide all types with
a common interface, so that a component can use all lock types
uniformly without being dependent on their implementation.
Configure the component with an instance of the appropriate lock
type to use at its creation or declaration time, for example by passing
a lock object to its constructor or parameterizing the component¡¯s
class with a particular lock type. Use this lock instance to protect all
critical sections within the component.
A STRATEGIZED LOCKING design offers several benefits. Rather than a separate implementation for each concurrency model, there is only one
core implementation. Enhancements and bug fixes for a component
therefore do not have to be duplicated. Configuring and customizing
a component for specific concurrency models is simple and non intrusive for the component, because the synchronization aspects of
components are strategized. Conversely, STRATEGIZED LOCKING exposes
a parameterization decision to the component user, which may be
considered intrusive for common cases. Nevertheless, such an open
and orthogonal approach allows the component to be used beyond
its original context.
To make locks ¡®pluggable,¡¯ define a lock acquisition and release
STRATEGY (455) interface that is implemented by all concrete lock
types. Use the same lock type to configure related components within
the same application. Where a variation in lock implementation is
with respect to the platform rather than the locking policy, make concrete lock types WRAPPER FACADES (459) that encapsulate the details of
a particular platform-specific locking mechanism. To optimize components for single-threaded environments where no locking is needed,
provide a null lock type, which is a NULL OBJECT (457) whose lock
acquisition and release methods are empty ¡®no-ops.¡¯
SCOPED LOCKING (390) helps to simplify and automate safe acquisition
and release of the lock within the component¡¯s implementation.
##%%&&
When providing locks for a shared component in a concurrent program, either by hard-coding a particular lock type into the component
or by implementing STRATEGIZED LOCKING (388) . . .
. . . a key issue in multi-threaded programming is to ensure that locks
are acquired and released automatically when entering and leaving
critical sections.
A critical section of code that should run sequentially is often
protected by a lock, which is acquired and released whenever
control enters and leaves the critical section. If programmers
must acquire and release this lock explicitly, however, it can be
hard to ensure that all paths through the code release the lock.
Control can leave a scope early due to an uncaught exception, or an
explicit ¡®exit statement¡¯ such as return, break, or goto. The lock¡¯s
release code may therefore be missed as a result of programmer
oversight. The more complex and verbose code becomes, the more
likely such oversights will occur. Code that tries to achieve exception safety through the explicit use of statements like try, catch, and
re-throw is error prone, losing both clarity and the intended safety.
Therefore:
Scope the critical section¡ªif this has not already been done¡ª
and acquire the lock automatically when control enters the
scope. Similarly, automate the release of the lock when control
leaves the scope via any exit path.
Entering a critical section becomes thread-safe, and leaving the critical section safely releases all acquired locks.
SCOPED LOCKING increases the robustness of concurrent software by
eliminating common programming errors related to synchronization
and multi-threading. In addition, locks are acquired and released
automatically when control enters and leaves critical sections, which
are defined by programming language method and block scopes.
The implementation of SCOPED LOCKING depends on the language used
to program concurrent software. For example, Java provides a language feature, the synchronized block, that instructs Java compilers
to generate a corresponding block of bytecode instructions in which a
monitor enter and a monitor exit bracket the block. To ensure that
the lock is always released, the compiler also generates an exception
handler to catch all exceptions thrown in the synchronized block
[Eng99]. If the locking scope covers the whole method, the method
itself can be marked as synchronized. However, because marking
the method is slightly simpler to write than a synchronized block,
it is tempting to fall back on this briefer approach. Unfortunately, in
many cases this means that the lock scope becomes larger than the
critical section, leading to a loss of concurrency.
C++, by contrast, does not provide direct language support for SCOPED
LOCKING, but it can be implemented as a C++ idiom via an EXECUTE AROUND OBJECT (451). Use this idiom to create a guard class whose
constructor acquires a lock and whose destructor releases it. A guard
object is thus declared as a local variable within the critical section¡¯s
scope and before its first statement. When control enters the critical
section, the guard¡¯s constructor is called and acquires the lock.
When control leaves the critical section via any exit path, the guard¡¯s
destructor is called automatically, due to C++ semantics, to release
the lock. Designing the guard object as a WRAPPER FACADE (459) helps
to encapsulate the details of a particular platform-specific locking
mechanism behind a uniform interface.
##%%&&
When working with code developed without threads or unanticipated
integration in mind, or when designing a WRAPPER FACADE (459) that
encapsulates an error-handling mechanism for concurrent C/C++
legacy systems running in a UNIX environment . . .
. . . the use of state is often considered global within an application
but, within a concurrent realization, its representation needs to be
expressed as physically local to each thread.
Access to an object that is tied to its environment makes it
appear logically global. However, if the environment needs to
appear specific to each thread, the object cannot simply be
physically global with a single copy of state.
Locking each access to the object, and looking up the appropriate value based on the thread, can degrade system performance if
the object is used frequently. Ideally, access to the object should
be atomic without incurring any locking overhead. In addition,
retrofitting its implementation to run in multiple threads it is often
infeasible, as is the case for many legacy components written without
concern for multi-threading that rely on specific objects being global
in some way.
Therefore:
Introduce a common access point for the environmentally bound
object, but maintain its physical object instances in storage that
is local to each thread.
Ensure that every thread-local copy of an object can be retrieved
from its corresponding thread-local storage via a globally unique
key. Using this key, forward all method calls on the object¡¯s global
access point to the particular copy of the thread-specific object that is
maintained in the thread from which the access point is called. Thus
no locking is required to access a thread-local object copy. Let client
threads manipulate the thread-specific object only through its global
access point, to preserve the object¡¯s logically global appearance.
A THREAD-SPECIFIC STORAGE design needs no locking to access thread specific data. In addition, THREAD-SPECIFIC STORAGE is easy for software developers to use when dealing with legacy code that makes
global assumptions about its objects, or that does not provide suitable parameters for distributing context information. However, this
pattern is not without its liabilities, foremost of which is the encouragement such a facility may provide to developers to make ¡®global¡¯
objects that should be considered local and passed more explicitly as
parameters.
Implement the global access point for the thread-specific object as
a PROXY (290). At runtime, instantiate the global access point to the
thread-specific object in every thread, but none of its thread-local
instances. When a client thread calls a method on the access point,
let a FACTORY METHOD (529) use LAZY ACQUISITION (507) to check first if
a globally unique key exists that identifies the thread-specific object,
and to create one if not. Finally, forward the original call on the
access point to this thread-specific object and execute the request.
Note that no locks are needed to protect the object¡¯s existence check,
its creation, and also its use from concurrent access. Although clients
invoke calls via a logically global access point, they actually operate
on object state that is not shared between threads.
Multiple thread-specific components can be maintained in an indexed
container whose index type is the key type, such as a map or an OBJECT
MANAGER (492), of which a separate instance is provided per thread.
##%%&&
When expressing a value in an application via a DOMAIN OBJECT (208),
dispatching events to event handlers in a REACTOR (259) or PROACTOR
(262), or when specifying a NULL OBJECT (457) . . .
. . . we must ensure that representations of values do not intro duce thread-safety problems or efficiency bottlenecks if they must
be known in different threads in a concurrent program.
Value objects are commonly distributed and stored in fields.
If value objects are shared between threads, however, state
changes caused by one object to a value can have unexpected and
unwanted side effects for any other object sharing the same value
instance. In a multi-threaded environment shared state must be
synchronized between threads, but this introduces costly overhead for frequent access.
Value objects form the principal type of representation and communication between many object types, such as entities and services.
Identity is not significant for a value object, but its state is. Aliasing of these objects can often cause surprises, particularly where
an object is used to represent the value of an attribute within an
object. Between threads, the aliasing becomes more deeply problematic, with the possibility of race conditions for any state modification.
Synchronizing methods addresses the question of valid individual
modifications, but does nothing for the general problem of sharing,
nor for the performance cost of synchronization of changes.
Therefore:
Define a value object type whose instances are copyable. When
a value is used in communication with another thread, ensure
that the value is copied.
Each thread receives its own copy of a passed value, so value objects
are not shared between threads.
The absence of any sharing means that there is no reason to synchronize. COPIED VALUES are disjoint across threads and incur no locking
overhead. Value objects can support state-changing operations, but
the effect of the state changes is localized.
C++¡¯s language¡¯s object model is essentially based on value objects,
and support for copying is provided easily and transparently to applications via copy constructors. Beyond ensuring appropriate copy
construction behavior and declaring the appropriate call signatures
for functions, therefore, developers do not need to make any extra
arrangements to communicate values as COPIED VALUES. However, if
the COPIED VALUES share representation, they are unsuitable if clients
modify their state.
In C#, fine-grained objects that require no special construction can
be expressed as struct objects, for which copying is an implicit operation. If a value requires complex construction, however, expressing
them as COPIED VALUES might become infeasible due to copying overhead, and an alternative representation should be considered.
Java lacks an automatic mechanism for ensuring that objects are
copied when communicated through methods between threads. The
use of COPIED VALUES therefore requires more manual support, creating
copies explicitly on call, and is therefore error-prone.
##%%&&
When expressing a fine-grained value via a DOMAIN OBJECT (208) or
designing a CLIENT PROXY (240) . . .
. . . we want to communicate values between different threads of an
application efficiently and safely.
References to value objects are commonly distributed and stored
in fields. However, state changes to a value caused by one object
can have unexpected and unwanted side-effects for any other
object sharing the same value instance. Copying the value can
reduce the synchronization overhead, but can also incur object
creation overhead.
Copying can be used for modifiable value objects to minimize aliasing
problems. Without proper language support, however, this practice
is tedious and error-prone. It may also cause excessive creation
of small objects, especially where values are frequently queried or
passed between components. In principle, values are unchanging, so
the creation of multiple instances of a value just to communicate
it may be wasteful and incur excessive space and time overhead,
depending on the underlying object creation model.
In a multi-threaded environment the problems of sharing and state
change are multiplied. Synchronizing methods addresses the question of valid individual modifications, but does nothing for the general
problem of sharing. Synchronization of change also incurs a performance cost.
Therefore:
Define a value object type whose instances are immutable. The
internal state of a value object is set at construction and no
subsequent modifications are allowed.
In an immutable value only query methods and constructors are
provided: no modifier methods are defined. A change of value thus
becomes a change in the value object referenced.
The absence of any possible state changes means there is no reason
to synchronize. Not only does this make IMMUTABLE VALUES implicitly
thread safe¡ªthe absence of locking means that their use in threaded
environment is also efficient. Sharing of IMMUTABLE VALUES is also safe
and transparent in other circumstances, so there is no need to copy
an IMMUTABLE VALUE.
In Java, declaring the fields of an IMMUTABLE VALUE final ensures that
no change promise is honored. This guarantee also implies that
either the class itself must be final, or that its subclasses must also
be IMMUTABLE VALUES. In C++ a fully const-qualified object can play a
similar role, distributed and shared by pointer. Restriction of further
inheritance, however, must be communicated through convention
rather than language mechanism.
If a value with different attributes is required, a new object is created or found with the desired value: references are changed, rather
than attributes. There are complementary techniques for creating
IMMUTABLE VALUES. A direct construction from a set of input values
is supported by a complete and intuitive set of constructors, or by
a number of class-level FACTORY METHODS (529). A MUTABLE COMPANION
[Hen00b] helps in constructing IMMUTABLE VALUES as a result of an operation on another value object, such as calculating a value that is twice
as large as another given value. A RESOURCE POOL (503) helps providing
access to a set of pre-defined IMMUTABLE VALUES, and an OBJECT MANAGER
(492) supports the construction of singleton IMMUTABLE VALUES.
Value objects do not represent resources, so managing finalization
is not the issue that it might be with resource objects. AUTOMATED
GARBAGE COLLECTION (519) can thus be used to reclaim the resources
of IMMUTABLE VALUES that are no longer used. In contrast, sharing
IMMUTABLE VALUES in a non-garbage collected environment needs to
involve careful management: thread lifetimes must either be shorter
than the lifetimes of the IMMUTABLE VALUES they share, or they should
be referred to via COUNTING HANDLES (522).
##%%&&
In a LAYERS (185), MODEL-VIEW-CONTROLLER (188), PRESENTATION ABSTRACTION-CONTROL (191), SHARED REPOSITORY (202), ITERATOR (298),
HALF-OBJECT PLUS PROTOCOL (324), REPLICATED COMPONENT GROUP (326),
INTERCEPTOR (444), MEDIATOR (410), COMPONENT CONFIGURATOR (490), or
DATABASE ACCESS LAYER (538) arrangement . . .
. . . we must provide a means to keep the state of a set of cooperating
component objects consistent with each other.
Consumer objects sometimes depend on the state of, or data
maintained by, another provider object. If the state of the
provider object changes without notice, however, the state of
the dependent consumer objects can become inconsistent.
Common solutions to this problem are to hard-code connections
from the provider object to all its dependent consumer objects, or to
have the consumers poll the provider. These approaches are often
impractical, however, since a consumer may not be dependent on the
provider indefinitely, and new instances and types of consumers may
emerge over an application¡¯s lifetime. Moreover, polling may either
consume excessive resources, or may not detect changes quickly
enough.
Therefore:
Define a change-propagation mechanism in which the provider¡ª
known as the ¡®subject¡¯¡ªnotifies registered consumers¡ªknown
as the ¡®observers¡¯¡ªwhenever its state changes, so that the
notified observers can perform whatever actions they deem necessary.
Observers must define a specific update interface that is notified by
the subject when its state changes. This interface is the primary coupling between the subject and its observers. Observers can register
and unregister from the subject dynamically. When the subject notifies its observes, it can either push the state to the observers along
with the state change notification, or the observers can selectively
pull the changed state from the subject at their discretional after
being notified.
In an OBSERVER arrangement, the dynamic registration of observers
with the change notification mechanism avoids hard-coding dependencies between the subject and its observers: they can join and leave
at any time, and new types of observer that implement the update
interface can be integrated without changing the subject. The active
propagation of changes by the subject avoids polling and ensures
that observers can update their own state immediately in response to
state changes in the subject.
In a typical OBSERVER implementation, an EXPLICIT INTERFACE (281)
defines the update interface to be supported by observers. Concrete
observers implement this interface to define their specific update policy in response to notifications by the subject. The subject, in turn,
offers an interface for observers to register with and unregister from,
the change notification mechanism. Internally, the subject manages
its registered observers within a collection, such as a hashed set or a
linked list.
The OBSERVER change notification protocol can be implemented in
several ways. The simplest option is based on a generic pull model:
when the subject changes its state, it notifies all registered observers
that a state change has occurred. The notified observers can then
call back to the subject to retrieve more detailed information. This
protocol works well if all observers depend on all state that the
subject maintains. If different observers depend on different state
in the subject, however, the generic pull model causes unnecessary
updates, because the subject notifies all observers, not just those
who depend on the state that has changed.
If subject and observer execute in different address spaces, the ¡®chattiness¡¯ of the generic pull model can consume network and processing
resources unnecessarily. In this case, a categorized pull model can
be used to allow observers to register with one or more different types
of state changes in the subject. These observers are only notified if
particular types of state change. The pull model therefore may still
be inappropriate for remote communication, due to the overhead of
having all the observers call back to the subject to obtain the state.
To minimize interactions between subject and observer, there are
two other options for implementing the change notification protocol,
both based on a push model rather than a pull model. In the generic
push model, the subject pushes a snapshot of its attribute state
to the observers along with each notification, using a DATA TRANSFER
OBJECT (418) to communicate the attributes. This model is useful if all
observers depend on the entire state being pushed, or when the cost
of communicating all the state is less than the cost of having each
observer call back for specific state.
A variant of the generic push model is the categorized push model,
which is based on some type of filtering [GoF95]. If a change in the
subject affects only a small portion of its entire state, this portion
is pushed only to those observers who are interested in it. If these
observers can also decide to not update themselves each time they
are notified, however, even the categorized push model produces
overhead, because many ¡®expensive¡¯ data pushes are unnecessary.
Choosing the best option for the change notification mechanisms
in a specific OBSERVER configuration is also influenced by coupling
issues. Pull models generally result in a looser coupling between the
subject and the observers than push models, and the generic pull
model decouples subject and observers better than the categorized
pull model.
##%%&&
When implementing a VISITOR (447) or when communicating between
class hierarchies . . .
. . . we must realize behavior that depends on the types of two objects.
A method¡¯s behavior clearly depends on its argument¡¯s values.
Sometimes, however, its behavior also depends on the type of an
argument. In most object models, this ¡®multi-method dispatch¡¯
is not supported, and only single-dispatch is available.
Polymorphism is normally expressed in terms of single dispatch: the
target of the method is the receiver of the call, and the selected method
implementation depends on its class. Sometimes, however, behavior
depends on both the type of the caller and the type of the called object,
so the caller passes itself as argument. One solution to express such a
configuration uses explicit selection, such as an if. . .else or switch
statement, based on the argument¡¯s runtime type in each concrete
class of the receiver¡¯s class hierarchy. This approach, however, is
brittle and verbose. Another approach uses a map whose key is made
up of a pairing of the receiver and the argument¡¯s runtime type, and
whose mapped value is a method reference of some kind, such as a
member function pointer in C++, or a delegate in C#. This approach
can suffer from undue brittleness and runtime overhead.
Therefore:
Pass the caller object to the receiver object as an extra argument.
Within the receiver, call back the caller object to run caller-class
dependent logic, passing the receiver as an additional argument,
so that the caller can behave appropriately.
Behavior is now split across the caller and receiver objects. The
caller object supports an interface that is based on the possible
types of the receiver object: a set of methods¡ªeither overloaded or
specifically named¡ªeach corresponding to a concrete receiver type.
Each implementation of the caller object¡¯s interface defines the type specific behavior in each receiver-matched method. The receiver is
responsible for calling back on to the caller with itself as argument,
selecting the method that corresponds to its type.
DOUBLE DISPATCH avoids executing caller-class dependent behavior via
large if. . .else or switch statements in the receiver object. Such
solutions are hard to maintain and extend, even though they appear
to centralize all programming logic in one place [Beck97]. Instead,
both the caller and receiver objects are involved in the computation¡ªhence the name DOUBLE DISPATCH. This callback arrangement
results in simpler and more cohesive code.
One liability with DOUBLE DISPATCH, however, is that it incurs additional communication overhead between remote objects. Moreover,
the maintainability of DOUBLE DISPATCH relies on the stability of the
receiver type¡¯s hierarchy. If new concrete classes are added to the
receiver hierarchy, the argument¡¯s interface must be updated, as
must all implementations of the argument interface. Depending on
how widely this interface is distributed, such changes range from
being tedious to impossible¡ªorganizational boundaries, widespread
release in a published API, binary stability, and so on, all restrict the
evolvability of an interface.
##%%&&
When implementing PRESENTATION-ABSTRACTION-CONTROL (191), a
MICROKERNEL (194), or a WHOLE-PART (317) . . .
. . . we must reduce the coupling between multiple cooperating objects.
Sometimes the relationships between a set of objects is complex:
each object in the set cooperates with several other objects.
Allowing each object to maintain all these cooperation relationships by itself, however, would overly couple the objects.
The resulting interdependencies would be hard to understand,
test, and maintain in subsequent modifications of the collaborating classes, since introducing participants to the collaborating set
of objects is subtle and error-prone. If the essence of the collaboration¡ªbut not the specific types involved¡ªis needed elsewhere, such
tight coupling can preclude effective reuse.
Therefore:
Decouple the objects via a separate mediator object that encapsulates the collective cooperative behavior of all objects in the
set. Collaboration is achieved indirectly via mediation, rather
than directly through point-to-point communication.
If an object wants to cooperate with another object, it does so by sending its request, message, or data to the mediator, which then routes
the information anonymously to the appropriate receiver. Results can
be returned accordingly.
A mediator preserves the self-containment and independence of
multiple cooperating objects, which need not maintain explicit relationships with their peers. Instead, they delegate the routing of
requests, messages, and data that they exchange with other objects
to the mediator. The mediator is the orchestrator that connects the
cooperating objects, maintains oversight of them, and controls their
collaborations.
Although a mediator preserves the cohesiveness, encapsulation, and
simplicity of individual collaborating objects, the centralization of control may make the mediator itself a potential maintenance problem.
Similarly, a mediator can also be a potential performance bottleneck
and a single point of failure in a distributed system.
There are two general approaches for implementing a mediator: either
it can hard-code the relationships between the objects whose cooperation it coordinates, or it can act as a subject in an OBSERVER (405)
design that provides requests, messages, and data to consuming
objects. The latter option follows an implicit invocation style of execution that is very loosely coupled. This approach, however, makes the
actual control model harder to see, and can require more development
effort than a hard-coded design.
##%%&&
When implementing LAYERS (185), MODEL-VIEW-CONTROLLER (188),
PRESENTATION-ABSTRACTION-CONTROL (191), ACTIVE OBJECT (365), ENUME RATION METHOD (300), HALF-OBJECT pLUS PROTOCOL (324), REPLICATED
COMPONENT GROUP (326), PAGE CONTROLLER (337), FRONT CONTROLLER (339),
or APPLICATION CONTROLLER (341) . . .
. . . we must invoke actions on a component independently of selecting
the actions to invoke.
Accessing an object typically involves calling one of its methods.
Sometimes, however, it is useful to decouple the sender of a
request from its receiver. It may also be useful to decouple the
selection of a request and receiver from the point of execution.
While explicit coupling is beneficial for many client/service relationships and deployments, it can introduce too much coupling if the
request chosen by the client is more important than the identity of
the receiver. Similarly, it is hard to invoke a method asynchronously
in programming languages that use the conventional synchronous
call/return model of method invocation. Moreover, housekeeping
functionality, such as logging and undo, cannot be supported transparently. Ideally, clients ought be able to issue a request and the
¡®right¡¯ things should just happen.
Therefore:
Encapsulate requests to the receiving object in command objects,
and provide these objects with a common interface to execute
the requests they represent.
Clients that want to issue a particular request create the corresponding command object. When invoked, the command object initiates
and controls the execution of the represented request with respect to
any arguments it receives when executed.
COMMAND decouples the requestor of behavior from the recipient, as
well as the selection of behavior from the point of execution. This
loose coupling ensures that requestors do not depend on a specific
receiver interface. Modifications to the receiver¡¯s interface therefore
do not ripple through to its clients. In addition, the reification of
requests into command objects allows the handling of requests as
first-class entities within an application, which in turn enables the
implementation of extra request-handling features such as undo and
logging.
To realize a COMMAND structure, first specify an EXPLICIT INTERFACE (281)
for uniform command execution. The interface will define one or more
methods for execution, receiving arguments as necessary. Concrete
commands implement this interface to reify particular requests. Each
concrete command is initialized with whatever state is needed to
support its execution, such as a receiver object or method arguments
for later use. An INTERPRETER (442) is a special form of COMMAND in which
a script from a simple language is transformed into an executable
runtime structure.
Command objects can offer an undo mechanism by retaining the
state necessary to roll back the behavior they execute. If the state
is heavyweight or hard to restore, however, a MEMENTO (414) that
snapshots the receiver¡¯s state before executing the command may
provide a simpler, more lightweight option.
A COMPOSITE (319) structure supports the construction and execution of macro commands, aggregating multiple command objects
uniformly behind a single interface, and executing or rolling them
back in a particular order. A separate COMMAND PROCESSOR (343) that
executes command objects on behalf of their sender can provide
additional request-handling support, such as for multiple undo/redo,
scheduling, and logging.
##%%&&
When implementing an EXPLICIT INTERFACE (281), INTROSPECTIVE INTERFACE
(286), or ENCAPSULATED IMPLEMENTATION (313), or when realizing an
OBSERVER (405), COMMAND (412), or COMPONENT CONFIGURATOR (490)
arrangement . . .
. . . we need to exchange state between participants without breaking
encapsulation.
It is often necessary to record the internal state of an object.
Allowing other objects to access an object¡¯s state directly breaks
encapsulation and introduces unnecessary complexity in dependent objects.
Storing and retrieving object state is common in distributed systems.
For example, a persistence service may need to extract an object¡¯s
state when it is passivated and hold this state so it is available the
next time the object is run. If the internal state of an object is exposed
for this purpose, however, it may lose control over which other objects
access¡ªand perhaps modify¡ªits state. This lack of encapsulation
can bind dependent objects to a particular state representation,
which makes it hard to modify the software.
Therefore:
Snapshot and encapsulate the relevant state of the originating
object within a separate memento object, and pass this memento
to the object¡¯s clients instead of letting them access the object¡¯s
internal state directly.
The originating object can recover and reactivate its previous state
from the memento if and when it is passed back by a client or a
state-recovery service.
A MEMENTO preserves the encapsulation of the originating object:
dependent objects cannot access its state directly, but the originating
object can. MEMENTO allows these dependent objects to receive the
latest state, in wrapped form, from the publishing object on request,
without being dependent on the representation of the state.
The internal representation stored in the MEMENTO is free to evolve over
different versions of the software without any need for source-level
changes in the dependent objects. In fact, binary compatibility may
also be preserved across versions, depending on the programming
language and class design techniques used.
Most MEMENTOS offer two interfaces. For the originating object they
provide a wide interface, with setter and getter methods to initialize
the MEMENTO and to access the state it maintains. Clients of the
originating object, however, often see only a narrow interface that
allows them to access the MEMENTO¡¯S state, but not to modify its
state. Some languages, such as Java and C#, support packaging
structures that allow different visibility for class methods inside a
package than outside it, which makes it easy to segregate the two
interfaces. C++ supports these differences in visibility via friend or
forward-declared classes, and source files as packages. Alternatively,
separation of interfaces via inheritance can help to differentiate the
wide and narrow interfaces.
##%%&&
When implementing an EXPLICIT INTERFACE (281), INTROSPECTIVE INTERFACE
(286), ENCAPSULATED IMPLEMENTATION (313), INTERCEPTOR (444), INTERPRETER
(442), LIFECYCLE CALLBACK (499), or TASK COORDINATOR (501) . . .
. . . we need to share information about the system or invocation
context of a component without introducing global coupling.
Loose coupling is a key design goal in distributed systems. In a
loosely coupled system, however, there may be a need to share
common information that relates to the program¡¯s execution
context, such as its externally configured values, client session
state, and logging, across its disparate parts or layers.
Although the scope of a program¡¯s execution context is generally more
global than the scope of its parts, global variables and SINGLETONS
[GoF95] provide uncontrolled access from all parts of a program
and introduce unnecessary coupling. Globals and SINGLETONS also do
not provide late binding easily: the implementation and instance on
which they are based is often hard-coded at design-time rather than
configurable at runtime. Conversely, propagating many items of fine grained information as individual variables, and services as individual
operations, can yield unmanageable and unstable argument lists.
Therefore:
Represent the information and services in an object that encapsulates the required context. Provide this object to the operations, components, and layers that need the context.
Clients that issue a request on a component create the context object,
which can be passed explicitly or implicitly along with other parameters of the invocation. The receiving component uses the information
and services in the context object to guide its own execution.
With a CONTEXT OBJECT, basic runtime substitutability can be supported explicitly via the standard argument passing mechanisms, or
implicitly via THREAD-SPECIFIC STORAGE. It is possible to modify the execution context of a component without necessarily having to modify
the configuration or code of other parts of a system. It is also possible
to run with multiple, different contexts within the same program,
perhaps in different threads. Over time, small changes to the information and services in the context are absorbed, since the unit of
stability is the CONTEXT OBJECT, not its individual items of information
and service operations.
If the dependency of a piece of code is simply on a passable value
rather than a broader notion of execution context, the value itself
should be passed rather than any kind of CONTEXT OBJECT, partitioned
or not. In such cases a CONTEXT OBJECT would draw in more context,
significance, and dependencies than were justified by the use of an
isolated value. Although execution context represents a cross-cutting
feature, this does not necessitate it being global, either as a global
variable or as a universal type.
A CONTEXT OBJECT introduces an inversion of dependencies [Mar04]
and responsibilities. It may also introduce an inversion of control
flow, but this is not necessarily always the case: an informational
context may be more passive, but a behavioral service context will
generally result in a more obvious inversion of control flow.
##%%&&
When realizing a MODEL-VIEW-CONTROLLER (188), PRESENTATION ABSTRACTION-CONTROL (191), MICROKERNEL (194), REFLECTION (197),
SHARED REPOSITORY (202), BLACKBOARD (205), EXPLICIT INTERFACE (281),
INTROSPECTIVE INTERFACE (286), COMBINED METHOD (296), WHOLE-PART (317),
MASTER-SLAVE (321), TRANSFORM VIEW (347), OBSERVER (405), or TABLE DATA
GATEWAY (544) configuration . . .
. . . we must make calls to query and update data in remote component
objects.
Many stateful component objects need an interface that supports
querying and optionally updating of their attributes. Remote
communication can incur significant overhead on each call, however, so individual methods for getting and setting each attribute
are inefficient.
Accessing individual attributes of remote objects via correspondingly
fine-grained calls is expensive. In addition, such an interface introduces coherence problems, because if a client needs to query more
than a single attribute at a time, another client may change the
state between the individual calls, even though each individual query
was synchronized. Similarly, locking the remote object for the duration of a client¡¯s attribute queries is inefficient. A COMBINED METHOD
(296) would make sense if only a single method were involved, but
if the same data items appear in the argument lists of multiple
methods, duplication and interface instability may occur if argument
lists change. Moreover, not all languages support the concept of out
parameters to support queries simply and directly.
Therefore:
Bundle all data items that might be needed into a single data
transfer object used for querying or updating attributes together.
A DATA TRANSFER OBJECT has little behavior of its own, containing only
the data corresponding to the attributes, queries to access them, and
a way of initializing and optionally setting the data values.
The resulting DATA TRANSFER OBJECT is more network-friendly, since
only a single remote call is needed to query or update a set of
attributes. If the set of attributes changes, moreover, the call interface of the remote object remains stable, even though the data
transfer object changes. DATA TRANSFER OBJECTS are also useful in non distributed systems, where they help to avoid many fine-grained data
access calls to component objects, and also dependencies of clients
on concrete data representations used within component objects.
Even if not all the attributes are needed from a query, the cost
of a single call to copy the transfer object across the network is
still often lower than the repeated cost of querying each attribute
individually. If only a few attributes need to be updated, the caller
must either resupply the existing values¡ªwhich could require an
additional call to query them and the additional risk of overwriting
another update¡ªor indicate that some attributes are not updated.
This can either be expressed in the data transfer object or in the
method signature.
A data transfer object needs some way of indicating that a value has
not been set, which for certain data types can be indicated using
nulls. For primitive types, however, additional flags may be needed.
Alternatively, a simple set of flags indicating which values have been
set can be passed along with the data transfer object, though this
solution can be brittle and awkward to use. A more loosely typed
approach can use a data transfer object made up of name¨Cvalue
pairs.
##%%&&
When specifying a LAYERS (185) design or a DOMAIN OBJECT (208) . . .
. . . we must consider that the implementation of objects can vary
independently of their interfaces.
An object may have one of several different implementations.
The difference between these implementations could be platform specific, or a runtime decision. Using inheritance to separate
interface and implementation, however, can expose the client of
the object to decisions about its implementation.
Inheritance is a typical approach to separating interface and implementation: an interface declares the object¡¯s visible functionality, and
an implementation realizes the methods declared in the interface. If
accessed via the concrete class, however, there is a coupling that
becomes a liability when striving for a stable design: client code
now depends on the underlying implementation type. If accessed
consistently through the interface, object users will be unaffected by
changes in the underlying implementation class, but they are not free
of all dependencies on the implementation: at the point of creation, a
client must make a decision concerning the underlying type, which
can clutter and overcomplicate client code.
Therefore:
Split the object into two parts: a handle abstraction that provides
its interface, and a separate implementor hierarchy that provides
the various implementations for the body.
Clients only communicate with the handle object, which in turn is
configured with a body that is an instance of the required implementor
class. Method calls on the handle are forwarded to the associated body
to perform the implementation. The handle may also be responsible
for managing the lifecycle of the body.
Using a BRIDGE makes clients of an object depend directly only on
an interface: the implementation decision and detail is encapsulated.
Consequently, clients are unaffected by implementation changes.
Similarly, changes in an object¡¯s interface do not ripple through
to its implementation if the new handle interface can be mapped
onto the existing body, for example with an OBJECT ADAPTER (438).
This strict separation of concerns can incur a performance overhead,
however, due to the additional level of indirection introduced by using
BRIDGE. The shorter the execution time of the body, the higher this
performance penalty becomes.
Either side of a BRIDGE structure can be concrete, or based on a
class hierarchy. In the latter case, it is often based on two EXPLICIT
INTERFACES (281): one for the abstraction hierarchy, one for the implementor hierarchy. Concrete implementations of the body hierarchy
are often realized as ENCAPSULATED IMPLEMENTATIONS (313). The handle
must choose a suitable, specific concrete implementation for the
body, which may be linked dynamically at runtime.
Normally the body instance is exclusive to a given handle, but if
it is immutable it can be shared. Sharing is sometimes also used
as a space optimization when the cost of modifying the body can
be deferred. AUTOMATED GARBAGE COLLECTION (519) helps to prevent the
accidental destruction of a body by multiple handle instances. If
garbage collection is not supported, the handle can be implemented
as a COUNTING HANDLE (522) to keep track of the number of handles
using the body.
##%%&&
When realizing LAYERS (185), DOMAIN OBJECT (208), BROKER (237),
MICROKERNEL (194), PROXY (290) ENCAPSULATED IMPLEMENTATION (313),
BRIDGE (436), CHAIN OF RESPONSIBILITY (440), or CONTAINER (488) . . .
. . . we may need to address the mismatch between the provided interface of an existing component and the interface required by clients.
Applications can benefit from reusing existing code. An existing
class, however, does not always provide the interface that its
clients expect, but instead might provide too much, too little, or
in the wrong style.
Using the original class interface directly may be simple, but is
often problematic because clients are coupled tightly to a particular
implementation and interface. Interface changes can affect all clients,
which is an unnecessary side-effect arising from reuse. In addition,
the original interface may not reflect the intended usage scenarios of
the application, which requires either extra glue code or bending the
usage code towards the existing interface. The former option would
pollute the clients with (duplicated) infrastructure code, which is
tedious and error-prone to maintain, and also does not contribute to
realizing their main responsibilities. The latter option could result in
client domain models that are suboptimal from the perspective of the
application using the component. Since reusing existing code should
ideally be a means to an end and not an end in itself, reused code
should offer an interface that allows clients to use it in new contexts
on their own terms.
Therefore:
Introduce a separate adapter between the component and its
clients that converts the provided interface of the component
into the interface that the clients expect, and vice versa.
Calling a method on the adapter maps the request onto the adaptee¡ª
the instance of the class being reused. Result data structures returned
by the component are transformed into the result data structures
expected by the calling client.
An OBJECT ADAPTER shields clients from a specific decision to use an
existing implementation in a new context, while still integrating with a
uniform client interface. If the adaptee interface changes, corresponding modifications in the application are localized in the adapter. These
interface changes are transparent to the adapter¡¯s clients.
The adapter class itself may be based on an EXPLICIT INTERFACE
(281) that its clients expect or require. The implementation of the
adapter normally relies on composition. During construction, either
the adapter initializes an instance of the adaptee directly or, in the
case of a looser relationship, it receives an instance of the adaptee.
Pluggable adapters can be defined either with respect to an adaptee
instance passed in on construction, or with respect to a parameterized type. Regardless of which construction variant is used, calls on
the adapter are forwarded to the adaptee.
An OBJECT ADAPTER may introduce an additional level of indirection and
an additional object creation. This cost is not necessarily incurred if
the adaptee¡¯s representation is embedded within that of the adapter.
For C++, a data member of the adaptee type is contained in this way,
and the adapter method calls can be inlined to minimize overhead.
For C#, the adaptee would have to be a struct type. In other cases,
such as accessing via a pointer, or in other languages such as Java,
the indirection and extra creation cost will always be incurred.
The more complex the mapping between the interface required by
clients and the interface provided by the adaptee becomes, the more
expensive the mapping can become, in terms of runtime resources,
performance, and development effort. If the application cannot afford
this overhead, consider not using the adaptee at all, or refactor the
code to allow simpler adaptation.
##%%&&
When passing user input from the view to the controller in a MODEL VIEW-CONTROLLER (188) architecture . . .
. . . we want to reduce the coupling between the sender of requests
and the objects that can handle these requests.
Sometimes more than one object in an application could handle
a particular client request, such as user input received from
an input device. However, clients are often not interested in
knowing which specific objects are handling their requests¡ªthey
only want the application to execute requests appropriately.
Clients may also not be interested in the dispatching logic that decides
which application object should process a specific request, so that
any changes in the object structure or the request dispatching-logic
do not change the clients. Factoring out the request dispatching logic
to a separate infrastructure object is not always practical, however.
If only a few application objects can handle a specific client request,
or if the request dispatching logic is fairly simple, the complexity of
developing a general request dispatching infrastructure can outweigh
its benefits. Nevertheless, we need a mechanism for dispatching a
client request to its receiver object, and this mechanism should be
simple and efficient.
Therefore:
Chain the objects that can handle the requests and let each
object decide whether or not it can execute a particular request.
If it can, the object executes the request, if it cannot, the object
forwards the request to the next object in the chain.
Clients initially issue their requests to the first object in the ¡®chain
of responsibility.¡¯ As the request passes through the chain, each
object checks whether or not it can execute the request. If it can, the
object performs its processing and returns any results. If it cannot,
the object forwards the request to the request handling interface of
its successor in the chain, if there is one, otherwise it returns an
error.
A CHAIN OF RESPONSIBILITY frees clients from knowing the ¡®right¡¯ object
that will process each request they issue. This flexibility allows
dynamic composition of arbitrary request-dispatching chains without
modifications in either the clients or existing objects in the chain. The
longer a CHAIN OF RESPONSIBILITY becomes, however, the more overhead
is incurred due to the increased number of hops that requests make
before an object processes them. In addition¡ªand perhaps more
significantly¡ªclients have no assurance that the chain contains an
object that handles the requests they issue.
To specify a CHAIN OF RESPONSIBILITY, provide all objects with a reference to a potential successor object and a common EXPLICIT INTERFACE
(281) to receive client requests. This interface is invoked on each
object until one of the them decides it is qualified to process the
request and return the results. If the objects do not have a common
EXPLICIT INTERFACE, use an OBJECT ADAPTER (438) to bring them into
conformance.
##%%&&
When designing a COMMAND PROCESSOR (343) that receives request
messages, or a COMMAND (412) or COMPONENT CONFIGURATOR (490) that
receives script-based configuration parameters, or¡ªmore generally¡ª
a parameterizable ENCAPSULATED IMPLEMENTATION (313) . . .
. . . a mechanism is needed to interpret the data or scripts and execute
the right services on the component.
Some problems are often resolved via interpretation rather than
by precompiled algorithms. For example, searching for strings
that match a particular pattern can be resolved by interpreting
regular expressions. Interpreting an input stream or a data model
in a little domain-specific language, however, requires grammar
representation and an implementation of execution semantics.
Automated code-generation language processing tools, such as lex,
yacc, Boost.Spirit, and ANTLR support parsing the source of a language, but they do not necessarily address representation of the
abstract syntax tree or the execution of the code. For little languages
[Ben86], such as a shell based on single-line commands, intermediate representation is not necessary, and direct execution is possible,
even bypassing the need for a more formal parsing stage. Languages
with control structures or context-free grammars cannot be run so
easily or directly off the back of a stream of lexing or parsing events.
Therefore:
Introduce an interpreter that represents the grammar of the language and its execution. The interpreter is a whole-part hierarchy
of classes, typically with one class per grammar rule.
The concrete dependencies between the expressions of the language
form the basis of its abstract syntax tree. Clients need to address
lexical analysis and parsing, and the tree structure forms the basis of
the INTERPRETER model. Leaves define the terminal expressions in the
grammar, non-leaves the non-terminal expressions, which consist
of multiple sub-expressions. Sub-expressions can be either non terminal or terminal.
The root of the tree represents the most general non-terminal expression of the language and thus defines its entry point for execution.
After all sub-expressions are interpreted, the root assembles the final
interpretation of the received sentence and returns it to the caller of
the interpreter.
An INTERPRETER design defines a direct and convenient way to represent and interpret grammars for little languages, such as structured
messages and scripts, and thus avoids the complexities of more
sophisticated representation models. The more complex is the gram mar to be represented, however, the less practical an INTERPRETER
arrangement becomes, because for each rule in the grammar there
is a separate class to implement and maintain. Other approaches
that separate the concerns of interpretation from those of grammar,
such as automated code-generation language processing tools, may
therefore become simpler in practice.
Typically, the syntax tree of the represented language is realized
as a COMPOSITE (319), to provide an infrastructure for managing the
structure of¡ªand the interactions within¡ªrecursive whole-part hierarchies of objects of similar type. Functionality that operates on the
entire object hierarchy of the interpreter, such as syntax checking,
type checking, and generation of output, can be localized within
a VISITOR (447), to avoid it being scattered across the interpreter¡¯s
construct classes.
The INTERPRETER structure is stateless with respect to any given execution: execution state is expressed in a CONTEXT OBJECT (416) that is
passed into the methods of an INTERPRETER arrangement.
##%%&&
When implementing the REQUESTOR (242) or INVOKER (244) parts of
communication middleware, a COMMAND PROCESSOR (343), or¡ªmore
generally¡ªan ENCAPSULATED IMPLEMENTATION (313) . . .
. . . it may be necessary to add out-of-band service extensions to a
component or framework.
It can be hard to anticipate how the behavior of a framework may
need to be tailored for different environments or applications.
Features and attributes of an otherwise stable core set of services
may need adaptation or extension. However, some behavioral
modifications might be cross-cutting and associated with certain
uses for all objects rather than just a few specific objects.
For example, communication middleware must provide remote IPC
services, but not every user requires load-balanced communication
or the same security policy. A gradual integration of such service
extensions into a middleware framework may not be practical, as it
will bloat both functionality and code over time, and all users (and
developers) will incur the overhead of many rarely used extensions.
Moreover, the original developers or maintainers of a middleware
framework may not have made the most appropriate choice when
implementing certain extensions. Yet some applications will need to
integrate out-of-band service extensions into a middleware framework
to meet their particular requirements.
Therefore:
Allow users to tailor a software framework by registering out of-band service extensions via predefined callback interfaces,
known as ¡®interceptors,¡¯ then let the framework trigger these
extensions automatically when specific events occur.
The interceptors realize predefined callback interfaces to implement
service extensions that process occurrences of the events in a (user-)
specific manner. Interceptors must register with the framework so
that it will notify them when events of interest occur. When an
interceptor is notified by the framework, it executes its out-of-band
functionality. Control flow returns to the framework after the interceptor finishes its execution.
An INTERCEPTOR infrastructure supports the configuration of a framework, or components within a framework, with new and unanticipated
features that only a few applications need, without incurring significant overhead for applications who do not use these features. The
framework thus stays lean, offering only the core functionality needed
by all users, thereby improving its usability and commonality. The
strict separation of framework and interceptors also allows independent variation and evolution of core functionality and service
extensions.
The high degree of flexibility supported by an INTERCEPTOR design can
also incur some costs, however. For example, the infrastructure for
notifying interceptors is always executed when relevant events occur,
even if no concrete interceptor is registered with the framework.
This may incur excessive time and space overhead if there are many
interception points and many events. In addition, the framework has
no direct control over registered interceptors and is unaware of their
operational qualities. It is therefore hard to specify a precise contract
for the framework with regard to its performance and behavior in the
event of errors or security holes caused by interceptors.
To realize an INTERCEPTOR arrangement, first select all events internal to
the product on whose occurrence users are likely want to perform out of-band service extensions. For example, in an Object Request Broker
(ORB) clients often require transaction or security support before and
after marshaling and demarshaling service requests. Partition the
selected events into two sets: a reader set and a writer set. When
events from the reader set occur, users are only allowed to obtain
information about the event. Intervention of the product¡¯s control
flow is prohibited, which supports product stability. When events
from the writer set occur, users are also allowed to modify the state
of the product: this supports sophisticated service extensions, but
can corrupt state in the product or cause misbehavior. Group related
events into disjoint interception groups, for example events that deal
with request sending in an ORB, because it is likely that users want
to execute the same out-of-band behavior in response these events.
For each interception group, define an interceptor callback inter face, an EXPLICIT INTERFACE (281) that specifies one method for each
of the group¡¯s events. Concrete interceptors derive from these interfaces to implement specific out-of-band service extensions. Ideally
interceptors should catch and handle all internal failures before
they return, but a robust INTERCEPTOR implementation should not
assume this and should assume the worse, or risk failure rather than
resilience in the face of faulty interception.
When an interceptable event occurs in the framework, notify the
corresponding interceptors via a dispatcher. The necessary notification chain can be implemented via two OBSERVER (405) arrangements.
The framework is a publisher to which dispatchers subscribe, to be
notified about the occurrence of particular events. Each dispatcher
is also a publisher to which interceptors subscribe, to be called back
when events of interest occur so that they can execute their functionality. Within the dispatchers, implement an appropriate interceptor
callback policy, such as the order in which interceptors register, or
interceptor priorities. The main benefit of introducing a dispatcher
rather than letting the framework notify interceptors directly is that it
keeps the product¡¯s design and implementation independent of interceptor callback interfaces, as well as of the interceptor registration
and notification infrastructure.
When a framework calls back to a interceptor it can pass along a
CONTEXT OBJECT (416) containing information about the event that
occurred. This design allows the interceptor to adapt its behavior
depending on the current execution context of the framework.
Two specific types of INTERCEPTOR are INTERCEPTING FILTERS [ACM01] and
INVOCATION INTERCEPTORS [VKZ04]. An INTERCEPTING FILTER allows service
requests on a component to be intercepted and manipulated before
they are executed. An INVOCATION INTERCEPTOR supports the injection
of optional infrastructure functionality into inter process communication, such as for load balancing and security.
##%%&&
When implementing an ENUMERATION METHOD (300), an ENCAPSULATED
IMPLEMENTATION (313), a WHOLE-PART (317) or COMPOSITE (319) assembly,
or an INTERPRETER (442) . . .
. . . at times we want to implement services that operate on an aggregate object structure.
Some services operate on complex, often heterogeneous object
structures, for example state summarization and queries in a
topological tree. Scattering the service implementation across
the classes that define the object structure, however, creates a
design that is hard to understand, maintain, and evolve.
These problems arise from a lack of modularity. A single functionality
is split across multiple parts, one for every object type on which
it operates, and its implementation cross-cuts the classes of all
objects in the structure. The more scattered the service, the harder
it becomes to see the big picture, since there is no single place
that contains the service implementation. Lack of modularity also
bloats the classes of the object structure with functionality that is
not their prime responsibility and whose boundary is not limited to
the implementing class. The alternative of collocating all behavior
in a single method that implements type-specific behavior through
runtime-type identification and a cascaded if else structure is not
much better.
Therefore:
Implement the service in terms of a separate visitor class that
expresses the type-specific behavior associated with each class
in the object structure. Extend the classes that define the object
structure with a method that accepts a visitor, and selects and
calls back the correct corresponding method to execute.
The visitor class defines a method for each class on whose instances
it operates. Each method implements that portion of the service that
operates on the instances of the corresponding class. Pass the visitor
to the object structure to be visited, and let each object in this
structure call back that method of the visitor that corresponds to the
object¡¯s class. The called method then executes the respective part of
the service.
The visitor modularizes services whose code would otherwise be
scattered across many application classes, thereby improving the
understandability and maintainability of service algorithms and code
by centralizing everything in one location: the visitor. The visitor
infrastructure also protects a stable design center [Gam97] from
uncontrolled changes: it is possible to attach new services to an
object structure without touching its design and implementation.
Stability is a two-way contract, however, so the object structure must
be stable for VISITOR to be effective. Adding to the set of classes
that define the object structure will ripple through the whole visitor
hierarchy, requiring the addition of a new method to every visitor
class.
VISITOR is commonly implemented using DOUBLE DISPATCH (408). An
EXPLICIT INTERFACE (281) declares the required set of visit methods,
each taking an instance of their corresponding object structure class
as a parameter. Concrete visitors realize this interface to implement
a particular service. Each visit method of a concrete visitor implements the portion of behavior that operates on the object structure
class with which it is associated.
All classes whose instances participate in the object structure should
have an accept method that takes an abstract visitor as an argument
and calls back the corresponding visit method on the received
visitor, passing the object to be visited along as a parameter¡ªthat
is, the object on which the accept method was invoked. Each visit
method then executes its service on the particular object that it
receives when being called back.
##%%&&
When realizing an adaptable and flexible ENCAPSULATED IMPLEMENTATION
(313) or FRONT CONTROLLER (339) . . .
. . . at times we want to add responsibilities to an individual object,
but not to the methods of its corresponding class.
It is sometimes necessary to extend the methods of an object
dynamically with additional pre- and post-processing behavior,
such as data compression, logging, or security checks. Such
extensions, however, should not affect other instances of the
same class if they do not need these extensions.
Integrating the additional behavior into the class directly, and allowing it to be switched on or off depending on the needs of an object¡¯s
clients, is at best a short-term solution to this problem. Over time
the class becomes polluted with a ¡®shopping list¡¯ of optional behavior
that only few clients need and use, but for which all clients (and
developers) must pay in terms of resource consumption, performance
overhead, and complexity. Separating the add-on behavior avoids
such bloat, but introduces a challenge: clients generally do not wish
to distinguish between accessing and using the ¡®core¡¯ object versus a
new object augmented with additional behavior.
Therefore:
Wrap the original object in a decorator object whose interface
conforms to the original object. Implement the optional, extra
functionality within the decorator object, and forward requests
on its interface to the original object after or before its add-on
functionality is executed.
If a client invokes a method on a decorator, the optional behavior is
executed in addition to the original object¡¯s core behavior: if a client
invokes a method on the original object, only its core behavior is
executed.
Decorating an object keeps its core implementation clean and lean,
but extensible. In addition, extensions are transparent to the object¡¯s
clients. Hiding extensions from clients, however, can introduce hidden costs, because clients are unaware of whether they access the
original object or its decorator.
To apply DECORATOR, organize both the original object and its decorators within a single class hierarchy. The root of this hierarchy is an
EXPLICIT INTERFACE (281) that defines the visible behavior of the object
that can be decorated. Two types of class realize this interface: the first
class represents the original object whose behavior can be extended
by decorators, and the second class forms the base for all concrete
decorators. This class maintains a reference to an object conforming
to the explicit interface of the hierarchy, which enables a concrete
decorator to ¡®decorate¡¯ either the original object or another decorator.
This design allows addition of nested decorators to the original object,
forming a chain in which each decorator extends the original object¡¯s
methods with additional behavior. Concrete decorators inherit from
the decorator base and implement a particular additional pre- and
post-processing for some the original object¡¯s methods.
Clients program only to the explicit interface of the class hierarchy.
Via polymorphism it is then possible to (dynamically) configure the
clients transparently with either the original object or a (nested) decorator. When a client invokes a method on the outermost decorator, it
and all of the associated decorators in the chain are run, terminating
at the original object.
##%%&&
When implementing an adaptable and extensible ENCAPSULATED
IMPLEMENTATION (313), a SCOPED LOCKING (390) arrangement, or a WRAPPER
FACADE (459) that encapsulates a resource . . .
. . . we need to execute a pair of related actions around a sequence of
C++ statements.
In C++, paired actions¡ªin which a function is called before
some statement sequence and a corresponding function after wards¡ªare commonly associated with resource acquisition and
release. Pre- and post-sequence actions are a common feature
of block-scoped resource management, for example to allocate
memory, use it, and deallocate it. Programming such action pairs
explicitly for each such sequence in an application, however, is
error-prone, not exception-safe, and leads to repetitive code.
Practice shows that it is easy to forget the action following the
sequence. First, developers must make sure that the action is executed in every return path out of the sequence, whether unlocking a
lock, releasing a resource, deleting an object. Second, when exceptions are thrown, they can bypass the post-sequence action, further
complicating matters.
Therefore:
Provide a helper class whose constructor implements the pre sequence action and whose destructor the post-sequence action.
Define an object of this class on the stack before the sequence
of statements, and provide its constructor with the necessary
arguments to perform the pre- and post-sequence actions.
In C++, a constructor is called on creation of an object for the sole
purpose of initializing it; that is, the constructor describes the ¡®boot
sequence¡¯ for an object. Conversely, a destructor is called automatically at the end of an object¡¯s life to ¡®shut it down¡¯ in an orderly fashion.
The calling of the destructor is deterministic: for stack variables
lifetime is tied to the enclosing scope, even if exceptions are thrown.
It is this determinism that allows constructor and destructor to straddle a sequence. A helper object can take advantage of this: executing
the pre-sequence action in the constructor and the post-sequence
action in the destructor. Where the helper object must make calls on
another object, the object and any additional state must be passed
to the constructor.
An EXECUTE-AROUND OBJECT addresses both exception safety and
control-flow abstraction, which results in less repetitive and less
error-prone code. Destructors are called for stack objects as the
stack is unwound upon leaving a scope. Stack unwinding occurs
as a result of either normal control flow out of the scope, or exceptional flow from a thrown exception. Clean-up actions therefore occur
independently of how control flow leaves a sequence of statements.
If the post-sequence action depends on whether or not an exception
has been thrown, the std::uncaught exception function can be
used to determine the reason the destructor is being called. The
result of std::uncaught exception is worthless, however, if the
stack was already unwinding when the helper object was created.
EXECUTE-AROUND OBJECT relies on two key language features, the combination of which is specific to C++: the deterministic and scope-bound
destruction of objects created locally, and the ability to execute code
automatically within an object just before its destruction. Other forms
of execution wrapping behavior are possible in other languages. For
example, in C# the combination of a using block and an object
that implements the IDisposable interface comes close to realizing
EXECUTE-AROUND OBJECT.
##%%&&
When implementing an ACCEPTOR-CONNECTOR (265), ENCAPSULATED
IMPLEMENTATION (313), WHOLE-PART (317), MASTER-SLAVE (321), PAGE
CONTROLLER (337), FRONT CONTROLLER (339), COMMAND PROCESSOR (343),
HALF-SYNC/HALF-ASYNC (359), LEADER/FOLLOWERS (362), ACTIVE OBJECT
(365), or THREAD-SAFE INTERFACE (384) arrangement . . .
. . . we often need to express objects that share a common structural
and behavioral core, but vary in particular behavioral aspects.
Some objects have a common structural and behavioral core,
but differ in specific behavioral aspects. Providing full, separate
classes for each behavioral variant, however, can duplicate code
and complicate maintenance of the object¡¯s invariant core.
Although stable, the common behavioral core may evolve. Whenever
it does so, each separate class must be updated¡ªwhich is not only
tedious, but also error-prone, since version skew can result if updates
are done inconsistently. Separating the object¡¯s invariant core from
its variant aspects by delegating to another object would avoid version
skew, but is not necessarily self-contained, since it involves creating
and managing two separate objects.
Therefore:
Create a superclass for the behavioral variants that provides a
template method that expresses the common behavioral core.
Within the template method, delegate execution of variant
actions to separate hook methods that are overridden by each
subclass that implements the variant behavior.
Clients using this pattern thus depend on only a single class and a
single object. Calling the template method executes the common core
inherited from its superclass, which in turn calls and executes the
hook method versions implemented in the subclass.
A TEMPLATE METHOD design allows all object variants to share a single
implementation of its structural and behavioral core, which avoids
code duplication and maintenance overhead for the object¡¯s invariant
parts. In addition, the clear separation of a service¡¯s variant parts from
its invariant parts supports independent modification and evolution
of the two aspects. The use of inheritance to separate the two allows
variant and invariant parts of the object to share the same data
structures directly.
TEMPLATE METHOD can also increase code duplication and maintenance
costs, however, if multiple service subclasses share implementations
of specific hook methods, and the problems this design is intended
to avoid for the object¡¯s invariant core are reintroduced at the level
of its variant parts. TEMPLATE METHOD is a class-level rather than an
object-level decision, so there is a strong coupling of variants to
the superclass. The fragile base class problem [Szy02] is therefore
a potential liability unless the superclass is a strong design center.
TEMPLATE METHOD is also not appropriate if a component needs to
export only interfaces for its plug-in behaviors, because it relies on a
partially implemented class.
One way to ensure the invariance of the common behavioral core
when implementing TEMPLATE METHOD is to make the public template
method non-polymorphic. It is also common to implement default
behaviors for private hook methods [Pree94]. If the default is more
complicated than a do-nothing implementation, however, this can
make the design less stable and overriding more subtle, since subclass developers must be more aware of what they are overriding.
##%%&&
In an ACCEPTOR-CONNECTOR (265), ENCAPSULATED IMPLEMENTATION (313),
WHOLE-PART (317), MASTER-SLAVE (321), PAGE CONTROLLER (337), FRONT
CONTROLLER (339), COMMAND PROCESSOR (343), HALF-SYNC/HALF-ASYNC
(359), LEADER/FOLLOWERS (362), ACTIVE OBJECT (365), STRATEGIZED LOCKING
(388), OBJECT MANAGER (492), EVICTOR (515), or ABSTRACT FACTORY (525)
arrangement . . .
. . . we often need to realize objects that share a common structural
and behavioral core, but vary in multiple behavioral aspects.
Some objects need to implement behavior across one or more
methods that differ on a case-by-case basis. To identify the case
with a flag, so that distinct behavior can be implemented by
explicit selection, is however a brittle and closed solution that
scales poorly.
The problems with flag-based approaches are that methods are coupled to the flag, and each flag-dependent method duplicates the same
switch selection structure. This approach suffers all the usual problems arising from duplication: adding new cases leads to repetition
of case structure, or, worse, the change is made incorrectly. Such
methods are typically long and get longer with time. Not only does
this approach scale poorly, it is also a closed solution that requires
modification of source code every time a new option is added.
Therefore:
Capture the varying behavioral aspects of the object separately
from its defining service class in a set of strategy classes. Plug
in an appropriate strategy instance, and delegate the execution
of the variant behavior to the appropriate strategy within the
implementation of the service class.
STRATEGY allows parameterization of the variant part of code that needs
to have a stable behavioral core.
In a given STRATEGY design, a service class defines the usage code, such
as a common behavioral core, and a separate strategy interface with
one or more hook methods expresses the variant aspects. The strategy
interface is an EXPLICIT INTERFACE (281) that the implementation of the
service class uses to delegate the execution of the variant service
behavior to a separate object. Concrete strategy classes implement
the strategy interface, to implement the variant behavior for a specific
service variant. The clear separation of variant parts from invariant
parts supports independent modification and evolution. If a strategy
class needs to perform an action that depends on some or all of the
state of the service object itself, the service object needs to pass a
reference to itself or to a CONTEXT OBJECT (416).
STRATEGY is useful for implementing behavioral extensions to an
object¡¯s services. The STRATEGY service class interface defines these
extension points, and pluggable extensions implement extended
behavior as appropriate. For this reason, the STRATEGY pattern is
also known as PLUGGABLE BEHAVIOR [Beck97]. If no behavior is needed,
a NULL OBJECT (457) offers a simpler and more consistent approach
than introducing null checks throughout the code.
There are two basic options for implementing STRATEGY: runtime
polymorphism, based on instance methods, and parametric polymorphism, based on generic mechanisms such as templates in C++
or generics in Java. In the runtime approach, strategy classes often
implement the strategy interface to implement the behavioral variants. Instances are used to parameterize behavior at runtime. The
templated approach, also known as POLICY [Ale01], fixes the parameterization decision at compile time, which reduces object creation
and indirection costs, and offers opportunities for compiler inlining.
The flexibility trade-off, where available, is in terms of type intrusion,
binding time¡ªcompile or runtime¡ªand performance. The templated
approach should be used if runtime reconfiguration of the context
object is unlikely or impossible, for example if strategies take advantage of operating system or hardware properties and APIs, or other
system environment aspects.
##%%&&
When realizing STRATEGIZED LOCKING (388), STRATEGY (455), or¡ªmore
generally¡ªvariant behavior that is expressed through polymorphism . . .
. . . we should consider that one option for implementing varying
behavior is to do nothing.
Some object behavior is executed only in the presence of some
other object. If this other object is absent, the behavior is either
to do nothing or to use some default value. Using explicit conditionals to check an object reference for null and then branching,
however, introduces a great deal of repetition and complexity
into the code.
Repeated checks against null break up the flow of code and are
often forgotten. In many cases, the conditional is simply a guard that
prevents null references from dereferencing: the alternative path of
action results in a null operation or an assignment of a simple default
value. In these situations the recurring null guard is overly visible
and begins to have all the drawbacks of programming with flags.
There is a strong case for factoring out repetition and making best
use of the language mechanisms available.
Therefore:
Provide something for nothing: a class that conforms to the
interface required of the object reference, implementing all of
its methods to do nothing, or to return suitable default values.
Use an instance of this class, a so-called ¡®null object,¡¯ when the
object reference would otherwise have been null.
Calling a method of a null object has no effects or side effects, since
nothing happens. In some situations, the overhead of the call can be
optimized away completely by the compiler.
A NULL OBJECT is encapsulated and cohesive: it does one thing¡ª
nothing¡ªand it does it well. This reification of the void and encapsulation of the emptiness eliminates superfluous and repeated conditionals that are not part of a piece of code¡¯s core logic, thus
making the absence of behavior easier to use. Selection and variation are expressed through polymorphism and (interface) inheritance
rather than procedural condition testing. A NULL OBJECT can be seen
to remove specific recurring conditional statements. The decision
structure has been objectified and concealed, taking advantage of
encapsulation and polymorphism: the object relationship moves from
being optional to mandatory, making its use uniform and consistent.
NULL OBJECT should not, however, be used indiscriminately as a
replacement for null references. If the absence of an object is significant to the code¡¯s logic and results in fundamentally different
behavior, using a NULL OBJECT is inappropriate. It is important to
ensure that access to, and the use of, the potentially nullable reference are encapsulated. In particular, expecting clients to pass around
NULL OBJECTs consistently and correctly instead of null references may
make interfaces more complicated to use.
A NULL OBJECT is stateless and immutable by definition¡ªit is thus
sharable and intrinsically thread-safe. NULL OBJECTs scale poorly
to remote objects, however, because method calls will always be
executed across the network and their arguments will always be
evaluated, which can incur significant overhead and introduces
an additional point of failure, thereby undermining their basic ¡®do
nothing¡¯ behavior. In a distributed context, the NULL OBJECT should
therefore be passed as a COPIED VALUE (394): transparent replication
rather than transparent sharing becomes the appropriate choice.
##%%&&
When realizing a BROKER (237), CLIENT REQUEST HANDLER (246),
SERVER REQUEST HANDLER (249), REACTOR (259), PROACTOR (262),
ACCEPTOR-CONNECTOR (265), MODEL-VIEW-CONTROLLER (188), ENCAPSULATED
IMPLEMENTATION (313), HALF-SYNC/HALF-ASYNC (359), FUTURE (382),
STRATEGIZED LOCKING (388), or SCOPED LOCKING (390) arrangement . . .
. . . we want to access low-level, function-based APIs in a convenient,
robust, and portable manner.
Applications often have some code that needs to use services
provided by low-level, non-object-oriented APIs. Programming
applications with these APIs directly, however, makes the code
hard to understand during development. It is also a poor choice
for testability, portability, and the long-term stability of the
code, since today¡¯s choice of platforms may not be tomorrow¡¯s.
When using low-level, function-based APIs¡ªtypically C¡ªthe code is
often repetitive, focused on API minutiae, and error-prone. It can also
be non-portable, even across different versions of the same platform.
It is often unclear how different functions in the same API are related.
Programming against such APIs scatters common code across an
application, making it hard to ¡®plug in¡¯ alternative solutions.
Therefore:
Avoid accessing low-level function-based APIs directly. Instead,
wrap each related group of functions and data within such an
API in a separate, cohesive wrapper facade class.
Calling a method on a wrapper facade maps the client request to a
corresponding API function, or executes a sequence of API functions
in a specific, predefined order.
WRAPPER FACADE provides a concise and robust set of classes for accessing functions of system APIs, such as those of operating systems and
GUI libraries. The code for accessing low-level APIs is encapsulated,
so the access code need not be repeated. WRAPPER FACADE also improves
application portability, since the wrapper interfaces can remain stable even if underlying APIs change. A WRAPPER FACADE is a class, thus
it can be used as the basis for pluggable components in a generative
programming model [CzEi02].
Within the API to be encapsulated, identify existing abstractions and
their relationships, which may be different than the ones documented
in the original API. Cluster each group of cohesive functions and data
structures into a separate wrapper facade. Errors signaled by the
API should be handled in the style appropriate for the target language¡ªwhich may not be the style of the API. For example, C APIs
typically signal failure via return values, whereas object-oriented languages use exceptions. In C/C++ legacy systems running on UNIX,
THREAD-SPECIFIC STORAGE (415) allows for the safe retention and handling of errors on a per-thread basis. To support the robustness of a
wrapper facade, automate resource acquisition and return where possible. In C++, for example, provide an EXECUTE-AROUND OBJECT (451).
A WRAPPER FACADE arrangement should make the common use of
low-level APIs simple and easy, but also allow more complex usage
scenarios. The interface of a wrapper facade may therefore need an
¡®escape hatch¡¯ that allows more direct access to the wrapped low-level
APIs. Although this design is a compromise, it avoids the need to
modify the wrapper facade for every special case, which would yield a
bloated, hard-to-use interface. C APIs are perhaps the most common
for low-level access. C++ allows simple, in-language wrapping, but the
WRAPPER FACADE pattern is more general than this specific scenario.
The same advice applies where a more sharply defined language
barrier exists, such as in Java or Ruby using C. In these situations,
a WRAPPER FACADE offers a more appropriate and cohesive design than
simply rewrapping the underlying API directly by exporting handles
as integers and functions as class static methods.
##%%&&
When realizing a DOMAIN OBJECT (208) or a CONTAINER (488) . . .
. . . we must tell the hosting environment of an application how to
handle the technical requirements of a specific component, such as
its transaction and security needs.
A runtime environment provides system resources and services
such as threads, network connections, security, and transactions
to components of an application. These resources and services
allow components to execute as specified. A runtime environment, however, cannot always anticipate the specific resources
and technical services each component requires, and how each
component wants to use these resources and services.
The runtime environment still needs this information to manage its
resources and components appropriately. Hard-coding the resource
and technical service requirements in the component¡¯s implementation is an impractical solution, however: functional aspects would
be mixed with technical aspects, although the two areas are generally independent of one another. Changing either independently
is awkward, and adapting the component implementation to handle
different aspects is manual and costly. Each adaptation would also
create a new component version, which must be maintained and
managed explicitly.
Therefore:
Specify a separate declarative component configuration for each
component that indicates to the runtime environment the system resources and services it needs to execute correctly, as well
as how it will use these resources and services.
Pass the declarative component configuration to the runtime environment during component deployment. The runtime environment can
use the specifications in the declarative component configuration to
configure itself so that each component can be managed accordingly.
A DECLARATIVE COMPONENT CONFIGURATION tells an application¡¯s runtime
environment how to manage the components it hosts, so the runtime environment can thus respond to the individual environmental
requirements of each component. This pattern supports flexibility
because components can shape the environment in which they are
located, rather than being forced to use a single ¡®one-size-fits-all¡¯ environment, or have application developers handcraft the environment
for each component. Keeping the DECLARATIVE COMPONENT CONFIGURATION
separate from the component interface and implementation simplifies changing the specification of component system resources and
service requirements without modifying the components themselves.
A DECLARATIVE COMPONENT CONFIGURATION often includes the component¡¯s
name, its resource requirements, dependencies to other component
interfaces, the security and transaction support it needs, its threading
model, and various quality of service parameters [VSW02]. Typically
it is provided in form of configuration scripts, for example as XML
files [OMG02] [MaHa99] [Ram02].
##%%&&
When realizing an ENCAPSULATED IMPLEMENTATION (313) . . .
. . . we sometimes need to support object behavior that alters significantly whenever an object changes its internal state.
The behavior of an object may be modal, where the mode depends
on its current state. Hard-wiring the corresponding multi-part
conditional code within the object¡¯s implementation, however,
can frustrate its comprehensibility and future development.
For example, an object that represents a controller for a user interface
needs to respond to common events in a way that is appropriate to the
current state of the view. The view may have many different modes,
depending on options and validation. Hard-coding a state machine
directly via switch and if statements does not scale effectively, and
is effective only for a small number of states that affect only a small
number of methods. Different state machine aspects cannot evolve
independently, such as code that represents the behavior of a specific
state, the transition logic that connects the states, or the integration
of new states.
Therefore:
Encapsulate the state-dependent behavior of the object into a
hierarchy of state classes, with one class per different modal
state. Use an instance of the appropriate class to handle the
state-dependent behavior of the object, forwarding method calls.
Whenever a client calls a method on the modal object whose behavior
is state-dependent, the object delegates the execution of the method
to an instance of the corresponding state class. Upon creation, the
modal object is associated with an instance of the state class that
implements the behavior to apply in its initial state. When the modal
object changes its state, the state object it uses is exchanged so that
it behaves correctly in the new state.
The encapsulation and organization of state-dependent behavior in a
class hierarchy allows the modal object to be configured dynamically
with instances of arbitrary state classes. An extension of the hierarchy
with new state classes is also simplified, as well as the modification
of an existing state class.
An OBJECTS FOR STATES design, however, distributes responsibility
across multiple classes, which can make it appear unnecessarily
complex when there are few states and few state-dependent methods.
This design can also be hard to manage if there are many states,
where the corresponding class explosion becomes the key cause of
complexity. Class nesting, or localizing state classes within packages
or files, can help to contain such complexity.
Implementations of the state classes are¡ªsomewhat ironically¡ª
normally stateless. Each state object receives the main object, or a
reference to its instance data, as an argument in each of its methods.
This statelessness allows modal objects to share the same instances
of the state objects, which can be accessed as static data. This style
of programming, however, may seem more indirect than necessary
for such a closely coupled system of classes. Conversely, it may be
simpler to make the state classes stateful if mode-specific state is
necessary, such as might exist during a transaction-based state.
In general, there are two implementation options for exchanging the
state instance that is used by the object. One option is to implement
this logic within the main object itself. The other is for the currently
associated state object to determine its own ¡®successor.¡¯ Either option
ensures that the currently used state object is exchanged correctly
whenever the object transitions into a new state. The trade-off to consider is central control over the state-transition logic versus flexibility
in its composition.
##%%&&
When realizing an ENCAPSULATED IMPLEMENTATION (313) . . .
. . . we sometimes need to support object behavior that changes significantly whenever an object alters its internal state.
The behavior of an object may be modal, where the mode depends
on its current state. Hard-wiring the modal behavior within
the object¡¯s methods, however, can make future development
unnecessarily awkward. Yet delegating the behavior to one of a
community of objects can also complicate the coordination of,
and data sharing between, different modes.
For example, an object that represents a network connection must
react differently if its methods are called before a connection is
established, when it is connected to a remote peer, or after the
connection is closed.
Conditional statements within an object¡¯s methods are one way to
express such behavior, but the more complex the state machine
becomes, the more complicated the object¡¯s evolution. Encapsulating each mode¡¯s behavior in a separate object untangles the modal
functionality, but can yield overly complex state machines if different
modes depend on the same data or require data-driven coordination.
Therefore:
Implement state-dependent behavior as internal methods of the
object, and use data structures to reference the methods that
represent the behavior of a specific state.
Whenever a client calls a method on the modal object whose behavior
is state-dependent, the object delegates the request to the internal
method referenced by the data structure that represents the current
state. Upon creation, the modal object should be associated with the
data structure that represents its initial state. When the modal object
changes its state, the data structure it uses is exchanged so that it
behaves correctly in the new state.
The encapsulation of state-dependent behavior in internal methods
allows the modal object to share data and context information among
different modes with maximal performance and minimal resource use.
The use of data structures to reference the state-specific methods simplifies the configuration and evolution of the object¡¯s state machine.
The data structure holding the method references can be a record-like
data structure with named fields, such as a C++ struct. Alternatively, a dictionary object can be used to locate the private method
reference that corresponds to each history-sensitive public method.
In effect, this configuration emulates the normal polymorphic method
lookup mechanism, such as a C++ vtable, with a little added customization, evolution, and intelligence. Where only a single public
method is state-dependent, no intermediate data structure is needed
to represent the mode: a single method reference will suffice. Global,
module, or class-wide variables can hold the single instance of the
data structure or method reference required for each mode.
The method references may be actual method references, such as
member function pointers in C++ or delegates in C#, or they may be
symbolic method names that are resolved using reflection, executed
by calling a DYNAMIC INVOCATION INTERFACE (288). This latter option is only
cost-effective for dynamic languages or for highly configurable objects
where the state machine can be specified externally to the class.
##%%&&
When managing service request objects in a COMMAND PROCESSOR (343),
AUTOMATED GARBAGE COLLECTION (519), or a similar collection-managing
arrangement . . .
. . . we often need to handle the lifecycle of objects or operate on them
collectively with respect to their current state.
Objects whose behavior depends on their current state may
be modeled as individual state machines. Sometimes, however,
their clients view the behavior of these objects as modal, whereas
the objects themselves are independent of any client-specific
state model.
For example, a garbage collector distinguishes between referenced
and unreferenced objects, but the objects themselves are, and should
be, unaware of this view. Making objects aware of their state and
allowing them to manage this state themselves would couple their
implementation too closely with the way their clients are using them.
Whenever a client changes its state model, all objects are affected,
which complicates their maintenance and evolution. This situation
is even worse if different clients have different state-dependent views
onto the objects. Additional resource and performance penalties can
occur if clients treat objects in the same state collectively, for example
all objects waiting for deletion by a garbage collector.
Therefore:
Within the client, represent each state of interest by a separate
collection that refers to all objects in that state.
Whenever an object changes its state, it is moved from the collection
that represents the source state to the collection that represents the
target state. The client can invoke only those methods on objects
referenced by a specific collection that are allowed to execute in the
state represented by that collection.
The current collection that references the object implicitly determines
its state, so there is no need to represent the state internally within the
object. Extrinsic representation of state may also be used, in addition
to intrinsic representation, as a speed optimization for selection of
objects in a particular state.
All objects of a particular state can be managed collectively, which
can yield a smaller object footprint and allow clients to execute
actions efficiently on them as a group. Client-specific state models
can be implemented without affecting the class of the objects, which
reduces the structural complexity of the application and supports the
independence and evolution of the objects and their class.
Within each client, there are at least as many collections as there
are states of interest: the simplest approach is to represent each
state exclusively from other states. It is also possible to add inclusive
collections, such as a collection that holds all the objects, in addition
to the inclusive ones that model a particular state.
With a growing number of states, therefore, COLLECTIONS FOR STATES
becomes less applicable, as the collections¡ªas well as the object
management functionality across all collections¡ªintroduce resource
management and performance overhead. Similarly, if the rate of state
changes is high, COLLECTIONS FOR STATES can become impractical, due
to the overhead of transferring objects between collections.
##%%&&
When realizing DOMAIN OBJECTS (208), a BROKER (237) middleware, or a
REFLECTION (197) or MICROKERNEL (194) architecture . . .
. . . we need to support flexible component configuration at runtime.
Prematurely committing an application to a particular set of
component implementations can be inflexible and near-sighted.
Some decisions cannot be made until late in the lifecycle, even
after deployment, and it is undesirable to force applications
either to carry the overhead of components they do not use, or
be unable to take advantage of better or newer components.
Components have their own lifecycle: they evolve and mature.
For example, new versions provide better algorithms or fix bugs.
Applications using these components should therefore benefit from
such improvements. Similarly, components whose implementations
depend on specific software or hardware environments must be
replaced when these environments change. Applications with high
availability requirements, however, cannot tolerate downtime, so
updates must have minimal affect on a running system.
Therefore:
Decouple component interfaces from their implementations and
provide a mechanism to (re)configure components in an application dynamically without having to shut down and restart it.
Organize components into suitable units of deployment so that they
are loadable dynamically, and provide a framework that supports
component (re)configuration under the explicit control of running
applications. Manage configured components centrally via a component repository, and offer an API or use some form of scripting to
(re)configure designated sets of components at runtime.
A COMPONENT CONFIGURATOR enhances flexibility by allowing replacement and redeployment of component implementations throughout
an application¡¯s lifecycle, even when it is executing. Similarly, applications only pay for the time and space overhead of components they
actually use, in contrast to unnecessary components linked into an
application statically.
To support effective component (re)configuration, all components
should define a common administrative LIFECYCLE CALLBACK (499) interface that includes operations to initialize a component and start its
execution, shut down a component and clean up its resources, suspend and resume a component¡¯s execution, and access information
about a component¡¯s current execution status. The component configurator framework uses this interface to (re)configure each component.
This administrative interface may also provide a protocol, such as
OBSERVER (405), to notify clients when a component terminates, as
well as to transfer state to a new version. State and relationships to
other components can be passed to a new version via a MEMENTO (414)
that is cached by the component repository during the replacement.
Organize components into dynamically linked libraries (DLLs) that
can be (un)loaded dynamically, and provide each DLL with FACTORY
METHODS (529) to create component objects and DISPOSAL METHODS (531)
to destroy these objects when they are no longer needed.
Internally, a component configurator consists of a component repository, which is an OBJECT MANAGER (492) that manages the configured
components. The interface of the component configurator is often a
FACADE (294) that shields clients from its internal structure and dele gates requests to its appropriate participants. It may also contain an
INTERPRETER (442) to process configuration directives if these are in a
simple scripting language.
##%%&&
When implementing a CLIENT REQUEST HANDLER (246), SERVER REQUEST
HANDLER (249), REACTOR (259), ACCEPTOR-CONNECTOR (265), ASYNCHRONOUS
COMPLETION TOKEN (268), MICROKERNEL (194), REFLECTION (197), THREAD SPECIFIC STORAGE (392), IMMUTABLE VALUES (396), CONTAINER (488),
COMPONENT CONFIGURATOR (490), or ROW DATA GATEWAY (542) arrangement. . . we must often manage the access to and lifetime of specific types
of objects, and their resources and relationships.
Some objects within an application, such as resource or server side component objects, require careful access control and lifecycle management to maintain and use them efficiently and
correctly. Implementing this functionality within the objects
themselves, however, burdens them with complex responsibilities and makes them hard to use and evolve.
Similarly, clients are not responsible for the actual management of
such objects, since that would couple them to the concrete type
of the objects, their access constraints, and the lifecycle policy. This
situation would also make discovery of objects harder, such as finding
an object via a key. Ultimately, these dependencies increase coupling
and complexity within an application. Ideally, therefore, a client
should depend only on an object¡¯s usage interfaces, not its housekeeping obligations.
Therefore:
Separate object usage from object lifecycle and access control.
Introduce a separate object manager whose responsibility is to
manage and maintain a set of objects.
Clients can use the object manager to access objects with specific
capabilities. If a requested object does not yet exist, the object manager can create it on demand. Clients may also request creation
of objects explicitly via the object manager. In some situations the
client may already have created the objects, and may hand custody
of them to a manager. The manager can also control the disposal of
its objects, either transparently or in response to client requests.
An OBJECT MANAGER frees managed objects and their clients from
detailed lifecycle-management and retrieval activities. It concentrates
and localizes object management for a particular kind of object into
a well-defined, easy to find, and encapsulated object type.
Some applications provide only one object manager for each type
of managed object. For example, one object manager could handle
threads, another could handle connections. Alternatively, an application can provide multiple object managers for different purposes and
different contexts. An example might be one object manager per group
of objects that are managed according to a specific set of policies.
If an object manager is shared between multiple threads, it should
offer a THREAD-SAFE INTERFACE (384). On the other hand, if objects are
only ever used within the thread that initiated their creation, an
object manager per thread would offer a simpler and more efficient
design.
Clients use the object manager retrieval services to request access to
objects. LOOKUP (495) services allow a client to search for a specific
object, for example based on object names, object properties, or other
types of key. An ITERATOR (298), ENUMERATION METHOD (300), or BATCH
METHOD (302) supports traversal of multiple objects without revealing
the internal structure of an object manager.
An object manager has several options for maintaining managed
objects. For widespread and diverse deployments, it may be appropriate to parameterize these options and policies with STRATEGY (455)
objects or types. In the typical case, however, a simple method interface for setting options is sufficient.
A RESOURCE POOL (503) can be used to keep a fixed number of objects
of equal type and identity constantly available, which is useful for
managing critical computing resources that are used continuously,
such as processes, threads, and connections. A RESOURCE CACHE (505),
in contrast, keeps specific objects available only for a certain amount
of time. To avoid degrading an application¡¯s quality of service, the
cache can dispose of unused objects and release their resources for
other objects to use. An EVICTOR (515) supports controlled removal
of infrequently used objects from a cache. Evicted objects may still
be referenced and accessed by clients, however, in which case they
can be reactivated by an ACTIVATOR (513). Alternatively, clients access
the objects via a VIRTUAL PROXY (497), and the proxy is responsible for
reactivation.
To prevent premature release of actively referenced objects, an object
manager can use various object removal policies. LEASING (517) enables
an object manager to specify the time for which references to objects
are valid, and offers clients the opportunity to renew their leases.
After a lease has expired, the object manager can destroy the objects
safely. COUNTING HANDLES (522), in contrast, initiate the removal of an
object as soon as it is known to be no longer referenced.
Objects maintained by an object manager must be created internally
or provided by clients. Registration functionality allows clients to
transfer custody of externally created objects to an object manager,
whereas FACTORY METHODS (529) support encapsulated and explicit
object creation. Objects can also be created for clients transparently, without their explicit intervention. Although client creation of
objects offers a certain flexibility, it also weakens design cohesion,
reduces the opportunities for resource-management optimization by
the manager, and increases the likelihood of custody-related errors.
Objects maintained by an object manager must be destroyed at
some point. Deregistration functionality allows clients to assume
responsibility for objects from the object manager. DISPOSAL METHODS
(531) request the deletion of objects explicitly. When shutting down
an application, the object manager often disposes of all remaining
managed objects before terminating, thereby ensuring proper release
of the resources used by the objects.
A set of LIFECYCLE CALLBACKS (499) common to all objects allows an
object manager uniform control over their lifecycle, including their
initial creation, eviction, reactivation, and final disposal.
##%%&&
When implementing a BROKER (237), a BUSINESS DELEGATE (292), a
REPLICATED COMPONENT GROUP (326), or an OBJECT MANAGER (492) . . .
. . . we typically need to discover and retrieve references to resources,
objects, and services that are held either locally or remotely.
In a distributed system, a server may offer many services to
clients. A client does not necessarily know which services are initially on offer when it is started. Similarly, services can be added
or removed over time. If clients do not know which services are
available, however, they cannot use them.
One way for clients to discover services in a distributed system is to
hard-code the addresses of the services into the client software. This
approach is clearly inflexible, however. Ideally, a server should be able
to publish services¡ªand clients to find these services¡ªefficiently and
scalably. However, a broadcast approach can be costly in its use of
bandwidth and processing time.
Therefore:
Provide a lookup service that allows services in a distributed
system to register their references when they become available,
and deregister their references when they become unavailable.
Clients in the system can use the lookup service to retrieve the
references of registered services.
LOOKUP is a ¡®clearing house¡¯ between clients and servers, allowing
clients to access server services without having clients hard-code the
location or references to the servers or the services they offer. Equally,
servers do not need to know the location of the clients that want to
access their services.
There are essentially two styles for organizing a LOOKUP service:
? Centralized, in which information about services resides in a single
location. The lookup service stores this information persistently to
ensure proper recover in case of system failures. This approach is
relatively straightforward to implement, but scales poorly and can
be a single point of failure.
? Distributed, in which a group of lookup services periodically publish the availability of their registered services. Some form of group
communication protocol may be used to multicast this information between the federated lookup services. Typically, a distributed
lookup service is realized as a HALF-OBJECT PLUS PROTOCOL (324).
Although a distributed lookup service is harder to implement, it
scales better and avoids a single point of failure.
The reference of a service can be associated with properties that
describe the service and the interfaces it offers. The lookup service
maintains this information internally to allow clients to select one
or more services based on queries. An ACTIVATOR (513) can (re)start a
lookup service when clients need to locate services, to minimize the
number of services that actively consume resources in a distributed
system.
To communicate with the lookup service, the clients and servers
need an access point. If the access point is not known, clients and
servers use a protocol to find it, which may involve contacting a
preconfigured set of bootstrap servers or broadcasting a message.
An available lookup service responds with a message containing
information about its access point.
The lookup service is the one resource that clients need to be able
to access simply, typically as part of its initial context, and by a
well-known name or via a CONTEXT OBJECT (416).
##%%&&
When implementing a PROXY-based (290) interface, or a FUTURE (382),
OBJECT MANAGER (492), CONTAINER (488), or ACTIVATOR (513) arrangement
using potentially expensive objects . . .
. . . we may need to reduce the cost of acquisition for optionally or
infrequently accessed resources.
Creating an object can be expensive in terms of memory or time.
This expense is particularly wasteful if the object is never used,
or is not used soon after creation. When it is needed, however,
the object must be present.
The cost of loading all objects that correspond to the rows in a large
database is costly and unnecessary, especially if only a few of the
objects are actually used. Similarly, a large collection of server objects
managed by a container could consume excessive memory and space
in the table of active objects. If these objects are used infrequently,
resources are needlessly overcommitted. Ideally, a resource user
should not have to incur overhead for resources they do not use, or
do not use for long periods. Managing a resource¡¯s lifecycle should
also not encumber resource users.
Therefore:
Introduce a proxy for an object that does not currently exist in
memory. The proxy may be able to handle simple requests, such
as a query of the intended target object¡¯s identifying key, but
when more complete object behavior is needed, the actual target
object is created and initialized as needed.
The proxy offers the same interface as the intended target object.
Each method is executed, either in terms of state that is stored in
advance of creating the actual target object, or in terms of on-demand
creation followed by forwarding. In the latter case, the existence of
the target is checked, the target is created if it does not already exist,
and the method invoked on the proxy is called in turn on the target
object.
VIRTUAL PROXY introduces a level of indirection to resource access,
using the offset in structure to support an offset in time: the time at
which the target object is created and committed is delayed until the
first point of use. There is always some cost associated with adding
an additional object to introduce a level of indirection, and this cost
must be balanced against the benefit and likelihood of optimization
under the expected application load.
The common implementation of VIRTUAL PROXY is simply in terms of
LAZY ACQUISITION (507) or a collocated ACTIVATOR (513). For objects that
can be decomposed into separately acquirable parts each of which
is associated with a different load cost, however, PARTIAL ACQUISITION
(535) offers an alternative that can spread the acquisition load more
evenly in some applications.
The cost of object creation is not eliminated in VIRTUAL PROXY, it is
simply deferred. Similarly, the likelihood that the late initialization
fails is also deferred. So instead of dealing with just the application
errors that might arise from using the resource, a client may also
have to deal with more fundamental resource errors, which affects
the transparency of the optimization.
The cost of first access is deferred when the option of consuming a
resource is exercised. A proxy to an infrequently accessed object will
hold onto the resource once initialized, however, even if it is never
or rarely used again. This retention can lead to resource hogging
and even resource exhaustion. The target object can be disposed
of immediately after the first access, as a follow-on action from
the forwarding. In some cases, however, this will simply lead to an
expensive (re)acquisition and release cycle. Alternatively an EVICTOR
(515) or LEASING (517) can be used to release the resource at a later
time so that is readily available for immediate subsequent use.
##%%&&
When implementing an EXTENSION INTERFACE (284), OBJECT MANAGER
(492), CONTAINER (488), or COMPONENT CONFIGURATOR (490) that is responsible for managing the lifecycle of framework objects . . .
. . . we need to ensure that framework objects are able to respond to
lifecycle-related events initiated by the framework.
The lifecycle of some objects is simple: their clients create them
before they are used, they stay alive as long as they are used,
and they are disposed of by their clients when no longer used.
However, some objects have a much more complex lifecycle,
driven by the needs and events of their component environment
and constrained by additional resource-managing techniques,
such as pooling and passivation.
Rather than ad hoc creation and disposal by their clients, the lifecycle
of the latter type of object is often controlled by frameworks according
to application-specific policies and architectural needs. An object may
also be passivated during its lifetime, for example to save memory
and other resources, and reactivated when it is accessed again. In
addition, the knowledge of how to perform these operations is specific
to the object, not the application. Yet it should be possible for the
application to control the lifecycle of these objects explicitly.
Therefore:
Define key lifecycle events as callbacks in an interface that is
supported by framework objects. The framework uses the call backs to control the objects¡¯ lifecycle explicitly.
Typically the set of callbacks includes operations for initializing and
finalizing the object, for passivating and activating it, and for passing
its state to and from persistent storage.
LIFECYCLE CALLBACKS enable a framework to control the lifecycle of
components explicitly, but without any knowledge of their internal
structure: the framework calls through an interface with appropriate
methods, and the object types implement this interface according
to their own structure and needs. All object types for a framework
often share the same LIFECYCLE CALLBACK interface, which allows the
framework to treat them uniformly.
It is one thing to create an object, but quite another to start using
it. A LIFECYCLE CALLBACK interface, therefore, often separates these two
phases explicitly: a FACTORY METHOD (529) is responsible for object
creation, and a separate initialization callback for the LAZY ACQUISITION
(507) of the resources used by the object. This two-phase construction is often mirrored by two-phase destruction, in which a lifecycle
callback for finalization is responsible for resource release, and a
DISPOSAL METHOD (531) for the actual object destruction.
Where the object is likely to need access to component environment
details or framework services, a CONTEXT OBJECT (416) can be passed
through to each callback.
OBJECT MANAGERS and CONTAINERS typically use lifecycle callbacks to
activate, passivate, and remove component objects that they manage.
##%%&&
When implementing a CONTAINER (488) . . .
. . . we need to ensure that partial failure of a task divided across
multiple, cooperating participants does not make the state of the
system inconsistent.
Partial failure of computers, networks, and software components
is a common problem in large-scale systems. If a portion of
system fails, however, it may leave applications in an inconsistent state, which may be worse that total failure. This problem
is exacerbated when a task has been distributed across multiple
components.
Many applications execute tasks that involve more than one participant, where participants may include resource provides and resource
users. Each participant executes part of the task in a sequence: for
the task to succeed as a whole, the work performed by each participant must succeed. If a task is successful, the changes made
should keep the system in a consistent state. If the work performed
by one participant fails, however, the work of other participants may
have modified the application state, but the participant that failed
would not have made the necessary changes. As a consequence, the
application could produce incorrect results.
Therefore:
Introduce a coordinator that supervises the execution and completion of a task by all participants. The coordinator ensures that
either all contributing participants complete successfully or, in
the event of even a single participating task failing, it appears
that the entire task did not execute at all.
A two-phase model is the simplest model of coordination that keeps
communication to a minimum, maximizes opportunities for distribution and parallelism, and keeps tasks separate from one another. On
completion of the task, successful or otherwise, all resources involved
in the task are released automatically.
A COORDINATOR ensures that a task involving multiple participants
appears to be atomic to clients that initiate the task. This coordination in turn maintains the consistency of the entire application by
ensuring that multi-step state transitions are controlled and fail-safe.
To implement a coordinator with a two-phase approach, split the
work performed by each of the participants as follows:
? Prepare. In this phase, the coordinator asks each participant to
check whether the execution of its part of the task could fail. If a
participant indicates a potential failure, the coordinator stops the
execution sequence of the entire task and asks all participants that
successfully completed their prepare phase to roll back by aborting
and restoring their original state. Since none of the participants
made any persistent changes, the system state remains consistent.
? Commit. If all participants pass the prepare phase, the coordinator
initiates the commit phase in which all participants do their actual
work. As each participant has indicated in the prepare phase that
its work would succeed, the commit phase should also succeed,
leading to the overall success of the task.
The resulting task execution is transactional: it appears atomic, and
the resulting state of the system is consistent. During the transaction,
state changes are isolated from one another: successful state change
is durable beyond the transaction. These are the so-called ¡®ACID¡¯
properties of a transaction. Failure is still a possibility during the
commit phase, and three-phase commit is sometimes considered a
more robust protocol.
However, in spite of the scalability and integrity offered by such
coordination, use of a coordinator introduces overhead in the form
of division and management of the task, as well as the need to
pass transactional context to the participants, typically via a CONTEXT
OBJECT (416).
##%%&&
When implementing a LEADER/FOLLOWERS (362) concurrency model,
IMMUTABLE VALUES (396), or an OBJECT MANAGER (492) configuration in
which there is a high turnover of resources . . .
. . . we may need to support rapid acquisition and release for a limited
set of stateless resources.
Acquiring and releasing system resources, such as network connections, threads, or memory, can incur performance overhead
that may vary for each acquisition and release. Applications with
a need for performance and scalability, however, require efficient
and predictable access to these resources.
Any given access strategy to resources must scale: it must be fast
and predictable even as the number of resources used, and the
number of resource users, increases. Moreover, to ensure predictable
performance, acquisition and release time for resources of the same
type should not vary significantly. For example, consider a server
on which each request is handled by a separate thread. For frequent
short requests the repeated cost of creating, preparing, and destroying
each thread can dominate the actual time taken to handle the request.
Therefore:
Keep a certain number of resources available in an in-memory
resource pool. Rather than repeatedly creating resources from
scratch, retrieve the resources from the pool quickly and predictably. When the application no longer needs a resource, it
must be returned to the pool so it becomes available for subsequent acquisition.
A resource pool encapsulates the knowledge of resource acquisition, access, and management. To acquire a resource, a resource
user must know the appropriate pool. Depending on the nature of
the resource and how it is managed by the pool, the resource user
may or may not need to return the resource to the pool explicitly.
Resources are released either when the pool is disposed of or by
explicit request¡ªassuming that the pool supports such an interface.
RESOURCE POOL avoids performance penalties due to the repeated overhead of creating and destroying resources on demand from scratch.
By storing the resources in a pool, the time to access them is shorter
and more predictable. All resources in a pool that have the same
properties are considered equivalent, that is, a client gets a resource
on request, not a particular resource.
The number of resources in a pool may be fixed at creation, or it may
be grown dynamically according to some policy, such as exponential
or fixed increment, with growth either bounded or unbounded. For
resource pools that are to be used in different application environments, the policy may be configurable, either through a simple set of
methods that can be used to set policy parameters, or via a STRATEGY.
For pools that grow, it may make sense to allow shrinkage, either
explicitly or transparently. A pool that tracks its resources can easily
resize itself. Alternatively, a more complex approach based on EVICTOR
(515) or LEASING (517) can be employed to manage resource retirement
from the pool.
Pooled resources are often created during the initialization of the
OBJECT MANAGER, using either EAGER ACQUISITION (509), PARTIAL ACQUISITION
(511), or LAZY ACQUISITION (507). LAZY ACQUISITION defers the creation
of an object until the first time it is accessed. Conversely, EAGER
ACQUISITION creates an object completely before it is accessed, so the
object is usable immediately after creation. If creation takes a long
time, PARTIAL ACQUISITION can be used to reduce the initial creation time
via stepwise object assembly.
Resources returned to the pool are reinitialized before they are reused
by other clients. Reinitialization ensures that resources are in a
defined and ready-to-use state, or that security requirements are met.
##%%&&
When realizing a CLIENT PROXY (240) or an OBJECT MANAGER (492) . . .
. . . we need to optimize the cost of repeated access to the same set of
resources.
Repeated creation and disposal of resources for a few resources
users can incur unnecessary performance overhead. For applications in which this overhead makes it hard to meet performance
requirements, there is a need to minimize the cost of initialization and disposal of frequently used resources.
An application that frequently uses and disposes of a particular kind
of resource, such as memory buffers or threads, may benefit from a
pooling arrangement. However, although a pool may offer a policy that
is good for an application as a whole, it may not necessarily benefit
a more local context such as a specific component or subsystem. An
appropriate optimization needs to be localized, simple to implement,
and low in execution cost.
Therefore:
Rather than destroying a resource after use, store it in an in memory cache. When the resource is needed again, fetch it from
the cache and return it, instead of creating it anew.
A resource cache stores a resource or small set of resources temporarily for fast retrieval. To acquire a specific resource, a resource user
looks in the cache first. If the cache does not contain the requested
resource, it then looks to the resource provider for the resource. The
presence of a cache allows rapid, localized recycling of a resource.
Resources are released either when the cache is disposed of or is full,
or by explicit request¡ªassuming that the cache supports such an
interface.
By storing frequently accessed resources instead of destroying them,
a RESOURCE CACHE minimizes the cost of (re)acquiring and releasing
resources. Ideally, resources are created only once, either before or
when they are accessed for the first time. Similarly, they are destroyed
only once, when they are no longer needed, or when the application
terminates.
All resources in a cache are considered different, even if they have
the same properties. If a specific resource requested by a client is
not in the cache, the request fails, even if resources with identical
properties are available.
Clearing a cache to release its resources can be an explicit activity executed by an EVICTOR (515), or an implicit operation through
LEASING (517). Some overhead can occur, however, when a resource
must be evicted from the cache even though an application has not
explicitly released it. For example, if a new resource is inserted into
the cache but not enough space is available for it, other cached
resources¡ªtypically those that are least-recently or least-frequently
used¡ªmust be destroyed to allocate space for the new resource.
These resources must be created from scratch, or otherwise be reactivated when they are again accessed by the application.
All caches make a trade-off between space and time, using extra space
to improve performance. The more complex a cache is, however, the
harder it is to maintain from a development point of view, and the
less likely it is that it will offer a performance advantage.
##%%&&
When realizing BUSINESS DELEGATE (292), THREAD-SPECIFIC STORAGE (392),
LIFECYCLE CALLBACK (499), VIRTUAL PROXY (497), RESOURCE POOL (503),
PARTIAL ACQUISITION (511), DATA MAPPER (540), or any potentially costly
or optional initialization . . .
. . . we may need to ensure that object creation and resource acquisition satisfy high throughput and availability demands.
Applications that access many resources, but which must also
satisfy high availability requirements, need a way to reduce the
initial cost of acquiring the resources they need or the resource
usage footprint that they have at any point in time.
In particular, acquiring all resources during system or subsystem
initialization can make start-up unnecessarily or even unacceptably
slow. Moreover, many resources may be acquired over-optimistically,
making the initial acquisition wasteful if they are not consumed
during the lifetime of the application. Over-acquisition can lead to
resource exhaustion and prevent resource recycling. Applications
that use many resources, however, need to access them when needed,
ideally without paying for the space overhead or early start-up cost
associated with early acquisition.
Therefore:
Acquire resources at the latest possible point in time. The
resource is not acquired until it is actually about to be used.
At the point at which a resource user is about to use a resource,
it is acquired and returned to the resource user.
Lazy acquisition is an optimistic optimization that defers, but does
not eliminate, the cost of resource acquisition. Its use should be fully
encapsulated by the resource provider, shielding the resource user
from the policy and mechanism details.
LAZY ACQUISITION follows a ¡®manana, ma ? nana¡¯ philosophy for resource ?
acquisition: never do today what you can put off until tomorrow.
Perhaps more positively, it can be seen to exercise the lean principle
¡®decide as late as possible,¡¯ deferring actual acquisition to ¡®the last
responsible moment¡¯ [PP03].
LAZY ACQUISITION ensures that each resource is acquired ¡®just in time,¡¯
that is, when the need for it is concrete and it is about to be used.
LAZY ACQUISITION therefore does not incur any resource acquisition
costs early in an application, component, or subsystem¡¯s lifecycle.
Further more, it does not waste time or space acquiring resources
that are never used.
However, there is normally some space overhead associated with LAZY
ACQUISITION, resulting from either introducing intermediate objects,
such as a VIRTUAL PROXY, or holding some additional state, such as a
status flag to indicate the state of the acquisition or attributes to be
used in acquiring the resource.
The cost of acquisition is moved rather than eliminated, so any code
that relies on an object that uses LAZY ACQUISITION will be slower to
execute the first time than subsequently. This deferral reduces the
predictability of execution. Another predictability-related risk is introduced by the lateness of acquisition: failure. There is no guarantee
that when the resource is needed, it will be available.
Because the actual acquisition is transparent, it may not be obvious when, if, or how a resource should be released. EVICTOR (515)
and LEASING (517) are both options for releasing resources in the
background.
In concurrent applications LAZY ACQUISITION often uses DOUBLE CHECKED
LOCKING (386) to prevent the same resource being accidentally acquired
multiple times by multiple threads running in parallel. A simple
locking approach may be thread-safe, but it can be costly, incurring
locking overhead unnecessarily for every access but the first.
##%%&&
When implementing PARTIAL ACQUISITION (511), RESOURCE POOL (503), or
any potentially costly initialization . . .
. . . we may need to ensure that object creation and resource acquisition satisfy high predictability and performance requirements.
Every application needs to access certain resources, such as
memory, threads, network connections, and file handles. Applications with stringent predictability and performance requirements, however, often cannot afford the overhead of acquiring
such resources on demand at runtime.
Resource acquisition can be a costly business, and an application
may not be able to afford the time taken to acquire a resource dynamically to fulfill a task. The time needed to acquire a specific resource
is often unpredictable, especially in general-purpose operating environments. As a result, applications may not be able to meet their
predictability requirements. In a layered system, in which primitive
resources from one layer are wrapped up by the next, lazy acquisition of primitive resources in one layer can reduce predictability of
acquisition in the next higher layer. Furthermore, handling resource
exhaustion may complicate the implementation of the task. Exhaustion may even be unacceptable from an operational perspective, so
the possibility should be minimized.
Therefore:
Eagerly acquire the resources before they are used. The resource
is then available to a resource user when immediately it requests
it.
The point in time at which resources should be acquired can depend
on several factors, including the time it takes to create them, when
they are needed, the number of resources to create, and their
dependencies to other resources. These factors should be considered
in the context of any application using EAGER ACQUISITION.
EAGER ACQUISITION is an optimization that applies Isabella Beeton¡¯s
kitchen-management maxim that ¡®there is no work like early work¡¯ to
resources.
In considering when to acquire resources, one option is to acquire
resources at system initialization. Such an immediate acquisition
allows an application to ensure that there is no dynamic resource
acquisition during the runtime of the application. Another option is
to acquire them at some later designated point in time after system
initialization but before their first use by applications. For example,
at component load-time for dynamically loaded components, or on
creation of a RESOURCE POOL or other responsible object. Regardless
of which strategy is applied, however, EAGER ACQUISITION ensures that
resources are properly acquired and are readily available before they
are actually used. The resource user is guaranteed that the resources
are available and at no (or minimal fixed) cost of access.
However, in applying EAGER ACQUISITION, a developer must be aware
that the optimization involves a trade-off rather than unconditional
benefits. The cost of resource acquisition is not eliminated, it is
simply moved to an earlier point in an application¡¯s lifecycle. If many
resources are managed in this fashion, an application¡¯s, component¡¯s,
or subsystem¡¯s start-up will be noticeably slowed, which is not acceptable for applications that need rapid start-up. EAGER ACQUISITION also
carries with it the risk of over-acquisition, tying up resources unnecessarily and, in conflict with one of the design objectives, incurring
the risk of resource exhaustion. There is no performance or space
benefit to acquiring resources eagerly that are never used.
##%%&&
When implementing RESOURCE POOL (503), VIRTUAL PROXY (497), or any
potentially costly or complex initialization . . .
. . . we may need to ensure that object creation and resource acquisition satisfy throughput and predictability demands.
Some applications with stringent performance, scalability, and
robustness requirements must access resources whose size is
large or unknown. Acquiring these resources eagerly during system initialization may therefore introduce excessive start-up
overhead. However, acquiring them on demand can incur an
untimely cost.
If memory or processing time is not available for acquiring all required
resources during system initialization, an application¡¯s overall quality
of service can suffer. Indeed, some of the resources may not be readily available at start-up. Similarly, acquiring these resources ¡®just in
time¡¯ when they are actually used may incur unpredictable performance overheads that also cannot be tolerated within an application¡¯s
operational constraints. For example, acquiring remote resources is
expensive, whenever it is done.
Therefore:
Split the acquisition of each resource into multiple stages. In
each stage, acquire only a part of the resource, so that its acquisition gradually completes over time, in accordance with overall
application quality of service needs.
Divide up the resource acquisition with respect to the cost, the
availability, the space overhead of holding or managing deferral of
acquisition, the perceived value or debt of holding a resource that is
not used, and any natural division of a resource¡¯s acquisition into
stages. For example, a remote resource may have separate instance
creation and initialization steps, which would map cleanly onto two
stages of partial acquisition.
PARTIAL ACQUISITION ensures that resource acquisition does not create
too much overhead and that each resource acquisition stage does not
incur excessive performance penalties. The original forces of large
resource size and long resource acquisition time are not completely
resolved, but instead are balanced against other demands.
The number of stages for a resource acquisition, the amount of
resource to acquire at each stage, and the timing of each stage depend
on various factors, such as available memory, required response time,
the availability of dependent resources, and the lifecycle of resources.
It is possible that some resources can be acquired asynchronously,
so that at an initial stage they are requested and at a later stage they
are committed. Managing such asynchronicity can be simplified, for
example by using a FUTURE (382).
After it has partially acquired a resource, an application can use the
resource as if it were fully acquired and initialized. PARTIAL ACQUISITION
strategies often build on LAZY ACQUISITION (507) and EAGER ACQUISITION
(509). However, PARTIAL ACQUISITION tends to be more complex in
implementation than both LAZY ACQUISITION and EAGER ACQUISITION, and
is subject to a combination of the liabilities of each of these two
approaches.
##%%&&
When implementing an OBJECT MANAGER (492) or a LOOKUP (495) service
that releases resources that could still be used by clients . . .
. . . we need to offer simple access to temporarily released resources.
Some types of services in a distributed system should only
consume resources when they are accessed actively by clients.
Clients should be shielded as much as possible from where services are located, how they are deployed on hosts in a network,
and how their lifecycle is managed.
Unconstrained use of resources such as communication channels,
threads, or memory can degrade the overall quality of service of an
application. Applications may periodically evict less frequently used
resources from memory to make space for other resources needed
by an application. If an evicted resource is re-accessed by its clients,
however, it must be re-activated, which can involve recreating the
resource, reloading its state, restarting it on its server, and reacquiring any resources that it in turn uses. Such resource reactivation
should be transparent to clients, however, so that it appears as if the
accessed resources are always available. It should not be part of the
client¡¯s responsibility to manage such reactivation.
Therefore:
Minimize resource consumption by activating services on
demand and deactivating services when they are no longer
accessed by clients. Use proxies to decouple client access transparently from service behavior and lifecycle management.
Introduce an activator that initiates and supervises the reactivation of
previously deactivated resources. Whenever a resource is evicted, the
activator receives information about the resource, such as its identity,
its location in the network, the location of its persistent state, and its
required computing resources. Whenever the client re-accesses the
de-activated resource, the activator reactivates it according to a given
policy using the information it maintains about the resource.
An ACTIVATOR frees clients from the responsibility of reactivating the
resources they use: it appears to them as if all resources were always
available. An ACTIVATOR also ensures that reactivating a resource
incurs minimal overhead, because it maintains information about
how to optimize this process. For example, the ACTIVATOR could reload
the resource¡¯s persistent state and reacquire the necessary computing
resources in parallel, thereby speeding resource initialization.
To make the use of an activator transparent, the resource must be
wrapped, such as with a VIRTUAL PROXY (497). There is normally some
space overhead associated with activation, however, resulting from
either introducing intermediate objects, such as a VIRTUAL PROXY, or
holding some or all attributes used when acquiring the resource. In
addition, the cost of acquisition is moved rather than eliminated, so
any code that relies on an object that needs reactivation will be slower
to execute the first time than subsequently. This deferral reduces the
predictability of execution.
##%%&&
When implementing an OBJECT MANAGER (492), Virtual Proxy (497),
RESOURCE POOL (503), or RESOURCE CACHE (505) for resource-constrained
applications . . .
. . . we need to ensure that infrequently used resources are released
in a timely manner.
The simplest model of resources is that a resource client acquires
a resource, uses it once, and then releases it. However, for clients
that need a resource more than once, albeit infrequently, a
repeating (re)acquire¡ªuse¡ªrelease cycle incurs overhead. But a
resource provider may not be able to afford unlimited use of its
resources.
The frequency of use, time of use, or some other quality of usage,
should influence the lifecycle of a resource. The lifecycle should be
constrained by system environment, not just by explicit actions by
resource clients, such as resource release. Ideally, the solution should
be as transparent as possible to the resource client, otherwise this
pushes the complexity of resource management detail back out to the
client.
Therefore:
Introduce an evictor to monitor the use of resources and control
their lifetime. Resources that are not accessed after a specific
period of time are removed to free up space for other resources.
Add a marker such as a flag, counter, or timestamp to each resource,
which indicates when the resource is used. Initialize the marker to
indicate that the resource is unused, then update it when the resource
is used. Periodically or on demand, evict unmarked or uncommonly
accessed resources, but keep resources that are marked because they
are currently, recently, or frequently in use. Other criteria for eviction
are possible, including ones based on resource footprint rather than
frequency of use.
EVICTOR prevents frequently or recently used resources from being
destroyed, thereby avoiding reacquisition costs when these resources
are re-accessed. Similarly, applications can control when less frequently used resources are released. This increases their predictability and performance, because resource housekeeping activities do
not interfere with critical application operations. If the application re-accesses an evicted resource, however, it must be created
from scratch or otherwise be reactivated, thereby incurring performance overhead.
Common eviction policies are Least Recently Used (LRU) and Least
Frequently Used (LFU). Application-specific policies are also possible.
For example, the size of resources in memory-constrained applications could be used to determine which resources to evict, so a large
resource could be evicted even if it was used recently. Similarly, it is
possible to use domain-specific knowledge. For example, if an application knows that a resource is scheduled for use soon, it may not
be evicted even if it has not been accessed recently. For overloaded
reactive systems, a policy of FRESH WORK BEFORE STALE [Mes96] results
in new tasks or requests¡ªas opposed to resources¡ªbeing given priority over old ones, which may be evicted. In all policies the state
of stateful resources is stored persistently before releasing them.
The policies can be either be hardwired or expressed in a pluggable
STRATEGY (455) form.
Although simple in principle, the subtlety of EVICTOR lies in finding
an appropriate eviction strategy and dealing with the question of
reactivation or access of evicted resources. The need for reactivation can introduce a slightly more complex lifecycle for resources,
involving two additional lifecycle events: (re)activate and passivate
(or deactivate).
##%%&&
When realizing an OBJECT MANAGER (492), RESOURCE POOL (503), RESOURCE
CACHE (505), LAZY ACQUISITION (507), LEASING (517), AUTOMATED GARBAGE
COLLECTION (519), or COUNTING HANDLE (522) structure in which resource
allocation crosses distribution boundaries . . .
. . . we need to ensure that resource-constrained applications always
release resources in a timely manner.
Applications working with a constrained set of resources need to
ensure that resources are returned after use. Unless resource
clients explicitly terminate their relationship with resource
providers and releases the resources, they may retain unused
resources needlessly. However, a crashed client is unable to
release resources, and a rogue client may be unwilling to do so.
In a typical scenario, a client asks a provider for one or more
resources. Assuming the provider grants the resources, the client
can then start using them. Either the client or the provider may
crash, or the provider may no longer offer some of its resources,
or a client may have a defect that prevents correct return of a
resource. However, unless resource providers are explicitly informed
that resources are no longer used, the resource will leak, which could
lead to resource exhaustion. And unless clients are explicitly informed
that the resources are no longer available, they may continue to hold
invalid resources.
Therefore:
Have the provider create a lease for each resource held by clients.
Include a time duration in the lease that specifies how long a
client can use the resource. After the time duration expires,
release the reference to the resource in the client and the
resource in the provider.
While a lease is active, the client can cancel the lease, in which case
the corresponding resource is freed in the provider. Before a lease
expires, the client can try to renew the lease from the lease provider.
As long as the lease is renewed, the corresponding resource will
continue to be available.
LEASING simplifies resource usage and management for both clients
and providers. Clients are freed from the responsibility of releasing
resources explicitly. They also know that the resource is available and
valid for the granted time. Providers can control resource usage more
efficiently: by bounding resource usage to a time-based lease, unused
resources are not wasted and are released as soon as possible so that
they can be granted to new clients.
The concept behind LEASING is simple, and the result is efficient, stable, and scalable. Alternative schemes, such as requiring resource
clients and providers to emit a heartbeat or to ping one another, tend
to be more complex and less efficient than LEASING. Overall, resource
management is simple for both resource provider and resource consumer, but both parties need to be timer-aware. It is possible for
a lease to be lost if a client or network is overloaded, so that the
resource provider does not receive the lease renewal in time. In such
cases, the client will find itself working with a stale resource and will
have to handle this runtime error, for example by acquiring a new
resource.
The duration of a lease depends very much on the kind of application,
and is often also a configurable parameter. In support of distributed
objects, as used in Java RMI and. NET Remoting, the default lease
period is of the order of minutes [VKZ04]. For IP addresses issued by
DHCP the default is of the order of days. For software licenses the
period is normally of the order of months.
##%%&&
When realizing a WHOLE-PART (317), ACTIVE OBJECT (365), IMMUTABLE
VALUE (396), or BRIDGE (436) configuration, or when providing runtime
support for objects allocated dynamically on the heap . . .
. . . we often need a safe and simple mechanism to reclaim memory
from objects that are no longer needed.
Heap memory is a finite resource managed by the runtime environment and consumed by any dynamically created object in an
application. Failure to return memory to the heap when it is no
longer needed can exhaust memory, or lower performance due
to thrashing virtual memory pages. Similarly, errors in manual
management can yield memory leaks and memory corruption.
It is relatively easy to manage objects that are created and used
only within other objects or within a single method. For example,
stack-based value objects in C++ or C# are scope-bound, and EXECUTE AROUND OBJECTS can automatically reclaim heap objects in C++.
Objects that are shared or involved in complex object relationships, however, are more likely to invite programmer error or complex
designs aimed at taming manual memory management. Object relationship graphs may also include cycles, in which one object refers
to another which directly or indirectly points back to the first.
Therefore:
Define a garbage collector that identifies which objects are no
longer referenced by live objects in the application, and reclaims
their memory. The garbage collector performs the identification
and reclamation automatically and transparently.
The set of unreferenced objects is determined by finding the set of
objects the running system uses and subtracting it from the set of
all allocated objects. The root set of references defines objects that
are known directly to be used by the system: objects referenced by
global (or static) variables and objects referenced by the stack in
each thread of control. From the root set it is possible to follow the
references from each object to determine which other objects are in
use, and so on.
AUTOMATED GARBAGE COLLECTION lives up to its name in the sense that
it classifies unreferenced objects as garbage and collects their memory without direct application intervention. The greatest benefit of
garbage collection is that it is simple to work with for many programs, programmers, and types of object, notably those that¡ªwith
the exception of memory¡ªare not resource consumers. Garbage collection is hard to implement deterministically, however, so quality
of service may vary, and it is hard to ensure that resource-based
objects will be released in a timely manner, if at all. Where such
control is needed, programmers are often required to resort to a more
manual scheme, such as calling DISPOSAL METHODS (531) explicitly, or
employing COUNTING HANDLES (522).
Garbage collectors can run synchronously and completely, or asynchronously and incrementally. The simplest model is the synchronous
mark-and-sweep approach: all objects reachable from the root set are
marked in one pass, then a second pass through all the objects in
memory collects the unmarked ones. Although simple to implement,
mark-and-sweep algorithms can have a ¡®stop-the-world¡¯ effect on an
application, which is inconvenient for user interfaces and intolerable for high-performance servers or real-time systems. Generational
garbage collectors take advantage of the longevity profile of objects:
most objects that are created only live for a short period of time,
but a minority of older objects live for a very long time. COLLECTIONS
FOR STATES (495) can divide objects in memory into young and old
generations, and manage each generation separately for collection.
Objects that survive a collection of the young generation are moved
to the old generation.
Garbage collectors can be configured in terms of the various parameters that govern their operation, such as maximum generation size for
generational collectors. Garbage collection APIs also normally allow
explicit execution of the garbage collector and, for some collectors,
explicit disabling and enabling of the collector. Explicit control of the
collector is particularly important for applications that must meet
real-time constraints, in which an inappropriate garbage collection
cycle could mean the difference between meeting a hard deadline or
missing it, causing a catastrophic application failure.
In a distributed environment, collection strategies based on traversing the set of all objects would perform and scale poorly. The
process of garbage collection would flood the network with many
fine-grained book-keeping messages and would introduce blocking
behavior across the network. Local garbage collection within the
same address space can be efficient, but to scale to a distributed
system a complementary strategy such as LEASING (529) is required.
A form of memory leakage often occurs in garbage-collected systems
in which an object that should be collected is not because some
type of lookup table retains a reference to it. These tables are often
intended to show available objects, rather than to use them directly
and keep them alive. By referring to them, however, that is just
what they end up doing. To resolve this problem, ordinary object
references can be complemented with ¡®weak references¡¯ that are not
counted when looking for reachable objects. If an object is collected,
any corresponding weak references will yield null when they are
checked.
It is still possible to employ garbage collection in runtime environments in which manual memory management is the native model,
such as in C and C++. A more conservative approach is needed,
however, to collect object pointers expressed in terms of raw memory
addresses rather than as special handles. Raw memory is untyped,
so any pointer-sized piece of memory that matches the address of
an allocated object, or points into such an object, is considered a
pointer. These faulty matches can not only leak memory, but can
also be fooled by addresses that have been encoded in some way,
such as in terms of offset values rather than actual pointers.
##%%&&
In a PROXY (290), WHOLE-PART (317), ACTIVE OBJECT (365), IMMUTABLE
VALUE (396), BRIDGE (436), OBJECT MANAGER (492), AUTOMATED GARBAGE
COLLECTION (519), or DISPOSAL METHOD (531) arrangement . . .
. . . we often need to arrange for guaranteed and deterministic disposal
of shared objects and their resources.
An object created dynamically on the heap must be destroyed
following its use to avoid leaking memory and other resources.
However, some languages, such as C++, manage heap object lifetime manually. Errors in management lead to memory leaks and
memory corruption. Even in garbage-collected environments,
issues can arise with resource reclamation, because garbage collection is not deterministic.
Instead, we must ensure that the shared object created on the heap
is disposed of reliably, safely, and in a timely fashion. The sooner
it is disposed of, the less perception there will be of any resource
leakage or starvation. If this is done too soon, however, there will
be dangling references to the object. As long as at least one client
is referencing the object, it must not be reclaimed: as soon as no
client is using the object, it is a candidate for disposal. Implementing
this logic within the object¡¯s clients is not practical, since it pollutes
clients with additional housekeeping code and couples them tightly
to the object.
Therefore:
Introduce or nominate a handle object as the only means of
accessing the shared object. Let this handle object encapsulate
the responsibility for tracking references to the shared object
and, consequently, for its disposal if it is no longer referenced.
The handle object ensures, transparently, that the shared object is
disposed of on behalf of its clients without producing either dangling
references or memory leaks.
The handle object¡¯s lifecycle methods are responsible for tracking the
number of references to, and managing the lifetime of, the shared
object. When there are no more references, when a handle itself is
about to be destroyed or re-bound to another object, is when the (no
longer) shared object can be disposed of. In C++ the operations in
question are the constructors, destructor, and assignment operator
for the handle.
There are two basic options for implementing handle objects. An
EXPLICITLY COUNTED OBJECT [Hen01b] tracks the references to the shared
object via reference counting, so that there is an explicit, physical
count. By contrast, LINKED HANDLES [Hen01b] introduces bidirectional
links between the handle objects, so that they are aware of both the
shared object and other handle objects referring to the shared object.
However, LINKED HANDLES cannot be made thread-safe with any kind
of reasonable efficiency, so they are not applicable for objects shared
between threads.
An explicit count avoids the penalties of linking COUNTING HANDLES
and makes the task of checking sharing against a specific limit such
as zero simple and explicitly visible. It is easy to determine when a
COUNTING HANDLE is the last one to point to the shared object and it
must delete it upon its own deletion.
There are three placement options for the reference count for an
EXPLICITLY COUNTED OBJECT. An EMBEDDED COUNT places the counter
within the shared object itself [Hen01b]. This option is efficient
time-wise and space-wise, requiring only a single heap allocation
not much larger than the allocation of an uncounted version of
the shared object. If it is impossible, hard, or inappropriate to add
an EMBEDDED COUNT to the shared object, for example if there is
no access to its source code, we can introduce a DETACHED COUNT,
which is a separate object that holds the reference count for the
shared object [Hen01b]. A DETACHED COUNT does not affect the type
of the shared object, and is managed exclusively by the COUNTING
HANDLES. The DETACHED COUNT is created when the shared object is
first introduced to a COUNTING HANDLE¡ªonly then does it become an
EXPLICITLY COUNTED OBJECT. When the last COUNTING HANDLE disposes
of the shared object, it also disposes of the corresponding DETACHED
COUNT.
A third placement option is a LOOKED-UP COUNT. This centralizes the
management of shared objects and their counts collectively in a
separate object, using some identity of the shared objects as the key
for its direct access from their COUNTING HANDLE [Hen01b]. A LOOKED-UP
COUNT can introduce an additional time overhead for the lookup, but
it can also provide the opportunity for collective operations on all the
shared objects if that is needed.
In multi-threaded applications any reference count implementation
must be thread-safe, to avoid corrupting its state due to race conditions caused by concurrent access from multiple COUNTING HANDLES.
Unless incrementing and decrementing the count are infrequent operations, lock-based solutions are not time- or resource-efficient enough
to be practical. The lock-free increment and decrement operations
offered by modern operating systems are the preferred approach, but
note that although these may be lock free, they are not cost free:
they still incur an overhead, especially on multiprocessor systems
and multi-core processors.
In a distributed environment, synchronous reference counting is
insufficient as a practical or scalable scheme when the lifetime of
a shared object in one address space is managed by handles in
remote address spaces. Increments and decrements can make for
a great deal of idle chatter on the network, and all it takes is for
a client to crash, or a rogue client to not adhere to the reference counting protocol, for the scheme to unravel. If COUNTING HANDLES
are introduced in such an environment, they need to be supplemented with other lifetime management schemes such as LEASING
(517). Similarly, COUNTING HANDLES fail for object relationships that
have cycles. In cyclic relationships the reference counts are never
zero, so they leak.
##%%&&
When implementing a DOMAIN OBJECT (208), a REFLECTION (197) architecture, or a BUILDER (527) . . .
. . . we often need to separate details of related implementation classes
from their client interfaces to keep a system loosely coupled.
Clients¡ªparticularly clients in a framework¡ªare generally not
interested in how families of related objects they use are created, configured, represented, and disposed of. Instead they are
only interested in the services that these objects offer, as well
as knowing that the objects in the families are semantically
compatible.
Frameworks, for example, such as graphical user interface toolkits
and object request brokers, as well as applications designed for
flexibility, often create families of objects that encapsulate variants
in the software such as control-flow extensions, algorithms, and data
representations. Creating each object individually and separately
can create a configuration nightmare if the created objects are not
semantically compatible with one another.
Therefore:
Define a factory interface for the creation and, optionally, the
disposal of families of related objects. Fulfill this interface with
specialized factories that actually carry out the creation work on
behalf of their clients.
An abstract factory specifies a uniform object creation and disposal
interface for all related object types. Concrete factory classes derive
from this abstract factory to control the creation and disposal of
particular object types, and to ensure that the objects created are
semantically compatible.
In general, factories separate object lifetime concerns from object
usage, so that clients do not depend on how the objects they use are
created and destroyed. Neither do they depend on structural details
of the objects that are relevant for proper creation and destruction.
An abstract factory interface contains two kinds of methods: one
or more FACTORY METHODS (529) create objects and, optionally, the
corresponding DISPOSAL METHODS (531) dispose of the objects. Abstract
factories are typically expressed in terms of their own class rather
than by adding new responsibilities to an existing class, because
most objects created are not related to each other via inheritance.
The variability sometimes needed for object creation can often be
accommodated by configuring the factory with objects that specify
the actual creation logic for the factory¡¯s creation methods. Such a
PLUGGABLE FACTORY [Vlis98b] [Vlis99] is essentially a framework into
which other parts are plugged and offers a simpler, more flexible,
and less repetitive alternative to creating many different concrete
factories. Pluggable creational behavior can be expressed through
STRATEGY (455) objects or types. Alternatively, prototypical instances
can be provided for each type created by the factory. A homomorphic
FACTORY METHOD¡ªa specialized form of FACTORY METHOD in which the
resulting product type is the same as the creating type¡ªcan express
polymorphic copying.
##%%&&
When implementing a DOMAIN OBJECT (208) or a REFLECTION (197) architecture . . .
. . . we often need to create objects that are complex or the cumulative
result of many steps.
Some objects are complex enough to prevent them being created
in a single ¡®atomic¡¯ step. Instead they must be created step-wise,
in which each step of the builder creates a portion of the complex
object. Alternatively, even if the object being created is simple
in representation, its state may be the outcome of complex or
ongoing calculations. The execution order of the steps may differ
for each object created.
Clients are generally not interested in the complexity and variability
of an object¡¯s creation. The details of the process should not result
in the creation of many temporary objects or the complication of
application logic. Clients just want a ¡®product¡¯ that is created correctly
and reliably so that they can access its functionality. Similarly, what
holds for an object¡¯s creation often also holds for its disposal.
Therefore:
Introduce a builder that provides separate methods for constructing and disposing of each different part of a complex object, or
for combining cumulative changes in the construction of whole
objects. Let a separate director implement the algorithms for
creating and disposing of the object by using the builder interface.
A builder encapsulates the knowledge of how to construct an appropriate product, whether it is encapsulation of process that can be used
to work on the product, or the encapsulation of assembly structure.
The builder offers a method for retrieving the resulting product object.
A director uses the builder to construct an appropriate product object.
Object disposal can optionally also be supported.
The separation into builder and director roles supports flexible variation of an object¡¯s creation and, optionally, disposal process. The
builder provides and encapsulates the basic mechanisms for the creation and deletion of the object¡¯s different parts via corresponding
FACTORY METHODS (529) and DISPOSAL METHODS (531). The director is
either an ad hoc client, or a factory such as an ABSTRACT FACTORY
(525), that implements the policy for assembling and disassembling
the object in a particular way, which may depend on client input or
global application. Both parts can vary independently: changes to the
builder implementation need not affect the director, and vice versa,
as long as the builder interface remains stable.
Hierarchically related objects can be created and deleted by introducing a builder hierarchy. An abstract builder declares the build and
disposal interface for elements of the object hierarchy, and concrete
builders that derive from this abstract base implement the build and
disposal steps for a particular object. Clients can retrieve the created
object from the builder directly, via the director, or pass the object to
the director for correct disposal.
A simple form of BUILDER, MUTABLE COMPANION [Hen00b], supports the
creation of IMMUTABLE VALUES (396) with less overhead and in more ways
than ordinary constructors, for example, the use of a StringBuffer
or StringBuilder in Java to construct a String by concatenation. In particular, a value builder eliminates the need for complex
expressions for constructing immutable values, and minimizes the
creation of temporary objects during this process. The modifier methods enable cumulative or complex state changes. These methods
should have a THREAD-SAFE INTERFACE (384) if used in a multi-threaded
environment. A single FACTORY METHOD (529) allows clients to access
the resulting immutable object.
##%%&&
When implementing a BROKER (237), ACCEPTOR-CONNECTOR (265), EXPLICIT
INTERFACE (281), ITERATOR (298), THREAD-SPECIFIC STORAGE (392), IMMUTABLE
VALUE (396), OBJECT MANAGER (492), COMPONENT CONFIGURATOR (490),
LIFECYCLE CALLBACK (499), ABSTRACT FACTORY (525), or BUILDER (527) configuration . . .
. . . we often need to encapsulate the details of object creation to
preserve looseness of coupling and stability of use.
Object creation is often a simple matter of using a new expression, with respect to a class, that allocates memory and initiates
a constructor. However, not all objects can be constructed so
easily. For example, the type of the objects may depend on the
type of another object, or some initialization steps may need to
be handled outside the constructor.
Direct object creation may inadvertently obfuscate and reduce the
independence of the calling code if any of the necessary ingredients
for correct object creation are not readily available. The concrete
class, the full set of constructor arguments, or the enforcement of
other constraints may not be known at the point of call. To require
them would increase the complexity of the calling code and reduce
its stability, making changes in creation detail harder to introduce.
Therefore:
Encapsulate the concrete details of object creation inside a
factory method, rather than letting clients create the object
themselves.
Clients call through the factory method to obtain a new object.
FACTORY METHOD frees clients from creating any complex objects that
they use, thereby making the clients easier to understand and maintain. This pattern also simplifies the instantiation of objects whose
constructor cannot easily contain their creation logic, for example if
external validation or object relationships must be established that
is beyond the scope of the object¡¯s immediate responsibility.
There are three main variants of FACTORY METHOD that address different
creational needs:
? Simple. A method on an object of concrete class type that simply encapsulates the logic of creation and the policies that may
surround it.
? Polymorphic. Where the possible types of the product object are
drawn from a class hierarchy, and the type to be created depends
on an existing object, a polymorphic factory method on the existing
object encapsulates the knowledge of the concrete class used.
? Class. Considered as a meta-object, a class creates its instances.
It is possible to provide class-level factory methods¡ªexpressed as
static methods in many languages¡ªto play the role of constructors.
These variants can be combined and refined to produce an appropriate design solution, depending on specific goals and context.
Note that unless created specifically for the purpose, including the
role of factory in an interface can sometimes be considered an addition
that weakens its cohesiveness a little. The solution is certainly more
encapsulated than the alternatives, but the cohesion can be considered less than in a design in which such creation was never needed.
##%%&&
When realizing an EXPLICIT INTERFACE (281), ITERATOR (298), OBJECT
MANAGER (492), COMPONENT CONFIGURATOR (490), LIFECYCLE CALLBACK (499),
AUTOMATED GARBAGE COLLECTION (519), ABSTRACT FACTORY (525), or BUILDER
(527) structure . . .
. . . we need to preserve the encapsulation of object creation and
resource acquisition when discarding an object.
Disposing of an object is not always as simple as destroying
it explicitly or leaving it to the mercy of a garbage collector.
Objects whose creation was encapsulated may have more to
them than can be addressed by simple collection of memory.
Reliance on manual deletion can introduce too much coupling,
whereas garbage collection lacks deterministic execution.
For manual memory management, as found in C++, and even with
COUNTING HANDLES, it is too much to assume that an object whose
creation was encapsulated is a suitable candidate for deletion. Its
allocation may have been optimized in a way that is incompatible
with deletion, or it may need to be recycled instead of deleted because
its creation is expensive. Making applications responsible for this
housekeeping functionality would make their code more complex
than necessary due to coupling to encapsulated lifecycle concerns.
Therefore:
Encapsulate the concrete details of object disposal within a dedicated method, instead of letting clients delete or discard objects
themselves.
When a client has finished using an object, it¡ªor a helper mechanism¡ªcalls the disposal method to ensure that the object is cleaned
up in an appropriate manner. This may be outright deletion, careful
dismantling, or recycling.
DISPOSAL METHOD frees clients from deleting or dismantling any complex objects that they use, thereby making the clients easier to
understand and maintain. In addition, DISPOSAL METHOD simplifies the
destruction of objects whose deletion logic cannot be implemented
easily in their destructor or by the garbage collector, for example
because the object or its parts should be recycled or made available
to other clients. DISPOSAL METHOD makes these variations in the way
an object is deleted transparent to its clients.
There are two basic options for placing a DISPOSAL METHOD. One option,
a factory DISPOSAL METHOD, places the disposal method on the factory
or object responsible for originally creating the object. This explicit
arrangement makes the intended lifecycle of the created objects
clearer in the interface of the factory. However, it means that the
object user must be aware of the object¡¯s origins in order to return
the object when it is done. The other option, self-DISPOSAL METHOD,
provides a method on the object to be disposed of. The lifecycle relationship is not as obvious in this design and, if the disposal action
involves return to a pool, the object may also require a back reference
to the factory so that it can, so to speak, ¡®return home.¡¯
Because manual use of a disposal method can be error-prone, its
use can be wrapped by an EXECUTE-AROUND METHOD [Hen01a] or a
COUNTING HANDLE. (522). In C#, the IDisposable interface provides
a general purpose self-disposal interface that can be used with the
using control-flow construct.
##%%&&
When realizing a DOMAIN MODEL (182), or when specifying a SHARED
REPOSITORY (202) or BLACKBOARD (205) architecture . . .
. . . we must often connect an object-oriented application to a relational database.
Object-oriented software systems often use relational databases
for persistence: object technology can simplify application
design, while relational databases support efficient persistence,
or are chosen for economic or historical reasons. There is a
disconnect, however, between the two approaches that makes it
hard to map objects and their various relationships¡ªinter-object
navigation and inheritance¡ªto relational tables.
While each model offers a good economic and technological fit for
its respective domain, the connection between the two models is
not straightforward. Neither applications nor database access should
suffer from the mismatch, however. In particular, an object-oriented
design should be usable for the application, and application code
should not be littered with database access statements and API
housekeeping. Database access code should similarly make best use
of mechanisms provided by the relational model.
Therefore:
Introduce a separate database access layer between the application and the relational database that provides a stable object oriented data-access interface for application use, backed by an
implementation that is database-centric.
Applications can store and retrieve their persistent data by calling an
appropriate method on the database access layer, or by asking the
data to persist itself, depending on the design. The database access
layer is responsible for mapping between the data structures used by
the applications and the format required by the database tables.
DATABASE ACCESS LAYER decouples an object-oriented application from
the details of the database. All concrete mappings of objects to tables
are encapsulated within this layer, so that it appears to the application as if it were storing and retrieving ¡®its own¡¯ objects rather than
table entries. DATABASE ACCESS LAYER thus offers a suitable bridge to the
underlying persistence technology. In addition, modifications to the
DATABASE ACCESS LAYER do not affect application components directly.
Many options exist for designing a DATABASE ACCESS LAYER. A DATA
MAPPER (540) shields objects from knowing even that there is a
database present, which is useful when the application design and the
database schema should evolve independently. A ROW DATA GATEWAY
(542) specifies objects that look exactly like a record in a record
structure, but which can be accessed with ordinary object-oriented
methods. This is useful if application data structures map directly
onto database records and instances of the records are accessed individually, rather than in record sets. A TABLE DATA GATEWAY (544), in
contrast, provides an interface with several find methods to retrieve
sets of data records from the database, with corresponding update,
insert, and delete methods. This is useful if application data structures map directly to database records, and the records are usually
accessed in entire record sets rather than individually. ACTIVE RECORDS
(546) uses the most obvious approach, and puts all data access
logic into application objects. This is useful when the object model
of an application maps directly onto the database schema and its
domain logic is not too complex¡ªfor example, if it is centered around
operations such as create, read, update, and delete.
The database access layer is also responsible for notifying the application if values in the database change because another application
accesses them. Typically the notification mechanism is realized via
some form of OBSERVER (405) arrangement, with the database notifying
the database access layer, and the database access layer notifying
the application. Without such a mechanism, the view the application
has of the database is stale, which could lead to incorrect application
behavior.
##%%&&
When designing a DATABASE ACCESS LAYER (538) . . .
. . . we must shield applications from the way in which data is represented in persistent storage.
Object-oriented applications and relational databases use different mechanisms for structuring data. While it is still necessary
to transfer data between the two, if the object-oriented domain
model knows about the relational database schema, and vice
versa, changes in one tend to ripple to the other.
Mapping between an object-oriented and a relational database
schema introduces accidental complexity into an application. For
example, collections and inheritance are not present in relational
databases, while relational constructs such as SQL queries are not
behavioral primitives in conventional object-oriented languages. It is
easy to add database access code to an application object, but this
makes the object¡¯s implementation unnecessarily complex and brittle
if the schema or database changes. A direct transliteration of classes
to tables, however, may not yield the most appropriate database
schema design either. A solution is needed that is loosely coupled
and stable.
Therefore:
Introduce a data mapper for each type of persistent application
object whose responsibility is to transfer data from the object to
the relational database, and vice versa.
A data mapper is a mediator between an object-oriented domain
model and a relational database. A client can use the data mapper
to retrieve an application object from the database, or ask it to
store an application object in it. The data mapper performs all
necessary data transformations and ensures consistency between the
two representations.
Using a DATA MAPPER, in-memory objects need not know that a
database is present. Moreover, they require no SQL interface code and
have no knowledge of the database schema. DATA MAPPER allows the
relational database schema and the object-oriented domain model to
evolve independently. This design also simplifies unit testing, allowing mappers to real databases to be replaced by mock objects that
support in-memory test fixtures.
If an application¡¯s domain model is simple and corresponds largely
to its physical representation in the database, the data mapper can
just map a database table to an equivalent application object on a
field-to-field basis. If the domain model is more complex, however,
several patterns can be used to support the implementation of a data
mapper. A REGISTRY [Fow03a] helps to find data belonging to a specific
application object. An IdENTITY MAP [Fow03a] ensures that data is
loaded only once if complex and cyclic dependencies exist between
application objects. If an application object contains a large data set,
a LAZY LOAD [Fow03a] enables the data mapper to load a subset of
key data during object creation, and defer loading less important
or infrequently used data when it is first accessed. In general, LAZY
LOAD combines PARTIAL ACQUISITION (511) and LAZY ACQUISITION (507) to
control and optimize the process of loading data from the database.
When inserting and updating data into the database, a UNIT of WORK
[Fow03a] helps the data mapper understand which objects have been
changed, created, or destroyed.
A data mapper that supports the handling of data from multiple
types of application objects can employ a METADATA MAPPING [Fow03a]
to avoid hard-coding different mapping schemes.
DATA MAPPER simplifies application objects both programmatically and
in terms of their dependencies. It offers a degree of isolation and stability, protecting both application objects and schemas from changes
in the other. DATA MAPPER is not without its own complexity, however,
and changes in either the application object model or the database
schema may require changes to a data mapper.
##%%&&
When designing a DATABASE ACCESS LAYER (538) . . .
. . . we need a means to access and manipulate a single data record.
In-process data structures in some applications map directly to
relational database schemas, so there is a one-to-one correspondence between rows and objects. Having each object access its
corresponding row directly, however, tightly couples and dilutes
application code with infrastructural code.
When application data structure types correspond to tables, instances
of each structure to rows, and data fields of the structures to columns
in the tables, it is tempting to access and use the relational data directly
within the application code, since memory footprint is small and code
is direct. The drawbacks to this design, however, are that application
code and database-access code can become tightly coupled, and the
cohesion of the application code suffers. If the schema changes, mapping code must be added to the application wherever the data is used.
As more such changes occur, the complexity of both application and
mapping code increases. Similarly, if the application supports multiple databases with different SQL dialects, the application code must
handle these variations explicitly, so the original advantage of a small
footprint and direct access becomes a disadvantage.
Therefore:
Wrap the data structures and their database access code within
row data gateways whose internal structure looks exactly like a
database record, but which offer a representation-independent
data access interface to clients.
There is one row data gateway instance per row in the table.
Clients can use the gateways as if they were ¡®ordinary¡¯ application objects. When the client creates a new gateway instance, its
data is inserted into a new row of the corresponding database table.
When a client commits changes to the row data gateway, the corresponding fields in the database table are updated. Disposal of the
gateway object triggers the deletion of the content of the corresponding table row.
A ROW DATA GATEWAY is most useful if database records are accessed,
manipulated, and stored explicitly and separately from other database
records. Each row data gateway instance acts as an object that mimics a single record, such as one row of a particular database table,
but hides all details of access to the database behind its interface.
Changes to the table representation of the data structures are thus
transparent to clients, as well as changes of the database access code
if the row data gateway is ported to another database that uses a
different SQL dialect.
The row data gateway is responsible for any type conversion from the
data source types to the in-memory types. This type conversion is
often straightforward, since each column in the database table typically corresponds to a field in the row data gateway¡ªwhich suggests
that the mapping code be generated via METADATA MAPPING [Fow03a].
A row data gateway has a simple interface: set and get methods to
access and modify the in-memory data structure, and update, insert,
and delete methods to execute the appropriate SQL for the action and
data against the database. A row data gateway does not implement
application logic that operates on the encapsulated data structure,
which keeps the two concerns independent.
Each row data gateway object must be associated with a network
connection of some kind. To keep this management code simple and
self-contained, use an OBJECT MANAGER (492) to create new records.
This also addresses the potential problem of creating more than a
single row data gateway instance for a particular row. Also consider
supporting row deletion via the manager rather than via the gateway
object. OBJECT MANAGER can also offer finder capabilities that search
the appropriate table in the database.
##%%&&
When designing a DATABASE ACCESS LAYER (538) . . .
. . . we must provide measures to manipulate whole collections of data.
In-process data structures in some applications map directly to
a relational database schema. Directly manipulating collections
of data within the application¡¯s business logic via SQL, however,
tightly couples the application model and the database schema.
When application data structure types correspond to tables, it is
tempting to access and use relational data directly within application code, since memory footprint is small and the code is direct.
The resulting coupling creates development friction, however, if
the schema evolves or the database technology changes. Moreover,
applications are burdened with non-cohesive infrastructure code.
Although ROW DATA GATEWAYs offer a simple solution to these problems, they introduce other problems, such as row management, and
can yield a proliferation of objects. While these problems may not
arise for small tables or when only a few tables are accessed during a session, they may significantly complicate large applications.
ROW DATA GATEWAYs also do not handle collections of objects, as they
manipulate individual objects directly, rather than sets.
Therefore:
Wrap the database access code for a specific database table within
a specialized table data gateway, and provide it with an interface that allows applications to work on domain-specific data
collections.
There is one table data gateway instance per table in the database.
Clients can use the gateway to manipulate collections of data of the
same type. When a client adds a new object to the collection, its
data is stored as a new row in the table accessed by the gateway.
Modification and deletion of rows in the table are negotiated via the
gateway.
TABLE DATA GATEWAY is most useful if database records are accessed,
modified, and stored in sets, rather than individually. Each table
data gateway encapsulates the details of access to the database, as
well as the transformation of that data into collections of domain specific objects and vice versa. Changes to the table representation
of domain-specific objects become largely transparent to clients, as
well as changes to database access code when porting the table
data gateway to another database that uses a different SQL dialect.
Significant changes to the schema that change the normalization
of rows and partitioning of tables, however, are not as well supported.
A table data gateway has a simple interface consisting of several find
methods to get data from the database, together with corresponding
update, insert, and delete methods. Each method maps the input
parameters and action into SQL and executes it against a database
connection. The table data gateway is usually stateless with respect
to domain data, because its role is to push data back and forth.
One constraint on the interface, however, is that update and deletion
actions may not be available when a table data gateway corresponds
to a view.
Many alternatives exist for returning the results of queries to clients,
which may contain multiple database records. One is to return a
simple data structure such as a map or a DATA TRANSFER OBJECT (418).
Some environments, such as ADO.NET and JDBC 2.0, can return
a RECORD SET [Fow03a], which is an in-memory representation of
tabular data. Since RECORD SET mirrors a table structure directly in
application code, however, it can tightly couple that code to database specific aspects. TABLE DATA GATEWAY can also act as a factory and
manager and return the appropriate domain-specific objects.
##%%&&
When designing a DATABASE ACCESS LAYER (538) . . .
. . . we may want to avoid complex data mapping for self-contained
data records that offer only simple data manipulation methods.
In most object-oriented applications, some parts of the domain
logic are bound to the data on which they operate. If the data is
actually in the database, however, direct manipulation is not possible. For data structures with only simple associated behaviors,
a multi-layered solution may be too complex.
Raw use of a database API in the main parts of an application can yield
code that is non-cohesive and brittle in the face of changes such as
schema evolution or database technology migration. Database access
code and application code should therefore not be mixed too freely
in such cases. Introducing too much separation between application
objects and database access, however, may be too cumbersome when
the associated object behavior for each record is simple, such as
attribute queries, or queries based on simple calculations over the
attributes.
Therefore:
Encapsulate the data, the corresponding database access code,
and the data-centered domain behavior in active record objects
that offer a domain-specific interface to clients.
Clients can use active record objects via their domain-specific interfaces as if they were ordinary application objects. Any change in the
encapsulated data values¡¯ results can lead to a change in the row
data via an additional update method. If a client creates a new active
record, a new row in the corresponding database table is created. If
a client disposes of an active record, the corresponding table row is
deleted.
ACTIVE RECORD can be an appropriate choice when the domain logic of
an application is relatively simple, for example if it is centered around
create, read, update, and delete operations, attribute manipulation,
and simple calculated values based on attributes. In such cases,
changes to the table representation of the data structures have only
a local and limited affect on application-specific code, as well as
changes to the database access code due to porting the active object
to another database that uses a different SQL dialect. If a domain
object type is more complex, for example if it uses inheritance, inter object relationships, and collections, an ACTIVE RECORD may be less
suitable.
An ACTIVE RECORD class typically offers methods for constructing
instances from database domain objects. These can include SQL
result sets, static finder methods that wrap commonly used SQL
queries, methods to update the database and to insert the data
encapsulated within the active record into the database, get and
set methods for the active record data fields, and domain-related
methods that implement some data-centric pieces of the application¡¯s business logic. The implementation can suffer, however, if it
mixes problem domain concepts with infrastructural concepts, which
results in tight coupling between application and database parts.
##%%&&
An enterprise has multiple applications that are being built independently, with different
languages and platforms.
How can I integrate multiple applications so that they work together and can exchange
information?
In an ideal world, you might imagine an organization operating from a single cohesive piece of
software, designed from the beginning to work in a unified and coherent way. Of course even the
smallest operations don't work like that. Multiple pieces of software handle different aspects of
the enterprise. This is due to a host of reasons.
People buy packages that are developed by outside organizations
Different system are built at different times, leading to different technology choices
Different systems are built by different people, whose experience and preferences lead them to
different approaches to building applications
Getting an application out and delivering value is more important than ensuring that integration
is addressed, especially when that integration isn't adding any value to the application under
development.
As a result, any organization has to worry about sharing information between very divergent
applications. These can be written in different languages, based on different platforms, and with
some different assumptions about how the business operates.
Tying such applications together requires a lot of knowledge of understanding how to link
together applications on both a business level and a technical level. To have any chance of getting
your head around it, you must minimize what you need to know about how each application
works.
What is needed is a common data transfer mechanism that can be used by a variety of languages
and platforms, yet seems natural to each. It should require a minimal amount of specialized
hardware and software, making use of that the enterprise already has available.
Files are a universal storage mechanism, built in to any enterprise operating system, available
from any enterprise language. The simplest approach would be to somehow integrate the
applications using files.
Have each application produce files containing information that other applications need to
consume. Integrators take the responsibility of transforming files into different formats.
Produce the files at regular intervals according to the nature of the business.
An important decision with files is what format to use. Very rarely will the output of one
application be exactly what's needed for another, so you'll have to do a fair bit of processing of
files along the way. Not just do all the applications that use a file have to read it, you also have to
be able to use processing tools on it. As a result, standard file formats have grown up over time.
Mainframe systems commonly use data feeds based on the file system formats of COBOL. Unix
systems use text based files. The modern fashion is to use XML. An industry of readers, writers,
and transformation tools has built up around each of these formats.
Another issue with files is when to produce them and consume them. Since there's a certain
amount of effort required to produce and process a file, you usually don't want to work with
them too frequently. Typically you have some regular business cycle that drives the decision:
nightly, weekly, quarterly, etc. Applications get used to when a new file is available and
processes it at its time.
The great advantage of files is that integrators need no knowledge of the internals of an
application. The application team itself usually provides the file. The file's contents and format
are negotiated with integrators, although if a package is used there's often limited choices. The
integrators then deal with the transformations required for other applications, or they leave it up
the consuming applications to decide how they want to manipulate and read the file.
As a result the different applications are quite nicely decoupled from each other. Each application
can make internal changes freely without affecting other applications, providing they still
produce the same data in the files in the same format. The files effectively become the interface of
each application.
Part of what makes File Transfer simple is that no extra tools or integration packages are needed,
but that also means that developers have to do a lot of the work themselves. The applications
must agree on file naming conventions and the directories they appear in. The writer of a file
must implement a strategy to keep the filenames unique. The applications must agree on which
one will delete old files, and that application will have to know when a file is old and no longer
needed. The applications will need to implement a locking mechanism or follow a timing
convention to ensure that one application is not trying to read the file while another is still
writing it. If all of the applications do not have access to the same disk, then some application
must take responsibility for transferring the file from one disk to another.
One of the most obvious issues with File Transfer is that updates tend to occur infrequently, as a
result systems can get out of synchronization. A customer management system can process a
change of address and produce an extract file each night, but the billing system may send out the
bill to old address on the same day. Sometimes lack of synchronization isn't a big deal. People
often expect a certain lag in getting information around, even with computers. At other times the
result of using stale information is a disaster. When deciding on when to produce files, you have
to take the freshness needs of consumers into account.
In fact, the biggest problem with staleness is often on the software development staff themselves,
who often have to live with data that isn't quite right. This can lead to inconsistencies that are
difficult to resolve. If a customer changes his address on the same day with two different systems,
but one of them makes an error and gets the wrong street name, you'll have two inconsistent
addresses for a customer. You'll need some way to figure out how to resolve this. The longer the
period between file transfer, the more likely and more painful this problem will become.
Of course, there's no reason that you can't produce files more frequently. Indeed you can think of
Messaging as File Transfer where you produce a file with every change in an application. The
problem then is managing all the files that get produced, ensuring they are all read and none get
lost. This goes beyond what file system based approaches can do, particularly since there are
expensive resource costs with processing a file, which get prohibitive if you want to produce lots
of files quickly. As a result, once you get to fine grained files like this, it's easier to think of them
as Messaging.
To make data available more quickly and enforce an agreed-upon set of data formats, use a Shared
Database. To integrate applications' functionality rather than their data, use Remote Procedure
Invocation. To enable frequent exchanges of small amounts of data, perhaps used to invoke remote
functionality, use Messaging.
##%%&&
An enterprise has multiple applications that are being built independently, with different
languages and platforms. The enterprise needs information to be shared rapidly and consistently.
How can I integrate multiple applications so that they work together and can exchange
information?
File Transfer enables applications to share data, but can lack timeliness, yet timeliness of
integration is often a critical issue. If changes do not work their way quickly through a family of
applications, you are likely to do incorrect things due to the staleness of data. For modern
businesses, you want everyone to have the latest data as much as possible. Not just does this
reduce errors, it also increases people's trust in the data.
Rapid updates also allow inconsistencies to be handled better. The more frequently you
synchronize, the less likely you are to get inconsistencies and the less effort they are to deal with.
But however rapid the changes, there's still going to be problems. If an address is updated
inconsistently in rapid succession, how do I decide which one is the true address? I can take each
piece of data and say that one application is the master source for that data, but then I have to
remember who is the master for what data.
File Transfer also may not enforce data format sufficiently. Many of the problems in integration
come from incompatible ways of looking at the data. Often these represent subtle business issues
that can have a huge effect. A geological database may define an oil well as a single drilled hole,
which may or may not produce oil. A production database may define a well as multiple holes
covered by a single piece of equipment. These cases of semantic dissonance are much harder to deal
with than inconsistent data formats. (For a much deeper discussion of these issues, it's really
worth reading Data and Reality [Kent].)
What is needed is a central, agreed-upon datastore that all of the applications share, so that any
of them have access to any of the shared data whenever they need it.
Integrate applications by having them store their data in a single Shared Database.
If a family of integrated applications all rely on the same database, then you can be pretty sure
that they are always consistent all of the time. If you do get simultaneous updates to a single
piece of data from different sources, then you have transaction management systems that handle
that about as gracefully as it ever can be managed. Since the time between updates is so small,
any errors are much easier to find and fix.
Shared Database is made much easier by the widespread use of SQL-based relational databases.
Pretty much all application development platforms can work with SQL, often with quite
sophisticated tools. So you don't have to worry about multiple file formats. Since any application
pretty much has to use SQL anyway this avoid adding another technology for everyone to
master.
Since everyone is using the same database, this forces out problems in semantic dissonance.
Rather than leaving these problems to fester until they are difficult to solve with transforms, you
are forced to confront them and deal with them before the software goes live and you collect
large amounts of incompatible data.
One of the biggest difficulties with Shared Database is coming up with a suitable design for the
shared database. Coming up with a unified schema that can meet the needs of multiple
applications is a very difficult exercise, often resulting in a schema that application programmers
find difficult to work with.
If the technical difficulties of designing a unified schema aren't enough, there are also severe
political difficulties. If a critical application is likely to suffer delays in order to work with a
unified schema, then often there is irresistible pressure to separate. Human conflicts between
departments often exacerbate this problem.
Another, harder limit to Shared Database is external packages. Often they won't work with a
schema other than their own. Even if there is some room for adaptation, it's likely to be much
more limited than integrators would like. This problem also extends to integration after
development. Even if you can organize all your applications, you still have an integration
problem should a merger of companies occur.
Multiple applications using a Shared Database to frequently read and modify the same data can
cause performance bottlenecks and even deadlocks as each application locks others out of the
data. When the applications are distributed across multiple computers, the database must be
distributed as well so that each application can access the database locally, which confuses the
issue of which computer the data should be stored on. A distributed database with locking
conflicts can easily become a performance nightmare.
To integrate applications' functionality rather than their data, use Remote Procedure Invocation. To
enable frequent exchanges of small amounts of data, using a format per data type rather than one
universal schema, use Messaging.
##%%&&
An enterprise has multiple applications that are being built independently, with different
languages and platforms. The enterprise needs to share data and processes in a responsive way.
How can I integrate multiple applications so that they work together and can exchange
information?
File Transfer and Shared Database enable applications to share their data, which is an important part
of application integration, but just sharing data is often not enough. Often changes in data lead to
things that have to be done across different applications. Changing an address may be a simple
change in data, or it may trigger registration and legal processes to take into account different
rules in different legal jurisdictions. Having one application to invoke such processes in other
would require applications to know far too much about the internals of other applications.
This problem mirrors classic problems in application design. One of the most powerful
structuring mechanisms in application design is that of encapsulation--where modules hide their
data through a function call interface. In this way, they can intercept changes in data to carry out
the various actions they need to do when the data is changed. Shared Database provides a large,
unencapsulated data structure, which makes it much harder to do this. File Transfer allows an
application to react to changes as it processes the file, but the process is delayed.
The fact that Shared Database has unencapsulated data also makes it more difficult to maintain a
family of integrated applications. Many changes in any application can trigger a change in the
database, and database changes have a considerable ripple effect through every application. As a
result, systems that use Shared Database are often very reluctant to change the database, which
means that the application development work is much less responsive to the changing needs of
the business.
What is needed is a mechanism for one application to invoke a function in another application,
passing the data that needs to be shared and invoking the function that tells the receiver
application how to process the data.
Develop each application as a large-scale object or component with encapsulated data. Provide
an interface to allow other applications to interact with the running application.
Remote Procedure Invocation applies the principle of encapsulation to integrating applications. If an
application needs some information that is owned by another application, it asks that application
directly. If one application needs to modify the data of another, then it does so by making a call to
the other application. Each application can maintain the integrity of the data it owns.
Furthermore, each application can alter its internal data without having every other application
be affected.
There are a number of Remote Procedure Call (RPC) approaches: CORBA, COM, .NET Remoting,
Java RMI, etc. These vary as to how many systems support them and their ease of use. Often
these environments add additional capabilities, such as transactions.
For sheer ubiquity, the current fashionable favorite is Web Services using standards such as
SOAP and XML. A particularly valuable feature of web services is that they work easily with
HTTP, which is easy to get through firewalls.
The fact that there are methods that wrap the data make it easier to deal with semantic
dissonance. Applications can provide multiple interfaces to the same data, allowing some clients
to see one style and others another. Even updates can use multiple interfaces. This provides a lot
more ability to support multiple points of view than relational views. However, it is awkward for
integrators to add transformation components, so each application has to negotiate its interface
with its neighbors.
Since software developers are used to procedure calls, Remote Procedure Invocation fits in nicely with
what they are already used to. Actually, this is more of a disadvantage than it is an advantage.
There are big differences in performance and reliability between remote and local procedure calls.
If people don't understand these, then Remote Procedure Invocation can lead to slow and unreliable
systems (see [Waldo]).
Although the encapsulation helps reduce the coupling of the applications, by eliminating a large
shared data structure, the applications are still fairly tightly coupled together. The remote calls
each system supports tends to tie the different systems into a growing knot. In particular,
sequencing--doing certain things in a particular order--can make it difficult to change systems
independently. Often these become problems because issues that aren't significant within a single
application become so when integrating applications. People often design the integration the way
they would design a single application, unaware that the rules change.
To enable frequent exchanges of small amounts of data, perhaps used to invoke remote
functionality, use Messaging.
##%%&&
An enterprise has multiple applications that are being built independently, with different
languages and platforms. The enterprise needs to share data and processes in a responsive way.
How can I integrate multiple applications so that they work together and can exchange
information?
File Transfer and Shared Database enable applications to share their data, but not their functionality.
Remote Procedure Invocation enables applications to share functionality, but tightly couples them in
the process. Often the challenge of integration is about making collaboration between separate
systems as timely as possible, without coupling systems together in such a way that makes them
unreliable, either in terms of application execution or application development.
File Transfer allows you keep the applications very well decoupled, but at the cost of timeliness.
Systems just can't keep up with each other. Collaborative behavior is way too slow. Shared
Database keeps data together in a responsive way, but at the cost of coupling everything to the
database. It also fails to handle collaborative behavior.
Faced with these problems, Remote Procedure Invocation seems an appealing choice. But extending a
model used for a single application to application integration runs into plenty of other
weaknesses. These weaknesses start with the essential problems of distributed development.
Despite the fact that remote procedure calls look like local calls, they don't act the same. Remote
calls are slower, and can fail. With multiple applications communicating across an enterprise, you
don't want one application's failure to bring down all of the other applications. Also, you don't
want to design a system assuming that calls are fast and you don't want each application
knowing details of other applications, even if it's only details about their interfaces.
What we need is something like File Transfer where lots of little data packets can be produced
quickly, transferred easily, and the receiver application is automatically notified when a new
packet is available for consumption. The transfer needs a retry mechanism to make sure it
succeeds. The details of any disk structure or database for storing the data needs to be hidden
from the applications so that, unlike Shared Database, the storage schema and details can be easily
changed to reflect the changing needs of the enterprise. One application should be able to send a
packet of data to another application to invoke behavior in the other application, like Remote
Procedure Invocation, but without being prone to failure. The data transfer should be asynchronous
so that the sender does not need to wait on the receiver, especially when retry is necessary.
Use Messaging to transfer packets of data frequently, immediately, reliably, and asynchronously,
using customizable formats.
Asynchronous messaging is fundamentally a pragmatic reaction to the problems of distributed
systems. Sending a message does not require both systems to be up and ready at the same time.
Furthermore, thinking about the communication in an asynchronous manner forces developers to
recognize that working with a remote application is slower, which encourages design of
components with high cohesion (lots of work locally) and low adhesion (selective work
remotely).
Messaging systems also allow much of the decoupling you get when using File Transfer. Messages
can be transformed in transit without either the sender or receiver knowing about the
transformation. Indeed the decoupling allows integrators to broadcast messages to multiple
receivers, support choosing one of many potential receivers, and other topologies that allow
integration to be separated from the development of the applications. Since human issues tend to
separate application development from application integration, this approach works with human
nature rather than against it.
The transformation means that separate applications can have quite different conceptual models.
Of course this means that semantic dissonance will occur, but the messaging viewpoint is the
measure that Shared Database takes to avoid semantic dissonance are too complicated to work in
practice, and can't be done after the fact with packages or in enterprise merges.
By sending small messages frequently, you also allow applications to collaborate behaviorally as
well as share data. If a process needs to be launched once an insurance claim is received, it can be
done immediately as a message when a single claim comes in. Information can be requested and
a reply made rapidly. While such collaboration isn't going to be as fast as Remote Procedure
Invocation, the caller needn't stop while the message is being processed and the response returned.
And messaging isn't as slow as many people think -- many messaging solutions originated in the
financial services industry where thousands of stock quotes or trades have to pass through a
messaging system every second.
This book is about Messaging, so you can therefore assume that we consider Messaging to be
generally the best approach to enterprise application integration. But you shouldn't assume that
we think it's free of problems.
The high frequency of messages in Messaging reduces many of the inconsistency problems that
bedevil File Transfer, but it doesn't entirely remove them. There is still going to be some lag
problems with systems not being updated quite simultaneously.
Asynchronous design is not the way most software people are taught, and as a result there's a
whole host of different rules and techniques in place. The messaging context makes this a bit
easier than programming in a asynchronous application environment like X windows, but
asynchrony still has a learning curve. Testing and debugging are also harder in this environment.
The ability to transform messages has the nice benefit of allowing applications to be much more
decoupled from each other than in Remote Procedure Invocation and File Transfer. But this
independence does mean that integrators are often left with writing a lot of messy glue code to fit
everything together.
Once you decide that you want to use Messaging for system integration, there are a number of new
issues to consider and practices you can employ. How do you transfer packets of data? A sender
sends data to a receiver by sending a Message via a Message Channel that connects the sender and
receiver. How do you know where to send the data? If the sender does not know where to
address the data to, it can send the data to a Message Router, which will direct the data to the
proper receiver. How do you know what data format to use? If the sender and receiver do not
agree on the data format, the sender can direct the data to a Message Translator that will convert the
data to the receiver's format and then forward the data to the receiver. If you're an application
developer, how do you connect your application to the messaging system? An application that
wishes to use messaging will implement Message Endpoints to perform the actual sending and
receiving.
##%%&&
An enterprise has two separate applications that need to communicate, preferably by using
Messaging.
How does one application communicate with another using messaging?
Once a group of applications have a messaging system available, it's tempting to think that any
application can communicate with any other application any time desired. Yet the messaging
system does not magically connect all of the applications.
Applications Magically Connected
Likewise, it's not like an application just randomly throws information out into the messaging
system while other applications just randomly grab whatever information they run across. (Even
if this would work, it'd be very inefficient.) Rather, the application sending out the information
knows what sort of information it is, and the applications that would like to receive information
aren't looking for just any information, but for particular sorts of information they can use.
So the messaging system isn't a big bucket that applications throw information into and pull
information out of. It's a set of connections that enable applications to communicate by
transmitting information in predetermined, predictable ways.
Connect the applications using a Message Channel, where one application writes information
to the channel and the other one reads that information from the channel.
When an application has information to communicate, it doesn't just fling the information into
the messaging system, it adds the information to a particular Message Channel. An application
receiving information doesn't just pick it up at random from the messaging system; it retrieves
the information from a particular Message Channel.
The application adding info doesn't necessarily know what particular application will end up
retrieving the info, but it can be assured that whatever application retrieves the info, that
application will be interested in the info. This is because the messaging system has different
Message Channels for different types of information the applications want to communicate. When
an application sends information, it doesn't randomly add the info to any channel available; it
adds the info to a channel whose specific purpose is to communicate that sort of information.
Likewise, an application that wants to receive particular information doesn't pull info off some
random channel; it selects what channel to get information from based on what type of
information it wants.
Channels are logical addresses in the messaging system; how they're actually implemented
depends on the messaging system product and its implementation. Perhaps every Message
Endpoint has a direct connection to every other endpoint, or perhaps they're all connected
through a central hub. Perhaps several separate logical channels are configured as one physical
channel that nevertheless keeps straight which messages are intended for which destination. The
set of defined logical channels hides these configuration details from the applications.
A messaging system doesn't automatically come preconfigured with all of the message channels
the applications need to communicate. Rather, the developers designing the applications and the
communication between them have to decide what channels will be needed for the
communication. Then the system administrator who installs the messaging system software must
also configure it to set up the channels that the applications expect. While some messaging
system implementations support creating new channels while the applications are running, this
isn't very useful because other applications besides the one that creates the channel have to know
about the new channel so that they can start using it too. Thus the number and purpose of
channels available tend to be fixed at deployment time. (There are exceptions to this rule; see
Introduction to Messaging Channels.)
Something that often fools developers when they first get started with using a messaging system
is what exactly needs to be done to create a channel. A developer can write JMS code that
includes calling the method createQueue, or .NET code that includes new MessageQueue, but
neither of these bits of code actually allocates a new queue resource in the messaging system.
Rather, these pieces of code provide access to a resource that already exists in the messaging
system and was already created in the messaging system separately using its administration
tools.
Another issue to keep in mind when designing the channels for a messaging system: Channels
are cheap, but they're not free. Applications need multiple channels for transmitting different
types of information and transmitting the same information to lots of other applications. Each
channel requires memory to represent the messages; persistent channels require disk space as
well. Even if an enterprise system has unlimited memory and disk space, any messaging system
implementation usually has some hard or practical limit to how many channels it can service
consistently. So plan on creating new channels as your application needs them, but if it needs
thousands of channels, or needs to scale in ways that may require thousands of channels, you'll
need to choose a highly scalable messaging system implementation and test that scalability to
make sure it meets your needs. Datatype Channel helps you determine when you need another
channel. Selective Consumer makes one physical channel act logically like multiple channels.
So what do we call the applications that communicate via a Message Channel? There are a number of terms out there that are
largely equivalent. The most generic terms are probably Sender and Receiver ¡ª an application sends a message to a Message
Channel to be received by another application. Another popular term is Producer and Consumer. Equally popular is Publisher and
Subscriber ¡ª they are more geared towards Publish-Subscribe Channels, but are often times used in generic form. Sometimes
we also say that an application listens on a channel that another application talks to. In the world of Web services, we generally
talk about a Requester and Provider. These terms usually imply that the requester sends a message to the provider and receives
a response back. In the old days we called these Client and Server ¡ª the terms are equivalent, but saying client and server is
less cool. Now it gets gets confusing: when dealing with Web services, sometimes the application that sends a message to the
provider is considered a Consumer of the service. We can think of it in such a way that consumer sends a message to the provider
and then consumes the response. Luckily, usage of the term in this way occurs only in RPC scenarios. An application that sends
or receives messages may be called a Client of the messaging system; a more specific term is Endpoint or Message Endpoint.
If channels are logical addresses, what do these addresses look like? Like in so many cases, the detailed answer depends on the
implementation of the messaging system. Nevertheless, in most cases channels are referenced by an alphanumeric name, such
as MyChannel. Many messaging systems support a hierarchical channel naming scheme, which enables you to organize channels
in a way that is similar to a file system with folders and subfolders. For example, MyCorp/Prod/OrderProcessing/NewOrders would
indicate a channel that is used in a production application at MyCorp and contains new orders.
There are two different kinds of message channels, Point-to-Point Channels and Publish-Subscribe
Channels. Mixing different data types on the same channel causes a lot of confusion; to avoid this
confusion, use separate Datatype Channels. Applications that use messaging often benefit from a
special channel for invalid messages, an Invalid Message Channel. Applications that wish to use
Messaging but do not have access to a messaging client can still connect to the messaging system
using Channel Adapters. A well designed set of channels forms a Message Bus that acts like a
messaging API for a whole group of applications.
When a stock trading application makes a trade, it puts the request on a Message Channel for trade
requests. Another application that processes trade requests will look for ones to process on that
same message channel. If the requesting application needs to request a stock quote, it will
probably use a different message channel, one designed for stock quotes, so that the quote
requests stay separate from the trade requests.
Let's look at how to create a Message Channel in JMS. The J2EE SDK ships with a reference
implementation of the J2EE services, including JMS. The reference server can be started with the
j2ee command. Message channels have to be configured using the j2eeadmin tool. This tool can
configure both queues and topics:
Once the channels have been administered (created), they can then be accessed by JMS client
code:
The JNDI lookup doesn't create the queue (or topic); it was already created by the j2eeadmin
command. The JNDI loookup simply creates a Queue instance in Java that models and provides
access to the queue structure in the messaging system.
If your messaging system implementation is IBM's WebSphere MQ for Java, which implements
JMS, you'll use the WebSphere MQ JMS administration tool to create destinations. This will
create a queue named ¡°myQueue¡±:
Once that queue exists in WebSphere MQ, an application can then access the queue.
WebSphere MQ, without the full WebSphere Application Server, does not include a JNDI
implementation, so we cannot use JNDI to lookup the queue as we did in the J2EE example.
Rather, we must access the queue via a JMS session, like this:
MSMQ provides a number of different ways to create a message channel, called a queue. You can
create a queue using the Microsoft Message Queue Explorer or the Computer Management
console (see picture). From here you can set queue properties or delete queues.
Alternatively, you can create the queue using code:
Once the queue is created, an application can access it by creating a MessageQueue instance:
##%%&&
In many enterprise integration scenarios, a single event triggers a sequence of processing steps,
each performing a specific function. For example, let's assume a new order arrives in our
enterprise in the form of a message. One requirement may be that the message is encrypted to
prevent eavesdroppers from spying on a customer's order. A second requirement is that the
messages contain authentication information in the form of a digital certificate to ensure that
orders are placed only by trusted customers. In addition, duplicate messages could be sent from
external parties (remember all the warnings on the popular shopping sites to click the 'Order
Now' button only once?). To avoid duplicate shipments and unhappy customers, we need to
eliminate duplicate messages before subsequent order processing steps are initiated. To meet
these requirements, we need to transform a stream of possibly duplicated, encrypted messages
containing extra authentication data into a stream of unique, simple plain-text order messages
without the extraneous data fields.
How can we perform complex processing on a message while maintaining independence and
flexibility?
One possible solution would be to write a comprehensive 'incoming message massaging module'
that performs all the necessary functions. However, such an approach would be inflexible and
difficult to test. What if we need to add a step or remove one? For example, what if orders can be
placed by large customers who are on a private network and do not require encryption?
Implementing all functions inside a single component also reduces opportunities for reuse.
Creating smaller, well-defined components allows us to reuse them in other processes. For
example, order status messages may be encrypted but do not need to be de-duped because
duplicate status requests are generally not harmful. Separating the decryption function into a
separate modules allows us to reuse the decryption function for other messages.
Integration solutions are typically a collection of heterogeneous systems. As a result, different
processing steps may need to execute on different physical machines, for example because
individual processing steps can only execute on a specific systems. For example, it is possible that
the private key required to decrypt incoming messages is only available on a designated machine
and cannot be accessed from any other machine for security reasons. This means that the
decryption component has to execute on this designated machine while the other steps may
execute on other machines. Likewise, different processing steps may be implemented using
different programming languages or technologies that prevent them from running inside the
same process or even on the same computer.
Implementing each function in a separate component can still introduce dependencies between
components. For example, if the decryption component calls the authentication component with
the results of the decryption, we cannot use the decryption function without the authentication
function. We could resolve these dependencies if we could 'compose' existing components into a
sequence of processing steps in such a way that each component is independent from the other
components in the system. This would imply that components expose generic external interfaces
so that they are interchangeable.
If we use asynchronous messaging we should take advantage of the asynchronous aspects of
sending messages from one component to another. For example, a component can send a
message to another component for further processing without waiting for the results. Using this
technique, we could process multiple messages in parallel, on inside each component.
Use the Pipes and Filters architectural style to divide a larger processing task into a sequence
of smaller, independent processing steps (Filters) that are connected by channels (Pipes).
Each filter exposes a very simple interface: it receives messages on the inbound pipe, processes
the message, and publishes the results to the outbound pipe. The pipe connects one filter to the
next, sending output messages from one filter to the next. Because all component use the same
external interface they can be composed into different solutions by connecting the components to
different pipes. We can add new filters, omit existing ones or rearrange them into a new sequence
-- all without having to change the filters themselves. The connection between filter and pipe is
sometimes called port. In the basic form, each filter component has one input port and one output
port.
When applied to our example problem, the Pipes and Filters architecture results in three filters,
connected by two pipes (see picture). We need one additional pipe to send messages to the
decryption component and one to send the clear-text order messages from the de-duper to the
order management system. This makes for a total of four pipes.
Pipes and Filters describe a fundamental architectural style for messaging systems: individual
processing steps ("filters") are chained together through the messaging channels ("pipes"). Many
patterns in this and the following sections, e.g. routing and transformation patterns, are based on
this Pipes and Filters architectural style. This allows us to easily combine individual patterns into
larger solutions.
The Pipes and Filters style uses abstract pipes to decouple components from each other. The pipe
allows one component to send a message into the pipe so that it can be consumed later by
another process that is unknown to the component. The obvious implementation for such a pipe
is the Message Channel we just described at the beginning of this chapter. Most Message Channels
provide language, platform and location independence between the filters. This affords us the
flexibility to move a processing step to a different machine for dependency, maintenance or
performance reasons. However, a Message Channel provided by a messaging infrastructure can be
quite heavyweight if all components can in fact reside on the same machine. Using a simple
in-memory queue to implement the pipes would be much more efficient. Therefore, it is useful to
design the components so that they communicate with an abstract pipe interface. The
implementation of that interface can then be swapped out to use a Message Channel or an
alternative implementation such as an in-memory queue. The Messaging Gateway describes how
to design components for this flexibility.
One of the potential downsides of a Pipes and Filters architecture is the larger number of required
channels. First, channels may not be an unlimited resource as channels provide buffering and
other functions that consume memory and CPU cycles. Also, publishing a message to a channel
involves a certain amount of overhead because the data has to be translated from the
application-internal format into the messaging infrastructure's own format. At the receiving end
this process has to be reversed. If we are using a long chain of filters, we are paying for the gain
in flexibility with potentially lower performance due to repeated message data conversion.
The pure form of Pipes and Filters allows each filter to have only a single input port and a single
output port. When dealing with Messaging we can relax this property somewhat. A component
may consume messages off more than one channel and also output messages to more than one
channel (for example, a Message Router). Likewise, multiple filter components can consume
messages off a single Message Channel. A Point-to-Point Channel ensures that only one filter
component consumes each message.
Using Pipes and Filters also improves testability, an often overlooked benefit. We can test each
individual processing steps by passing a Test Message to the component and comparing the
results to the expected outcome. It is more efficient to test and debug each core function in
isolation because we can tailor the test mechanism to the specific function. For example, to test
the encryption / decryption function we can pass in a large number of messages containing
random data. After we encrypt and decrypt each message we compare it with the original. On
the other hand, to test authentication, we need to supply messages with specific authentication
codes that match known users in the system.
Connecting components with asynchronous Message Channels allows each unit in the chain to
operate in its own thread or its own process. When a unit has completed processing one message
it can send the message to the output channel and immediately start processing another message.
It does not have to wait for the subsequent components to read and process the message. This
allows multiple messages to be processed concurrently as they pass through the individual stages.
For example, after the first message has been decrypted, it can be passed on to the authentication
component. At the same time, the next message can already be decrypted (see picture). We call
such a configuration a processing pipeline because messages flow through the filters like liquid
flows through a pipe. When compared to strictly sequential processing, a processing pipeline can
significantly increase system throughput.
Pipeline Processing with Pipes-and-Filters
However, the overall system throughput is limited by the slowest process in the chain. To
improve throughput we can deploy multiple parallel instances of that process to improve
throughput. In this scenario, a Point-to-Point Channel with Competing Consumers is needed to
guarantee that each message on the channel is consumed by exactly one of N available processors.
This allows us to speed up the most time-intensive process and improve overall throughput.
However, this configuration can cause messages to be processed out of order. If the sequence of
messages is critical, we can only run one instance of each component or use a Resequencer.
For example, if we assume that decrypting a message is much slower than authenticating it, we
can use the above configuration (see picture), running three parallel instances of the decryption
component. Parallelizing filters works best if each filter is stateless, i.e. it returns to the previous
state after a message has been processed. This means that we cannot easily run multiple parallel
de-dup components because the component maintains a history of all messages that it already
received and is therefore not stateless.
Pipes and Filters architectures are by no means a new concept. The simple elegance of this
architecture combined with the flexibility and high throughput makes it easy to understand the
popularity of Pipes and Filters architectures. The simple semantics also allow formal methods to
be used to describe the architecture.
Kahn described Kahn Process Networks in 1974 as a set of parallel processes that are connected
by unbounded FIFO (First-In, First-Out) channels [Kahn]. [Garlan] contains a good chapter on
different architectural styles, including Pipes and Filters. [Monroe] gives a detailed treatment of
the relationships between architectural styles and design patterns. [PLOPD1] contains Regine
Meunier's "The Pipes and Filters Architecture" which formed the basis for the Pipes and Filters
pattern included in [POSA]. Almost all integration-related implementations of Pipes and Filters
follow the 'Scenario IV' presented in [POSA], using active filters that pull, process and push
independently from and to queuing pipes. The pattern described by Buschmann assumes that
each element undergoes the same processing steps as it is passed from filter to filter. This is
generally not the case in an integration scenario. In many instances, messages have to be routed
dynamically based on message content or external control. In fact, routing is such a common
occurrence in enterprise integration that it warrants its own patterns, the Message Router.
Pipes and Filters share some similarities with the concept of Communicating Sequential Processes
(CSPs). Introduced by Hoare in 1978 [CSP], CSPs provide a simple model to describe
synchronization problems that occur in parallel processing systems. The basic mechanism
underlying CSPs is the synchronization of two processes via input-output (I/O). I/O occurs
when process A indicates that it is ready to output to process B, and process B states that it is
ready to input from process A. If one of these happens without the other being true, the process is
put on a wait queue until the other process is ready. CSPs are different from integration solutions
in that they are not as loosely coupled, nor do the "pipes" provide any queuing mechanisms.
Nevertheless, we can benefit from the extensive treatment of CSPs in the academic world.
When discussing Pipes and Filters architectures we need to be cautious with the term 'filter'. We later define two additional
patterns, the Message Filter and the Content Filter. While both of these are special cases of a generic filter, so are many other
patterns in this pattern language. In other words, a pattern does not have to involve a filtering function (e.g. eliminating fields
or messages) in order to be a filter in the sense of Pipes and Filters. We could have avoided this confusion be renaming the Pipes
and Filters architectural style. However, we felt that Pipes and Filters are such an important and widely discussed concept that
it would be even more confusing if we gave it a new name. We are trying to use the word 'filter' cautiously throughout these
patterns and try to make it clear whether we are talking about a generic filter a la Pipes and Filters or a Message Filter / Content
Filter to filter messages. In places where we felt that there is still room for confusion, we generally termed the generic filter as
'component' which is a generic enough (and often enough abused) term that should not get us into trouble.
The following code snippet shows a generic base class for a filter with one input port and one
output port. The base implementation simply prints the body of the received message and sends
it to the output port. A more interesting filter would subclass the Processor class and override the
ProcessMessage method to perform additional actions on the message, e.g. transform the message
content or route it to different output channels.
You notice that the Processor requires references to an input and output channel in order to be
instantiated. The class is not tied to specific channels nor any other filter. This allows us to
instantiate multiple filters and chain them together in arbitrary configurations.
This implementation is a Event-Driven Consumer. The Process method registers for incoming
messages and instructs the messaging system to invoke the method OnReceiveCompleted every
time a message arrives. This method extracts the message data from the incoming event object
and calls the virtual method ProcessMessage.
This simple filter example is not transactional. If an error occurs while processing the message
(before it is sent to the output channel) the message is lost. This is generally not desirable in a
production environment. See Transactional Client for a solution to this problem.
##%%&&
Multiple processing steps in a Pipes and Filters chain are connected by Message Channels.
How can you decouple individual processing steps so that messages can be passed to different
filters depending on a set of conditions?
The Pipes and Filters architectural style connects filters directly to one another with fixed pipes.
This makes sense because many applications of the Pipes and Filters pattern (e.g., [POSA]) are
based on a large set of data items, each of which undergoes the same, sequential processing steps.
For example, a compiler will always execute the lexical analysis first, the syntactic analysis
second and the semantic analysis least. Message-based integration solutions, on the other hand,
deal with individual messages which are not necessarily associated with a single, larger data set.
As a result, individual messages are more likely to require a different series of processing steps.
A Message Channel decouples the sender and the receiver of a Message. This means that multiple
applications can publish Messages to a Message Channel. As a result, a message channel can
contain messages from different sources that may have to be treated differently based on the type
of the message or other criteria. You could create a separate Message Channel for each message
type (a concept explained in more detail later as a Datatype Channel) and connect each channel to
the required processing steps for that message type. However, this would require the message
originators to be aware of the selection criteria for different processing steps, so that they can
publish the message to the correct channel. It could also lead to an explosion of the number of
Message Channels. Also, the decision on which steps the message undergoes may not just depend
on the origin of the message. For example, we could imagine a situation where the destination of
a message changes by the number of messages that have passed through the channel so far. No
single originator would know this number and would therefore be unable to send the message to
the correct channel.
Message Channels provide a very basic form of routing capabilities. An application publishes a
Message to a Message Channel and has no further knowledge of that Message's destination.
Therefore, the path of the Message can change depending on which component subscribes to the
Message Channel. However, this type of 'routing' does not take into account the properties of
individual messages. Once a components subscribes to a Message Channel it will by default
consume all messages from that channel regardless of the individual messages' specific
properties. This behavior is similar to the use of the pipe symbol in Unix. It allows you to
compose processes into a Pipes and Filters chain but for the lifetime of the chain all lines of text
undergo the same steps.
We could solve this problem by making the receiving component itself responsible for
determining whether it should process the message or not. This is problematic, though, because
once the message is consumed and the component determines that it does not want the message
it can't just put the message back on the channel for another component to check out. Some
messaging systems allow receivers to inspect message properties without removing the message
from the channel so that it can decide whether to consume the message or not. However, this is
not a general solution and will also tie the consuming component to a specific type of message
because the logic for message selection is now built right into the component. This would reduce
the potential for reuse of that component and eliminate the composability that is the key strength
of the Pipes and Filters model.
Many of these alternatives assume that we can modify the participating components. In most
integration solutions, however, the building blocks ('components') are large applications which in
most cases cannot be modified at all, for example because they are packaged applications or
legacy applications. This makes it uneconomical or even impossible to adjust the message
producing or consuming applications to the needs of the messaging system or other applications.
An advantage of the Pipes and Filters is the composability of the individual components. It allows
us to insert additional steps into the chain without having to change existing components. This
opens up the option of decoupling two filters by inserting another filter in between that
determines what step to execute next.
Insert a special filter, a Message Router, which consumes a Message from one Message Channel
and republishes it to a different Message Channel channel depending on a set of conditions.
The Message Router differs from the most basic notion of Pipes and Filters in that it connects to
multiple output channels. Thanks to the Pipes and Filters architecture the components
surrounding the Message Router are completely unaware of the existence of a Message Router. A
key property of the Message Router is that it does not modify the message contents. It only
concerns itself with the destination of the message.
The key benefit of using a Message Router is that the decision criteria for the destination of a
message are maintained in a single location. If new message types are defined, new processing
components are added, or the routing rules change, we need to change only the Message Router
logic and all other components remain unaffected. Also, since all messages pass through a single
Message Router, incoming messages are guaranteed to be processed one-by-one in the correct
order.
While the intent of a Message Router is to decouple filters, using a Message Router can actually
cause the opposite effect. The Message Router needs to have knowledge of all possible message
destinations in order to send the message to the correct channel. In some situations, the list of
possible destinations may change frequently and turn the Message Router into a maintenance
bottleneck. In those cases, it would be better to let the individual recipients decide which
messages they are interested in. You can accomplish this by using a Publish-Subscribe Channel and
an array of Message Filters. We contrast these two alternatives by calling them predictive routing
and reactive filtering (for more detail see Message Filter).
Because a Message Router requires the insertion of an additional processing step it can degrade
performance. Many message-based systems have to decode the message from one channel before
it can be placed on another channel, which causes computational overhead if the message itself
does not really change. This overhead can turn a Message Router, into a performance bottleneck.
By using multiple routers in parallel or adding additional hardware, this effect can be minimized.
As a result, the message throughput (number of messages processed per time unit) may not be
impacted, but the latency (time for one message to travel through the system) will almost
certainly increase.
Deliberate use of Message Routers can turn the advantage of loose coupling into a disadvantage.
Loosely coupled systems can make it difficult to understand the "big picture" of the solution, i.e.
the overall flow of messages through the system. This is a common problem with messaging
solutions and the use of routers can exacerbate the problem. If everything is loosely coupled to
everything else it becomes impossible to understand which way messages actually flow. This can
complicate testing and debugging and maintenance. A number of tools can help alleviate this
problem. First, we can use the Message History to inspect messages at runtime and see which
components they traversed. Alternatively, we can compile a list of all channels that each
component in the system subscribes or publishes to. With this knowledge we can draw a graph of
all possible message flows across components. Many EAI packages maintain channel
subscription information in a central repository, making this type of static analysis easier.
A Message Router can use any number of criteria to determine the output channel for an incoming
message. The most trivial case is a fixed router. In this case, only a single input channel and a
single output channel are defined. The fixed router consumes one message off the input channel
and publishes it to the output channel. Why would we ever use such a brainless router? A fixed
router may be useful to intentionally decouple subsystems. Or we may be relaying messages
between multiple integration solutions. In most cases, a fixed router will be combined with a
Message Translator or a Channel Adapter to transform the message content or send the message
over a different channel type.
Many Message Routers decide the message destination only on properties of the message itself, for
example the message type or the values of specific message fields. We call such a router a
Content-Based Router. This type of router is so common that the Content-Based Router pattern
describes it in more detail.
Other Message Routers decide the message's destination based on environment conditions. We call
these routers Context-Based Routers. Such routers are commonly used to perform load balancing,
test or failover functionality. For example, if a processing component fails, the Context-Based
Router can re-route message to another processing component and thus provide fail-over
capability. Other routers split messages evenly across multiple channels to achieve parallel
processing similar to a load balancer. A Message Channel already provides basic load balancing
capabilities without the use of a Message Router because multiple competing consumers can each
consume messages off the same channel as fast as they can. However, a Message Router can have
additional built-in intelligence to route the messages as opposed to a simple round-robin
implemented by the channel.
Many Message Routers are stateless, i.e. they only look at one message at a time to make the
routing decision. Other routers take the content of previous messages into account when making
a routing decision. For example, we can envision a router that eliminates duplicate messages by
keeping a list of all messages it already received. These routers are stateful.
Most Message Routers contain hard-coded logic for the routing decision. However, some variants
connect to a Control Bus so that the middleware solution can change the decision criteria without
having to make any code changes or interrupting the flow of messages. For example, the Control
Bus can propagate the value of a global variable to all Message Routers in the system. This can be
very useful for testing to allow the messaging system to switch from 'test' to 'production' mode.
The Dynamic Router configures itself dynamically based on control messages from each potential
recipient.
Chapter Introduction to Message Routing introduces further variants of the Message Router.
The notion of a Message Router is central to the concept of a Message Broker, implemented in
virtually all commercial EAI tools. These tools accept incoming messages, validate them,
transform them and route them to the correct destination. This architecture alleviates the
participating applications from having to be aware of other applications altogether because the
message broker brokers between the applications. This is a key function in EAI because most
applications to be connected are packaged or legacy applications and the integration has to
happen non-intrusively, i.e. without changing the application code. This requires the middleware
to incorporate all routing logic so the applications do not have to. The Message Broker is the
integration equivalent of a Mediator presented in [GoF].
This code example demonstrates a very simple router that routes an incoming message to one of
two possible output channels.
The code is relatively straightforward. The example implements an event-driven consumer of
messages using C# delegates. The constructor registers the method OnMessage as the handler for
messages arriving on the inQueue. This causes the .NET framework to invoke the method
OnMessage for every message that arrives on the inQueue. OnMessage figures out where to route the
message by calling the method IsConditionFulfilled. In this trivial example
IsConditionFulfilled simply toggles between the two channels, dividing the messages evenly
between outQueue1 and outQueue2. In order to keep the code to a minimum, this simple router is
not transactional, i.e. if the router crashes after it consumed a message from the input channel
and before it published it to the output channel, we would lose a message. Later chapters will
explain how to make endpoints transactional (see Transactional Client).
##%%&&
The previous patterns describe how to construct messages and how to route them to the correct
destination. In many cases, enterprise integration solutions route messages between existing
applications such as legacy systems, packaged applications, homegrown custom applications, or
applications operated by external partners. Each of these applications is usually built around a
proprietary data model. Each application may have a slightly different notion of the Customer
entity , the attributes that define a Customer and which other entities a Customer is related to. For
example, the accounting system may be more interested in the customer's tax payer ID numbers
while the customer-relationship management (CRM) system stores phone numbers and
addresses. The application¡¯s underlying data model usually drives the design of the physical
database schema, an interface file format or a programming interface (API) -- those entities that
an integration solution has to interface with. As a result, the applications expect to receive
messages that mimic the application's internal data format.
In addition to the proprietary data models and data formats incorporated in the various
applications, integration solutions often times interact with standardized data formats that seek
to be independent from specific applications. There are a number of consortia and standards
bodies that define these protocols, such as RosettaNet, ebXML, OAGIS and many other, industry
specific consortia. In many cases, the integration solution needs to be able to communicate with
external parties using the ¡®official¡¯ data formats while the internal systems are based on
proprietary formats.
How can systems using different data formats communicate with each other using messaging?
We could avoid having to transform messages if we could modify all applications to use a
common data format. This turns out to be difficult for a number of reasons (see Shared Database).
First, changing an application¡¯s data format is risky, difficult, and requires a lot of changes to
inherent business functionality. For most legacy applications, data format changes are simply not
economically feasible. We may all remember the effort related to the Y2K retrofits where the
scope of the change was limited to the size of a single field!
Also, while we may get multiple applications to use the same data field names and maybe even
the same data types, the physical representation may still be quite different. One application may
use XML documents, while the other application uses COBOL copybooks.
Furthermore, if we adjust the data format of one application to match that of another application
we are tying the two applications more tightly to each other. One of the key architectural
principles in enterprise integration is loose coupling between applications (see Canonical Data
Model). Modifying one application to match another application's data format would violate this
principle because it makes two applications directly dependent on each other's internal
representation. This eliminates the possibility of replacing or changing one application without
affecting the other application, a scenario that is fairly common in enterprise integration.
We could incorporate the data format translation directly into the Message Endpoint. This way, all
applications would publish and consume messages in a common format as opposed to the
application internal data format. However, this approach requires access to the endpoint code,
which is usually not the case for packaged applications. In addition, hard-coding the format
translation to the endpoint would reduce the opportunities for code reuse.
Use a special filter, a Message Translator, between other filters or applications to translate one
data format into another.
The Message Translator is the messaging equivalent of the Adapter pattern described in [GoF]. An
adapter converts the interface of a component into a another interface so it can be used in a
different context.
Message translation may need to occur at a number of different levels. For example, data
elements may share the same name and data types, but may be used in different representations
(e.g. XML file vs. comma-separated values vs. fixed-length fields). Or, all data elements may be
represented in XML format, but use different tag names. To summarize the different kinds of
translation, we can divide it into multiple layers (loosely borrowing from the OSI Reference
Model model):
The Transport Layer at the bottom of the ¡°stack¡± provides data transfer between the different
systems. It is responsible for complete and reliable data transfer across different network
segments and deals with lost data packets and other network errors. Some EAI vendors provide
their own transport protocols (e.g. TIBCO RendezVous) while other integration technologies
leverage TCP/IP protocols (e.g. SOAP). Translation between different transport layers can be
provided by the Channel Adapter pattern.
The Data Representation layer is also referred to as the ¡°syntax layer¡±. This layer defines the
representation of data that is transported. This translation is necessary because the Transport
Layer can only transport character or byte streams. This means that complex data structures have
to be converted into a character string. Common formats include XML, fixed-length fields (e.g.
EDI records) or proprietary formats. In many cases, data is also compressed or encrypted, carries
check digits or digital certificates. In order to interface systems with different data
representations, data may have to be decrypted, uncompressed and parsed, then the new data
format rendered, and possibly compressed and encrypted as well.
The Data Types layer defines the application data types that the application (domain) model is
based on. Here we deal with decisions whether date fields are represented as strings or as native
date structures, whether dates carry a time-of-day component, which time zone they are based on,
etc. We may also consider whether the field Postal Code denotes only a US ZIP code or can contain
Canadian postal codes. In case of a US ZIP code, do we include a ZIP+4; is it mandatory? Is it
stored in one field or two? Many of these questions are usually addressed in so-called Data
Dictionaries. The issues related to Data Types go beyond whether a field is of type string or integer.
Consider sales data that is organized by region. The application used by one department may
divide the country into 4 regions: Western, Central, Southern and Eastern, identified by the
letters ¡®W¡±, ¡®C¡¯, ¡®S¡¯ and ¡®E¡¯. Another department may differentiate the Pacific Region from the
Mountain Region and distinguishes the Northeast from the Southeast. Each region is identifies by
a unique two-digit number. What number does the letter ¡®E¡¯ correspond to?
The Data Structures describes the data at the level of the application domain model. It is therefore
also referred to as the Application Layer. This layer defines the logical entities that the
application deals with, such as Customer, Address or Account. It also defines the relationships
between these entities: Can one customer have multiple accounts? Can a customer have multiple
addresses? Can customers share and address? Can multiple customers share an account? Is the
address part of the account or the customer? This is the domain of entity-relationship diagrams
and class diagrams.
Many of the design trade-offs in integration are driven by the need to decouple components or
applications. Decoupling is an essential tool to enable the management of change. Integration
typically connects existing applications and has to accommodate changes to these applications.
Message Channels decouple applications from having to know each other's location. A Message
Router can even decouple applications from having to agree on a common Message Router.
However, this form of decoupling only achieves limited independence between applications if
they depend on each other's data formats. A Message Translator can help remove this level of
dependency.
Many business scenarios require transformations at more than one layer. For example, let's
assume an EDI 850 Purchase Order record represented as a fixed-format file has to be translated
to an XML document sent over http to the order management system which uses a different
definition of the Order object. The required transformation spans all four levels: the transport
changes from a file to HTTP, the data format changes from fixed-format to XML, data types and
data formats have to be converted to comply with the Order object defined by the order
management system. The beauty of a layered model is that we can treat one layer without regard
to the lower layers and therefore can choose to work at different levels of abstraction.
Correspondingly, we can talk about transformation at each layer of abstraction (see picture).
Chaining multiple Message Translator units using Pipes and Filters results in the following
architecture (see picture). Creating one Message Translator for each layer allows us to reuse these
components in other scenarios. For example, the Channel Adapter and the EDI-to-XML Message
Translator can be generic enough to deal with any incoming EDI document.
This approach also makes individual layers interchangeable. You could use the same structural
transformation mechanisms, but instead of converting the data representation into a fixed format
you could convert it into a comma-separated file by swapping out the data representation
transformation.
There are many specializations and variations of the Message Translator pattern. A Content
Enricher augments the information inside a message while the Content Filter removes information.
The Claim Check removes information but stores it for later retrieval. The Normalizer can convert a
number of different message formats into a consistent format. Lastly, the Canonical Data Model
shows how to leverage multiple Message Translators to achieve data format decoupling. Inside
each of those patterns, complex structural transformations can occur (e.g. mapping a
many-to-many relationship into a one-to-one relationship). The Messaging Bridge performs a
translation of the transport layer by connecting multiple messaging systems to each other.
Transformation is such a common need that the W3C defined a standard language for the
transformation of XML documents, the Extensible Stylesheet Language (XSL). Part of XSL is the
XSL Transformation (XSLT) language, a rules-based language that translates one XML document
into a different format. Since this is a book on integration and not on XSLT, we just show a simple
example (for all the gory detail see the spec [XSLT 1.0] or to learn by reviewing code examples
see [Tennison]). In order to keep things simple, we explain the required transformation by
showing example XML documents as opposed to XML schemas.
For example, let's assume we have an incoming XML document and need to pass it to the
accounting system. If both systems use XML, the Data Representation layer is identical and we
need to cover any differences in field names, data types and structure. Let's assume the incoming
document looks like this:
This XML document contains customer data. Each customer can be associated with multiple
addresses, each of which can contain multiple phone numbers. The XML represents addresses as
independent entities so that multiple customers could share an address.
Let's assume the accounting system needs the following representation. If you think that the
German tag names are bit far fetched, keep in mind that one of the most popular pieces of
The resulting document has a much simpler structure. Tag names are different and some fields
are merged into a single field. Since there is room for only one address and phone numbers, we
need to pick one from the original document based on business rules. The following XSLT
program transforms the original document into the desired format. It does so by matching
elements of the incoming document and translating them into the desired document format.
XSL is based on pattern matching and can be a bit hairy to read if you are used to procedural
programming like most of us. In a nutshell, the <xsl:template> are called whenever an element in
the incoming XML document matches the expression specified in the match attribute. For
example, the line
causes the subsequent lines to be executed for each <customer> element in the source document.
The next statements concatenate first and last name and output it inside the <Name> element.
Getting the address is a little trickier. The XSL code looks up the correct instance of the <address>
element and calls the "subroutine" getaddr. getaddr extracts the address and phone number from
the original <address> element. It uses the cell cell phone number if one is present and the home
phone number otherwise.
If you find XSL programming a bit cryptic, you are in good company. Therefore, most integration
vendors provide visual transformation editors that displays the structure of the two document
formats on the left-hand side and right-hand side of the screen, respectively. The users can then
drag and drop between the two sides to associate elements between the formats. This can be a lot
simpler than coding XSL. Some vendors specialize entirely in transformation tools, for example
Contivo, Inc..
The following screen shots shows the Microsoft BizTalk Mapper editor that is integrated into
Visual Studio. The diagram shows the mapping between individual elements more clearly than
the XSL script. On the other hand, some of the details (e.g., how the address is chosen) are hidden
underneath the functoid icons.
Creating Transformations the Drag-Drop Style
Being able to drag and drop transformations shortens the learning curve for developing a
Message Translator dramatically. As so often though, visual tools can also become a liability when
it comes to debugging or when you need to create complex solutions. Therefore, many tools let
you switch back and forth between XSL and the visual tool.
##%%&&
Applications are communicating by sending Messages to each other via Message Channels.
How does an application connect to a messaging channel to send and receive messages?
The application and the messaging system are two separate sets of software. The application
provides functionally for some type of user, whereas the messaging system manages messaging
channels for transmitting messages for communication. Even if the messaging system is
incorporated as a fundamental part of the application, it is still a separate, specialized provider of
functionality much like a database management system or a web server. Because the application
and the messaging system are separate, they must have a way to connect and work together.
Applications disconnected from a message channel
A messaging system is a type of server, capable of taking requests and responding to them. Like
a database accepting and retrieving data, a messaging server accepts and delivers messages. A
messaging system is a messaging server.
A server needs clients, and an application that uses messaging is a client of the messaging server.
But applications do not necessarily know how to be messaging clients, any more than they know
how to be database clients. The messaging server, like a database server, has a client API that the
application can use to interact with the server. The API is not application-specific; it is
domain-specific, where messaging is the domain.
The application must contain a set of code that connects and unites the messaging domain with
the application to allow the application to perform messaging.
Connect an application to a messaging channel using a Message Endpoint, a client of the
messaging system that the application can then use to send or receive messages.
Message Endpoint code is custom to both the application and the messaging system¡¯s client API.
The rest of the application knows little about message formats, messaging channels, or any of the
other details of communicating with other applications via messaging. It just knows that it has a
request or piece of data to send to another application, or is expecting those from another
application. It is the messaging endpoint code that takes that command or data, makes it into a
message, and sends it on a particular messaging channel. It is the endpoint that receives a
message, extracts the contents, and gives them to the application in a meaningful way.
The Message Endpoint encapsulates the messaging system from the rest of the application, and
customizes a general messaging API for a specific application and task. If an application using a
particular messaging API were to switch to another, developers would have to rewrite the
message endpoint code, but the rest of the application should remain the same. If a new version
of a messaging system changes the messaging API, this should only affect the message endpoint
code. If the application decides to communicate with others via some means other than
messaging, developers should ideally be able to rewrite the message endpoint code as something
else, but leave the rest of the application unchanged.
A Message Endpoint can be used to send messages or receive them, but one instance does not do
both. An endpoint is channel-specific, so a single application would use multiple endpoints to
interface with multiple channels. An application may use more than one endpoint to interface to
a single channel, usually to support multiple concurrent threads.
A Message Endpoint is a specialized Channel Adapter, one that has been custom developed for and
integrated into its application.
A Message Endpoint should be designed as a Messaging Gateway to encapsulate the messaging
code and hide the message system from the rest of the application. It can employ a Messaging
Mapper to transfer data between domain objects and messages. It can be structured as a Service
Activator to provide asynchronous message access to a synchronous service or function call. An
endpoint can explicitly control transactions with the messaging system as a Transactional Client.
Sending messages is pretty easy, so many endpoint patterns concern different approaches for
receiving messages. A message receiver can be a Polling Consumer or an Event-Driven Consumer.
Multiple consumers can receive messages from the same channel either as Competing Consumers
or via a Message Dispatcher. It can decide which messages to consume or ignore using a Selective
Consumer . It can use a Durable Subscriber to make sure a subscriber does not miss messages
published while the endpoint is disconnected. And the consumer can be an Idempotent Receiver
that correctly detects and handles duplicate messages.
In JMS, the two main endpoint types are MessageProducer, for sending messages, and
MessageConsumer, for receiving messages. A Message Endpoint uses an instance of one of these
types to either send or receive messages to/from a particular channel.
In .NET, the main endpoint class is the same as the main Message Channel class, MessageQueue. A
Message Endpoint uses an instance of MessageQueue to send or receive messages to/from a
particular channel.
##%%&&
An application is using Messaging to make remote procedure calls (RPC¡¯s) or transfer documents.
How can the caller be sure that exactly one receiver will receive the document or perform the
call?
One advantage of an RPC is that it¡¯s invoked on a single remote process, so either that receiver
performs the procedure or it does not (and an exception occurs). And since the receiver was only
called once, it only performs the procedure once. But with messaging, once a call is packaged as a
Message and placed on a Message Channel, potentially many receivers could see it on the channel
and decide to perform the procedure.
The messaging system could prevent more than one receiver from monitoring a single channel,
but this would unnecessarily limit callers that wish to transmit data to multiple receivers. All of
the receivers on a channel could coordinate to ensure that only one of them actually performs the
procedure, but that would be complex, create a lot of communications overhead, and generally
increase the coupling between otherwise independent receivers. Multiple receivers on a single
channel may be desirable so that multiple messages can be consumed concurrently, but any one
receiver should consume any single message.
Send the message on a Point-to-Point Channel, which ensures that only one receiver will
receive a particular message.
A Point-to-Point Channel ensures that only one receiver consumes any given message. If the
channel has multiple receivers, only one of them can successfully consume a particular message.
If multiple receivers try to consume a single message, the channel ensures that only one of them
succeeds, so the receivers do not have to coordinate with each other. The channel can still have
multiple receivers to consume multiple messages concurrently, but only a single receiver
consumes any one message.
When a Point-to-Point Channel only has one consumer, the fact that a message only gets consumed
once is not surprising. When the channel has multiple consumers, then they become Competing
Consumers, and the channel ensures that only one of the consumers receives each message. The
effort to consume messages is highly scalable because that work can be load-balanced across
multiple consumers running in multiple applications on multiple computers.
Whereas a Point-to-Point Channel sends a message to only one of the available receivers, to send a
message to all available receivers, use a Publish-Subscribe Channel. To implement RPC's using
messaging, use Request-Reply with a pair of Point-to-Point Channels. The call is a Command Message
whereas the reply is a Document Message.
In a stock trading system, the request to make a particular trade is a message that should be
consumed and performed by exactly one receiver, so the message should be placed on a
Point-to-Point Channel.
In JMS, a point-to-point channel implements the Queue interface. The sender uses a QueueSender to
send messages; each receiver uses its own QueueReceiver to receive messages. [JMS11, pp.75-78],
[Hapner, p.18]
An application uses a QueueSender to send a message like this:
An application uses a QueueReceiver to receive a message like this:
Note: JMS 1.1 unifies the client API¡¯s for the point-to-point and publish/subscribe domains, so
the code shown here can be simplified to use Destination, ConnectionFactory, Connection,
Session, MessageProducer, and MessageConsumer, rather than their Queue-specific counterparts.
In .NET, the MessageQueue class implements a point-to-point channel. [SysMsg] MSMQ, which
implements .NET messaging, only supported point-to-point messaging prior to version 3.0, so
point-to-point is what .NET supports. Whereas JMS seperates the responsibilities of the
connection factory, connection, session, sender, and queue, a MessageQueue does it all.
Send a message on a MessageQueue like this:
Receive a message on a MessageQueue like this:
##%%&&
An application is using Messaging to announce events.
How can the sender broadcast an event to all interested receivers?
Luckily, there are well-established patterns for implementing broadcasting. The Observer pattern
[GoF] describes the need to decouple observers from their subject so that the subject can easily
provide event notification to all interested observers no matter how many observers there are
(even none). The Publisher-Subscriber pattern [POSA] expands upon Observer by adding the
notion of an event channel for communicating event notifications.
That¡¯s the theory, but how does it work with messaging? The event can be packaged as a Message
so that messaging will reliably communicate the event to the observers (subscribers). Then the
event channel is a Message Channel. But how will a messaging channel properly communicate the
event to all of the subscribers?
Each subscriber needs to be notified of a particular event once, but should not be notified
repeatedly of the same event. The event cannot be considered consumed until all of the
subscribers have been notified. But once all of the subscribers have been notified, the event can be
considered consumed and should disappear from the channel. Yet having the subscribers
coordinate to determine when a message is consumed violates the decoupling of the Observer
pattern. Concurrent consumers should not be considered to compete, but should be able to share
the event message.
Send the event on a Publish-Subscribe Channel, which delivers a copy of a particular event to
each receiver.
A Publish-Subscribe Channel works like this: It has one input channel that splits into multiple
output channels, one for each subscriber. When an event is published into the channel, the
Publish-Subscribe Channel delivers a copy of the message to each of the output channels. Each
output channel has only one subscriber, which is only allowed to consume a message once. In
this way, each subscriber only gets the message once and consumed copies disappear from their
channels.
A Publish-Subscribe Channel can be a useful debugging tool. Even though a message is destined to
only a single receiver, using a Publish-Subscribe Channel allows you to eavesdrop on a message
channel without disturbing the existing message flow. Monitoring all traffic on a channel can be
tremendously helpful when debugging messaging applications. It can also save you from
inserting a ton of print statements into each application that participates in the messaging
solution. Creating a program that listens for messages on all active channels and logs them to a
file can realize many of the same benefits that a Message Store brings.
The ability to eavesdrop on a Publish-Subscribe Channel can also turn into a disadvantage. If your
messaging solution transmits payroll data between the payroll system and the accounting system,
you may not want to allow anyone to write a simple program to listen to the message traffic.
Point-to-Point Channels alleviate the problem to some extent because the eavesdropper would
consume the messages and the situation would be detected very quickly. However, some
implementations of message queues implementing peek functions that let consumers look at
messages inside a queue without consuming any of the messages. As a result, subscribing to a
Message Channel is an operation that should be restricted by security policies. Many (but not all)
commercial messaging implementations implement such restrictions. In addition, creating a
monitoring tool that logs active subscribers to Message Channels can be a useful systems
management tool.
For more details on how to implement Observer using messaging, see JMS Publish/Subscribe
Example.
Many messaging systems allow subscribers to Publish-Subscribe Channels to specify special wild card characters. This is a
powerful technique to allow subscribers to subscribe to multiple channels at once. For example, if an application publishes
messages to the channels MyCorp/Prod/OrderProcessing/NewOrders and MyCorp/Prod/OrderProcessing/CancelledOrders an
application could subscribe to MyCorp/Prod/OrderProcessing/* and receive all messages related to order processing. Another
application could subscribe to MyCorp/Dev/** to receive all messages sent by all applications in the development environment.
Only subscribers are allowed to use wildcards, publishers are always required to publish a message to a specific channel. The
specific capabilities and syntax for wildcard subscribers vary between the different messaging vendors.
An Event Message is usually sent on a Publish-Subscribe Channel because multiple dependents are
often interested in an event. A subscriber can be durable or non-durable¡ªsee Durable Subscriber.
If notifications should be acknowledged by the subscribers, use Request-Reply, where the
notification is the request and the acknowledgement is the reply.
In a stock trading system, many systems may need to be notified of the completion of a trade, so
make them all subscribers of a Publish-Subscribe Channel that publishes trade completions.
In JMS, a Publish-Subscribe Channel implements the Topic interface. The sender uses a
TopicPublisher to send messages; each receiver uses its own TopicSubscriber to receive messages.
[JMS11, pp.79-85], [Hapner, p.18]
An application uses a TopicPublisher to send a message like this:
An application uses a TopicSubscriber to receive a message like this:
Note: JMS 1.1 unifies the client API¡¯s for the point-to-point and publish/subscribe domains, so
the code shown here can be simplified to use Destination, ConnectionFactory, Connection,
Session, MessageProducer, and MessageConsumer, rather than their Topic-specific counterparts.
A new feature in MSMQ 3.0 [MSMQ01] is a one-to-many messaging model, which has two different
approaches:
1. Real-Time Messaging Multicast ¨C This most closely matches publish-subscribe, but its implementation is
entirely dependent on IP multicasting via the Pragmatic General Multicast (PGM) protocol.
2. Distribution Lists and Multiple-Element Format Names ¨C A Distribution List enables the sender to
explicitly send a message to a list of receivers (but this violates the spirit of the Observer pattern). A
Multiple-Element Format Name is a symbolic channel specifier that dynamically maps to multiple real channels,
which is more the spirit of the publish-subscribe pattern but still forces the sender to choose between real and
not-so-real channels.
The .NET CLR does not provide direct support for using the one-to-many messaging model.
However, this functionality can be accessed through the COM interface [MDMSG], which can be
embedded in .NET code.
##%%&&
An application is using Messaging to transfer different types of data, such as different types of
documents.
How can the application send a data item such that the receiver will know how to process it?
All messages are just instances of the same message type, as defined by the messaging system,
and the contents of any message are ultimately just a byte array. While this simple structure--a
bundle of bytes--is specific enough for a messaging system to be able to transmit a message, it is
not specific enough for a receiver to be able to process a message¡¯s contents.
A receiver must know the message content¡¯s data structure and data format. The structure could
be character array, byte array, serialized object, XML document, etc. The format could be the
record structure of the bytes or characters, the class of the serialized object, the DTD of the XML
document, etc. All of this knowledge is loosely referred to as the message¡¯s type, meaning the
structure and format of the message¡¯s contents.
The receiver must know what type of messages it¡¯s receiving, or it doesn¡¯t know how to process
them. For example, a sender may wish to send different objects such as purchase orders, price
quotes, and queries. Yet a receiver will probably take different steps to process each of those, so it
has to know which is which. If the sender simply sends all of these to the receiver via a message
channel, the receiver will not know how to process each one.
Mixed Data Types
The sender knows what message type it¡¯s sending, so how can this be communicated to the
receiver? The sender could put a flag in the message¡¯s header (see Format Indicator), but then the
receiver will need a case statement. The sender could wrap the data in a Command Message with a
different command for each type of data, but that presumes to tell the receiver what to do with
the data when all that the message is trying to do is transmit the data to the receiver.
A similar problem that is completely separate from messaging occurs using non-array collections:
collections can be heterogeneous (each item can be of any object type), but as a practical matter
collections need to be homogeneous (each item should be of the same object type, meaning that
they all implement the same abstract class or interface). Homogeneous collections are much more
useful because an iterator on the collection knows what type each item will be and can
manipulate each item using the methods that type understands.
The same principle applies to messaging because in this context, a channel is like a collection and
a receiver is like an iterator. Although a particular channel doesn¡¯t require that all of its messages
be of the same type, they ought to be so that a receiver on that channel knows what type of
message it¡¯s receiving.
Use a separate Datatype Channel for each data type, so that all data on a particular channel is
of the same type.
By using a separate Datatype Channel for each type of data, all of the messages on a given channel
will contain the same type of data. The sender, knowing what type the data is, will need to select
the appropriate channel to send it on. The receiver, knowing what channel the data was received
on, will know what its type is.
As discussed in Message Channel, channels are cheap but not free. An application may need to
transmit many different data types, too many to create a separate Datatype Channel for each. In
this case, multiple data types can share a single channel by using a different Selective Consumer for
each type. This makes a single channel act like multiple data type channels. Whereas Datatype
Channel explains why all messages on a channel must be of the same format, Canonical Data Model
explains how all messages on all channels in an enterprise should follow a unified data model.
A Message Dispatcher, besides providing concurrent message consumption, can be used to process
a generic set of messages in type-specific ways. Each message must specify its type; the
dispatcher detects the message's type and dispatches it to a type-specific performer for
processing. The messages on the channel are still all of the same type, but that type is the more
general one that the dispatcher supports, not the more specific ones that the various performers
require.
In a stock trading system, if the format of a quote request is different from that of a trade request,
the system should use a separate Datatype Channel for communicating each kind of request.
Likewise, a change-of-address announcement may have a different format from a
change-of-portfolio-manager announcement, so each kind of announcement should have its own
Datatype Channel.
Reconsidering our earlier example, since the sender wants to send three different types of data
(purchase orders, price quotes, and queries), it should use three different channels. When sending
an item, the sender must select the appropriate Datatype Channel for that item. When receiving an
item, the receiver knows the item¡¯s type because of which datatype channel it received the item
on.
##%%&&
An application is using Messaging to receive Messages.
How can a messaging receiver gracefully handle receiving a message that makes no sense?
In theory, everything on a Message Channel is just a message and message receivers just process
messages. However, to process a message, a receiver must be able to interpret its data and
understand its meaning. This is not always possible: the message body may cause parsing errors,
lexical errors, or validation errors. The message header may be missing needed properties, or the
property values may not make sense. A sender might put a perfectly good message on the wrong
channel, transmitting it to the wrong receiver. A malicious sender could purposely send an
incorrect message just to mess-up the receiver. A receiver may not be able to process all messages
it receives, so it needs to have some other way to handle messages it does not consider valid.
A Message Channel is a Datatype Channel, where each of the messages on the channel is supposed
to be of the proper datatype for that channel. If a sender puts a message on the channel that is not
of the proper datatype, the messaging system will transmit the message successfully, but the
receiver will not recognize the message and will not know how to process it.
An example of a message with an improper datatype or format is a byte message on a channel
that is supposed to contain text messages. Another example is a message whose format is not
correct, such as an XML document that is not well-formed, or that is not valid for the
agreed-upon DTD or schema. There's nothing wrong with these messages, as far as the
messaging system is concerned, but the receiver will not be able to process them, so they are
invalid.
Messages that do not contain the header field values that the receiver expects are also invalid. If a
message is supposed to have header properties such as a Correlation Identifier, Message Sequence
identifiers, a Return Address, etc., but the message is missing the properties, then the messaging
system will deliver the message properly but the receiver will not be able to process it
successfully.
Invalid Message
When the receiver discovers that the message it's trying to process is not valid, what should it do
with the message? It could put the message back on the channel, but then the message will just be
re-consumed by the same receiver or another like it. Meanwhile, invalid messages that are being
ignored will clutter the channel and hurt performance. The receiver could consume the invalid
message and throw it away, but that would tend to hide messaging problems that need to be
detected. What the system needs is a way to clean improper messages out of channels and put
them somewhere out of the way, yet in a place where these invalid messages can be detected to
diagnose problems with the messaging system.
The receiver should move the improper message to an Invalid Message Channel, a special
channel for messages that could not be processed by their receivers.
When designing a messaging system for applications to use, the administrator will need to define
one or more Invalid Message Channels for the applications to use.
The Invalid Message Channel will not be used for normal, successful communication, so its being
cluttered with improper messages will not cause a problem. An error handler that wants to
diagnose improper messages can use a receiver on the invalid channel to detect messages as they
become available.
A Invalid Message Channel is like an error log for messaging. When something goes wrong in an
application, it's a good idea to log the error. When something goes wrong processing a message,
it's a good idea to put the message on the channel for invalid messages. If it won't be obvious to
anyone browsing the channel why this message is invalid, the application should also log an
error with more details.
Keep in mind that a message is neither inherently valid or invalid--it is the receiver's context and
expectations that make this determination. A message that may be valid for one receiver may be
invalid for another receiver; two such receivers should not share the same channel. A message
that is valid for one receiver on a channel should be valid for all other receivers on that channel;
likewise, if one receiver considers a message invalid, all other receivers should as well. It is the
sender's responsibility to make sure that a message it sends on a channel will be considered valid
by the channel's receivers; otherwise, the receivers will ignore the sender's messages by rerouting
them to the Invalid Message Channel.
A similar but separate problem is when a message is structured properly, but its contents are
semantically incorrect. For example, a Command Message may instruct the receiver to delete a
database record that does not exist. This is not a messaging error, but an application error. As
such, while it may be tempting to move the message to the Invalid Message Channel, there is
nothing wrong with the message, so treating it as invalid is misleading. Rather, an error like this
should be handled as an invalid application request, not an invalid message.
This differentiation between message-processing errors and application errors becomes simpler
and clearer when the receiver is implemented as a Service Activator or Messaging Gateway. These
patterns separate message-processing code from the rest of the application. If an error occurs
while processing the message, the message is invalid and should be moved to the Invalid Message
Channel. If it occurs while the application processes the data from the message, that is an
application error that has nothing to do with messaging.
An Invalid Message Channel whose contents are ignored is about as useful as an error log that is
ignored. Messages on the Invalid Message Channel indicate application integration problems, so
those messages should not be ignored; rather, they should be analyzed to determine what went
wrong so that the problem can be fixed. Ideally, this would be an automated process that
consumed invalid messages, determined their cause, and fixed the underlying problems.
However, the cause is often a coding or configuration error which requires a developer or system
analyst to evaluate and repair. At the very least, applications which use messaging and Invalid
Message Channels should have a process that monitors the Invalid Message Channel and alerts
system administrators whenever the channel contains messages.
A similar concept implemented by many messaging systems is a Dead Letter Channel. Whereas an
Invalid Message Channel is for messages that can be delivered and received but not processed, a
Dead Letter Channel is for messages that the messaging system cannot deliver properly.
In a stock trading system, an application for executing trade requests might receive a request for
a current price quote, or a trade request that does not specify what security to buy or how many
shares, or a trade request that does not specify who to send the trade confirmation to. In any of
these cases, the application has received an invalid message--one that does not meet the
minimum requirements necessary for the application to be able to process the trade request. Once
the application determines the message to be invalid, it should resend the message onto the
Invalid Message Channel. The various applications that send trade requests may wish to monitor
the Invalid Message Channel to determine if their requests are being discarded.
In JMS, the specification suggests that if a MessageListener gets a message it cannot process, a
well-behaved listener should divert the message ¡°to some form of application-specific
¡®unprocessable message¡¯ destination.¡± [JMS11, p.69] This unprocessable message destination is an
Invalid Message Channel.
JMS Request/Reply Example and .NET Request/Reply Example show an example of how to
implement receivers that reroute messages they cannot process to an Invalid Message Channel.
##%%&&
An enterprise is using Messaging to integrate applications.
What will the messaging system do with a message it cannot deliver?
If a receiver receives a message it cannot process, it should move the invalid message to an Invalid
Message Channel. But what if the messaging system cannot deliver the message to the receiver in
the first place?
There are a number of reasons the messaging system may not be able to deliver a message. The
messaging system may not have the message¡¯s channel configured properly. The message¡¯s
channel may be deleted after the message is sent but before it can be delivered or while it is
waiting to be received. The message may expire before it can be delivered (see Message Expiration).
A message without an explicit expiration may nevertheless timeout if it cannot be delivered for a
very long time. A message with a Selective Consumer that everyone ignores will never be read and
may eventually die. A message could have something wrong with its header that prevents it
from being delivered successfully.
Once the messaging system determines that it cannot deliver a message, it has to do something
with the message. It could just leave the message wherever it is, cluttering up the system. It could
try to deliver the message back to the sender, but the sender is not a receiver and cannot detect
deliveries. It could just delete the message and hope no one misses it, but this may well cause a
problem for the sender that has successfully sent the message and expects it to be delivered (and
received and processed).
When a messaging system determines that it cannot or should not deliver a message, it may
elect to move the message to a Dead Letter Channel.
The specific way a Dead Letter Channel works depends on the specific messaging system¡¯s
implementation, if it provides one at all. The channel may be called a ¡°dead message queue¡±
[Monson-Haefel, p.125] or ¡°dead letter queue.¡± [MQSeries], [Dickman, pp.28-29] Typically, each
machine the messaging system is installed on has its own local Dead Letter Channel so that
whatever machine a message dies on, it can be moved from one local queue to another without
any networking uncertainties. This also records what machine the message died on. When the
messaging system moves the message, it may also record the original channel the message was
supposed to be delivered on.
The difference between a dead message and an invalid one is that the messaging system cannot
successfully deliver what it then deems a dead message, whereas an invalid message is properly
delivered but cannot be processed by the receiver. Determining if a message should be moved to
the Dead Letter Channel is an evaluation of the message¡¯s header performed by the messaging
system; whereas the receiver moves a message to an Invalid Message Channel because of the
message¡¯s body or particular header fields the receiver is interested in. To the receiver,
determination and handling of dead messages seems automatic, whereas the receiver must
handle invalid messages itself. A developer using a messaging system is stuck with whatever
dead message handling the messaging system provides, but can design his own invalid message
handling, including handling for seemingly dead messages that the messaging system doesn¡¯t
handle.
In a stock trading system, an application that wishes to perform a trade can send a trade request.
To make sure that the trade is received in a reasonable amount of time (less than five minutes,
perhaps), the requestor sets the request's Message Expiration to five minutes. If the messaging
system cannot deliver the request in that amount of time, or if the trading application does not
receive the message (e.g., read it off of the channel) in time, then the messaging system will take
the message off of the trade request channel and put the message on the Dead Letter Channel. The
trading system may wish to monitor the system's Dead Letter Channels to determine if it is missing
trades.
##%%&&
An enterprise is using Messaging to integrate applications.
How can the sender make sure that a message will be delivered, even if the messaging system
fails?
One of the main advantages of asynchronous messaging over RPC is that the sender, the receiver,
and network connecting the two don¡¯t all have to be working at the same time. If the network is
not available, the messaging system has to store the message until the network becomes available.
If the receiver is unavailable, the messaging system has to store the message and retry delivery
until the receiver becomes available. This is the store and forward process that messaging is based
on. So where should the message be stored before it is forwarded?
By default, the messaging system stores the message in memory until it can successfully forward
the message to the next storage point. This works as long as the messaging system is running
reliably, but if the messaging system crashes (for example, because one of its computers loses
power or the messaging process aborts unexpectedly), all of the messages stored in memory are
lost.
Most applications have to deal with similar problems. All data that is stored in memory is lost if
the application crashes. To prevent this, applications use files and databases to persist data to
disk so that it survives system crashes.
Messaging systems need a similar way to persist messages more permanently so that no message
gets lost even if the system crashes.
Use Guaranteed Delivery to make messages persistent so that they are not lost even if the
messaging system crashes.
With Guaranteed Delivery, the messaging system uses a built-in data store to persist messages.
Each computer the messaging system is installed on has its own data store so that the messages
can be stored locally. When the sender sends a message, the send operation does not complete
successfully until the message is safely stored in the sender¡¯s data store. Subsequently, the
message is not deleted from one data store until it is successfully forwarded to and stored in the
next data store. In this way, once the sender successfully sends the message, it is always stored on
disk on at least one computer until is successfully delivered to and acknowledged by the receiver.
Persistence increases reliability, but at the expense of performance. Thus if it¡¯s OK to loose
messages when the messaging system crashes or is shut down, avoid using Guaranteed Delivery so
that messages will move through the messaging system faster.
Also consider that Guaranteed Delivery can consume a large amount of disk space in high-traffic
scenarios. If a producer generates hundreds or thousands of messages per second, then a network
outage that lasts multiple hours could use up a huge amount of disk space. Because the network
is unavailable, the messages have to be stored on the producing computer¡¯s local disk drive
which may not be designed to hold this much data. For these reasons, some messaging systems
allow you to configure a retry timeout parameter that specifies how long messages are buffered
inside the messaging system. In some high-traffic applications (e.g., streaming stock quotes to
terminals), this timeout may have to be set to a short time span, for example a few minutes.
Luckily, in many of these applications, messages are used as Event Messages and can safely be
discarded after a short amount of time elapses (see Message Expiration).
It can also be useful to turn off Guaranteed Delivery during testing and debugging. This makes it
easy to purge all message channels by stopping and restarting the messaging server. Messages
that are still queued up can make it very tedious to debug even simple messaging programs. For
example, you may have a sender and a receiver connected by a Point-to-Point Channel. If a
message is still stored on the channel, the receiver will process that message before any new
message that the sender produces. This is a common debugging pitfall in asynchronous,
guaranteed messaging. Many commercial messaging implementations also allow you to purge
queues individually to allow a fresh restart during testing.
How guaranteed is guaranteed messaging?
It is important to keep in mind that reliability in computer systems tends to be measured in the "number of 9s", e.g. 99.9%. This
tells us that something is rarely 100% reliable, with the cost already increasing exponentially to move from 99.9% to 99.99%.
The same caveats apply to Guaranteed Delivery. There is always going to be a scenario where a message can get lost. For
example, if the disk that stores the persisted messages fails, messages may get lost. You can make your disk storage more
reliable by using redundant disk storage to reduce the likelihood of failure. This will possibly add another ¡®9¡¯ to the reliability
rating, but likely not make it a true 100%. Also, if the networks is unavailable for a long time, the messages that have to be
stored may fill up the computer¡¯s disk, resulting in lost messages. In summary, Guaranteed Delivery is designed to make the
message delivery resilient against expected outages, such as machine failures or network failures, but it is usually not 100%
bullet-proof.
With .NET¡¯s MSMQ implementation, for a channel to be persistent, it must declared transactional,
which means senders usually have to be Transactional Clients. In JMS, with Publish-Subscribe
Channel, Guaranteed Delivery only assures that the messages will be delivered to the active
subscribers. To assure that a subscriber receives messages even when it¡¯s inactive, the subscriber
will need a Durable Subscriber.
In a stock trading system, trade requests and trade confirmations should probably be sent with
Guaranteed Delivery, to help ensure that none are lost. Likewise, change-of-address
announcements should probably be sent with Guaranteed Delivery. On the other hand, price
updates probably do not require Guaranteed Delivery; loosing some of them is not significant, and
their frequency makes the overhead of Guaranteed Delivery prohibitive.
In Durable Subscriber, the stock trading example says that some price-change subscribers may
wish to be durable. If so, then perhaps the price-change channel should guarantee delivery as
well. Yet other subscribers may not need to be durable nor want to suffer the overhead of
Guaranteed Delivery. How can these different needs be met? The system may wish to implement
two price-change channels, one with Guaranteed Delivery and another without. Only subscribers
that require all updates should subscribe to the persistent channel, and their subscriptions should
be durable. The publisher may wish to publish updates less frequently on the persistent channel
because of its increased overhead.
In JMS, message persistence can be set on a per-message basis. In other words, some messages on
a particular channel may be persistent while others might not be. [JMS11, pp.71-72], [Hapner,
pp.58-59]
When a JMS sender wants to make a message persistent, it uses its MessageProducer to set the
message¡¯s JMSDeliveryMode to PERSISTENT. The sender can set persistency on a per-message basis
like this:
If the application wants to make all of the messages persistent, it can set that as the default for the
message producer:
(And, in fact, the default delivery mode for a message producer is persistent). Now, messages
sent by this producer are automatically persistent, so they can simply be sent:
Meanwhile, messages sent by other message producers on the same channel may be persistent,
depending on how those producers configure their messages.
In WebSphere MQ, Guaranteed Delivery can be set on a per-channel basis or a per-message basis.
If the channel is not persistent, the messages cannot be persistent. If the channel is persistent, the
channel can be configured such that all messages sent on that channel are automatically
persistent, or such that an individual message can be sent persistently or non-persistently.
A channel is configured to be persistent (or not) when it is created in the messaging system. For
example, the channel can be configured so that all of its messages will be persistent:
Or, the channel can be configured so that the message sender can specify with each message
whether the message is persistent or transient:
If the channel is set to allow the sender to specify persistency, then a JMS MessageProducer can set
that delivery-mode property as described earlier. If the channel is set to make all messages
persistent, then the delivery-mode settings specified by the MessageProducer are ignored. [WSMQ,
pp.45-56]
With .NET, persistent messages are created by making a MessageQueue transactional:
All messages sent on this queue will automatically be persistent. [Dickman, p.257]
##%%&&
Many enterprises use Messaging to integrate multiple, disparate applications.
How can you connect an application to the messaging system so that it can send and receive
messages?
Most applications were not designed to work with a messaging infrastructure. There are a
number of reasons for this. Many applications were developed as self-contained, stand-alone
solutions but contain data or functionality that can be leveraged by other systems. For example,
many mainframe applications were designed as a one-in-all application that does not need to
interface with other applications.
Many message-oriented middleware systems expose proprietary API's so that an application
developer or vendor would have to provide multiple implementations of a messaging interface
for the application.
If applications need to exchange data with other applications, they often are designed to use
more generic interface mechanisms such as file exchange or database tables. Reading and writing
files is a basic operating system function and does not depend on vendor-specific API's. Likewise,
most business applications already persists data into a database, so little extra effort is required to
store data destined for other systems in a database table. Or an application can expose internal
functions in a generic API that can be used by any other integration strategy, including
messaging.
Other applications may be capable of communicating via a simple protocols like HTTP or
TCP/IP. However, these protocols do not provide the same reliability as a Message Channel and
the data format used by the application is usually specific to the application and not compatible
with a common messaging solution.
In the case of custom applications, we could add code inside the application to send and receive
messages. However, this can introduce additional complexity into the application and we need to
be careful not to introduce any undesired side-effects when making these changes. Also, this
approach requires developers who are skilled with both the application logic and the messaging
API. If we deal with a packaged application that we purchased from a third-party software
vendor, we may not even have the option of changing the application code.
Use a Channel Adapter that can access the application's API or data and publish messages on a
channel based on this data, and that likewise can receive messages and invoke functionality
inside the application.
The adapter acts as a messaging client to the messaging system and invokes applications
functions via an application-supplied interface. This way, any application can connect to the
messaging system and be integrated with other applications as long as it has a proper Channel
Adapter.
The Channel Adapter can connect to different layers of the application's architecture, depending on
that architecture and the data the messaging system needs to access.
A Channel Adapter Connecting to Different Layers of an Application
? User Interface Adapter. Sometimes disparagingly called "screen scraping," these types of adapters can be
very effective in many situations. For example, an application may be implemented on a platform that is not
supported by the messaging system. Or, the owner of the application may have little interest in supporting the
integration. This eliminates the option of running the Channel Adapter on the application platform. However,
the user interface is usually available from other machines and platforms (e.g. 3270 terminals). Also, the
surge of Web-based thin-client architectures has caused a certain revival of user interface integration.
HTML-based user interfaces make it very easy to make an HTTP request and parse out the results. Another
advantage of user-interface integration is that no direct access to the application internals is needed. In some
cases, it may not be desirable or possible to expose internal functions of a system to the integration solution.
Using a user-interface adapter, other applications have the exact same access to the application as a regular
user. The downside of user interface adapters is the potential brittleness and low speed of the solution. The
application has to parse "user" input and render a screen in response, just so that the Channel Adapter can
parse the screen back into raw data. This process involves many unnecessary steps and can be slow. Also,
user interfaces tend to change more frequently than the core application logic. Every time the user interface
changes, the Channel Adapter is likely to have to be changed as well.
? Business Logic Adapter. Most business applications expose their core functions as an API. This interface
may be a set of component (e.g. EJB's, COM objects, CORBA components) or a direct programming API
(e.g., a C++, C#, or Java library). Since the software vendor (or developer) exposes these API's expressly for
access by other applications, they tend to be more stable than the user interface. In most cases, accessing the
API is also more efficient. In general, if the application exposes a well-defined API, this type of Channel
Adapter is likely to be the best approach.
? Database Adapter. Most business applications persist their data inside a relational database. Since the
information is already in the database, Channel Adapter can extract information directly from the database
without the application ever noticing, which is very non-intrusive. The Channel Adapter can even add a
trigger to the relevant tables and send messages every time the data in these tables changes. This type of
Channel Adapter can also be very efficient and is quite universal, aided by the fact that only two or three
database vendors dominate the market for relational databases. This allows us to connect to many
applications with a relatively generic adapter. The downside of a database adapter is that we are poking
around deep in the internals of an application. This may not be as risky if we simply read data, but making
updates directly to the database can be very dangerous. Also, many application vendors consider the database
schema "unpublished," meaning that they reserve the right to change it at will, which can make a database
adapter solution brittle.
An important limitation of Channel Adapters is that they can convert messages into application
functions, but require message formatting that closely resembles the implementation of the
components being adapted. For example, a database adapter typically requires the message field
names of incoming messages to be the same as the names of tables and fields in the application
database. This kind of message format is driven entirely by the internal structure of the
application and is not a good message format to use when integration with other applications.
Therefore, most Channel Adapters require the combination with a Message Translator to convert the
application-specific message into a message format that complies with the Canonical Data Model.
Channel Adapters can often times run on a different computer than the application or the database
itself. The Channel Adapter can the connect to the application logic or the database via protocols
such as HTTP or ODBC. While this setup allows us to avoid installing additional software on the
application or database server, these protocols do not provide the same quality-of-service that a
messaging channel provides, such as guaranteed delivery.
Some Channel Adapters may be unidirectional. For example, if a Channel Adapter connects to an
application via HTTP, it may only be able to consume messages and invoke functions on the
application, but it may not be able to detect changes in the application data.
An interesting variation of the Channel Adapter is the Metadata Adapter, sometimes called
Design-Time Adapter. This type of data does not invoke application functions, but extracts
metadata, data that describes the internal data formats of the application. This metadata can then
be used to configure Message Translators or to detect changes in the application data formats (see
Introduction to Message Transformation). Many application interfaces support the extraction of
metadata. For example, most commercial databases provide a system tables that contain a
description of the application tables. Likewise, most component frameworks (e.g. J2EE, .NET)
provide special "reflection" functions that allow a component to enumerate methods provided by
another component.
A special form of the Channel Adapter is the Messaging Bridge. The Messaging Bridge connects the
messaging system to another messaging system as opposed to a specific application. Typically, a
Channel Adapter is implemented as a Transactional Client to ensure that each piece of work the
adapter does succeeds in both the messaging system and the other system being adapted.
A stock trading system may wish to keep a log of all of a stock's prices in a database table. The
messaging system may include a relational database adapter that logs each message from a
channel to a specified table and schema. This channel-to-RDBMS adapter is a Channel Adapter.
The system may also be able to receive external quote requests from the Internet (TCP/IP or
HTTP) and send them on its internal quote-request channel with the internal quote requests. This
Internet-to-channel adapter is a Channel Adapter.
Commercial EAI vendors provide a collection of Channel Adapters as part of their offerings.
Having adapters to all major application packages available simplifies development of an
integration solution greatly. Most vendors also provide more generic database adapters as well as
software development kits (SDK's) to develop custom adapters.
A number of vendors provide adapters from common messaging system to legacy systems
executing on platforms such as as UNIX, MVS, OS/2, AS/400, Unisys, and VMS. Most of these
adapters are specific to a certain messaging system. For example, Envoy Technologies' EnvoyMQ
is a Channel Adapter that connects many legacy platforms with MSMQ. It consists of a client
component that runs on the legacy computer and a server component that runs on a Windows
computer with MSMQ.
Many messaging systems provide Channel Adapters to convert SOAP messages between HTTP
transport and the messaging system. This way, SOAP messages can be transmitted over an
intranet using the messaging system, and over the global Internet (and through firewalls) using
HTTP. One example is the Web Services Gateway for IBM's WebSphere Application Server.
##%%&&
An enterprise is using Messaging to enable applications to communicate. However, the enterprise
uses more than one messaging system, which confuses the issue of which messaging system an
application should connect to.
How can multiple messaging systems be connected so that messages available on one are also
available on the others?
A common problem is an enterprise that uses more than one messaging system. This can occur
because of a merger or acquisition between two different companies that have standardized
around different messaging products. Sometimes a single enterprise that uses one messaging
system to integrate their mainframe/legacy systems chooses another for their J2EE or .NET web
application servers, and then needs to integrate the two messaging systems. Another common
occurrence is an application that participates as part of multiple enterprises, such as a B2B client
that wants to be a bidder in multiple auctioning systems; if the various auction clusters use
different messaging systems, the bidder applications within an enterprise may wish to
consolidate the messages from several external messaging systems onto a single internal
messaging system. Another example: An extremely large enterprise with a huge number of
Message Channels and Message Endpoints may require more than one instance of the messaging
system, which means those instances must be connected somehow.
If the messages on one system are of no interest to the applications using the other messaging
system, then the systems can remain completely separate. But because the applications are part of
the same enterprise, often some applications using one messaging system will be interested in
messages being transmitted on another messaging system.
A common misconception is that a standardized messaging API such as JMS solves this problem;
it does not. JMS makes two compliant messaging systems look the same to a client application,
but it does nothing to make the two messaging systems work with each other. For the messaging
systems to work together, they need to be interoperable, meaning that they use the same message
format and transmit a message from one message store to the next in the same way. Messaging
systems from two different vendors are rarely interoperable; a message store from one vendor
can only work with other message stores from the same vendor.
Each application in the enterprise could choose to implement a client for each messaging system
in the enterprise, but that would increase complexity and duplication in the messaging layer.
This redundancy would become especially apparent if the enterprise added yet another
messaging system and all of the applications had to be modified. On the other hand, each
application could choose to only interface with one messaging system and ignore data on the
other messaging systems. This would make the application simpler but could cause it to ignore a
great deal of enterprise data.
What is needed is a way for messages on one messaging system that are of interest to
applications on another messaging system to be made available on the second messaging system
as well.
Use a Messaging Bridge, a connection between messaging systems, to replicate messages
between systems.
Typically, there is no practical way to connect two complete messaging systems, so instead we
connect individual, corresponding channels between the messaging systems. The Messaging
Bridge is a set of Channel Adapters, where the non-messaging client is a actually another
messaging system, and where each pair of adapters connects a pair of corresponding channels.
The bridge acts as map from one set of channels to the other, and also transforms the message
format of one system to the other. The connected channels may be used to transmit messages
between traditional clients of the messaging system, or strictly for messages intended for other
messaging systems.
You may need to implement the Messaging Bridge for your enterprise yourself. The bridge is a
specialized Message Endpoint application that is a client of both messaging systems. When a
message is delivered on a channel of interest in one messaging system, the bridge consumes the
message and sends another with the same contents on the corresponding channel in the other
messaging system.
Many messaging system vendors have product extensions for bridging to messaging systems
from other vendors. Thus you may be able to buy a solution rather than build it yourself.
If the other "messaging system" is really a simpler protocol, such as HTTP, apply the Channel
Adapter pattern.
Messaging Bridge is necessary because different messaging system implementations have their
own proprietary approaches for how to represent messages and how to forward them from one
store to the next. Web services may be standardizing this, such that two messaging system
installs, even from different vendors, may be able to act as one by transferring messaging using
web services standards. See the discussion of WS-Reliability and WS-ReliableMessaging in
Emerging Standards and Futures in Enterprise Integration.
A brokerage house may have one messaging system that the applications in its various offices
use to communicate. A bank may have a different messaging system that the applications in its
various branches use to communicate. If the brokerage and the bank decide to merge into a single
company that offers bank accounts and investment services, which messaging system should the
combined company use? Rather than redesigning half of the company's applications to use the
new messaging system, the company can use a Messaging Bridge to connect the two messaging
systems. This way, for example, a banking application and a brokerage application can
coordinate to transfer money between a savings account and a securities trading account.
MSMQ defines an architecture based on connector servers that enables connector applications to
send and receive messages using other (non-MSMQ) messaging systems. An MSMQ application
using a connector server can perform the same operations on channels from other messaging
systems that it can perform on MSMQ channels. [Dickman, pp.42-45]
Microsoft's Host Integration Server product contains an MSMQ-MQSeries Bridge service that
makes the two messaging systems work together. It lets MSMQ applications send messages via
MQSeries channels and vise versa, making the two messaging systems act as one.
Envoy Technologies, licenser of the MSMQ-MQSeries Bridge, also has a related product called
Envoy Connect. It connects MSMQ and BizTalk servers with messaging servers running on
non-Windows platforms, especially the J2EE platform, coordinating J2EE and .NET messaging
within an enterprise.
Sonic Software's SonicMQ has SonicMQ Bridge products that support IBM MQSeries, TIBCO
TIB/Rendezvous, and JMS. This enables messages on Sonic channels to be transmitted on other
messaging systems' channels as well.
##%%&&
An enterprise contains several existing systems that must be able to share data and operate in a
unified manner in response to a set of common business requests.
What is an architecture that enables separate applications to work together, but in a decoupled
fashion such that applications can be easily added or removed without affecting the others?
An enterprise often contains a variety of applications that operate independently, yet need to
work together in a unified manner. Enterprise Application Integration (EAI) describes a solution
to this problem but doesn¡¯t describe how to accomplish it.
For example, consider an insurance company that sells different kinds of insurance products (life,
health, auto, home, etc.). As a result of corporate mergers, and of the varying winds of change in
IT development, the enterprise consists of a number of separate applications for managing the
company¡¯s various products. An insurance agent trying to sell a customer several different types
of policies must log into a separate system for each policy, wasting effort and increasing the
opportunity for mistakes.
Insurance Company EAI Scenario
The agent needs a single, unified application for selling customers a portfolio of policies. Other
types of insurance company employees, such as claims adjusters and customer service
representatives, need their own applications for working with the insurance products, but also
want their applications to present a unified view. The individual product applications need to be
able to work together, perhaps to offer a discount will purchasing more than one policy, and
perhaps to process a claim that is covered by more than one policy.
The IT department could rewrite the product applications to all use the same technology and
work together, but the amount of time and money to replace systems that already work (even
though they don¡¯t work together) is prohibitive. IT could create a unified application for the
agents, but this application needs to connect to the systems that actually manage the policies.
Rather than unifying the systems, this new application creates one more system that doesn¡¯t
integrate with the others.
The agent application could integrate with all of these other systems, but that would make it
much more complex. The complexity would be duplicated in the applications for claims adjusters
and customer service representatives. Furthermore, these unified user applications would not
help the product applications integrate with each other.
Even if all of these applications could be made to work together, any change to the enterprise¡¯s
configuration could make it all stop working. Not all applications will be available all of the time,
yet the ones that are running need to be able to continue with minimal impact from those that are
not running. Over time, applications will need to be added to and removed from the enterprise,
with minimal impact on the other applications.
What is needed is an integration architecture that enables the product applications to coordinate
in a loosely coupled way, and for user applications to be able to integrate with them.
Structure the connecting middleware between these applications as a Message Bus that
enables them to work together using messaging.
A Message Bus is a combination of a common data model, a common command set, and a
messaging infrastructure to allow different systems to communicate through a shared set of
interfaces. This is analogous to a communications bus in a computer system, which serves as the
focal point for communication between the CPU, main memory, and peripherals. Just as in the
hardware analogy, there are a number of pieces that come together to form the message bus:
? Common communication infrastructure ¡ª Just as the physical pins and wires of a PCI
bus provide a common, well-known physical infrastructure for a PC, a common
infrastructure must serve the same purpose in a message bus. Typically, a messaging
system is chosen to serve as the physical communications infrastructure, providing a
cross-platform, cross-language universal adapter between the applications. The
infrastructure may include Message Router capabilities to facilitate the correct routing of
messages from system to system. Another common option is to use Publish-Subscribe
Channels to facilitate sending messages to all receivers.
? Adapters ¡ª The different systems must find a way to interface with the message bus.
Most commonly, this is done with commercial or custom Channel Adapters and Service
Activators that can handle things like invoking CICS transactions with the proper
parameters, or representing the general data structures flowing on the bus in the specific
and particular way they should be represented inside each system. This also requires a
Canonical Data Model that all systems can agree on.
? Common Command Structure ¡ª Just like PC architectures have a common set of
commands to represent the different operations possible on the physical bus (read bytes
from an address, write bytes to an address), there needs to be common commands that
are understood by all the participants in the Message Bus. Command Message illustrates
how this feature works. Another common implementation for this is the Datatype Channel,
where a Message Router makes an explicit decision as to how to route particular messages
(like Purchase Orders) to particular endpoints. It is at the end that the analogy breaks
down, since the level of the messages carried on the bus are much more fine-grained than
the ¡°read/write¡± kinds of messages carried on a physical bus.
In our EAI example, a Message Bus could serve as a universal connector between the various
insurance systems, and as a universal interface for client applications that wish to connect to the
insurance systems.
Insurance Company Message Bus
Here we have a two GUI¡¯s that only know about the Message Bus¡ªthey are entirely unaware of
the complexities of idiosyncrasies of the underlying systems. The bus is responsible for routing
Command Messages to the proper underlying systems. In some cases, the best way to handle the
command messages is to build an adapter to the system that interprets the command and then
communicates with the system in a way it understands (invoking a CICS transaction, for instance,
or calling a C++ API). In other cases, it may be possible to build the command-processing logic
directly into the existing system as an additional way to invoke current logic.
Once the Message Bus has been developed for the agent GUI, it is easy to reuse for other GUIs
such as those for claims processors, customer service representatives, and a web interface for
customer to browse their own accounts. The features and security control of these GUI
applications differ, but their need to work with the backend applications is the same.
A Message Bus forms a simple, useful service-oriented architecture for an enterprise. Each service
has at least one request channel that accepts requests of an agreed-upon format, and probably a
corresponding reply channel that supports a specified reply format. Any participant application
can make use of these services by making requests and waiting for replies. The request channels,
in effect, act as a directory of the services available.
A Message Bus requires that all of the applications using the bus use the same Canonical Data
Model. Applications adding messages to the bus may need to depend on Message Routers to route
the messages to the appropriate final destinations. Applications not designed to interface with a
messaging system may require Channel Adapters and Service Activators.
A stock trading system may wish to offer a unified suite of services including stock trades, bond
auctions, price quotes, portfolio management, etc. This may require several separate back-end
systems that have to coordinate with each other. To unify the services for a front-end customer
GUI, the system could employ an intermediate application that offered all of these services and
delegated their performance to the back-end systems. The back-end systems could even
coordinate through this intermediary application. However, the intermediary application would
tend to become a bottleneck and a single point of failure.
Rather than an intermediary application, a better approach might be a Message Bus with channels
for requesting various services and getting their responses. This bus could also enable to
back-end systems to coordinate with each other. A front-end system could simply connect to the
bus and use it to invoke services. The bus could relatively easily be distributed across multiple
computers to provide load distribution and fault tolerance.
Once the Message Bus is in place, connecting front-end GUI¡¯s would be relatively easy; they each
just need to send and receive messages from the proper channels. One GUI might enable a retail
broker to manage his customers¡¯ portfolios. Another web-based GUI could enable any customer
with a web browser to manage his own portfolio. Another non-GUI front-end might support
personal finance programs like Intuit¡¯s Quicken and Microsoft¡¯s Money, enabling customers
using those programs to download trades and current prices. Once the Message Bus is in place,
developing new user applications is much simpler.
Likewise, the trading system may want to take advantage of new back-end applications such as
switching one trading application for another or spreading price quote requests across multiple
applications. Implementing a change like this is a simple as adding and removing applications
from the Message Bus. Once the new applications are in place, none of the other applications have
to change; they just keep sending messages on the bus¡¯ channels as usual.
##%%&&
An application needs to invoke functionality provided by other applications. It would typically
use Remote Procedure Invocation, but would like to take advantage of the benefits of using
Messaging.
How can messaging be used to invoke a procedure in another application?
The advantage of Remote Procedure Invocation is that it¡¯s synchronous, so the call is performed
immediately while the caller¡¯s thread blocks. But that¡¯s also a disadvantage. If the call cannot be
made right now¡ªeither because the network is down or because the remote process isn¡¯t
running and listening¡ªthen the call doesn¡¯t work. If the call were asynchronous, it could keep
trying until the procedure in the remote application is successfully invoked.
Messaging is asynchronous. If the procedure were somehow invoked by a Message, the procedure
would not even have to be remotely accessible, it could just be a local procedure invoked within
its own process. So the question is how to make a procedure call into a message.
Luckily, there¡¯s a well-established pattern for how to encapsulate a request as an object. The
Command pattern [GoF] shows how to turn a request into an object that can be stored and
passed around. If this object were a message, then it could be stored in and passed around
through a Message Channel. Likewise, the command¡¯s state (if any) can be stored in the message¡¯s
state.
Use a Command Message to reliably invoke a procedure in another application.
There is no specific message type for commands; a Command Message is simply a regular message
that happens to contain a command. In JMS, the command message could be any type of message;
examples include an ObjectMessage containing a Serializable command object, a TextMessage
containing the command in XML form, etc. In .NET, a command message is a Message with a
command stored in it. A Simple Object Access Protocol (SOAP) request is a command message.
Command Messages are usually sent on a Point-to-Point Channel so that each command will only be
consumed and invoked once.
With the SOAP protocol [SOAP 1.1] and WSDL service description [WSDL 1.1], when using
RPC-style SOAP messages, the request message is an example of this Command Message pattern.
With this usage, the SOAP message body (an XML document) contains the name of the method
to invoke in the receiver and the parameter values to pass into the method. This method name
must be the same as one of the message names defined in the receiver's WSDL.
This example from the SOAP spec invokes the receiver's GetLastTradePrice method with a single
parameter called symbol:
In a SOAP command, one might expect the method name to be the value of some standard
<method> element; actually, the method name is the name of the method element, prefixed by the
m namespace. Having a separate XML element type for each method makes validating the XML
data much more precise, because the method element type can specify the parameters' names,
types, and order.
##%%&&
An application would like to transfer data to another application. It could do so using File
Transfer or Shared Database, but those approaches have shortcomings. The transfer might work
better using Messaging.
How can messaging be used to transfer data between applications?
This is a classic problem in distributed processing: One process has data another one needs.
File Transfer is easy to use, but doesn¡¯t coordinate applications very well. A file written by one
application may sit unused for quite a while before another application reads it. If several
applications are supposed to read it, it¡¯ll be unclear who should take responsibility for deleting it.
Shared Database requires adding new schema to the database to accommodate the data, or
force-fitting the data into the existing schema. Once the data is in the database, there¡¯s the risk
that other applications which should not have access to the data now do. Triggering the receiver
of the data to now come read it can be difficult, and coordinating multiple readers confuses who
should delete the data.
Remote Procedure Invocation can be used to send the data, but then the caller is also telling the
receiver¡ªvia the procedure being invoked¡ªwhat to do with the data. Likewise, a Command
Message would transfer the data, but would be overly specific about what the receiver should do
with the data.
Yet we do want to use Messaging to transfer the data. Messaging is more reliable than an RPC. A
Point-to-Point Channel can be used to make sure that only one receiver gets the data (no
duplication), or a Publish-Subscribe Channel can be used to make sure that any receiver who wants
the data gets a copy of it. So the trick is to take advantage of Messaging without making the
Message too much like an RPC.
Use a Document Message to reliably transfer a data structure between applications.
Whereas a Command Message tells the receiver to invoke certain behavior, a Document Message just
passes data and lets the receiver decide what, if anything, to do with the data. The data is a single
unit of data, a single object or data structure which may decompose into smaller units.
Document Messages can seem very much like Event Messages; the main difference is a matter of
timing and content. The important part of a Document Message is its content, the document.
Successfully transferring the document is important; the timing of when it is sent and received is
less important. Guaranteed Delivery may be a consideration; Message Expiration probably is not.
A Document Message can be any kind of message in the messaging system. In JMS, the document
message may be an ObjectMessage containing a Serializable data object for the document, or it
may be a TextMessage containing the data in XML form. In .NET, a document message is a
Message with the data stored in it. A Simple Object Access Protocol (SOAP) reply message is a
document message.
Document Messages are usually sent using a Point-to-Point Channel to move the document from
one process to another without duplicating it. Messaging can be used to implement simple
workflow by passing a document to an application that modifies the document and then passes it
to another application. In some cases, a document message can be broadcast via a
Publish-Subscribe Channel, but this creates multiple copies of the document. Either the copies need
to be read-only, or if the receivers change the copies, there will be multiple copies of the
document in the system that contain different data. In Request-Reply, the reply is usually a
Document Message where the result value is the document.
The following example (drawn from the example XML schema in [Graham02]) shows how a
simple purchase order can be represented as XML and sent as a message using JMS.
With the SOAP protocol [SOAP 1.1] and WSDL service description [WSDL 1.1], when using
document-style SOAP messages, the SOAP message is an example of this pattern. The SOAP
message body is an XML document (or some kind of data structure that has been converted into
an XML document), and the SOAP message transmits that document from the sender (e.g., client)
to the receiver (e.g., server).
When using RPC-style SOAP messages, the response message is an example of this pattern. With
this usage, the SOAP message body (an XML document) contains the return value from the
method that was invoked.
This example from the SOAP spec returns the answer from invoking the GetLastTradePrice
method:
##%%&&
Several applications would like to use event-notification to coordinate their actions, and would like to use Messaging
to communicate those events.
How can messaging be used to transmit events from one application to another?
Sometimes an event occurs in one object that another object needs to know about. The classic example is a model that
changes its state and must notify its views so that they can redraw themselves. Such change notification can also be
useful in distributed systems. For example, in a B2B system, one business may need to notify others of price changes
or a whole new product catalog.
A process can use Remote Procedure Invocation to notify other applications of change events, but that requires that the
receiver accept the event immediately, even if it doesn¡¯t want events right now. RPC also requires that the announcing
process know every listener process and invoke an RPC on each listener.
The Observer pattern [GoF] describes how to design a subject that announces events and observers that consume
events. A subject notifies an observer of an event by calling the observer¡¯s Update() method. Update() can be
implemented as an RPC, but that would have all of RPC¡¯s shortcomings.
It would be better to send the event notification asynchronously, as a Message. This way, the subject can send the
notification when it¡¯s ready and each observer can receive the notification if and when it¡¯s ready.
Use an Event Message for reliable, asynchronous event notification between applications.
When a subject has an event to announce, it will create an event object, wrap it in a message, and send it on a channel.
The observer will receive the event message, get the event, and process it. Messaging does not change the event
notification, just makes sure that the notification gets to the observer.
A Event Message can be any kind of message in the messaging system. In Java, an event can be an object or data such
as an XML document. Thus they can be transmitted through JMS as an ObjectMessage, TextMessage, etc. In .NET,
an event message is a Message with the event stored in it.
The difference between an Event Message and a Document Message is a matter of timing and content. An event¡¯s
contents is typically less important. Many events are empty; their mere occurrance tells the observer to react. An
event¡¯s timing is very important; the subject should issue an event as soon as a change occurs, and the observer should
process it quickly while it¡¯s still relevant. Guaranteed Delivery is usually not very helpful with events because they¡¯re
frequent and need to be delivered quickly. Message Expiration can be very helpful to make sure that an event is
processed quickly or not at all.
Our B2B example could use Event Messages, Document Messages, or a combination of the two. If a message says that
the price for computer disk drives has changed, that¡¯s an event. If the message provided information about the disk
drive, including its new price, that¡¯s a document being sent as an event. Another message that announces the new
catalog and its URL is an event, whereas a similar message that actually contains the new catalog is an event that
contains a document.
Which is better? The Observer pattern describes this as a trade-off between a push model and a pull model. The push
model sends information about the change as part of the update, whereas the pull model sends minimal information
and observers that want more information request it by sending GetState() to the subject. The two models relate to
messaging like this:
Push model ¨C The message is a combined document/event message; the message¡¯s delivery announces that the state
has occurred and the message¡¯s contents are the new state. This is more efficient if all observers want these details, but
otherwise can be the worst of both worlds: A large message that is sent frequently and often ignored by many
observers.
Pull model ¨C There are three messages:
? Update ¨C An Event Message that notifies the observer of the event.
? State Request ¨C A Command Message an interested observer uses to request details from the subject.
? State Reply ¨C A Document Message the subject uses to send the details to the observer.
The advantage of the pull model is that the update messages are small, only interested observers request details, and
potentially each interested observer can request the details it specifically is interested in. The disadvantage is the
channels needed and traffic caused by three messages instead of one.
For more details on how to implement Observer using messaging, see JMS Publish/Subscribe Example.
There is usually no reason to limit an event message to a single receiver via a Point-to-Point Channel; the message is
usually broadcast via a Publish-Subscribe Channel so that all interested processes receive notification. Whereas a
Document Message needs to be consumed so that the document is not lost, a receiver of Event Messages can often
ignore the messages when it¡¯s too busy to process them, so the subscribers can often be non-durable (not Durable
Subscribers). Event Message is a key part of implementing the Observer pattern using messaging.
##%%&&
When two applications communicate via Messaging, the communication is one-way. The
applications may want a two-way conversation.
When an application sends a message, how can it get a response from the receiver?
Messaging provides one-way communication between applications. Messages travel on a Message
Channel in one direction, from the sender to the receiver. This asynchronous transmission makes
the delivery more reliable and decouples the sender from the receiver.
The problem is that communication between components often needs to be two-way. When a
program calls a function, it receives a return value. When it executes a query, it receives query
results. When one component notifies another of a change, it may want to receive an
acknowledgement.
How can messaging be two-way?
Perhaps a sender and receiver could share a message simultaneously. Then each application
could add information to the message for the other to consume. But that is not how messaging
works. A message is first sent, then received, such that the sender and receiver cannot both access
the message at the same time.
Perhaps the sender could keep a reference to the message. Then, once the receiver placed its
response into the message, the sender could pull the message back. This may work for notes
clipped to a clothesline, but it is not how a Message Channel works. A channel transmits messages
in one direction.
What is needed is a two-way message on a two-way channel.
Send a pair of Request-Reply messages, each on its own channel.
Request-Reply has two participants:
1. Requestor ¨C Sends a request message and waits for a reply message.
2. Replier ¨C Receives the request message and responds with a reply message.
The request channel can be a Point-to-Point Channel or a Publish-Subscribe Channel. The difference
is whether the request should be broadcast to all interested parties or should only be processed
by a single consumer. The reply channel, on the other hand, is almost always point-to-point,
because it usually makes no sense to broadcast replies¨Cthey should only be returned to the
requestor.
When a caller performs a Remote Procedure Invocation, the caller's thread must block while it waits
for the response. With Request-Reply, the requestor has two approaches for receiving the reply:
1. Synchronous Block ¨C A single thread in the caller sends the request message, blocks (as a Polling Consumer)
to wait for the reply message, then processes the reply. This is simple to implement, but if the requestor
crashes, it will have difficulty re-establishing the blocked thread. The request thread awaiting the response
implies that there is only one outstanding request, or that the reply channel for this request is private for this
thread.
2. Asynchronous Callback ¨C One thread in the caller sends the request message and sets up a callback for the
reply. A separate thread listens for reply messages. When a reply message arrives, the reply thread invokes
the appropriate callback, which re-establishes the caller's context and processes the reply. This approach
enables multiple outstanding requests to share a single reply channel, and a single reply thread to process
replies for multiple request threads. If the requestor crashes, it can recover by simply restarting the reply
thread. An added complexity, however, is the callback mechanism that must re-establish the caller's context.
By itself, two applications sending requests and replies to each other are not very helpful. What is
interesting is what the two messages represent.
1. Messaging RPC ¨C This is how to implement Remote Procedure Invocation using messaging. The
request is a Command Message that describes the function the replier should invoke. The reply is a
Document Message that contains the function's return value or exception.
2. Messaging Query ¨C This is how to perform a remote query using messaging. The request is a
Command Message containing the query, and the reply is the results of the query, perhaps a Message
Sequence.
3. Notify/Acknowledge ¨C This provides for event notification with acknowledgement using messaging.
The request is an Event Message that provides notification and the reply is a Document Message
acknowledging the notification. The acknowledgement may itself be another request, one seeking
details about the event.
The request is like a method call. As such, the reply is one of three possibilities:
1. Void ¨C Simply notifies the caller that the method has finished so that the caller can proceed.
2. Result value ¨C A single object that is the method's return value.
3. Exception ¨C A single exception object indicating that the method aborted before completing
successfully, and indicating why.
The request should contain a Return Address to tell the replier where to send the reply. The reply
should contain a Correlation Identifier that specifies which request this reply is for.
SOAP messages come in Request-Reply pairs. A SOAP request message indicates a service the
sender wishes to invoke on the receiver, whereas a SOAP response message contains the result of
the service invocation. The response message either contains a result value or a fault¨Cthe SOAP
equivalent of an exception. [SOAP 1.1]
Whereas SOAP 1.1 has response messages and they are loosely described, SOAP 1.2 introduces
an explicit Request-Response Message Exchange Pattern. [SOAP 1.2 Part 2] This patterns
describes a separate, potentially asynchronous response to a SOAP request.
JMS includes a couple of features that can be used to implement Request-Reply.
A TemporaryQueue is a Queue that can be created programatically and that only lasts as long as the
Connection used to create it. Only MessageConsumers created by the same connection can read
from the queue, so effectively it is private to the connection. [JMS11, pp.61-62]
How are MessageProducers going to know about this newly created, private queue? A requestor
will create a temporary queue and specify it in the reply-to property of a request message. (See
Return Address.) A well-behaved replier will send the reply back on the specified queue, one that
the replier wouldn't even know about if it weren't a property of the request message. This is a
simple approach the requestor can use to make sure that the replies always come back to it.
The downside with temporary queues is that when their Connection closes, the queue and any
messages in it are deleted. Likewise, temporary queues cannot provide Guaranteed Delivery; if the
messaging system crashes, then the connection is lost, so the queue and its messages are lost.
JMS also provides QueueRequestor, a simple class for sending requests and receiving replies. A
requestor contains a QueueSender for sending requests and a QueueReceiver for receiving replies.
Each requestor creates its own temporary queue for receiving replies and specifies that in the
request's reply-to property. [JMS11, p.78] A requestor makes sending a request and receiving a
reply very simple:
One method¨Crequest¨Csends the request message and blocks until it receives the reply message.
TemporaryQueue, used by QueueRequestor, is a Point-to-Point Channel. Its Publish-Subscribe Channel
equivalents are TemporaryTopic and TopicRequestor.
##%%&&
My application is using Messaging to perform a Request-Reply.
How does a replier know where to send the reply?
Messages are often thought of as completely independent, such that any sender sends a message
on any channel whenever it likes. However, messages are often associated, such as Request-Reply
pairs, two messages which appear independent but where the reply message has a one-to-one
correspondence with the request message that caused it. Thus the replier that processes the
request message cannot simply send the reply message on any channel it wishes, it must send it
on the channel the requestor expects the reply on.
Each receiver could automatically know which channel to send replies on, but hard coding such
assumptions makes the software less flexible and more difficult to maintain. Furthermore, a
single replier could be processing calls from several different requestors, so the reply channel is
not the same for every message; it depends on what requestor sent the request message.
Uncertain Where to Send Replies
A requestor potentially may not want a reply sent back to itself. Rather, it may have an associated
callback processor to process replies, and the callback processor may monitor a different channel
than the requestor does (or the requestor may not monitor any channels at all). The requestor
could have multiple callback processors such that replies for different requests from the same
requestor should be sent to different processors.
The reply channel will not necessarily transmit replies back to the requestor; it will transmit them
to whomever the requestor wants to process the replies, because it¡¯s listening to the channel the
requestor specified. So knowing what requestor sent a request or what channel it was sent on
does not necessarily tell the replier what channel to send the reply on. Even if it did, the replier
would still have to infer which reply channel to use for a particular requestor or request channel.
It¡¯s easier for the request to explicitly specify which reply channel to use.
What is needed is a way for the requestor to tell the replier where and how to send back a reply.
The request message should contain a Return Address that indicates where to send the reply
message.
This way, the replier does not need to know where to send the reply, it can just ask the request. If
different messages to the same replier require replies to different places, the replier knows where
to send the reply for each request. This encapsulates the knowledge of what channels to use for
requests and replies within the requestor so those decisions do not have to be hard coded within
the replier. A Return Address is put in the header of a message because it¡¯s not part of the data
being transmitted.
A message¡¯s Return Address is analogous to the reply-to field in an e-mail message. The ¡°reply-to¡±
e-mail address is usually the same as the ¡°from¡± address, but the sender can set it to a different
address to receive replies in a different account than the one used to send the original message.
When the reply is sent back the channel indicated by the Return Address, it may also need a
Correlation Identifier. The Return Address tells the receiver what channel to put the reply message
on; the correlation identifier tells the sender which request a reply is for.
JMS messages have a predefined property for Return Addresses, JMSReplyTo. Its type is a
Destination (a Topic or Queue), rather than just a string for the destination name, which ensures
that the destination (e.g., Message Channel) really exists, at least when the request is sent. [JMS11,
p.33], [Monson-Haefel, pp.192-193]
A sender that wishes to specify a reply channel that is a queue would do so like this:
Then the receiver would send the reply message like this:.NET messages also have a predefined property for Return Addresses, ResponseQueue. Its type is a
MessageQueue, the queue that the application should send a response message to. [SysMsg],
[Dickman, p.122]
SOAP 1.2 incorporates the Request-Response Message Exchange Pattern [SOAP 1.2 Part 2], but
the address to send the reply to is unspecified and therefore implied. This SOAP pattern will
need to support an optional Return Address to truely make SOAP messages asynchronous and
delink the responder from the requestor.
The emerging WS-Addressing standard helps address this issue by specifying how to identify a
web service endpoint and what XML elements to use. Such an address can be used in a SOAP
message to specify a Return Address. See the discussion of WS-Addressing in Emerging Standards
and Futures in Enterprise Integration.
##%%&&
My application is using Messaging to perform a Request-Reply and has received a reply message.
How does a requestor that has received a reply know which request this is the reply for?
When one process invokes another via Remote Procedure Invocation, the call is synchronous, so
there is no confusion about which call produced a given result. But Messaging is asynchronous, so
from the caller¡¯s point of view, it makes the call, then sometime later a result appears. The caller
may not even remember making the request, or may have made so many requests that it no
longer knows which one this is the result for. With confusion like this, when the caller finally gets
the result, it may not know what to do with it, which defeats the purpose of making the call in
the first place.
Cannot Match Reply to Request
There are a couple of approaches the caller can use to avoid this confusion. It can make just one
call at a time, waiting for a reply before sending another request, so there is at most one
outstanding request at any given time. This will greatly slow processing throughput, however.
The call could assume that it¡¯ll receive replies in the same order it sent requests, but messaging
does not guarantee what order messages are delivered in and all requests may not take the same
amount of time to process, so the caller¡¯s assumption would be faulty. The caller could design its
requests such that they do not need replies, but this constraint would make messaging useless for
many purposes.
What the caller needs is for the reply message to have a pointer or reference to the request
message, but messages do not exist in a stable memory space such that they can be referenced by
variables. However, a message could have some sort of foreign key, a unique identifier like the
key for a row in a relational database table. Such a unique identifier could be used to identify the
message from other messages, clients that use the message, etc.
Each reply message should contain a Correlation Identifier, a unique identifier that indicates
which request message this reply is for.
There are six parts to Correlation Identifier:
1. Requestor ¡ª An application that performs a business task by sending a request and
waiting for a reply.
2. Replier ¡ª Another application that receives the request, fulfills it, then sends the reply. It
gets the request ID from the request and stores it as the correlation ID in the reply.
3. Request ¡ª A Message sent from the requestor to the replier containing a request ID.
4. Reply ¡ª A Message sent from the replier to the requestor containing a correlation ID.
5. Request ID ¡ª A token in the request that uniquely identifies the request.
6. Correlation ID ¡ª A token in the reply that has the same value as the request ID in the
request.
This is how a Correlation Identifier works: When the requestor creates a request message, it assigns
the request a request ID¡ªan identifier that is different from those for all other currently
outstanding requests (e.g., requests that do not yet have replies). When the replier processes the
request, it saves the request ID and adds that ID to the reply as a correlation ID. When the
requestor processes the reply, it uses the correlation ID to know which request the reply is for.
This is called a correlation identifier because of the way the caller uses the identifier to correlate
(e.g., match; show the relationship) each reply to the request that caused it.
As is often the case with messaging, the requestor and replier must agree on several details. They
must agree on the name and type of the request ID property, and they must agree on the name
and type of the correlation ID property. Likewise, the request and reply message formats must
define those properties or allow them to be added as custom properties. For example, if the
requestor stores the request ID in a first-level XML element named request_id and the value is an
integer, the replier has to know this so that it can find the request ID value and process it
properly. The request ID value and correlation ID value are usually of the same type; if not, the
requestor has to know how the replier will convert the request ID to the reply ID.
This pattern is a simpler, messaging-specific version of the Asynchronous Completion Token
pattern. [POSA2] The requestor is the Initiator, the replier is the Service, the consumer in the
requestor that processes the reply is the Completion Handler, and the Correlation Identifier that
consumer uses to match the reply to the request is the Asynchronous Completion Token.
A correlation ID (and also the request ID) is usually put in the header of a message rather than
the body. The ID is not part of the command or data the requestor is trying to communicate to the
replier. In fact, the replier does not really use the ID at all; it just saves the ID from the request
and adds it to the reply for the requestor¡¯s benefit. Since the message body is the content being
transmitted between the two systems, and the ID is not part of that, the ID goes in the header.
The gist of the pattern is that the reply message contains a token (the correlation ID) that
identifies the corresponding request (via its request ID). There are several different approaches
for achieving this.
The simplest approach is for each request to contain a unique ID, such as a message ID, and for
the response's correlation ID to be the request's unique ID. This relates the reply to its
corresponding request. However, when the requestor is trying to process the reply, knowing the
request message often isn't very interesting. What the requestor really wants is a reminder of
what business task caused it to send the request in the first place, so that the requestor can
complete the business task using the data in the reply.
The business task, such as needing to execute a stock trade or ship a purchase order, probably has
its own unique business object identifier (such as an order ID), so that business task¡¯s unique ID
can be used as the request-reply correlation ID. Then when the requestor gets the reply and its
correlation ID, it can bypass the request message and go straight to the business object whose
task caused the request in the first place. In this case, rather than use the messages¡¯ built-in
request message ID and reply correlation ID properties, the requestor and replier should use a
custom business object ID property in the request and the reply that identifies the business object
whose task this request-reply message pair is performing.
A compromise approach is for the requestor to keep a map of request ID's and business object
ID's. This is especially useful when the requestor wants to keep the object ID's private, or when
the requestor has no control over the replier's implementation and can only depend on the replier
copying the request's message ID into the reply's correlation ID. In this case, when the requestor
gets the reply, it looks up the correlation ID in the map to get the business object ID, then uses
that to resume performing the business task using the reply data.
Messages have separate message ID and correlation ID properties so that request-reply message
pairs can be chained. This occurs when a request causes a reply, and the reply is in turn another
request that causes another reply, and so on. A message's message ID uniquely identifies the
request it represents; if the message also has a correlation ID, then the message is also a reply for
another request message, as identified by the correlation ID.
Request-Reply Chaining
Chaining is only useful if an application wants to retrace the path of messages from the latest
reply back to the original request. Often all the application wants to know is the original request,
regardless of how many reply-steps occurred in between. In this situation, once a message has a
non-null correlation ID, it is a reply and all subsequent replies caused by it should also use the
same correlation ID.
Correlation Identifier is a simple version of an Asynchronous Completion Token [POSA2], where
the token is simply a primitive value. Both help a caller to process the responses generated by
asynchronous requests.
While a Correlation Identifier is used to match a reply with its request, the request may also have a
Return Address that states what channel to put the reply on. Whereas a correlation identifier is
used to match a reply message with its request, a Message Sequence¡¯s identifiers are used to
specify a message¡¯s position within a series of messages from the same sender.
JMS messages have a predefined property for correlation identifiers, JMSCorrelationID, which is
typically used in conjunction with another predefined property, JMSMessageID. [JMS11, p.32],
[Monson-Haefel, pp.194-195] A reply message¡¯s correlation ID is set from the request¡¯s message
ID like this:
Each Message in .NET has a CorrelationId property, a string in an acknowledgement message
that is usually set to the Id of the original message. MessageQueue also has a special peek and
receive methods, PeekByCorrelationId(string) and ReceiveByCorrelationId(string), for peeking
at and consuming the message on the queue (if any) with the specified correlation ID. (See
Selective Consumer [SysMsg], [Dickman, pp.147-149]
Web services standards, as of SOAP 1.1 [SOAP 1.1], do not provide very good support for
asynchronous messaging, but SOAP 1.2 starts to plan for it. SOAP 1.2 incorporates the
Request-Response Message Exchange Pattern [SOAP 1.2 Part 2], a basic part of asynchronous
SOAP messaging. However, the request/response pattern does not mandate support for
"multiple ongoing requests," so it does not define a standard Correlation Identifier field, even an
optional one.
As a practical matter, service requestors often do require multiple outstanding requests. "Web
Services Architecture Usage Scenarios" [WSAUS] discusses several different asynchronous web
services scenarios. Four of them¡ªRequest/Response, Remote Procedure Call (where the
transport protocol does not support [synchronous] request/response directly), Multiple
Asynchronous Responses, and Asynchronous Messaging¡ªuse message-id and response-to fields
in the SOAP header to correlate a response to its request. This is the request/response example:
Example: SOAP request message containing a message identifier
Example: SOAP response message containing correlation to original request
Like the JMS and .NET examples, in this SOAP example, the request message contains a unique
message identifier, and the response message contains a response to (e.g., a correlation ID) field
whose value is the message identifier of the request message.
##%%&&
My application needs to send a huge amount of data to another process, more than may fit in a
single message. Or my application has made a request whose reply contains too much data for a
single message.
How can messaging transmit an arbitrarily large amount of data?
It¡¯s nice to think that messages can be arbitrarily large, but there are practical limits to how much
data a single message can hold. Some messaging implementations place an absolute limit on how
big a message can be. Other implementations allow messages to get quite big, but large messages
nevertheless hurt performance. Even if the messaging implementation allows large messages, the
message producer or consumer may place a limit on the amount of data it can process at once.
For example, many COBOL- and mainframe-based systems will only consume or produce data in
32 Kb chunks.
So how do you get around this? One approach is to limit your application to never need more
data that what the messaging layer can handle. This is an arbitrary limit, though, which can
prevent your application from producing the desired functionality. If the large amount of data is
the result of a request, the caller could issue multiple requests, one for each result chunk, but that
assumes the caller even knows how many result chunks will be needed. The receiver could listen
for data chunks until there are not anymore (but how does it know there are not anymore?), then
try to figure out how to reassemble the chunks into the original, large piece of data, but that
would be error-prone.
Inspiration comes from the way a mail order company sometimes ships an order in multiple
boxes. If there are three boxes, the shipper will mark them as ¡°1 of 3,¡± ¡°2 of 3,¡± and ¡°3 of 3¡± so
that the receiver will know which ones he¡¯s received and whether he has all of them. The trick is
to apply the same technique to messaging.
Whenever a large set of data may need to be broken into message-size chunks, send the data as
a Message Sequence and mark each message with sequence identification fields.
The three Message Sequence identification fields are:
1. Sequence identifier ¨C Distinguishes this cluster of messages from others.
2. Position identifier ¨C Uniquely identifies and sequentially orders each message in a sequence.
3. Size or End indicator ¨C Specifies the number of messages in the cluster, or marks the last message in the
cluster (whose position identifier then specifies the size of the cluster).
The sequences are typically designed such that each message in a sequence indicates the total size
of the sequence, e.g. the number of messages in that sequence. As an alternative, you can design
the sequences such that each message indicates whether it is the last message in that sequence.
Message Sequence with End Indicator
Let¡¯s say a set of data needs to be sent as a cluster of three messages. The sequence identifier of
the three-message cluster will be some unique ID. The position identifier for each message will be
different¡ªeither 1, 2, or 3 (assuming that numbering starts from 1, not 0). If the sender knows the
total number of messages from the start, the sequence size for each message is 3. If the sender
does not know the total number of messages until it runs out of data to send (e.g., the sender is
streaming the data), each message except the last will have a ¡°sequence end¡± flag that is false;
when the sender is ready to send the final message in the sequence, it will set that message¡¯s
sequence end flag is true. Either way, the position identifiers and sequence size/end indicator
will give the receiver enough information to reassemble the parts back into the whole, even if the
parts are not received in sequential order.
If the receiver expects a Message Sequence, then every message sent to it should be sent as part of a
sequence, even if it is only a sequence of one. Otherwise, when a single-part message is sent
without the sequence identification fields, the receiver may become confused by the missing
fields and may conclude that the message is invalid (see Invalid Message Channel).
If a receiver gets some of the messages in a sequence but never does get all of them, it should
reroute the ones it did receive to the Invalid Message Channel.
An application may wish to use a Transactional Client for sending and receiving sequences. The
sender can send all of the messages in a sequence using a single transaction. This way, none of
the messages will be delivered until all of them have been sent. Likewise, a receiver may wish to
use a single transaction to receive the messages so that it does not truly consume any of the
messages until it receives all of them. If any of the messages in the sequence are missing, the
receiver can choose to rollback the transaction so that the messages can be consumed later. In
many messaging system implementations, if a sequence of messages are sent in one transaction,
they will be received in the order they are sent, which simplifies the receiver's job of putting the
data back together.
When the Message Sequence is the reply message in a Request-Reply, the sequence identifier and the
Correlation Identifier are usually the same thing. They would be separate if the application sending
the request expected multiple responses to the same request and one or more of the responses
could be in multiple parts. When only one response is expected, then uniquely identifying the
response and its sequence is permissible, but redundant.
Message Sequence tends not to be compatible with Competing Consumers nor Message Dispatcher. If
different consumers/performers receive different messages in a sequence, none of the receivers
will be able to reassemble the original data without exchanging message contents with each other.
Thus a message sequence should be transmitted either via a Message Channel with a single
consumer.
An alternative to Message Sequence is to use a Claim Check. Rather than transmitting a large
document between two applications, if the applications both have access to a common database
or file system, store the document and just transmit the receipt in a single message.
Imagine a sender needs to send an extremely large document to a receiver, so large that it will not
fit within a single message, or is impractical to send all at once. Then break the large document
into parts, each of which can be sent as a message. Each message needs to indicate its position in
the sequence and indicate how many messages total to expect.
For example, the maximum size of an MSMQ message is 4 MB. [Dickman, pp.169-172] discusses
how to send a multipart message sequence in MSMQ.
Consider a query that requests a list of all books by a certain author. Because this could be a very
large list, the messaging design might choose to return each match as a separate message. Then
each message needs to indicate the query this reply is for, the message¡¯s position in the sequence,
and how many messages total to expect.
Consider a query that is performed in parts by multiple receivers. If the parts have some order to
them, this will need to be indicated in the reply messages so that the complete reply can be
assembled properly. Each receiver will need to know its position in the overall order and will
need to indicate that position is the reply's message sequence.
Neither JMS nor .NET have built-in properties for supporting message sequences. Therefore,
messaging application must implement their own sequence fields. In JMS, an application can
define its own properties in the header, so that is an option. .NET does not provide
application-defined properties in the header. The fields could also be defined in the message
body. Keep in mind that if a receiver of the sequence needs to filter for messages based on their
sequence, such filtering is much simpler to do if the field is stored in the header rather than the
body.
Web services standards currently do not provide very good support for asynchronous messaging,
but the W3C has started to think about how it could. "Web Services Architecture Usage
Scenarios" [WSAUS] discusses several different asynchronous web services scenarios. One of
them¡ªMultiple Asynchronous Responses¡ªuse message-id and response-to fields in the SOAP
header to correlate a responses to their request, and sequence-number and total-in-sequence
fields in the body to sequentially identify the responses. This is the multiple responses example:
Example: SOAP request message containing a message identifier
Example: First SOAP response message containing sequencing and correlation to original request
Example: Final SOAP response message containing sequencing and correlation to original
request
The message-id in the header is used as the sequence identifier in the responses. The
sequence-number and total-in-sequence in each response are a position identifier and side
indicator, respectively.
##%%&&
My application is using Messaging. If a Message¡¯s data or request is not received by a certain time,
it is useless and should be ignored.
How can a sender indicate when a message should be considered stale and thus shouldn¡¯t be
processed?
Messaging practically guarantees that the Message will eventually be delivered to the receiver.
What it cannot guarantee is how long the delivery may take. For example, if the network
connecting the sender and receiver is down for a week, then it could take a week to deliver a
message. Messaging is highly reliable, even when the participants (sender, network, and receiver)
are not, but messages can take a very long time to transmit in unreliable circumstances. (For more
details, see Guaranteed Delivery.)
Often, a message¡¯s contents have a practical limit for how long they¡¯re useful. A caller issuing a
stock quote request probably looses interest if it does not receive an answer within a minute or so.
That means the request should not take more than a minute to transmit, but also that the answer
had better transmit back very quickly. A stock quote reply more than a minute or two old is
probably too old and therefore unreliable.
Once the sender sends a message and does not get a reply, it has no way to cancel or recall the
message. Likewise, a receiver could check when a message was sent and reject the message if it¡¯s
too old, but different senders under different circumstances may have different ideas about how
long is too long, so how does the receiver know which messages to reject?
What is needed is a way for the sender to specify the message¡¯s lifetime.
Set the Message Expiration to specify a time limit how long the message is viable.
Once the time for which a message is viable passes, and the message still has not been consumed,
then the message will expire. The messaging system¡¯s consumers will ignore an expired message;
they treat the message as if it were never sent in the first place. Most messaging system
implementations reroute expired messages to the Dead Letter Channel, while others simply
discard expired messages; this may be configurable.
A Message Expiration is like the expiration date on a carton of milk¡ªafter that date, you¡¯re not
supposed to drink the milk. If the expiration date passes while the milk is on the grocery store
shelf, the grocer is supposed to pull the milk off the shelf and not sell it. If you end up with a
carton on milk that expires, you¡¯re supposed to pour it out. Likewise, when a message expires,
the messaging system should no longer deliver it. If a receiver nevertheless receives a message
but cannot process it before the expiration, the receiver should throw away the message.
A Message Expiration is a timestamp (date and time) that specifies how long the message will live
or when it will expire. The setting can be specified in relative or absolute terms. An absolute
setting specifies a date and time when the message will expire. A relative setting specifies how
long the message should live before it expires; the messaging system will use the time when the
message is sent to convert the relative setting into an absolute one. The messaging system is
responsible for adjusting the timestamp for receivers in different timezones from the sender, for
adjustments in daylight savings times, and any other issues that can keep two different clocks
from agreeing on what time it is.
The message expiration property has a related property, sent time, which specifies when the
message was sent. A message¡¯s absolute expiration timestamp must be later than its sent
timestamp (or else the message will expire immediately). To avoid this problem, senders usually
specify expiration times relatively, in which case the messaging system calculates the expiration
timestamp by adding the relative timeout to the sent timestamp (expiration time = sent time +
time to live).
When a message expires, the messaging system may simply discard it or may move it to a Dead
Letter Channel. A receiver that finds itself in possession of an expired message should move it to
the Invalid Message Channel. With a Publish-Subscribe Channel, each subscriber gets its own copy of
the message; some copies of a message may reach their subscribers successfully while other
copies of the same message expire before their subscribers consume them. When using
Request-Reply, a reply message with an expiration may not work well¡ªif the reply expires, the
sender of the request will never know whether the request was ever received in the first place. If
reply expirations are used, the request sender has to be designed to handle the case where
expected replies are never received.
Message expiration is what the JMS spec calls ¡°message time-to-live.¡± [JMS11, p.71], [Hapner,
pp.59-60] JMS messages have a predefined property for message expiration, JMSExpiration, but a
sender should not set it via Message.setJMSExpiration(long) because the JMS provider will
override that setting when the message is sent. Rather, the sender should use its MessageProducer
(QueueSender or TopicPublisher) to set the timeout for all messages it sends; the method for this
setting is MessageProducer.setTimeToLive(long). A sender can also set the time-to-live on an
individual message using the MessageProducer.send(Message message, int deliveryMode, int
priority, long timeToLive) method, where the forth parameter is the time-to-live in milliseconds.
Time-to-live is a relative setting specifying how long after the message is sent it should expire.
A .NET Message has two properties for specifying expiration: TimeToBeReceived and
TimeToReachQueue. The reach queue setting specifies how long the message has to reach its
destination queue, after which the message might sit in the queue indefinitely. The be received
setting specifies how long the message has to be consumed by a receiver, which limits the total
time for transmitting the message to its destination queue plus the amount of time the message
can spend sitting on the destination queue. TimeToBeReceived is equivalent to JMS¡¯s
JMSExpiration property. Both time settings have a value of type System.TimeSpan, a length of time.
##%%&&
Several applications are communicating via Messages that follow an agreed upon data format,
perhaps an enterprise-wide Canonical Data Model. However, that format may need to change over
time.
How can a message¡¯s data format be designed to allow for possible future changes?
Even when you design a data format that works for all participating applications, future
requirements may change. New applications may be added that have new format requirements,
new data may need to be added to the messages, or developers may find better ways to structure
the same data. Whatever the case, designing a single enterprise data model is difficult enough;
designing one that will never need to change in the future is darn near impossible.
When an enterprise¡¯s data format changes, there would be no problem if all of the applications
change with it. If every application stopped using the old format and started using the new
format, and all did so at exactly the same time, then conversion would be simple. The problem is
that some applications will be converted before others, while some less-used applications may
never be converted at all. Even if all applications could be converted at the same time, all
messages would have to be consumed so that all channels are empty before the conversion could
occur.
Realistically, applications are going to have to be able to support the old format and the new
format simultaneously. To do this, applications will need to be able to tell which messages follow
the old format and which use the new.
One solution might be to use a separate set of channels for the messages with the new format.
That, however, would lead to a huge number of channels, duplication of design, and
configuration complexity as each application has to be configured for an ever-expanding
assortment of channels.
A better solution is for the messages with the new format to share the same channels the
old-format messages have already been using. This means that receivers will need a way to
differentiate messages from the same channel that have different formats. The message must
specify what format it is using. Each message needs a simple way to indicate its format.
Design a data format that includes a Format Indicator, so that the message specifies what
format it is using.
The format indicator enables the sender to tell the receiver the format of the message. This way, a
receiver expecting several possible formats knows which one a message is using and therefore
how to interpret the message¡¯s contents.
There are three main alternatives for implementing a format indicator:
1. Version Number ¨C A number or string that that uniquely identifies the format. Both the
sender and receiver must agree on which format is designated by a particular indicator.
2. Foreign Key ¨C A unique ID¡ªsuch as a filename, a database row key, a home primary key,
or an Internet URL¡ªthat specifies a format document. The sender and receiver must
agree on the mapping of keys to documents, and the format of the schema document.
3. Format Document ¨C A schema that describes the data format. The schema document does
not have to be retrieved via a foreign key or inferred from a version number, it is
embedded in the message. The sender and the receiver must agree on the format of the
schema.
A version number or foreign key can be stored in a header field that the senders and receivers
agree upon. Receivers that are not interested in the format version can ignore the field. A format
document may be too long or complex to store in a header field, in which case the message body
will need to have a format that contains two parts, the schema and the data.
XML documents have examples of all three approaches. One example is an XML declaration, like
this:
Here, 1.0 is a version number that indicates the document¡¯s conformance to that version of the
XML specification. Another example is the document type declaration, which can take two forms.
It can be an external ID containing a system identifier like this:
The system identifier, hello.dtd, is a foreign key that indicates the file containing the DTD
document that describes this XML document¡¯s format. The declaration can also be included
locally, like this:
The markup declaration, [<!ELEMENT greeting (#PCDATA)>], is a format document, an embedded
schema document that describes the XML¡¯s format. [XML 1.0]
##%%&&
Assume that we are building an order processing system. When an incoming order is received,
we first validate the order and then verify that the ordered item is available in the warehouse.
This function is performed by the inventory system. This sequence of processing steps is a perfect
candidate for the Pipes and Filters style. We create two filters, one for the validation step and one
for the inventory system, and route the incoming messages through both filters. However, in
many enterprise integration scenarios more than one inventory system exists with each system
being able to handle only specific items.
How do we handle a situation where the implementation of a single logical function (e.g.,
inventory check) is spread across multiple physical systems?
Integration solutions connect existing applications so that they work together. Because many of
these applications were developed without integration in mind, integration solutions rarely find
an ideal scenario where a business function is well encapsulated inside a single system. For
example, acquisitions or business partnerships often result in multiple systems performing the
same business function. Also, many businesses that act as aggregators or resellers typically
interface with multiple systems that perform the same functions (e.g. check inventory, place
order etc). To make matters more complicated, these systems may be operated within the
company or may be under the control of business partners or affiliates. For example, large
e-tailors like Amazon allow you to order anything from books to chain saws and clothing.
Depending on the type of item, the order may be processed by a different "behind-the-scenes"
merchant's order processing systems.
Let us assume that the company is selling widgets and gadgets and has two inventory systems,
one for widgets and one for gadgets. Let's also assume that each item is identified by a unique
item number. When the company receives an order, it needs to decide which inventory system to
pass the order to based on the type of item ordered. We could create separate channels for
incoming orders based on the type of item ordered. However, this would require the customers
to know our internal system architecture when in fact they may not even be aware that we
distinguish between widgets and gadgets. Therefore, we should hide the fact that the
implementation of the business function is spread across multiple systems from the remainder of
the integration solution, including customers. Therefore, we need to expect messages for different
items arriving on the same channel.
We could forward the order to all inventory systems (using a Publish-Subscribe Channel), and let
each system decide whether it can handle the order. This approach makes the addition of new
inventory systems easy because we do not have to change any of the existing components when a
new inventory system comes on-line. However, this approach assumes distributed coordination
across multiple systems. What happens if the order cannot be processed by any system? Or if
more than one system can process the order? Will the customer receive duplicate shipments?
Also, in many cases an inventory system will treat an order for an item that it cannot handle as an
error. If this is the case, each order would cause errors in all inventory systems but one. It would
be hard to distinguish these errors from 'real' errors such as an invalid order.
An alternative approach would be to use the item number as a channel address. Each item would
have its dedicated channel and the customers could simply put publish the order to the channel
associated with the item's number without having to know about any internal distinctions
between widgets and gadgets. The inventory systems could listen on all the channels for those
items that it can process. This approach leverages the channel addressability to route messages to
the correct inventory system. However, a large number of items could quickly lead to an
explosion of the number of channels, burdening the system with run-time and management
overhead. Creating new channels for each item that is offered would quickly result in chaos.
We should also try to minimize message traffic. For example we could route the order message
through one inventory system after the other. The first system that can accept the order consumes
the message and processes the order. If it cannot process the order it passes the order message to
the next system. This approach eliminates the danger of orders being accepted by multiple
systems simultaneously. Also, we know that the order was not processed by any system if the
last system passes it back. The solution does require, however, that the systems know enough
about each other in order to pass the message from one system to the next. This approach is
similar to the Chain of Responsibility pattern described in [GoF]. However, in the world of
message-based integration passing messages through a chain of systems could mean significant
overhead. Also, this approach would require collaboration of the individual systems, which may
not be feasible if some systems are maintained by external business partners and are therefore not
under our control.
In summary, we need a solution that encapsulates the fact that the business function is split
across systems, is efficient in its usage of message channels and message traffic, and ensures that
the order is handled by exactly one inventory system.
Use a Content-Based Router to route each message to the correct recipient based on message
content.
The Content-Based Router examines the message content and routes the message onto a different
channel based on data contained in the message. The routing can be based on a number of criteria
such as existence of fields, specific field values etc. When implementing a Content-Based Router,
special caution should be taken to make the routing function easy to maintain as the router can
become a point of frequent maintenance. In more sophisticated integration scenarios, the
Content-Based Router can take on the form of a configurable rules engine that computes the
destination channel based on a set of configurable rules.
Content-Based Router is a frequently used form of the more generic Message Router. It uses
predictive routing, i.e. it incorporates knowledge of the capabilities of all other systems. This makes
for efficient routing because each outgoing message is sent directly to the correct system. The
downside is that the Content-Based Router has to have knowledge of all possible recipients and
their capabilities. As recipients are added, removed or changed, the Content-Based Router has to be
changed every time. This can become a maintenance nightmare.
We can avoid the dependency of the Content-Based Router on the individual recipients if the
recipients assume more control over the routing process. These options can be summarized as
reactive filtering because they allow each participant to filter relevant messages as they come by.
The distribution of routing control eliminates the need for a Content-Based Router but the solution
is generally less efficient. These solutions and associated trade-offs are described in more detail in
the Message Filter and Routing Slip.
The Dynamic Router describes a compromise between the Content-Based Router and the reactive
filtering approach by having each recipient inform the Content-Based Router of its capabilities. The
Content-Based Router maintains a list of each recipient's capabilities and routes incoming messages
accordingly. The price we pay for this flexibility is the complexity of the solution and the
difficulty of debugging such a system when compared to a simple Content-Based Router.
This code example demonstrates a very simple Content-Based Router that routes messages based
on the first character in the message body. If the body text starts with 'W', the router routes the
message to the widgetQueue, if it starts with 'G', it goes to the gadgetQueue. If it is neither, the
router sends it to the dunnoQueue. This queue is actually an example of a Invalid Message Channel.
This router is stateless, i.e. it does not "remember" any previous messages when making the
routing decision.
The example uses an event-driven message consumer by registering the method OnMessage as the
handler for messages arriving on the inQueue. This causes the .NET framework to invoke the
method OnMessage for every message that arrives on the inQueue. The message queue Formatter
property tells the framework what type of message we expect. In our example, we only deal with
simple string messages. OnMessage figures out where to route the message and tells .NET that it is
ready for the next message by calling the BeginReceive method on the queue. In order to keep the
code to a minimum, this simple router is not transactional, i.e. if the router crashes after it
consumed a message from the input channel and before it published it to the output channel, we
would lose a message. Later chapters will explain how to make endpoints transactional (see
Transactional Client).
Message routing is such a common need that most EAI tool suites provide built-in tools to
simplify the construction of the routing logic. For example, in the C# example we had to code the
logic to read a message off the incoming queue, deserialize it, analyze it and republish it to the
correct outgoing channel. In many EAI tools this type of logic can be implemented with simple
drag-and-drop operations instead of writing code. The only code to write is the actual decision
logic for the Content-Based Router.
One such EAI tool that implements message routing is the TIBCO ActiveEnterprise suite. The
suite includes TIB/MessageBroker which is designed to create simple message flows that include
transformation and routing. The widget router that routes incoming messages based on the first
letter of the item number looks like this when implemented in TIB/MessageBroker:
We can read the message flow from left to right. The component on the left (represented by a
triangle pointing to the right) is the subscriber component that consumes messages off the
channel router.in. The channel name is specified in a properties box not shown in this picture.
The message content is directed to the message publisher (represented by the triangle on the
right side of the screen) The direct line from the Data output of the subscriber to the Message input
of the publisher represents the fact that a Content-Based Router does not modify the message. In
order to determine the correct output channel, the function ComputeSubject (in the middle)
analyzes the message content. The function uses a so-called dictionary (labeled as 'Map' in the
picture) as a translation table between message contents and the destination channel name. The
dictionary is configured with the following values:
The ComputeSubject functions uses the first letter of the incoming message's order item number to
look up the destination channel from the dictionary. To form the complete name of the output
channel, it appends the dictionary result to the string "router.out", to form a channel name like
"router.out.widget". The result of this computation is passed to the publisher component on the
right to be used as the name of the channel. As a result, any order item whose item number starts
with a 'G' is routed to the channel router.out.gadget, whereas any item whose item number
starts with a 'W' is routed to the channel router.out.widget.
The TIBCO implementation of the ComputeSubject function looks like this:
The function extracts the first letter of the order number (using the Left function) and converts it
to uppercase (using the Upper function ). The function uses the result as the key to the dictionary
to retrieve the name of the outgoing channel (using the DGet function).
This example demonstrates the strengths of commercial EAI tools. Instead of a few dozen lines of
code we only need to code a single function to implement the widget router. Plus, we get features
like transactionality, thread management, systems management etc. for free. But this example
also highlights the difficulties of presenting a solution created with UI tools. We had to relegate
to screen shots to describe the solution. Many important settings are hidden in property fields
that are not shown on the screen. This can make it difficult to document a solution built using UI
tools.
##%%&&
Continuing with the order processing example, let's assume that company management
publishes price changes and promotions to large customers. Whenever a price for an item
changes, we send a message notifying the customer. We do the same if we are running a special
promotion, e.g. all widgets are 10% off in the month of November. Some customers may be
interested in receiving price updates or promotions only related to specific items. If I purchase
primarily gadgets, I may not be interested in knowing whether widgets are on sale or not.
How can a component avoid receiving uninteresting messages?
The most basic way for a component to receive only relevant messages is to subscribe only to
those channels that carry relevant messages. This options leverages the inherent routing abilities
of Publish-Subscribe Channels. A component receives only those messages that travel through
channels to which the component subscribes. For example, we could create one channel for
widget updates and another one for gadget updates. Customers would then be free to 'subscribe'
to one or the other channel or both. This has the advantage that new subscribers can join in
without requiring any changes to the system. However, subscription to Publish-Subscribe Channel
is generally limited to a simple binary condition: if a component subscribes to a channel, it
receives all messages on that channel. The only way to achieve finer granularity it to create more
channels. If we are dealing with a combination of multiple parameters, the number of channels
can quickly explode. For example, if we want to allow consumers to receive all messages that
announce all price cuts of widgets or gadgets by more than 5%, 10% or 15%, we already need 6 (2
item types multiplied by 3 threshold values) channels. This approach would ultimately become
difficult to manage and will consume significant resources due to the large number of allocated
channels. So we need to look for a solution that allows for more flexibility than channel
subscription.
We also need a solution that can accommodate frequent change. For example, me could modify a
Content-Based Router to route the message to more than one destination (a concept described in
the Recipient List). This predictive router sends only relevant messages to each recipient so that
the recipient does not have to take any extra steps. However, now we burden the message
originator with maintaining the preferences for each and every subscriber. If the list of recipients
or their preferences change quickly this solution would prove to be a maintenance nightmare.
We could simply broadcast the changes to all components and expect each component to filter
out the undesirable messages. However, this approach assumes that we have control over the
actual component. In many integration scenarios this is not the case because we deal with
packaged applications, legacy applications or applications that not under the control of our
organization. Also, incorporating filtering logic inside the component makes the component
dependent on a specific type of message. For example, if a customer uses a generic price watch
component he or she may want to use it for both widgets and gadgets, but with different criteria.
Use a special kind of Message Router, a Message Filter, to eliminate undesired messages from
a channel based on a set of criteria.
The Message Filter has only a single output channel. If the message content matches the criteria
specified by the Message Filter, the message is routed to the output channel. If the message
content does not match the criteria, the message is discarded.
In our example we would define a single Publish-Subscribe Channel that each customer is free to
listen on. The customer can then use a Message Filter to eliminate messages based on criteria of his
or her choosing, such as the type of item or the magnitude of the price change.
The Message Filter can be portrayed as a special case of a Content-Based Router that routes the
message either to the output channel or the null channel, a channel that discards any message
published to it. Such a channel would be similar to /dev/null present in many operating systems
or the Null Object .
The widget and gadget example described a stateless Message Filter, i.e., the Message Filter
inspects a single message and decides whether to pass it on or not based solely on information
contained in that message. Therefore, the Message Filter does not need to maintain state across
messages and is considered stateless. Stateless components have the advantage that they allow us
run multiple instances of the component in parallel to speed up processing. However, a Message
Filter does not have to be stateless. For example, there are situations where the Message Filter
needs to keep track of the message history. A common example is the use of a Message Filter to
eliminate duplicate messages. Assuming that each message has a unique message identifier, the
Message Filter would store the identifiers of past messages to that it can recognize a duplicate
message by comparing each message's identifier with the list of stored identifiers.
Some messaging systems incorporate aspects of a Message Filter inside the messaging
infrastructure. For example, some publish-subscribe systems allow you to define a hierarchical
structure for Publish-Subscribe Channels (many publish-subscribe systems including most JMS
implementations allow this). For example, one can publish promotions to the channel
'wgco.update.promotion.widget'. A subscriber can then use wildcards to subscribe to a specific
subset of messages, e.g. if a subscriber listens to the topic 'wgco.update.*.widget' he would
receive all updates (promotions and price changes) related to widgets. Another subscriber may
listen to 'wgco.update.promotion.*', which would deliver all promotions related to widgets and
gadgets, but no price changes. The channel hierarchy lets us refine the semantics of a channel by
appending qualifying parameters, so that instead of a customer subscribing to all updates,
customers can filter messages by specifying additional criteria as part of the channel name.
However, the flexibility provided by the hierarchical channel naming is still limited when
compared to a Message Filter. For example, a Message Filter could decide to only pass on 'price
change' message only if the price changed by more than 11.5%, something that would be hard to
express by means of channel names.
Other messaging systems provide API support for Selective Consumers inside the receiving
application. Message Selectors are expressions that evaluate header or property elements inside
an incoming message before the application gets to see the message. If the condition does not
evaluate to true the message is ignored and not passed on to the application logic. A message
selector acts as a Message Filter that is built into the application. While the use of a message
selector still requires you to modify the application (something that is often not possible in EAI),
the execution of the selection rules is built into the messaging infrastructure. One important
difference between a Message Filter and a Selective Consumer is that a consumer using a Selective
Consumer does not consume messages that do not match the specified criteria. On the other hand,
a Message Filter removes all messages from the input channel, publishing only those to the output
channel that match the specified criteria.
One advantage of registering the filter expression with the messaging infrastructure is the fact
that the infrastructure is able make smart internal routing decisions based on the filter criteria.
Let's assume that the message receiver sits on a different network segment from the message
originator (or even across the Internet). It would be rather wasteful to route the message all the
way to the Message Filter just to find out that we want to discard the message. On the other hand,
we want to use a Message Filter mechanism so that the recipients have control over the message
routing instead of a central Message Router. If the Message Filter is part of the API that the
messaging infrastructure provides to the message subscriber, the infrastructure is free to
propagate the filter expression closer to the source. This will maintain the original intent of
keeping control with the message subscriber, but allows the messaging infrastructure to avoid
unnecessary network traffic. This behavior resembles that of a dynamic Recipient List.
We can use a broadcast channel that routes a message to a set of Message Filters who eliminate
unwanted messages to implement functionality equivalent to that of a Content-Based Router. The
following diagrams illustrate the two options:
Option 1: Using a Content-Based Router
In this simple example, we have 2 receivers: receiver Gadget is only interested in gadget messages
while receiver widget is only interested in widget messages. The Content-Based Router evaluates
each message's content and routes it predicatively to the appropriate receiver.
Option 2: Using a broadcast channel and a set of Message Filters
The second option broadcasts the message to a Publish-Subscribe Channel. Each recipient is
equipped with a Message Filter to eliminate unwanted messages. For example, the Widget receiver
employs a widget filter that lets only widget messages pass.
The following table characterizes some of the differences between the solutions:
Content-Based Router Pub-Sub Channel with Message Filter
Exactly one consumer receives each message. More than one consumer can consume a message.
Central control and maintenance -- predictive routing. Distributed control and maintenance -- reactive filtering.
Router needs to know about participants. Router may need
to be updated if participants are added or removed.
No knowledge of participants required. Adding or
removing participants is easy.
Often used for business transactions, e.g. orders.
Often used for event notifications / informational
messages.
Generally more efficient with queue-based channels. Generally more efficient with publish-subscribe channels.
How do we decide between the two options? In some cases, the decision is driven by the
required functionality, e.g. if we need the ability for multiple recipients to process the same
message, we need to use a Pub-Sub Channel with Message Filters. In most cases though, we
decide by which party has control over (and needs to maintain) the routing decision. Do we want
to keep central control or farm it out to the recipients? If message contain sensitive data that is
only to be seen by certain recipients we need to use a Content-Based Router -- we would not want
to trust the other recipients to filter out messages. For example, let's assume we offer special
discounts to out premium customers we would not send those to our non-premium customers
and expect them to ignore these special offers.
Network traffic considerations can drive the decision as well. If we have an efficient way to
broadcast information (e.g. using IP multicast on an internal network), using filters can be very
efficient and avoids the potential bottleneck of a single router. However, if this information is
routed over the Internet, we are limited to point-to-point connections. In this case a router is
much more efficient as it avoids sending individual messages to all participants. If we want to
pass control to the recipients but need to use a router for reasons of network efficiency we can
employ a dynamic Recipient List. This Recipient List allows recipients to express their preferences
and stores them in a database or a rule base. When a message arrives the Recipient List forwards
the message to all interested recipients whose criteria match the message.
##%%&&
You are using a Message Router to route messages between multiple destinations.
How can you avoid the dependency of the router on all possible destinations while
maintaining its efficiency?
A Message Router is very efficient because it can route a message directly to the correct destination.
Other solutions to message routing, especially reactive filtering solutions (see Message Filter and
Routing Slip) are less efficient because they use a trial-and error approach: they route each
message to the first possible destination. If that destination is the correct one, it accepts the
message, otherwise the message is passed to the second possible destination and so on.
Distributed routing solutions also suffer from the risk that there are multiple recipients of a
message or none at all. Both situations can go undetected unless we use a central routing element.
In order to achieve this accuracy, the Message Router has to incorporate knowledge about each
destination and the rules for routing messages to the destination. This can turn the Message Router
into a maintenance burden if the list of possible destinations changes frequently.
Use a Dynamic Router, a Router that can self-configure based on special configuration
messages from participating destinations.
Besides the usual input and output channels the Dynamic Router uses an additional control channel.
During system start-up, each potential recipient sends a special message to the Dynamic Router on
this control channel, announcing its presence and listing the conditions under which it can
handle a message. The Dynamic Router stores the 'preferences' for each participant in a rule base.
When a message arrives, the Dynamic Router evaluates all rules and routes the message to the
recipient whose rules are fulfilled. This allows for efficient, predictive routing without the
maintenance dependency of the Dynamic Router on each potential recipient.
In the most basic scenario each participant announces its existence and routing preferences to the
Dynamic Router on start-up time. This requires each participant to be aware of the control queue
used by the Dynamic Router. It also requires the Dynamic Router to store the rules in a persistent
way. Otherwise, if the Dynamic Router fails and has to restart it would not be able to recover the
routing rules. Alternatively, the Dynamic Router could send a broadcast message to all possible
participants to trigger them to reply with the control message. This configuration is more robust
but requires the use of an additional Publish-Subscribe Channel.
It might make sense to enhance the control channel to allow participants to send both 'subscribe'
and 'unsubscribe' messages to the Dynamic Router. This would allow recipients to add or remove
themselves from the routing scheme during runtime.
Because the recipients are independent from each other, the Dynamic Router has to deal rules
conflicts, i.e. multiple recipients announcing interest in the same type of message. The Dynamic
Router can employ a number of different strategies to resolve such conflicts:
? Ignore control messages that conflict with existing messages. This option assures that the
routing rules are free of conflict. However, the state of the routing table may depend on the
sequence in which the potential recipients start up. If all recipients start up at the same time,
this may lead to unpredictable behavior because all recipients would announce their
preferences at the same time to the control queue.
? Send the message to the first recipient whose criteria match. This option allows the
routing table to contain conflicts, but resolves them as messages come in.
? Send the message to all recipients whose criteria match. This option is tolerant of conflicts
but turns the Dynamic Router into a Recipient List. generally, the behavior of a Content-Based
Router implies that is publishes one output message for each input message. This strategy
violates that rule.
The main liability of the Dynamic Router is the complexity of the solution and the difficulty of
debugging a dynamically configured system.
A Dynamic Router is another example where message-based middleware performs similar
functions to lower level IP networking. A Dynamic Router works very similar to the dynamic
routing tables used in IP routing to route IP packets between networks. The protocol used by the
recipients to configure the Dynamic Router is analogous to the IP Routing Information Protocol
(RIP -- for more information see [Stevens]).
A common use of the Dynamic Router is dynamic service discovery in service-oriented
architectures. If a client application wants to access a service it sends a message containing the
name of the service to the Dynamic Router. The Dynamic Router maintains a service directory, a list
of all services with their name and the channel they listen on. The Dynamic Router matches the
name of the requested service to the service directory and routes the message to the correct
channel. This setup allows services to be provided by more than one provider. The client
application can continue to send command messages to a single channel without having to worry
about the nature or location of the specified service provider.
[POSA] describes the Client-Dispatcher-Server pattern as a way for a client to request a specific
service without knowing the physical location of the service provider. The dispatcher uses a list
of registered services to establish a connection between the client and the physical server
implementing the requested service. The Dynamic Router is different from the Dispatcher in that it
can be more intelligent than a simple table lookup.
This example builds on the example presented in the Content-Based Router and enhances it to act
as a Dynamic Router. The new component listens on two channels, the inQueue and the
controlQueue. The control queue can receive messages of the format "X:QueueName", causing the
Dynamic Router to route all messages whose body text begins with the letter X to the queue
QueueName.
This example uses a very simple conflict resolution mechanism -- last one wins. If two recipients
express interest in receiving messages that start with the letter 'X', only the second recipient will
receive the message because the hashmap stores only one queue for each key value. Also note
that the dunnoQueue can now receive two types of messages: incoming messages that have no
matching routing rules or control messages that do not match the required format.
##%%&&
A Content-Based Router allows us to route a message to the correct system based on message
content. This process is transparent to the original sender in the sense that the originator simply
sends the message to a channel, where the router picks it up and takes care of everything.
In some cases, though, we may want to specify one or more recipients for the message. A
common analogy are the recipient lists implemented in most e-mail systems. For each e-mail
message, the sender can specify a list of recipients. The mail system then ensures transport of the
message content to each recipient. An example from the domain of enterprise integration would
be a situation where a function can be performed by one or more providers. For example, we
may have a contract with multiple credit agencies to assess the credit worthiness of our
customers. When a small order comes in we may simply route the credit request message to one
credit agency. If a customer places a large order, we may want to route the credit request
message to multiple agencies and compare the results before making a decision. In this case, the
list of recipients depends on the dollar value of the order.
In another situation, we may want to route an order message to a select list of suppliers to obtain
a quote for the requested item. Rather than sending the request to all vendors, we may want to
control which vendors receive the request, possibly based on user preferences
How do we route a message to a list of dynamically specified recipients?
Because this problem is an extension to the problem that a Content-Based Router solves, some of
the same forces and alternatives described in that pattern come into play here as well.
Most messaging systems provide Publish-Subscribe Channels, which send a copy of a published
messages to each recipient who subscribes to the channel. The set of recipients is based on
subscription to the specific channel or subject. However, the list of active subscribers to a channel
is somewhat static and cannot change on a message-by-message basis.
Because subscription to a Publish-Subscribe Channels is binary (you are either subscribed to all
messages on the channel or none), each potential recipient would have to filter messages
incoming messages based on message content, most likely using a Message Filter or Selective
Consumer. This distributes the logic over who receives the message to the individual subscribers.
Going down this route we could maintain a central point of maintenance for the list of recipients
by attaching a list of intended recipients to the message. When the message is broadcast to all
possible recipients, each recipient would then look in the recipient list associated with the
message. If the recipient is not part of the recipient list, it will discard the message. The problem
with either approach its inefficiency by requiring each potential recipient to process every
message just to possibly discard it. The configuration also relies on a certain 'honor' system on
part of the recipients as we cannot really prevent a recipient from processing the message. This is
definitely not desirable in situations where we forward a request for quote to a select subset of
suppliers and expect others to ignore the message they are receiving.
We could also require the message originator to publish the message individually to each desired
recipient. In that case, though we would place the burden of delivery to all recipients on the
message originator. If we originator is a packaged application, this is generally not an option.
Also, it would embed decision logic inside the application which would couple the application
more tightly to the integration infrastructure. In many cases, the applications that are being
integrated are unaware of the fact that they even participate in an integration solution, so
expecting the application to contain message routing logic is not realistic.
Define a channel for each recipient. Then use a Recipient List to inspect an incoming message,
determine the list of desired recipients, and forward the message to all channels associated
with the recipients in the list.
The logic embedded in a Recipient List can be pictured as two separate parts even though the
implementation is often coupled together. The first part computes a list of recipients. The second
part simply traverses the list and sends a copy of the received message to each recipient. Just like
a Content-Based Router, the Recipient List usually does not modify the message contents.
A Recipient List Can Compute the Recipients (left) or Have Another Component Provide A List (right)
The recipient list can be derived from a number of sources. The creation of the list can be external
to the Recipient List so that the message originator or another component attaches the list to the
incoming message. The Recipient List only has to iterate through this ready-made list. In this
situation, the Recipient List usually removes the recipient list from the message to reduce the size
of the outgoing messages and prevent individual recipient from seeing who else is on the list.
Providing the list with the incoming message makes sense if the destinations of each message are
based on user selection.
In most cases, the Recipient List computes the list of recipients cased on the content of the message
and a set of rules embedded in the Recipient List. The rules may be hard-coded or configurable
(see below).
The Recipient List is subject to the same considerations regarding coupling as discussed under the
Message Router, pattern. Routing messages predictively to individual recipients can lead to tighter
coupling between components because a central has to have knowledge of a series of other
components.
In order for the Recipient List to control flow of information we need to make sure that recipients
cannot subscribe directly to the input channel into the Recipient List, bypassing any control the
Recipient List exercises.
The Recipient List component is responsible for sending the incoming message to each recipient
specified in the recipient list. A robust implementation of the Recipient List must be able to
process the incoming message but only 'consume' it after all outbound messages have been
successfully sent. As such, the Recipient List component has to ensure that the complete
operation is atomic. If the Recipient List fails, it needs to be restartable. This can be accomplished
in multiple ways:
? Single transaction - The Recipient List can use transactional channels and places the message on the
outbound channels as part of a single transaction. It does not commit the messages until all messages are placed
on the channels. This guarantees that either all or no messages are sent.
? Persistent recipient list - The Recipient List can "remember" which messages it already sent so that on failure
and restart can send messages to the remaining recipients. The recipient list could be stored on disk or a database
so that it survives a crash of the Recipient List component.
? Idempotent receivers Alternatively, the Recipient List could simply resend all messages on restart. This
options requires all potential recipients to be idempotent (see Idempotent Receiver). Idempotent functions are
those that do not change the state of the system if they are applied to themselves, i.e. the state of the component is
not affected if the same message is processed twice. Messages can be inherently idempotent (e.g. the messages
"All Widgets on Sale until May 30" or "get me a Quote for XYZ widgets" are unlikely to do harm if they are
received twice) or the receiving component can be made idempotent by inserting a special Message Filter that
eliminates duplicate messages. Idempotence is very handy because it allows us to simply resend messages when
we are in doubt whether the recipient has received it. The TCP/IP protocol uses a similar mechanism to ensure
reliable message delivery without unnecessary overhead (see [Stevens]).
Even though the intent of the Recipient List is to maintain control, it can be useful to let the
recipients themselves configure the rule set stored in the Recipient List, for example if recipients
want to subscribe to specific messages based on rules that can not easily be represented in form
of publish-subscribe channel topics. We mentioned these types of subscription rules under the
Message Filter Pattern, for example "accept the message if the price is less than $48.31". To
minimize network traffic we would still want to send the messages only to interested parties as
opposed to broadcasting it and letting each recipient decide whether to process the message or
not. To implement this functionality, recipients can send their subscription preferences to the
Recipient List via a special control channel. The Recipient List stores the preferences in a rules base
and uses it to compile the recipient list for each message. This approach gives the subscribers
control over the message filtering but leverages the efficiency of the Recipient List to distribute the
messages. This solution combines the properties of a Dynamic Router with a Recipient List to create
a Dynamic Recipient List (see picture).
A Dynamic Recipient List Is Configured by the Recipients via a Control Channel
This approach would work well for the 'price update' example discussed in the Message Filter
pattern. Since it assigns control to the individual recipients it is not suitable for the bidder
example mentioned at the beginning of this pattern, though.
Whether it is more efficient to send one message to all possible recipients who then filter the
message or to send individual messages to each recipient depends very much on the
implementation of the messaging infrastructure. Generally, we can assume that the more
recipients a message has, the more network traffic it causes. However, there are exceptions. Some
publish-subscribe messaging systems are based on IP Multicast functionality and can route
messages to multiple recipients with a single network transmission (requiring retransmission
only for lost messages). IP Multicast takes advantage of Ethernet's bus architecture. When an IP
packet is sent across the network, all network adapters (NIC) on the same Ethernet segment
receive the packet. Normally, the NIC verifies the intended recipient of the packet and ignores it
if the packet is not addressed to the IP address the NIC is associated with. Multicast routing
allows all receivers that are part of a specified multicast group to read the packet of the bus. This
results in a single packet being able to be received by multiple NIC's who then pass the data to
the respective application associated with the network connection. This approach can be very
efficient on local networks due to the Ethernet bus architecture. It does not work across the
Internet where point-to-point TCP/IP connections are required. In general, we can say that the
further apart the recipients are, the more efficient it is to use a Recipient List vs. a Publish-Subscribe
Channel.
Whether a broadcast is more efficient depends not only on the network infrastructure, but also on
the proportion between the number of recipients that are supposed to process the message over
all recipients. If on average, most recipients are in the recipient list, it may be more efficient to
simply broadcast the message and have the (few) non-participants filter the message out. If
however, on average only a small portion of all possible recipients are interested in a particular
message, the Recipient List is almost guaranteed to be more efficient.
A number of times we have contrasted implementing the same functionality using predictive
routing with a Recipient List or using reactive filtering using a Publish-Subscribe Channel and an
array of Message Filters. Some of the decision criteria equal those of the comparison between
the Content-Based Router and the Message Filter array. However, in case of a Recipient List, the
message can travel to multiple recipients, making the "filter" option more attractive.
Recipient List vs. MessageFilter Array
The following table compares the two solutions:
If we send a message to multiple recipients we may need to reconcile the results later. For
example, if we send a request for a credit score to multiple credit agencies we should wait until
all results come back so that we can compare the results and choose the best alternative . With
other less critical functions we may just take the first available response to optimize message
throughput. These types of strategies are typically implemented inside an Aggregator.
Scatter-Gather describes situations where we start with a single message, send it to multiple
recipients and re-combine the responses into a single message.
A dynamic Recipient List can be used to implement a Publish-Subscribe Channel if a messaging
system provides only Point-to-Point Channels but no Publish-Subscribe Channel. The Recipient List
would keep a list of all Point-to-Point Channels that are subscribed to the 'topic', represented by
this specific instance of the Recipient List. This solution can also be useful if we need to apply
special criteria to allow a recipient to subscribe to a source of data. The Recipient List could easily
implement logic that controls access to the source data as long as the messaging system can
ensure that the recipients don't have direct access to the input channel into the Recipient List.
The composed messaging example in the interlude at the end of this chapter (see Introduction to
Composed Messaging Examples) uses a Recipient List to route a loan quote request only to qualified
banks. The interlude shows implementations of the Recipient List in Java, C# and TIBCO.
This example builds on the Dynamic Router example to turn it into a dynamic Recipient List. The
code structure is very similar. The DynamicRecipientList listens on two input queues, one for
incoming messages (inQueue) and a control queue (controlQueue) where recipients can hand in
their subscription preferences. Messages on the control queue have to be formatted as a string
consisting of two parts separated by a colon (':'). The first part is a list of characters that indicate
the subscription preference of the recipient. The recipient expresses that it wants to receive all
messages starting with one of the specified letters. The second part of the control message
specifies the name of the queue that the recipient listens on. For example, the control message
"W:WidgetQueue" tells the DynamicRecipientList to route all incoming messages that begin with
"W" to the queue WidgetQueue. Likewise, the message "WQ:WidgetGadgetQueue" instructs the
DynamicRecipientList to route messages that start with either "W" or "G" to the queue
DynamicRecipientList.
The DynamicRecipientList uses a bit more clever (read complicated) way to store the recipient's
preferences. To optimize processing of incoming messages, the DynamicRecipientList maintains a
Hashtable keyed by the first letter of incoming messages. Unlike the Dynamic Router example, the
Hashtable contains not a single destination, but an Arraylist of all subscribed destinations. When
the DynamicRecipientList receives a message it locates the correct destination list from the
Hashtable and then iterates over the list to send one message to each destination.
This example does not use a dunnoChannel (see Content-Based Router or Dynamic Router ) for
incoming messages that do not match any criteria. Typically, a Recipient List does not consider it
an error of there are zero recipients for a message.
This implementation does not allow recipients to unsubscribe. It also does not detect duplicate
subscription. For example, if a recipient subscribes twice for the same message type it will receive
duplicate messages. This is different from the typical publish-subscribe semantics where a
specific recipient can subscribe to one channel only once.The DynamicRecipientListcould easily
be changed to disallow duplicate subscriptions if that is desired.
##%%&&
Many messages passing through an integration solution consist of multiple elements. For
example, an order placed by a customer consists of more than just a single line item. As outlined
in the description of the Content-Based Router, each line item may need to be handled by a
different inventory system. Thus, we need to find an approach to process a complete order, but
treat each orderitem contained in the order individually.
How can we process a message if it contains multiple elements, each of which may have to be
processed in a different way?
The solution to this routing problem should be generic enough so that it can deal with varying
numbers and types of elements. For example, an order can contain any number of items, so we
would not want to create a solution that assumes a fixed number of items. Nor would we want to
make too many assumptions about what type of items the message contains. For example, if the
Widget & Gadget company starts selling books tomorrow, we want to minimize the impact on
the overall solution.
We also want to maintain control over the orderitems and avoid duplicated or lost processing.
For example, we could send the complete order to each order management system using a
Publish-Subscribe Channel and let it pick out the items that it can handle. This approach has the
same disadvantages described in the Content-Based Router. It would be very difficult to avoid
missing or duplicate shipment of individual items.
The solution should also be efficient in its usage of network resources. Sending the complete
order message to each system that may only process a portion of the order can cause additional
message traffic, especially as the number of destinations increases.
To avoid sending the complete message multiple times we could split the original message into
as many messages as there are inventory systems. Each message would then contain only the line
items that can be handled by the specific system. This approach is similar to a Content-Based
Router except we are splitting the message and the routing the individual messages. This
approach would be efficient but ties the solution to knowledge about the specific item types and
associated destinations. What if we want to change the routing rules? We would now have to
change this more complex "item router" component. We use the Pipes and Filters architectural to
break out processing into well-defined, composable components as opposed to lumping multiple
functions together, so we should be able to take advantage of this architecture here as well.
Use a Splitter to break out the composite message into a series of individual messages, each
containing data related to one item.
use a Splitter that consumes one message containing a list of repeating elements, each of which
can be processed individually. The Splitter publishes a one message for each single element (or a
subset of elements) from the original message.
In many cases, we want to repeat some common elements in each resulting message. These extra
elements are required to make the resulting child message self-contained and therefore enables
state-less processing of the child message. It also allows reconciliation of associated child
messages later on. For example, each orderitem message should contain a copy of the order
number so we can properly associate the orderitem back to the order and all associated entities
such as the customer placing the order (see picture).
As mentioned earlier, many enterprise integration systems store message data in a tree structure.
The beauty of a tree structure is that it is recursive. Each child node underneath a node is the root
of another subtree. This allows us to extract pieces of a message tree and process them further as
a message tree on their own. If we use message trees , the Splitter can be easily be configured to
iterate through all children under a specified node and send one message for each child node.
Such a Splitter implementation would be completely generic because it does not make any
assumptions about the number and type of child elements. Many commercial EAI tools provide
this type of functionality under the term Iterator or Sequencer. Since we are trying to avoid vendor
vocabulary to reduce potential for confusion, we call this style of Splitter an Iterating Splitter.
Using a Splitter is not limited to repeating elements, though. A large message may be split into
individual messages to simplify processing. For example, a number of B2B information exchange
standards specify very comprehensive message formats. These huge messages are often a result
of design-by-committee and large portions of the messages may rarely be used. In many
instances it is helpful to split these mega-messages into individual messages, each centered
around a specific portion of the large message. This makes subsequent transformations much
easier to develop and can also save network bandwidth since we can route smaller messages to
those components that deal only with a portion of the mega-message. The resulting messages are
often published to different channels rather than the same channel because they represent
messages of different sub-types. In this scenario, the number of resulting messages is generally
fixed whereas in the more general Splitter assumes a variable number of items. To distinguish this
style of Splitter we call it Static Splitter. A Static Splitter is functionally equivalent to using a
broadcast channel followed by a set of Content Filters.
In some cases it is useful to equip child messages with sequence numbers to improve massage
traceability and simplify the task of an Aggregator. Also, it is a good idea to equip each message
with a reference to the original (combined) message so that processing results from the individual
messages can be correlated back to the original message. This reference functions as a Correlation
Identifier.
If message envelopes are used (see Envelope Wrapper), each new message should be supplied with
its own message envelope to make it compliant with the messaging infrastructure. For example,
if the infrastructure requires a message to carry a timestamp in the message header, we would
propagate the timestamp of the original message to each message's header.
Many messaging systems use XML messages. For example, let's assume an incoming order look
as follows:
We want the Splitter to split the order into individual orderitems. For the example document the
Splitter should generate the following two messages:
Each orderitem message is being enriched with the order date, the order number, and the
customer ID. The inclusion of the customer ID and the order date make the message
self-contained and keeps the message consumer from having to store context across individual
messages. This is important if the messages are to be processed by stateless servers. The addition
of the order number field is necessary for later re-aggregation of the items (see Aggregator. In this
example we assume that the specific order of items is not relevant for completion of the order, so
we did not have to include an item number.
Let's see what the Splitter code looks like in C#.
Most of the code centers around the XML processing. The XMLSplitter uses the same
Event-Driven Consumer structure as the other routing examples. Each incoming message invokes
the method OnMessage. Onmessage converts the message body into an XML document for
manipulation. First, we extract the relevant values from the order document. Then, we iterate
over each <item> child element. We do this by specifying the XPath expression
/order/orderitems/item. A simple XPath expression is very similar to a file path -- it descends
down the document tree, matching the element names specified in the path. For each <item> we
assemble a new XML document, copying the fields carried over from the order and the item's
child nodes.
Instead of manipulating XML nodes and elements manually, we can also create an XSL document
to transform the incoming XML into the desired format and then create output messages from
the transformed XML document. That is more maintainable when the document format is likely
to change. All we have to do is change the XSL transformation without any changes to the C#
code.
The new code uses the Transform method provided by the XslTransform class to convert the input
document into an intermediate document format. The intermediate document format has one
child element orderitem for each resulting message. The code simply traverses all child elements
and publishes one message for each element.
We read the XSL document from a separate file to make it easier to edit and test. Also, it allows
us to change the behavior of the Splitter without recompiling the code.
XSL is a declarative language, so it is not easy to make sense of unless you have written a fair bit
of XSL yourself (or read a good XSL book like [Tennison]). This XSL transform looks for any
occurrence of the order element (there is one in our document). Once it finds this element it
creates a new root element for the output document (all XML documents have to have a single
root element) and goes on to process all item elements inside the orderitems element of the input
document. The XSL specifies a new 'template' for each item that is found. This template copies
the date, ordernumber and customerid from order element (which is the item's parent's parent)
and then appends any element from the item. The resulting document has one orderitem element
for each item element in the input document. This make it easy for the C# code to iterate over the
elements and publish them as messages
We were curious as how to the two implementations would perform. We decided to run a real
quick, non-scientific performance test. We simply piped 5000 order messages into the input
Queue, started the Splitter and measured the time it took for 10,000 item messages to arrive on the
output queue. We executed this all inside a single program on one machine using local message
queues. We measured 7 seconds for the XMLSplitter that uses the DOM to extract elements and
5.3 seconds for the XSL-based Splitter. To establish a baseline, a dummy processor that consumes
one message of the input queue and publishes the same message twice on the output queue took
just under 2 seconds for 5000 messages. This time includes the dummy processor consuming
5,000 messages and publishing 10,000, and the test harness consuming the 10,000 messages the
processor published. So it looks like the XSL manipulation is a little more efficient than moving
elements around 'by hand' (if we subtract the baseline, the XSL is about 35% faster). We are sure
that either program could be tuned for maximum performance, but it was interesting to see them
execute side-by-side.
##%%&&
A Splitter is useful to break out a single message into a sequence of sub-messages that can be
processed individually. Likewise, a Recipient List or a Publish-Subscribe Channel is useful to
forward a request message to multiple recipients in parallel in order to get multiple responses to
choose from. In most of these scenarios, the further processing depends on successful processing
of the sub-messages. For example, we want to select the best bid from a number of vendor
responses or we want to bill the client for an order after all items have been pulled from the
warehouse.
How do we combine the results of individual, but related messages so that they can be
processed as a whole?
The asynchronous nature of a messaging system makes collecting information across multiple
messages challenging. How many messages are there? If we broadcast a message to a broadcast
channel, we may not know how many recipients listened to that channel and therefore cannot
know how many responses to expect.
Even if we use a Splitter, the response messages may not arrive in the same sequence they were
created in. As individual messages can be routed through different network paths, the messaging
infrastructure can usually guarantee the delivery of each message, but may not be able to
guarantee the order in which the individual messages are delivered. In addition, the individual
messages may be processed by different parties with different processing speeds. As a result,
response messages may be delivered out of order (see the Resequencer for a more detailed
description of this problem).
In addition, most messaging infrastructures operate in a "guaranteed, ultimately" delivery mode.
That means, that messages are guaranteed to be delivered to the intended recipient, but there are
no guarantees as to when the message will be delivered. How long should we wait for a message?
If we wait too long, we may delay subsequent processing. If we decide to move ahead without
the missing message, we have to find a way to work with incomplete information. Even so, what
should we do when the missing message (or messages) finally arrives? In some cases we may be
able to process the message separately, but in general other cases that may lead to duplicate
processing. On the other hand, if we ignore the late-comer messages, we permanently lose the
information content contained in these messages.
All these issues can complicate the combined processing of multiple, but related messages. It
would be much easier to implement the business logic if a separate component could take care of
these complexities and pass a single message to the subsequent processing business that depends
on the presence of all individual sub-messages.
Use a stateful filter, an Aggregator, to collect and store individual messages until a complete
set of related messages has been received. Then, the Aggregator publishes a single message
distilled from the individual messages.
The Aggregator is a special Filter that receives a stream of messages and identifies messages that
are correlated. Once a complete set of messages has been received (more on how to decide when
a set is 'complete' below), the Aggregator collects information from each correlated message and
publishes a single, aggregated message to the output channel for further processing.
Unlike most of the previous routing patterns, the Aggregator is a stateful component. Simple
routing patterns like the Content-Based Router are often stateless, which means the component
processes incoming messages one-by-one and does not have to keep any information between
messages. After processing a message, the component is in the same state as it was before the
message arrived. Therefore, we call such a component stateless. The Aggregator cannot be
stateless since it needs to store each incoming message until all the messages that belong together
have been received. Then, it needs to distill the information associated with each message into the
aggregate message. The Aggregator does not necessarily have to store each incoming message in
its entirety. For example, if we are processing incoming auction bids, we may only need to keep
the highest bid and the associated bidder ID without having to keep the history of all individual
bid messages. Still, the Aggregator has to store information across messages and is therefore
stateful.
When designing an Aggregator, we need to specify the following items:
? Correlation - which incoming messages belong together?
? Completeness Condition - when are we ready to publish the result message?
? Aggregation Algorithm - how do we combine the received messages into a single result
message?
Correlation is typically achieved by either the type of the incoming messages or an explicit
Correlation Identifier. Common choices for the completeness condition and aggregation algorithm
are described below.
Due to the event-driven nature of a messaging system, the Aggregator may receive related
messages at any time and in any order. To associate messages, the Aggregator maintains a list of
active aggregates, i.e. aggregates for which the Aggregator has received some messages already.
When the Aggregator receives a new message, it needs to check whether the message is part of an
already existing aggregate. If no aggregate related to this message exists, the Aggregator assumes
that this is the first message of a a set and creates a new aggregate. It then adds the message to
the new aggregate. If an aggregate already exists, the Aggregator simply adds the message to the
aggregate. After adding the message, the Aggregator evaluates the completeness condition for the
aggregate (described in more detail below). If the condition evaluates to true, a new aggregate
message is formed from the aggregate and published to the output channel. If the completeness
condition evaluates to false, no message is published and the Aggregator keeps the aggregate
active for additional messages to arrive. The following diagram illustrates this strategy. In this
simple scenario, we assume an aggregate to be complete whenever it contains at least three
messages.
This strategy creates a new aggregate whenever it receives a message that cannot be associated to
an existing aggregate. Therefore, the Aggregator does not need prior knowledge of the aggregates
that is may produce. Accordingly, we call this variant a Self-starting Aggregator.
Depending on the aggregation strategy the Aggregator may have to deal with the situation that an
incoming message belongs to an aggregate that has already been closed out, i.e. after the
aggregate message has been published. In order to avoid starting a new aggregate, the Aggregator
needs to keep a list of aggregates that have been closed out. We need to provide a mechanism to
purge this list periodically so that it does not grow indefinitely. This assumes that we can make
some basic assumptions about the time frame in which related messages will arrive. Since we do
not need to store the complete aggregate, but just the fact that it has been closed, we can store the
list of closed aggregates quite efficiently and build a sufficient safety margin into the purge
algorithm. We can also use Message Expiration to ignore messages that have been delayed for an
inordinate amount of time.
In order to increase the robustness of the overall solution we can also allow the Aggregator to
listen on a specific control channel which allows the manual purging of all active aggregates or a
specific one. This feature can be useful if we want to recover from an error condition without
having to restart the Aggregator component. Along the same lines, allowing the Aggregator to
publish a list of active aggregates to a special channel upon request can be a very useful
debugging feature. Both functions are excellent examples of the kind of features typically
incorporated into a Control Bus.
There are a number of strategies for aggregator completeness conditions. The available strategies
primarily depend on whether we know how many messages to expect or not. The Aggregator
could know the number of sub-messages to expect because it received a copy of the original
composite message or because each individual message contains the total count (as described in
the Splitter example). Depending on how much the Aggregator knows about the message stream,
the most common strategies are as follows:
? "Wait for All" Wait until all responses are received. This scenario is most likely in the
order example we discussed earlier. An incomplete order may not be meaningful. So if
not all items are received within a certain time-out period an error condition should be
raised by the Aggregator. This approach may give us the best basis for decision-making,
but may also be the slowest and most brittle (plus we need to know how many messages
to expect). A single missing or delayed message will prevent further processing of the
whole aggregate. Resolving such error conditions can be a complicated matter in
loosely-coupled asynchronous systems because the asynchronous flow of messages
makes it hard to reliably detect error conditions (how long should we wait before a
message is "missing"?). One way to deal with missing messages is to re-request the
message. However, this approach requires the Aggregator to know the source of the
message, which may introduce additional dependencies between the Aggregator and
other components.
? "Time Out" Wait for a specified length of time for responses and then make a decision by
evaluating those responses received within that time limit. If no responses are received,
the system may report an exception or retry. This heuristic is useful if incoming responses
are scored and only the message (or a small number of messages) with the highest score
is used. This approach is common in "bidding" scenarios.
? "First Best" Wait only until the first (fastest) response is received and ignore all other
responses. This approach is the fastest, but ignores a lot of information. It may be
practical in a bidding or quoting scenario where response time is critical.
? "Time Out with Override" Wait for a specified amount of time or until a message with a
preset minimum score has been received. In this scenario, we are willing to abort early if
we find a very favorable response; otherwise, we keep on going until time is up. If no
clear winner was found at that point, rank ordering among all the messages received so
far occurs.
? "External Event" Sometimes the aggregation is concluded by the arrival of an external
business event. For example, in the financial industry, the end of the trading day may
signal the end of an aggregation of incoming price quotes. Using a fixed timer for such an
event reduces flexibility because it does not other variability. Also, a designated business
even in form of an Event Message allows for central control of the system. The Aggregator
can listen for the Event Message on a special control channel or receive a specially
formatted message that indicates the end of the aggregation.
Closely tied to the selection of a completeness condition is the selection of the aggregation
algorithm. The following strategies are common to condense multiple messages into a single
message:
? Select the "best" answer. This approach assumes that there is a single best answer, e.g.
the lowest bid for an identical item. This makes it possible for the Aggregator to make the
decision and only pass the "best" message on. However, in real life, selection criteria are
rarely this simple. For example, the "best" bid for an item may depend on time of delivery,
the number of available items, whether the vendor is on the preferred vendor list etc.
? Condense data. An Aggregator can be used to reduce message traffic from a high-traffic
source. In these cases it may make sense to compute an average of individual messages or
add numeric fields from each message into a single message. This works best if each
message represents a numeric value, for example, the number of orders received.
? Collect data for later evaluation. it is not always possible to for an Aggregator to make the
decision of how to select the best answer. In those cases it makes still sense to use an
Aggregator to collect the individual messages and combine them into a single message.
This message may simply be a compilation of the individual's messages data. The
aggregation decision may be made later by a separate component or a human being.
In many instances, the aggregation strategy is driven by parameters. For example, a strategy that
waits for a specified amount of time can be configured with the maximum wait time. Likewise, if
the strategy is to wait until an offer exceeds a specific threshold we will most likely let the
Aggregator know in advance what the desired threshold is. If these parameters are configurable at
run-time, an Aggregator may feature an additional input that can receive control messages such as
these parameter settings. The control messages may also contain information such as the number
of correlated messages to expect, which can help the Aggregator implement more effective
completion conditions. In such a scenario, the Aggregator does not simply start a new aggregate
when the first message arrives, but rather receives up-front information related to an expected
series of messages. This information can be a copy of the original request message (e.g., an
Scatter-Gather message), augmented by any necessary parameter information. The Aggregator then
allocates a new aggregate and stores the parameter information with the aggregate (see figure).
When the individual messages come in, they are associated with the corresponding aggregate.
We call this variation an Initialized Aggregator as opposed to the Self-starting Aggregator. This
configuration is obviously only possible if we have access to the originating message, which may
not always be the case.
Aggregators are useful in many applications. The Aggregator is often coupled with a Splitter or a
Recipient List to form a composite pattern. See Composed Message Processor and Scatter-Gather for a
more detailed description of these composite patterns.
The composed messaging example in the interlude at the end of this chapter (see Introduction to
Composed Messaging Examples) uses an Aggregator to select the best loan quote from the loan quote
messages returned by the banks. The loan broker example uses an initialized Aggregator -- the
Recipient List informs the Aggregator of the number of quote messages to expect. The interlude
shows implementations of the Aggregator in Java, C# and TIBCO.
Joe Walnes showed me a creative use of an Aggregator. His system sends a message through a
sequence of components, which are unfortunately quite unreliable. Even using Guaranteed
Delivery will not correct this problem because typically the systems themselves fail after the
consumed a message. Because the applications are not Transactional Clients, the
message-in-progress is lost. To help remedying this situation, Joe routes an incoming through
two parallel paths -- once through the required, but unreliable components and once around the
components using Guaranteed Delivery. An Aggregator recombines the messages from the two
paths (see picture).
An Aggregator with Time-out Detects Missing Messages
The Aggregator uses a "Time Out with Override" completeness condition, which means that the
Aggregator completes if either the time-out is reached or the two associated message have been
received. The aggregation algorithm depends on which condition is fulfilled first. If two
messages are received, the processed message is passed on without modification. If the time-out
event occurs, we know that one of the components failed and "ate" the message. As a result, we
instruct the Aggregator to publish an error message that alerts the operators that one of the
components has failed. Unfortunately, the components have to be restarted manually, but a more
sophisticated configuration could likely restart the component and re-send any lost messages.
This example show the implementation of an Aggregator using the Java Messaging Service (JMS)
API. The Aggregator receives bid messages on one channel, aggregates all related bids and
publishes a message with the lowest bid to another channel. Bids are correlated through an
Auction ID property that acts as a Correlation Identifier for the messages. The aggregation strategy
is to receive a minimum of 3 bids. The Aggregator is self-starting and does not require external
initialization.
The Aggregator Example Selects the Lowest Bid
The solution consists of the following main classes:
? Aggregator - contains logic to receive messages, aggregate them and send result
messages. Interfaces with aggregates via the Aggregate interface.
? AuctionAggregate - implements the Aggregate interface. This class acts as an
Adapter (see [GoF]) between the Aggregate interface and the Auction class. This
setup allows the Auction class to be free of references to the JMS API.
? Auction - a collection of related bids that have been received. The Auction class
implements the aggregation strategy, e.g. finding the lowest bid and
determining when the aggregate is complete.
? Bid - is a convenience class that holds the data items associated with a bid. We
convert incoming message data into a bid object so that we can access the bid
data through a strongly-typed interface, making the the Auction logic
completely independent from the JMS API.
The code of the solution is the Aggregator class. This class requires two JMS destinations, an input
destination and an output destination. Destination is the JMS abstraction for a queue or a topic
(Publish-Subscribe Channel). This abstraction allows us to write JMS code independent from the
type of channel. This feature can be very useful for testing and debugging. For example, during
testing we may use publish-subscribe topics so that we can easily "listen in" on the message
traffic. In production may want to switch to queues.
The Aggregator is a Event-Driven Consumer and implements the MessageListener interface which
requires it to expose the onMessage method. Setting the current instance of the Aggregator as the
message listener for the MessageConsumer causes JMS to invoke the method onMesssage every time
a new message is received on the destination specified by the MessageConsumer. For each
incoming message the Aggregator extracts the correlation ID (stored as a message property) and
checks whether an active aggregate exists for this correlation ID. If no aggregate is found, the
Aggregator instantiates a new AuctionAggregate. The Aggregator then checks whether the
aggregate is still active (i.e. not complete). If the aggregate is no longer active, it discard the
incoming message. If the aggregate is active, it adds the message to the aggregate and tests
whether the termination condition has been fulfilled. If so, it gets the best bid entry and publishes
it.
The Aggregator code is very generic and depends on this specific example application only in two
lines of code. First, it assumes that the correlation ID is stored in the message property AuctionID.
Second, it creates an instance of the class AuctionAggregate. We could avoid this reference if we
used a factory that returns an object of type Aggregate and internally creates an instance of type
AuctionAggregate. Since this is a book on enterprise integration and not on object-oriented design,
we kept things simple and let this dependency pass.
The AuctionAggregate class needs to implement the Aggregate interface. The interface is rather
simple, specifying only three methods. One to add a new message (addMessage), one to determine
whether the aggregate is complete (isComplete) and one to get the best result (getBestMessage).
Instead of implementing the aggregation strategy inside the AuctionAggreagte class, we decided
to create a separate class Auction that implements the aggregation strategy but is not dependent
on the JMS API:
The Auction is actually quite simple. It provides three methods similar to the Aggregate interface,
but the method signatures differ in that they use the strongly typed Bid class instead of the
Message class. For this example, the aggregation strategy is very simple, simply waiting until
three bids have been received. However, by separating the aggregation strategy from the Auction
class and the JMS API is easy to enhance the Auction class to incorporate more sophisticated
logic.
The AuctionAggregate class acts as an Adapter between the Aggregate interface and the Auction
class. An adapter is a class that converts the interface of a class into another interface.
The following sequence diagram summarizes the interaction between the classes:
This simple example assumes that Auction IDs are universally unique. This allows us to not
worry about cleaning up the open auction list -- we just let it grow. In a real-life application we
would need to decide when to purge old auction records to avoid memory leaks.
Because this code only references JMS destinations we can run it with wither topics or queues. In
a production environment, this application may be more likely to employ a Point-to-Point Channel
(equivalent to a JMS queue) because there should only be a single recipient for a bid, the
Aggregator. As described in Publish-Subscribe Channel, topics can simplify testing and debugging.
It is very easy to add an additional listener to a topic without affecting the flow of messages.
Many times when I debug a messaging application, I run a separate 'listener' window that tracks
all messages. Many JMS implementations allow you to use wildcards in topic names so that a
listener can simply subscribe to all topics by specifying a topic name of '*'. It is very handy to
have a simple listener tool that displays all messages traveling on a topic and also logs the
messages into a file for later analysis.
##%%&&
A Message Router can route messages from one channel to different channels based on message
content or other criteria. Because individual messages may follow different routes, some
messages are likely to pass through the processing steps sooner than others, resulting in the
messages getting out of order. However, some subsequent processing steps do require
in-sequence processing of messages, for example to maintain referential integrity.
How can we get a stream of related but out-of-sequence messages back into the correct order?
The obvious solution to the out-of-sequence problem is to keep messages in sequence in the first
place. Keeping things in order is in fact easier than getting them back in order. That's why many
university libraries like to prevent readers from putting books back into the (ordered) bookshelf.
By controlling the insert process, correct order is (almost) guaranteed at any point in time. But
keeping things in sequence when dealing with an asynchronous messaging solution can be about
as difficult as convincing a teenager that keeping their room in order is actually the more efficient
approach.
One common way things get out of sequence is the fact that different messages may take
different processing paths. Let's look at a simple example. Let's assume we are dealing with a
numbered sequence of messages. If all even numbered messages have to undergo a special
transformation whereas all odd numbered messages can be passed right through, then odd
numbered messages will appear on the resulting channel while the even ones queue up at the
transformation. If the transformation is quite slow, all odd messages may appear on the output
channel before a single even message makes it, bringing the sequence completely out of order
(see picture).
To avoid getting the messages out of order, we could introduce a loop-back (acknowledgment)
mechanism that makes sure that only one message at a time passes through the system. The next
message will not be sent until the last one is done processing. This conservative approach will
resolve the issue, but has two significant drawbacks. First, it can slow the system significantly. If
we have a large number of parallel processing units, we would severely underutilize the
processing power. In many instances, the reason for parallel processing is that we need to
increase performance, so throttling traffic to one message at a time would complete erase the
purpose of the solution. The second issue is that this approach requires us to have control over
messages being sent into the processing units. However, often we find ourselves at the receiving
end of an out-of-sequence message stream without having control over the message origin.
An Aggregator can receive a stream of messages, identify related messages and aggregate them
into a single message based on a number of strategies. During this process, the Aggregator also
needs to be able to deal with the fact that individual messages can arrive at any time and in any
order. The Aggregator solves this problem by storing messages until all related messages arrive
before it publishes a result message.
Use a stateful filter, a Resequencer, to collect and re-order messages so that they can be
published to the output channel in a specified order.
The Resequencer can receive a stream of messages that may not arrive in order. The Resequencer
contains in internal buffer to store out-of-sequence messages until a complete sequence is
obtained. The in-sequence messages are then published to the output channel. It is important that
the output channel is order-preserving so messages are guaranteed to arrive in order at the next
component. Like most other routers, a Resequencer usually does not modify the message contents.
For the Resequencer to function, each message has to have a unique Sequence Number (see
Message Sequence). This sequence number is different from a Message Identifier or Correlation
Identifier. A Message Identifier is a special attribute that uniquely identifies each message.
However, in most cases Message Identifiers are not comparable, they are basically random values
and often times not even numeric. Even if they happen to numerical values it is generally a bad
idea to overload the Sequence Number semantics over an existing Message Identifier element.
Correlation Identifiers are designed to match incoming messages to original outbound requests.
The only requirement for Correlation Identifiers is uniqueness, they do not to have to be numeric
or in sequence. So if we need to preserve the order of a series of messages we should define a
separate field to track the Sequence Number. Typically, this field can be part of the Message
Header.
Generating Sequence Numbers can be more time-consuming than generating unique identifiers.
Often times unique identifiers can be generated in a distributed fashion by combining unique
location information (e.g., the MAC address of the network interface card) and current time. Most
GUID (Globally Unique Identifier) algorithms work this way. To generate in-sequence numbers
we generally need a single counter that assigns numbers across the system. In most cases, it is not
sufficient for the numbers to be simply in ascending order, but they need to be consecutive as
well. Otherwise it will be difficult to identify missing messages. If we are not careful, this
Sequence Number generator could easily become a bottleneck for the message flow. If the
individual messages are the result of using a Splitter it is best to incorporate the numbering right
into the Splitter. The Identify Field pattern in [EAA] contains a useful discussion on how to
generate keys and sequence numbers.
Sequence Numbers ensure that the Resequencer can detect messages arriving out of sequence. But
what should the Resequencer do when a out-of-sequence message arrives? An out-of-sequence
message implies that a message with a higher sequence number arrives before a message with a
lower sequence number. The Resequencer has to store the message with the higher sequence
number until it receives all the "missing" messages with lower sequence numbers. Meanwhile, it
may receive other out-of-sequence messages as well, which have to be stored as well. Once the
buffer contains a consecutive sequence of messages, the Resequencer sends this sequence to the
output channel and then removes the sent messages from the buffer (see picture).
In this simple example, the Resequencer receives messages with the sequence numbers 1, 3 ,5 ,2.
We assume that the sequence starts with 1, so the first message can be sent right away and
removed from the buffer. The next message has the sequence number 3, so we are missing
message 2. Therefore, we store message 3 until we have a proper sequence of messages. We do
the same with the next message, which has a sequence number of 5. Once message 2 comes in,
the buffer contains a proper sequence of the messages 2 and 3. Therefore, the Resequencer
publishes these messages and removes them from the buffer. Message 5 remains in the buffer
until the remaining "gap" in the sequence is closed.
How big should the buffer be? If we are dealing with a long stream of messages the buffer can get
rather large. Worse yet, let's assume we have a configuration with multiple processing units each
of which deals with a specific message type. If one processing unit fails, we will get a long stream
of out-of-sequence messages. A buffer-overrun is almost certain. In some cases we can use the
message queue to absorb the pending messages. This works only if the messaging infrastructure
allows us to read messages from the queue based on selection criteria as opposed to always
reading the oldest message first. That way we can poll the queue and see whether the first
'missing' message has come in yet without consuming all the messages in between. At some point,
though, even the storage allocated to the message queue will fill up.
One robust way to avoid buffer overruns is to throttle the message producer by using active
acknowledgement (see picture).
As we discussed above, sending only a single message at a time is very inefficient. So we need to
be a little smarter than that. One way we can be more efficient is for the Resequencer to tell the
producer how many slots it has available in its buffer. The message throttle can then fire off that
many messages since even if they get completely out of order the Resequencer will be able to hold
all of them in the buffer and re-sequence them. This approach presents a good compromise
between efficiency and buffer requirements. However, it does require that we have access to the
original in-sequence message stream in order to insert the send buffer and throttle.
This approach is very similar to the way the TCP/IP network protocol works. One of the key
features of the TCP protocol is to ensure in-sequence delivery of packets over the network. In
reality, each packet may be routed through a different network path so that out-of-sequence
packets occur quite frequently. The receiver maintains a circular buffer that is used as a sliding
window. Receiver and sender negotiate on the number of packets to send before each
acknowledgement. Because the sender waits for an acknowledgment from the receiver, a fast
sender cannot outpace the receiver or cause the buffer to overflow. Specific rules also prevent the
so-called Silly Window Syndrome where sender and receiver could fall into a very inefficient
one-packet-at-a-time mode.
Another solution to the buffer overrun problem is to compute stand-in messages for the missing
message. This works if the recipient is tolerant towards "good enough" message data and does
not require precise data for each message or if speed is more important than accuracy. For
example, in voice over IP transmissions implement filling in a blank packet results in a better
user experience than issuing a re-request for a lost packet.
Most of us application developers take reliable network communication for granted. When
designing messaging solutions, it is actually helpful to look into some of the internals of TCP
because at its core, IP traffic is asynchronous and unreliable and has to deal with many of the
same issues enterprise integration solutions do. For a thorough treatment of IP protocols see
[Stevens] and [Wright].
To demonstrate the function of a Resequencer in a real-life scenario, we use the following setup:
Resequencer Test Configuration
The test setup consists of four main components, each implemented as a C# class. The
components communicate via MSMQ message queues, provided by the Message queuing service
that is part of Windows 2000 and Windows XP.
? MQSend acts as the Test Message generator. The message body contains a simple text
string. MQSend equips each message with a sequence number inside the AppSpecific
property of each message. The sequence starts with 1 and the number of messages can be
passed in from the command line. MQSend publishes the messages to the private queue
inQueue.
? DelayProcessor reads messages off the inQueue. The only 'processing' consists of a timed
delay before the identical message is republished to the outQueue. We use three
DelayProcessors in parallel to simulate a load balanced processing unit. The processors act as
Competing Consumers, so that each message is consumed by exactly one processor. All
processors publish messages to the outQueue. Because of the different processing speed,
messages on the outQueue are out of sequence.
? The Resequencer buffers incoming out-of-sequence messages and republishes them in
sequence to the sequenceQueue.
? MQSequenceReceive reads messages off the sequenceQueue and verifies that the
sequence numbers in the AppSpecific property are in ascending order.
If we fire up all components, we see debug output similar to the following picture. From the size
of the processor output windows we can see the different speeds at which the processors are
working. As expected, the messages arriving at the Resequencer are not in sequence (in this run,
the messages arrived as 3, 4, 1, 5, 7, 2 ...). We can see from the Resequencer output how the
Resequencer buffers the incoming messages if a message is missing. As soon as the missing
message arrives, the Resequencer publishes the now completed sequence.
Output from the test components
Looking at the test setup, we realize that both the DelayProcessor and the Resequencer have a
few things in common: they both read messages from an input queue and publish them to an
output queue. The only difference is in what happens in between -- the actual processing of the
message. Therefore, we created a common base class that encapsulates the basic functionality of
this generic Filter (see Pipes and Filters. It contains convenience and template methods for queue
creation and asynchronous receiving, processing and sending of messages. We call this base class
Processor (see picture).
Both the DelayProcessor and the Resequencer inherit from the common Processor class
The default implementation of the Processor simply copies messages from the input queue to the
output queue. To implement the Resequencer, we have to override the default implementation of
the ProcessMessage method. In the case of the Resequencer, the processMessage method adds the
received message in the buffer, which is implemented as a Hashtable. The messages in the buffer
are keyed by the message sequence number which is stored in the AppSpecific property. Once
the new message is added, the method SendConsecutiveMessages checks whether we have a
consecutive sequence starting with the next outstanding messages. If so, the method sends all
consecutive messages and removes them from the buffer.
As you can see, the Resequencer assumes that the message sequence starts with 1. This works well
if the message producer also starts the sequence from 1 and the two components maintain the
same sequence over the lifetime of the components. To make the Resequencer more flexible, the
message producer should negotiate a sequence start number with the Resequencer first before
sending the first message of the sequence. This process is analogous to the SYN messages
exchanged during the connect sequence of the TCP protocol (see [Stevens]).
The current implementation also has no provisions for a buffer overrun. Let's assume, a
DelayProcessor aborts or malfunctions and 'eats' a message. The Resequencer will wait indefinitely
for the missed message until the buffer overflows. In hi-volume scenarios the message and the
Resequencer need to negotiate a window size describing the maximum number of messages the
Resequencer can buffer. Once the buffer is full, an error handler has to determine how to deal with
the missing message. For example, the producer could resend the message or a 'dummy' message
could be injected.
The Processor is relatively simple. It uses asynchronous message processing by using the
BeginReceive and EndReceivemethods. Because it is easy to forget to call BeginReceive at the end
of the message processing, we used a template method that incorporates this step. Subclasses can
the override the ProcessMessage method without having to worry about the asynchronous
processing.
##%%&&
The order-processing example presented in the Content-Based Router and Splitter patterns processes an incoming
order consisting of individual line items. Each line item requires an inventory check with the respective inventory
system. After all items have been verified we want to pass the validated order message to the next processing step.
How you maintain the overall message flow when processing a message consisting of multiple elements, each of
which may require different processing?
This problem seems to contain elements of multiple patterns we have already defined. A Splitter can split a single
message into multiple parts. A Content-Based Router could then route individual sub-messages through the correct
processing steps based on message content or type. The Pipes and Filters architectural style allows us to chain
together these two patterns so that we can route each item in the composed message to the appropriate processing
steps:
In our example, this means that each order item is routed to the proper inventory system to be verified. The inventory
systems are decoupled from each other and each system receives only items that can be processed by it.
The shortcoming of the setup so far is that we cannot find out whether all items that have been ordered are actually in
stock and can be shipped. We also need to retrieve the prices for all items (factoring volume discounts) and assemble
them into a single invoice. This requires us to continue processing as if the order is still a single message even though
we just chopped it up into many sub-messages
One approach would be to just reassemble all those items that are passing through a specific inventory system into a
separate order. This order can be processed as a whole from this point on: the order can be fulfilled and shipped, a bill
can be sent. Each sub-order is treated as an independent process. In some instances, lack of control over the
downstream process may make this approach the only available solution. For example, Amazon follows this approach
for a large portion of the goods it sells. Orders are routed to different fulfillment houses and managed from there.
However, this approach may not provide the best customer experience. The customer may receive more than one
shipment and more than one invoice. Returns or disputes may be difficult to accommodate. This is not a big issue with
consumers ordering books, but may prove difficult if individual order items depend on each other. Let's assume that
the order consists of furniture items that make up a shelving system. The customer would not be pleased to receive a
number of huge boxes containing furniture elements just to find out that the required mounting hardware is
temporarily unavailable and will be shipped at a later time.
The asynchronous nature of a messaging system makes distribution of tasks more complicated than synchronous
method calls. We could dispatch each individual order item and wait for a response to come back before we check the
next item. This would simplify the temporal dependencies, but would make the system very inefficient. We would like
to take advantage of the fact that each system can process orders simultaneously.
Use Composed Message Processor to process a composite message. The Composed Message Processor splits the
message up, routes the sub-messages to the appropriate destinations and re-aggregates the responses back into
a single message.
The Composed Message Processor uses an Aggregator to reconcile the requests that were dispatched to the multiple
inventory systems. Each processing unit sends a response message to the aggregator stating the inventory on hand for
the specified item. The Aggregator collects the individual responses and processes them based on a predefined
algorithm as described under Aggregator.
Because all sub-messages originate from a single message, we can pass additional information, such as the number of
submessages to the Aggregator to define a more efficient aggregation strategy. Nevertheless, the Composed Message
Processor still has to deal with issues around missing or delayed messages. If an inventory system is unavailable, do
we want to delay processing of all orders that include items from that system? Or should we route them to an
exception queue for a human to evaluate manually? If a single response is missing, should we re-send the inventory
request message? For a more detailed discussion of these trade-offs see the Aggregator pattern.
This patterns demonstrates the composability of individual patterns into a larger patterns. We can combine individual
patterns into a larger pattern. To the rest of the system, the Composed Message Processor appears like a simple filter
with a single input channel and a single output channel. As such, it provides an effective abstraction of the more
complex internal workings.
##%%&&
In the order processing example introduced in the previous patterns , each order item that is not
currently in stock could be supplied by one of multiple external suppliers. However, the
suppliers may or may not have the respective item in stock themselves, they may charge a
different price and may be able to supply the part by a different date. To fill the order in the best
way possible, we should request quotes from all suppliers and decide which one provides us
with the best term for the requested item.
How do you maintain the overall message flow when a message needs to be sent to multiple
recipients, each of which may send a reply?
The solution should allow for flexibility in determining the recipients of the message. We can
either determine the list of approved suppliers centrally or we can let any interested supplier
participate in the 'bid'. Since we have no (or little) control over the recipients, we must be
prepared to receive responses from some, but not all recipients. Such changes to the bidding rules
should not impact the structural integrity of the solution.
The solution should hide the number and identity of the individual recipients from any
subsequent processing. Encapsulating the distribution of the message locally keeps other
components independent from the route of the individual messages.
We also need to coordinate the subsequent message flow. The easiest solution might be for each
recipient to post the reply to a channel and let subsequent components deal with the resolution of
the individual messages. However, this would require subsequent components to be aware of the
message being sent to multiple recipients. It might also be harder for subsequent components to
process the individual messages without having any knowledge over the routing logic that has
been applied.
It makes sense to combine the routing logic, the recipients and the post-processing of the
individual messages into one logical component.
Use a Scatter-Gather that broadcasts a message to multiple recipients and re-aggregates the
responses back into a single message.
The Scatter-Gather routes a request message to the a number of recipients. It then uses an
Aggregator to collect the responses and distill them into a single response message.
There are two variants of the Scatter-Gather that use different mechanisms to send the request
messages to the intended recipients:
? Distribution via a Recipient List allows the Scatter-Gather to control the list of recipients
but requires the Scatter-Gather to be aware of each recipient's message channel.
? Auction-style Scatter-Gathers use a Publish-Subscribe Channel to broadcast the request to
any interested participant. This option allows the Scatter-Gather to use a single channel but at
the same time relinquishes control.
The solution shares similarities with the Composed Message Processor. Instead of using a Splitter,
we broadcast the complete message to all involved parties using a Publish-Subscribe Channel. We
will most likely add a Return Address so that all responses can be processed through a single
channel. As with the Composed Message Processor, responses are aggregated based on defined
business rules. In our example, the Aggregator might take the best bids from suppliers that can fill
the order. Aggregating the responses can be more difficult with a Scatter-Gather as compared to
the Composed Message Processor because we may not know how many recipients participate in the
interaction.
Both the Scatter-Gather and the Composed Message Processor route a single message to multiple
recipients and combine the individual reply messages back into a single message by using an
Composed Message Processor. The Composed Message Processor performs the task of synchronizing
multiple parallel activities. If the individual activities take widely varying amounts of time this
results in subsequent processing being held up even though many subtasks (or even all but one)
have been completed. This consideration needs to be weighed against the simplicity and
encapsulation the Scatter-Gather brings. A compromise between the two choices may be a
cascading Aggregator. This design allows subsequent tasks to be initiated with only a subset of the
results being available.
The Loan Broker example (see Introduction to Composed Messaging Examples) uses a Scatter-Gather
to route requests for a loan quote to a number of banks and select the best offer from the
incoming responses. The example implementations demonstrate both a solution based on a
Recipient List (see Asynchronous Implementation with MSMQ and a Publish-Subscribe Channel (see
Asynchronous Implementation with TIBCO ActiveEnterprise).
We can now use the Scatter-Gather to implement the widget and gadget order processing example.
We can combine the Scatter-Gather with the Composed Message Processor to process each incoming
order, sequence it into individual items, then pass each item up for a bid, then aggregate the bids
for each item into a combined bid response, and lastly aggregate all bid responses into a complete
quote. This is a very real example how multiple integration patterns can be combined into a
complete solution. The composition of individual patterns into larger patterns allows us to
discuss the solution at a higher level of abstraction. It also allows us to modify details of the
implementation without affecting other components.
Combining a Scatter-Gather and a Composite Message Processor
This example also shows the versatility of the Aggregator. The solution uses two Aggregators for
quite different purposes. The first Aggregator (part of the Scatter-Gather) chooses the best bid from
a number of vendors. This aggregator may not require a response from all vendors (speed may
be more important than a low price) but may require a complex algorithm to combine responses.
For example, the order may contain 100 widgets and the lowest price supplier has only 60
widgets in stock. The Aggregator needs to be able to decide whether to accept this offer and fill the
remaining 40 items from another supplier. The second Aggregator (part of the Composed Message
Processor) might be simpler because it simply concatenates all responses received from the first
Aggregator. However, this Aggregator needs to make sure that all responses are in fact received
and needs to deal with error conditions such as missing item responses.
##%%&&
Most of the routing patterns presented in this section route incoming messages to one or more
destinations based on a set of rules. Sometimes, though, we need to route a message not just to a
single component, but through a whole series of components. Let's assume, for example, that we
use a Pipes and Filters architecture to process incoming messages that have to undergo a sequence
of processing steps and business rule validations. Since the nature of the validations varies
widely and may depend on external systems (e.g., credit card validations), we implement each
type of step as a separate filter. Each filter inspects the incoming message, and applies the
business rule(s) to the message. If the message does not fulfill the conditions specified by the
rules it is routed to an exception channel. The channels between the filters determine the
sequence of validations that the message needs to undergo.
Now let's assume, though, that the set of validations to perform against each message depends
on the message type (for example, purchase order request do not need credit card validation or
customers who send orders over a VPN may not require decryption and authentication). To
accommodate this requirement we need to find a configuration that can route the message
through a different sequence of filters depending on the type of the message.
How do we route a message consecutively through a series of processing steps when the
sequence of steps is not known at design-time and may vary for each message?
The Pipes and Filters architectural style give us an elegant approach to represent a sequence of
processing steps as independent filters, connected by pipes (channels). In its default
configuration, the filters are connected by fixed pipes. If we want to allow message to be routed
to different filters dynamically, we can use special filters that act as Message Routers. The routers
dynamically determine the next filter to route the message to.
The key requirements to a good solution to our problem can be summarized as follows:
? Efficient message flow - Messages should only flow through the required steps and
avoid unnecessary components.
? Efficient use of resources - The solution should not use a huge amount of channels,
routers and other resources.
? Flexible - The route that individual messages that should be easy to change.
? Simple to maintain - If a new type of message needs to be supported, we would like to
have a single point of maintenance to avoid introducing errors.
The following diagram illustrates our alternative solutions to the problem. We are assuming that
the system offers three separate processing steps A, B, and C and that the current message is
required to pass only through steps A and C. The actual flow for this example message is marked
with thick, red arrows.
We could form one long Pipes and Filters chain of all possible validation steps and add code to
each router to bypass the validation if the step is not required for the type of message being
passed through (see Option A). This option employs the reactive filtering approach described in
the Message Filter. While the simplicity of this solution is appealing, the fact that the components
blend both business logic (the validation) and routing logic (deciding whether to validate) will
make them harder to reuse. Also, it is conceivable that two types of messages undergo similar
processing steps but in different order. This hard-wired approach would not easily support this
requirement.
In order to improve separation of concerns and increase the composability of the solution we
should replace the 'gating' logic inside each component with Content-Based Routers. We would
then arrive at a chain of all possible validation steps, each prefixed by a Content-Based Router (see
Option B). When a message arrives at the router, it checks the type of the message and
determines whether this type of message requires the validation step at hand. If the step is
required, the router routes the message through the validation. If the step is not required, the
router bypasses the validation and routes the message directly to the next router. This
configuration works quite well in cases where each step is independent from any other step and
the routing decision can be made locally at each step. On the downside, this approach ends up
routing the message though a long series of routers even though only a few validation steps may
be executed. In fact, each message will be transmitted across a channel at a rate of 2 times the
number of possible components. If we have a large component library, this will cause an
enormous amount of message flow for a rather simple function. Also, the routing logic is
distributed across many filters, making it hard to understand which validation steps a message of
a specific type will actually undergo. Likewise, if we introduce a new message type we may have
to update each and every router. Lastly, this option suffers from the same limitation as Option A
in that messages are tied to executing steps in a common order.
If we desire a central point of control, an up-front Content-Based Router tended to be a good choice
in our prior pattern discussions. We could envision a solution where we setup individual Pipes
and Filters chains for each message type. The chain would contain the sequence of validations
relevant to the specific type. We would then use a Content-Based Router to route the incoming
message to the correct validation chain based on message type (see Option C). This approach
routes messages only through the relevant steps (plus the initial router). Thus, is the most
efficient approach so far because we only add a single routing step in order to implement the
desired functionality. The solution also highlights the path that a message of a specific type will
take quite nicely. However, it requires us to hard-wire any possible combination of validation
rules. Also, the same component may be used in more than one path. This approach would
require us to run multiple instances of such components which leads to unnecessary duplication.
For a large set of message types, this approach could result in a maintenance nightmare because
of the large number of component instances and associated channels that we have to maintain. In
summary, this solution is very efficient at the expense of maintainability.
If we want to avoid hard-wiring all possible combinations of validation steps, we need to insert a
Content-Based Router between each validation step (see Option D). In order not to run into the
same issues associated with the reactive filtering approach (presented in Option B), we would
insert the Content-Based Router after each step instead of before (we need one additional router in
front of the very first step to get started). The routers would be smart enough to relay the
message directly to the next required validation step instead of routing it blindly to the next
available step in the chain. In the abstract, this solution looks similar to the reactive filtering
approach because the message traverses an alternating set of routers and filters. However, in this
case the routers possess more intelligence than a simple yes/no decision which allows us to
eliminate unnecessary steps. For example, in our simple scenario, the message passes only
through 2 routers as opposed to 3 with Option B. This option provides efficiency and flexibility,
but does not solve our goal of obtaining central goal-- we still have to maintain a potentially large
number of routers because the routing logic is spread out across a series of independent routers .
To address this last shortcoming we could combine all routers into a single 'super-router' (See
Option E). After each validation step, the message would be routed back to the super-router who
would determine the next validation step to be executed. Since all the routing decisions are now
incorporated into a single router, we need to devise a mechanism to remember which steps we
already finished processing. Therefore, the super-router would have to be stateful or each filter
would have to attach a tag to the message telling the super-router the name of the last filter the
message went through. Also, we are still dealing with the fact that each validation step requires
the message to be passed through two channels: to the component and the back to the
super-router. This results in about two times as much traffic as Option C.
Attach a Routing Slip to each message, specifying the sequence of processing steps. Wrap each
component with a special message router that reads the Routing Slip and routes the message to
the next component in the list.
We insert a special component into the beginning of the process that computes the list of required
steps for each message. It then attaches the list as a Routing Slip to the message and starts the
process by routing the message to the first processing step. After successful processing, each
processing step looks at the Routing Slip and passes the message to the next processing step
specified in the routing table.
This pattern works very similar to the routing slip attached to a magazine for circulation in a
department. The only difference is that the Routing Slip has a defined sequence of components it
traverses whereas in most companies you can hand the magazine after reading it to any person
on the list who has not read it (of course, the boss usually comes first).
The Routing Slip combines the central control of the 'super-router' approach (Option E) with the
efficiency of the hard-wired solutions (Option C). We determine the complete routing scheme
up-front and attach it to the message so we do not have to return to the central router for further
decision making. Each component is augmented by simple routing logic. In the proposed
solution we assume that this routing logic is built into the processing component itself. If we look
back at Option A, we remember that we dismissed this approach partway because we had to
hard-code some logic into each component. How is the Routing Slip better? The key difference is
that the router used in the Routing Slip is generic and does not have to change with changes in the
routing logic. The routing logic incorporated into each component is similar to a Return Address
where the return address is selected from a list of addresses. Similar to the Return Address, the
components retain their reusability and composability even though a piece of routing logic is
built into the component. Additionally, the computation of the routing table can now be done in
a central place without ever touching the code inside any of the processing components.
As always, there is no free lunch. So we can expect the Routing Slip to have some limitations. First,
the message size increases slightly. In most cases this should be insignificant, but we need to
realize that we are now carrying process state (which steps have been completed) inside the
message. This can cause other side effects. For example, if we lose a message we lose not only the
message data but also the process data (i.e., where the message was going to go next). In many
cases it may be useful to maintain the state of all messages in a central place to perform reporting
or error recovery.
Another limitation of the Routing Slip is the fact that the path of a message cannot be changed
once it is under way. This implies that the message path cannot depend on intermediate results
generated by a processing step along the way. In many real-life business processes the message
flow does change based on intermediate results, though. For example, depending on the
availability of the ordered items (as reported by the inventory system) we may want to continue
with a different path. This also means that a central entity has to be able to determine all steps a
message should undergo in advance. This can lead to some brittleness in the design, similar to
the concerns about using a Content-Based Router.
The Routing Slip assumes that we have the ability to augment the individual components with the
router logic. If we are dealing with legacy applications or packaged applications we may not be
able to influence the functionality of the component itself. Rather, we need to use an external
router that communicates with the component via messaging. This inevitably increases the
number of channels and components in usage. However, the Routing Slip still provides the best
trade-off between our goals of efficiency, flexibility and maintainability.
Implementing a Routing Slip with Legacy Applications
The Routing Slip is most useful in the following scenarios:
? A sequence of binary validation steps. By not adding information to the message, the
limitation that we cannot change the routing once the message is underway is no longer a
factor. We still appreciate the flexibility to change the sequence of validation steps by
reconfiguring the central Routing Slip. Each component has the choice between aborting
the sequence due to error or to pass the message on to the next step.
? Each step is a stateless transformation. For example, let's assume that we receive orders
from a variety of business partners. All orders arrive on a common channel. Depending
in the partner, the message may require different transformation steps. Messages from
some partners may require decryption, others may not. Some may require transformation
or enrichment while others may not. Keeping a Routing Slip for each partner gives us an
easy way to reconfigure the steps for each partner in a central location.
? Each step gathers data, but makes no decisions. In some cases, we receive a message that
contains reference identifiers to other data. For example, if we receive an order for a DSL
line, the message may contain only the home phone number of the applicant. We need to
go to external sources to determine the customer's name, the central office servicing the
line, the distance from the central office etc. Once we have a complete message with all
relevant data we can decide what package to offer to the customer. In this scenario the
decision is postponed until the end so we can use a Routing Slip. In this scenario we need
to assess though whether we really require the flexibility of the Routing Slip. Otherwise a
simple hard-wired chain of Pipes and Filters may be sufficient.
One of the downsides of a Content-Based Router was that it has to incorporate knowledge about
each possible recipient and the routing rules associated with that recipient. Under the spirit of
loose coupling it may be undesirable to have a central component that incorporates knowledge
about many other components. An alternative solution to the Content-Based Router was a
Publish-Subscribe Channel combined with an array of Message Filters. This solution allows each
recipient to decide which messages to process but suffered from risk of duplicate message
processing. Another option to enable individual recipients to decide whether to process a given
message is to use a modified version of a Routing Slip acting as a Chain of Responsibility as
described in [GoF]. The Chain of Responsibility allows each component to accept a message or
route it to the next component in the list. The Routing Slip is a static list of all participants. This
still implies that a central component has to have knowledge of all possible recipients. However,
the component does not need to know which messages each component consumes.
Using a Routing Slip avoids the risk of duplicate message processing. Likewise it is easy to
determine if a message was not processed by any component. The main trade-off is the slower
processing and increased network traffic. While a Content-Based Router publishes a single
message regardless of the number of systems, the Routing Slip approach publishes an average
number of messages equal to 1/2 the number of systems. We can reduce this number if we can
arrange the systems in such a way that the first systems to receive the message have a higher
chance of handling the message, but the number of messages will likely remain higher than with
a predictive Content-Based Router.
There are cases where we need more control than a simple sequential list or we need to change
the flow of a message based on intermediate results. The Process Manager can fulfill these
requirements because it supports branching conditions, forks and joins. In essence, the Routing
Slip is a special case of a dynamically configured business process. The trade-offs between using a
Routing Slip and using a central Process Manager should be carefully examined. A dynamic
Routing Slip combines the benefits of a central point of maintenance with the efficiency of a
hard-wired solution. However, as the complexity grows, analyzing and debugging the system
may become increasingly difficult as the routing state information is distributed across the
messages. Also, as the semantics of the process definition begin to include constructs such as
decisions, forks and joins, the configuration file may become hard to understand and maintain.
We could include conditional statements inside the routing table and augment the routing
modules in each component to interpret the conditional commands to determine the next routing
location. We need to be careful, though, to not overburden the simplicity of this solution with
additional functionality. Once we require this type of complexity it may be a good idea to give up
the run-time efficiency of a Routing Slip and to start using a much more powerful Process Manager.
When creating a service-oriented architecture, a single logic function is often composed of
multiple independent steps. This situation occurs commonly for two primary reasons. First,
packaged applications tend to expose fine-grained interfaces based on their internal APIs. When
integrating these packages into an integration solution, we want to work at a higher level of
abstraction. For example, the operation "New Account" may require multiple steps inside a
billing system: create a new customer, select a service plan, set address properties, verify credit
data etc. Second, a single logical function may be spread across more than one system. We want
to hide this fact from other systems so we have the flexibility to reassign responsibilities between
systems without affecting the rest of the integration solution. We can easily use a Routing Slip
to execute multiple internal steps for a single request message. The Routing Slip gives us the
flexibility to execute different requests from the same channel. The Routing Slip executes the
sequence of individual steps but appears to the outside like a single step (see picture).
1. The incoming request message, specifying the intended operation and any necessary data,
is sent to the lookup component.
2. The lookup component retrieves the list of required processing steps associated with the
intended operation from a service directory. It adds the list of channels (each channel
equals one fine-grained operation) to the message header. The lookup component adds
the return channel to the list so that completed messages are returned to the lookup
component.
3. The lookup component publishes the message to the channel for the first activity.
4. Each router reads the request from the queue and passes it to the service provider. After
the execution, the router marks the activity as completed and routes the message to the
next channel specified in the routing table.
5. The lookup component consumes the message off the return channel and forwards it to
the requestor. To the outside, this whole process appears like a simple request-reply
message exchange.
Frequently, a Web service request has to be routed through multiple intermediaries. For this
purpose, Microsoft defined the Web Services Routing Protocol (WS-Routing) specification.
WS-Routing is a SOAP-based protocol for routing messages from a sender through a series of
intermediaries to a receiver. The semantics of WS-Routing are richer than those of the Routing Slip,
but a Routing Slip can be easily implemented in WS-Routing. The following example shows the
SOAP header for a message that is routed from node A to node D via the intermediaries B and C.
Like most services specs, WS-Routing is likely to evolve over time and/or be merged with other
specs. We included the example here as a snapshot of where the Web Services community is
going with respect to routing.
##%%&&
The Routing Slip demonstrates how a message can be routed through a dynamic series of
processing steps. The solution of the Routing Slip is based on two key assumptions: the sequence
of processing steps has to be determined up-front and the sequence is linear. In many cases, these
assumptions may not be fulfilled. For example, routing decisions might have to be made based
on intermediate results. Or, the processing steps may not be sequential, but multiple steps might
be executed in parallel.
How do we route a message through multiple processing steps when the required steps may
not be known at design-time and may not be sequential?
One of the primary advantages of a Pipes and Filters architectural style is the composability of
individual processing units ("filters") into a sequence by connecting them with channels ("pipes").
Each message is then routed through the sequence of processing units (or components). If we
need to be able to change the sequence for each message, we can use multiple Content-Based
Routers. This solution provides the maximum flexibility, but has the disadvantage that the
routing logic is spread across many routing components. The Routing Slip provides a central
point of control by computing the message path up-front, but does not provide the flexibility to
re-route the message based on intermediate results or to execute multiple steps simultaneously.
We can gain flexibility and maintain a central point of control, if after each individual processing
unit we return control back to a central component. That component can then determine the next
processing unit(s) to be executed. Following this approach, we end up with an alternating
process flow: central component, processing unit, central component, processing unit and so on.
As a result, the central unit receives a message after each individual processing step. When the
message arrives, the central component has to determine the next processing step(s) to be
executed based on intermediate results and the current 'step' in the sequence. This would require
the individual processing units to return sufficient information to the central unit to make this
decision. However, this approach would make the processing units dependent on the existence of
the central unit because they might have to pass through extraneous information that is not
relevant to the processing unit, but only to the central component. If we want to decouple the
individual processing steps and the message formats from the central unit, we need to provide
the central unit with some form of 'memory' that tells it what step in the sequence was executed
last.
Use a central processing unit, a Process Manager, to maintain the state of the sequence and
determine the next processing step based on intermediate results.
First of all, let me clarify that the design and configuration of a Process Manager is a pretty
extensive topic. We could probably fill a whole book (Volume 2, maybe?) with patterns related to
the design of workflow or business process management. Therefore, this pattern is intended
primarily to "round off" the topic of routing patterns and to provide a pointer into the direction of
workflow and process modeling. By no means is it a comprehensive treatment of business
process design.
Using a Process Manager results in a so-called hub-and-spoke pattern of message flow (see
diagram). An incoming message initializes the Process Manager. We call this message the trigger
message. Based on the rules inside the Process Manager, it sends a message (1) to the first
processing step, implemented by Processing Unit A. After unit A completes its task it sends a
reply message back to the Process Manager. Next, the Process Manager determines the next step to
be executed and sends message (2) to the next processing unit. As a result, all message traffic
runs through this central 'hub', hence the term hub-and-spoke. The downside of this central
control element is the danger of turning the Process Manager into a performance bottleneck.
The versatility of a Process Manager is at the same time its biggest strength and weakness. A
Process Manager can execute any sequence of steps, sequential or in parallel. Therefore, almost
any integration problem can be solved with a Process Manager. Likewise, most of the patterns
introduced in this chapter could be implemented using a Process Manager. In fact, many EAI
vendors make you believe that every integration problem is a process problem. We think that
using a Process Manager for every situation may be overkill. It can distract from the core design
issue and also cause significant performance overhead.
One of the key functions of the Process Manager is to maintain state between messages. For
example, when the second processing unit returns a message to the Process Manager, the Process
Manager needs to remember that this is step 2 out of a sequence of many steps. We do not want to
tie this knowledge to the processing unit because the same unit may appear multiple times inside
the same process. For example, Processing Unit B may be step 2 and step 4 of a single process. As
a result, the same reply message sent by Processing Unit B may trigger the Process Manager to
execute step 3 or step 5, based on the process context. To accomplish this without complicating
the processing unit, the Process Manager needs to maintain the current position in the process
execution.
It is useful for the Process Manager to be able to store additional information besides the current
position in the process. The Process Manager can store intermediate results from previous
processing if it is relevant to later steps. For example, if the results of step 1 are relevant to a later
step, the Process Manager can store this information without burdening the messages to and from
subsequent processing units with this data back. This allows the individual processing steps to be
independent of each other because they do not have to worry about data produced or consumed
by other units. Effectively, the Process Manager plays the role of a Claim Check explained later.
Because the process execution can span many steps and can therefore take a long time, the Process
Manager needs to be prepared to receive new messages while another process is still executing. In
order to manage multiple parallel executions, the Process Manager creates a new process instance
for each incoming trigger message. The process instance stores the state associated with the
execution of the process triggered by the trigger message. The state includes the current
execution step of the process and any associated data. Each process instance is identified by a
unique process identifier.
It is important to separate the concepts of a process definition (also referred to as process template)
and a process instance. The process definition is a design construct that defines the sequence of
steps to be executed. The process instance is an active execution of a specific template. The
diagram below shows a simple example with one process definition and two process instances.
The first instance (process identifier 1234) is currently executing step 1 while the second process
instance (process identifier 5678) is executing steps 2 and 5 in parallel.
Multiple Process Instances based on one Process Definition
Because multiple process instances may be executing simultaneously, the Process Manager needs
to be able to associate an incoming message with the correct instance. For example, if the Process
Manager in the above example receives a message from a processing unit, which process instance
is the message meant for? Multiple instances may be executing the same step so the Process
Manager cannot derive the instance from the channel or the type of message. The requirement to
associate an incoming message with a process instance reminds us of the Correlation Identifier. The
Correlation Identifier allows a component to associate an incoming reply message with the original
request by storing a unique identifier in the reply message that correlates it to the request
message. Using this identifier the component can match up the reply with the correct request
even if the component has sent multiple requests and the replies arrive out of order. The Process
Manager requires a similar mechanism. When the Process Manager receives a message from a
processing unit, it needs to be able to associate the message with the process instance that sent
the message to the processing unit. The Process Manager needs to include a Correlation Identifier
inside messages that it sends to processing units. The component needs to return this identifier in
the reply message as a Correlation Identifier.
It is apparent that state management is an important feature of the Process Manager. How, then,
did the previous patterns get away without managing state? In a traditional Pipes and Filters
architecture, the pipes (i.e., the Message Channels) manage the state. To continue the example
above, if we were to implement the process with hard-wired components connected by Message
Channel, it would look like the picture below. If we assume that this system is in the same state as
the example above (i.e. two process instances), it equates to one message with the identifier 1234
sitting in a channel waiting to be processed by component (1) and two messages with the
identifier 5678 waiting to be processed by the components (2) and (5) respectively. As soon as
component (1) consumes the message and completes its processing tasks, it broadcasts a new
message to components (2) and (4) -- exactly the same behavior as the Process Manager in the
above example.
It is striking how much the message flow notation used for this example resembles an UML
activity diagram that is often used to model the behavior of Process Manager components.
Effectively, we can use an abstract notation to model the behavior of the system during design,
and then decide whether we want to implement the behavior as a distributed pipes-and-filters
architecture or as a hub-and-spoke architecture using a central Process Manager. Even though we
don't have room in this book to dive too deeply into the design of process models, many of the
patterns in this language do apply when designing a process model.
As with most architectural decisions, implementing a central Process Manager or a distributed
pipes-and-filters architecture is not a simple yes/no decision. In many cases, it makes most sense
to use multiple Process Manager components, each of which houses a particular aspect of a larger
process. The Process Manager components can then communicate with each other through a
pipes-and-filters architecture.
Managing state explicitly inside a Process Manager may require a more complex component, but it
allows much more powerful process reporting. For example, most implementations of a Process
Manager provide the ability to query process instance state. This makes it easy to see how many
orders are currently waiting for approval or have been put on hold because of lacking inventory.
We can also tell each customer the status of his or her order. If we used hard-wired channels, we
would have to inspect all channels to obtain the same information. This property of the Process
Manager is not only important for reporting, but also for debugging. Using a central Process
Manager makes it easy to retrieve the current state of a process and the associated data.
Debugging a fully distributed architecture can be a lot more challenging and is almost impossible
without the assistance of such mechanisms as the Message History or Message Store.
Most commercial EAI implementations include a Process Manager component, combined with
visual tools to model the process definition. Most visual tools use a notation that resembles UML
Activity Diagrams because the semantics of a Process Manager and those of an activity diagram
are fairly similar. Also, activity diagrams are a good visual representation of multiple tasks
executing in parallel. Until recently, most vendor tools converted the visual notation into an
internal, vendor-proprietary process definition to be executed by the process engine. However,
the push to standardize various aspects of distributed systems under the umbrella of Web
services has not ignored the important role of process definitions. Three proposed 'languages'
have emerged as a result of these efforts. Microsoft defined XLANG which is supported by its
family of BizTalk orchestration modeling tools. IBM drafted the WSFL, the Web Services Flow
Language [WSFL]. Recently, both companies have joined forces to create the specification for
BPEL4WS, the Business Process Execution Language for Web Services (see [BPEL4WS]). The
BPEL4WS is a powerful language that describes a process model as an XML document. The
intent is to define a standardized intermediate language between the process modeling tools and
the Process Manager engines. This way, I could model my processes with Vendor X's product and
decide to execute the process on Vendor Y's process engine implementation. For more
information on the impact of Web services standards in integration, see Emerging Standards and
Futures in Enterprise Integration.
The semantics of a process definition can be described in rather simple terms. The basic building
block is an activity (sometimes called task or action). Usually, an activity can send a message to
another component, wait for an incoming message or execute a specific function internally (e.g, a
Message Translator). Activities can be connected in serial fashion or multiple activities can be
executed in parallel using a fork and join construct. A fork allows multiple activities to execute at
the same time. It is semantically equivalent to a Publish-Subscribe Channel in a hard-wired
pipes-and-filters architecture. A join synchronizes multiple parallel threads of execution back into
a single thread. Execution after a join can only continue if all parallel threads have completed
their respective activities. In the pipes-and-filters style an Aggregator often serves this purpose.
The process template also needs to be able to specify a branch, or decision point, so that the path
of execution can change based on the content of a message field. This function is equivalent to a
Content-Based Router. Many modeling tools include the ability to design a loop construct, but this
is really a special case of the branch. The following picture highlights the semantic similarities
between an process definition (depicted as an UML Activity Diagram) and a pipes-and-filters
implementation using the patterns defined in this pattern language, even though the physical
implementation is very different.
Example UML Activity Diagram and Corresponding Pipes-And-Filters Implementation
A number of times we have contrasted a basic Pipes and Filters architecture, the Routing Slip and
the Process Manager. We compiled the key differences into the following table to highlight the
trade-offs involved in choosing the correct architecture.
A central point of control and state management can also mean a central point of failure or a
performance bottleneck. For this reason, most Process Manager implementations allow persistent
storage of process instance state in a file or in a database. The implementation can then leverage
redundant data storage mechanisms typically implemented in enterprise-class database systems.
It is also common to run multiple Process Manager in parallel. Parallelizing Process Manager is
generally easy because process instances are independent from each other. This allows us to
distribute process instances across multiple process engines. If the process engine persists all
state information in a shared database, the system can become robust enough to survive the
failure of a process engine -- another engine can simply pick up where the previous one left of.
The downside of this approach is that the state of each process instance has to be persisted in a
central database after each processing step. This could easily turn the database into a new
performance bottleneck. As so often, the architect has to find the correct balance between
between performance, robustness, cost and maintainability.
The MSMQ implementation of the Loan Broker example at the end of this chapter (see
Asynchronous Implementation with MSMQ) implements a simple Process Manager. This example
creates the Process Manager functionality from scratch, defining a process manager and process
instances. The TIBCO implementation of the same example (see Asynchronous Implementation with
TIBCO ActiveEnterprise) uses a commercial process management tool.
Most commercial EAI tools include process design and execution capabilities. For example,
Microsoft BizTalk lets users design process definitions via the Orchestration Designer tool that is
integrated into the Visual Studio .NET programming environment.
This simple example orchestration receives an order message and executes two parallel activities.
One activity creates a request message to the inventory systems and the other activity creates a
request message to the credit system. Once both responses are received the process continues.
The visual notation makes it easy to follow the process definition.
##%%&&
Many patterns in this chapter present ways to route messages to the proper destination without
the originating application being aware of the ultimate destination of the message. Most of the
patterns focused on specific types of routing logic. However, in aggregate, these patterns solve a
bigger problem.
How can you decouple the destination of a message from the sender and maintain central
control over the flow of messages?
Using a simple Message Channel already provides a level of indirection between sender and
receiver -- the sender only knows about the channel, but not the receiver. However, if each
receiver has its own channel, this level of indirection becomes less meaningful. Instead of
knowing the receiver's address, the sender has to know the correct channel name that is
associated with the receiver.
All but the most trivial messaging solutions connect a number of different applications. If we
created individual message channels to connect each application to each other application, the
number of channels in the system would quickly explode into an unmanageable number,
resulting in integration spaghetti (see diagram).
Integration Spaghetti as a Result of Point-to-Point Connections
This diagram illustrates that direct channels between individual applications can lead to an
explosion of the number of channels and reduce many of the benefits of routing messages
through channels in the first place. These types of integration architectures are often a result of a
solution that grew over times. First the customer care system had to talk to the accounting system.
Then the customer care system was also expected to retrieve information from the inventory
system and the shipping system was to update the accounting system with the shipping charges.
It is easy to see how "adding one more piece" can quickly compromise the overall integrity of
solution.
Requiring an application to explicitly communicate with each other application can quickly
hamper the maintainability of the system. For example, if the customer address changes in the
customer care system this system would have to send a message to all systems that maintain
copies of the customer address. Every time a new system is added, the customer care system
would have to know whether that system uses addresses and be changed accordingly.
Publish-Subscribe Channels provide some form of basic routing -- the message is routed to each
application that subscribed to the specific channel. This works in simple 'broadcast' scenarios but
often times routing rules are much more complicated. For example, an incoming order message
may have to be routed to a different system based on the size or the nature of the order. To avoid
making the applications responsible for determining a message's ultimate destination, the
middleware should include a Message Router that can route messages to the appropriate
destination.
Individual message routing patterns have helped us decouple the sender from the receiver(s). For
example, a Recipient List can help pull the knowledge about all recipients out of the sender and
into the middleware layer. Moving the logic into the middleware layer helps us in two ways.
First, many of the commercial middleware and EAI suites provide tools and libraries that are
specialized to performing these kind of tasks. This simplifies the coding effort because we do not
have to write the Message Endpoint-related code, such as Event-Driven Consumers or thread
management. Also, implementing the logic inside the middleware layer allows us to make the
logic "smarter" than would be practical inside of the application. For example, using a dynamic
Recipient List can avoid coding changes when new systems are added to the integration solution.
However, having a large number of individual Message Router components can be almost as hard
to manage as the integration spaghetti we were trying to resolve.
Use a central Message Broker that can receive messages from multiple destinations, determine
the correct destination and route the message to the correct channel. Implement the internals
of the Message Broker using the design patterns presented in this chapter.
Using a central Message Broker is sometimes referred to as hub-and-spoke architectural style, which
appears to be a descriptive name when looking at the diagram above.
The Message Broker pattern has a slightly different scope than most of the other patterns presented
in this chapter. It is an architecture pattern as opposed to individual design patterns we
presented in this chapter. As such, it is comparable to the Pipes and Filters architectural style,
which gives us a fundamental way of chaining components together to form more complex
message flows. Rather than just chaining individual components, the Message Broker concerns
itself with larger solutions and helps us deal with the inevitable complexity of managing such a
system.
The Message Broker is not a monolithic component. Internally, it uses many of the message
routing patterns presented in this chapter. So once you decide to use the Message Broker as an
architectural pattern, you can choose the correct Message Router design patterns to implement the
Message Broker.
The advantage of central maintenance of a Message Broker can also turn into a disadvantage.
Routing all messages through a single Message Broker can turn the Message Broker into a serious
bottleneck. A number of techniques can help us alleviate this problem. For example, the Message
Broker pattern only tells us to develop a single entity that performs routing. It does not prescribe
how many instances of this entity we deploy in the system at deployment time. If the Message
Broker design is stateless (i.e. if it is composed only of stateless components), we can easily deploy
multiple instances of the broker to improve throughput. The properties of a Point-to-Point Channel
ensure that only one instance of the Message Broker consumes any incoming message. Also, as in
most real-life situations, the ultimate solution ends up being a combination of a patterns.
Likewise, in many complex integration solutions it may make sense to design multiple Message
Broker components, each specializing on a specific portion of the solution. This avoids creating
the ¨¹ber-Message Broker that is so complex as to become unmaintainable. The apparent flip-side is
that we no longer have a single point of maintenance and could create a new form of "Message
Broker spaghetti". One excellent architectural style to use a combination of Message Brokers is a
Message Broker hierarchy (see picture). This configuration resembles a network configuration
composed out of individual subnets. If a message has to travel only between two applications
inside a "subnet" the local Message Broker can manage the routing of the message. If the message
is destined for another subnet, the local Message Broker can pass the message to the central
Message Broker who then determines the ultimate destination. The central Message Broker performs
the same functions as a local Message Broker, but instead of decoupling individual applications it
decouples whole subsystems consisting of multiple applications.
Because the purpose of the Message Broker is to reduce coupling between individual applications,
it usually has to deal with translating message data formats between applications. Having a
Message Broker abstract the routing of the message does not help the sending application if it has
to format the message in the (supposedly hidden) destination's message format. The next chapter
introduces a series of message transformation patterns to address these issues. In many cases, a
Message Broker uses a Canonical Data Model internally to avoid the N-square problem (the number
of translators required to translate between each and every recipient in a system grows with the
square of the number of participants).
Most commercial EAI tools provide tools to greatly simplify the creation of Message Broker
components for integration solutions. These tool suites typically provide a number of features
that support the development and deployment of Message Brokers:
? Built-in Endpoint code. Most EAI suites incorporate all code to send and receive
messages to and from the message bus. The developer does not have to concern itself with
writing any of the transport-related code.
? Visual Design tools. These tools allow the developer to compose the functionality of a
Message Broker using visual components, such as Routers, Decision Points, Transformers.
These tools make the flow of message visually intuitive and can reduce the coding effort for
many of these components to single-lines of code, e.g. an evaluation function or a rule.
? Runtime support. Most EAI packages also provide sophisticated run-time support in
both deploying the solution and monitoring the traffic flowing through the Message Broker.
##%%&&
Most messaging systems divide the message data into a header and a body. The header contains
fields that are used by the messaging infrastructure to manage the flow of messages. However,
most endpoint systems that participate in the integration solution generally are not aware of
these extra data elements. In some cases, systems may even consider these fields as erroneous
because they do not match the message format used by the application. On the other hand, the
messaging components that route the messages between the applications may require the header
fields and would consider a message invalid if it does not contain the proper header fields.
How can existing systems participate in a messaging exchange that places specific
requirements on the message format, such as message header fields or encryption?
For example, assume the messaging system is using a proprietary security scheme. A valid
message would have to contain security credentials for the message to be accepted for processing
by other messaging components. Such a scheme is useful to prevent unauthorized users from
feeding messages into the system. Additionally, the message content may be encrypted to
prevent eavesdropping by unauthorized listeners -- a particularly important issue with
publish-subscribe mechanisms. However, existing applications that are being integrated via the
messaging systems are most likely not aware of the concepts of user identity or message
encryption. As a result, 'raw' messages need to be translated into messages that comply with the
rules of the messaging system.
Some large enterprises use more than one messaging infrastructure. As a result, a message may
have to be routed across messaging systems using a Messaging Bridge. Each messaging system is
likely to have different requirements for the format of the message body as well as the header.
This scenario is a case where we can learn by looking at existing TCP/IP-based network
protocols. In many cases, connectivity to another system is restricted to a specific protocol, for
example telnet or secure shell. In order to enable communication using another protocol (for
example, FTP), that protocol format has to be encapsulated into packets that conform to the
supported protocol. At the other end, the packet payload can be extracted. This process is called
'tunneling'.
When one message format is encapsulated inside another, the system may lose access to the
information inside the data payload. Most messaging systems allow components (for example, a
Content-Based Router) to access only data fields that are part of the defined message header. If one
message is packaged into a data field inside another message, the component may not be able to
use the fields from the original message to perform routing or transformation functions.
Therefore, some data fields may have to be elevated from the original message into the message
header of the new message format.
Use a Envelope Wrapper to wrap application data inside an envelope that is compliant with the
messaging infrastructure. Unwrap the message when it arrives at the destination.
The process of wrapping and unwrapping a message consists of five steps:
? The Message Source publishes a message in a raw format. This format is typically
determined by the nature of the application and does not comply with the requirements of
the messaging infrastructure.
? The Wrapper takes the raw message and transforms it into a message format that
complies with the messaging system. This may include adding message header fields,
encrypting the message, adding security credentials etc.
? The Messaging System processes the compliant messages.
? A resulting message is delivered to the Unwrapper. The unwrapper reverses any
modifications the wrapper made. This may include removing header fields, decrypting the
message or verifying security credentials.
? The Message Recipient receives a 'clear text' message.
An envelope typically wraps both the message header and the message body or payload. We can
think of the header as being the information on the outside of the envelope -- it is used by the
messaging system to route and track the message. The contents of the envelope is the payload or
body -- the messaging infrastructure does not care much about it (within certain limitations) until
it arrives at the destination.
It is typical for wrappers to add information to the raw message. For example, before an internal
message can be sent through the postal system, a ZIP code has to be looked up. In that sense,
wrappers incorporate some aspects of a Content Enricher. However, wrappers do not enrich the
actual information content, but add information that is necessary for the routing, tracking and
handling of messages. This information can be created on the fly (e.g. creation of a unique
message ID or adding a time stamp), it can be extracted from the infrastructure (e.g. retrieval of a
security context), or the data may be contained in the original message body and the split by the
wrapper into the message header (e.g. a key field contained in the 'raw' message). The last option
is sometimes referred to as 'promotion' because a specific field is 'promoted' from being hidden
inside the body to being prominently visible in the header.
Frequently, multiple wrappers and unwrappers are chained (see Postal System example below),
taking advantage of the layered protocol model. This results in a situation where the payload of a
message contains a new envelope, which in turn wraps a header and a payload section (see
picture)
The basic SOAP message format [SOAP 1.1] is relatively simple. It specifies an envelope that
contains a message header and a message body. The following example illustrates how the body
can contain another envelope, which in turn contains another header and body. The combined
message is sent to an intermediary who unwraps the outside message and forwards the inside
message. This chaining of intermediates is very common when crossing trust boundaries. I may
encode all my messages and wrap them inside another message so that no intermediary can see
the message content or header (e.g. the address of a message may be confidential). The recipient
then unwraps the message, decodes the payload and passes the unencoded message through the
trusted environment.
While we commonly use "TCP/IP" as one term, it actually comprises two protocols. The IP
protocol provides basic addressing and routing services while TCP provides a reliable,
connection-oriented protocol that is layered on top of IP. Following the OSI layer model, TCP is
Transport protocol while IP is a Network protocol. Typically, TCP/IP data is transported over an
Ethernet network, which implements the Link layer.
As a result, application data is wrapped into a TCP envelope first, which is then wrapped into an
IP envelope, which is then wrapped into an Ethernet envelope. Since networks are
stream-oriented an envelope can consists of both a header and a trailer. The following diagram
illustrates the structure of application data traveling over the Ethernet:
Application Data is Wrapped Inside Multiple Envelopes to be Transported over the Network
If you are interested in more details about TCP/IP, [Stevens] is guaranteed to quench your thirst
for knowledge.
The Envelope Wrapper pattern can be compared to the postal system (see Figure). Let's assume an
employee creates an internal memo to a fellow employee. Any sheet of paper will be an
acceptable format for this message. In order to deliver the memo it has to be 'wrapped' into an
intra-company envelope that contains the recipient's name and department code. If the recipient
works in a separate facility, this intra-company 'message' will be stuffed into a large envelope
and mailed via the postal service. In order to make the new message comply with the USPS
requirements, it needs to feature a new envelope with ZIP code and postage. The US Postal
Service may decide to transport this envelope via air. To do so, it stuffs all envelopes for a specific
region into a mailbag, which is addressed with a bar code featuring the three-letter airport code
for the destination airport. Once the mailbag arrives at the destination airport, the wrapping
sequence is reversed until the original memo is received by the coworker. This example
illustrates the term "tunneling": Postal mail may be "tunneled" through air freight just like UDP
mulitcast packets may be tunneled over a TCP/IP connection in order to reach a different WAN
segment.
The postal system example illustrates the common practice of chaining wrappers and
unwrappers using the Pipes and Filters pattern. Messages may be wrapped by more than one step
and need to be unwrapped by a symmetric sequence of unwrapping steps. As laid out in the
Pipes and Filters pattern, keeping the individual steps independent from each other gives the
messaging infrastructure the flexibility to add or remove wrapping and unwrapping steps. For
example, encryption may no longer be required because all traffic is routed across a VPN as
opposed to the public Internet.
##%%&&
When sending messages from one system to another it is common for the target system to require
more information than the source system can provide. For example, incoming Address messages
may just contain the ZIP code because the designers felt that storing a redundant state code
would be superfluous. Likely, another system is going to want to specify both a state code and a
ZIP code field. Yet another system may not actually use state codes, but spell the state name out
because it uses free-form addresses in order to support international addresses. Likewise, one
system may provide us with a customer ID, but the receiving system actually requires the
customer name and address. An order message sent by the order management system may just
contain an order number, but we need to find the customer ID associated with that order, so we
can pass it to the customer management system. The scenarios are plentiful.
How do we communicate with another system if the message originator does not have all the
required data items available?
This problem is a special case of the Message Translator, so some of the same considerations apply.
However, this problem is slightly different from the basic examples described in the Message
Translator. The description of the Message Translator assumed that the data needed by the
receiving application is already contained in the incoming message, albeit in the wrong format. In
this new case, it is not a simple matter of rearranging fields, we actually need to inject additional
information to the message.
The Accounting System Requires More Information Than The Scheduling System Can Deliver
Let's consider the following example (see picture). A hospital scheduling system publishes a
message announcing that the patient has completed a doctor's visit. The message contain's the
patient's first name, his or her patient ID and the date of the visit. In order for the accounting
system to log this visit and inform the insurance company, it requires the full patient name, the
insurance carrier and the patient's social security number. However, the scheduling system does
not store this information, it is contained in the customer care system. What are our options?
Possible solutions for the Enricher problem
Option A: We could modify the scheduling system so it can store the additional information.
When the customer's information changes in the customer care system (e.g. because the patient
switches insurance carriers) the changes need to be replicated to the scheduling system. The
scheduling system can now send a message that includes all required information. Unfortunately,
this approach has two significant drawbacks. First, it requires a modification to the scheduling
system's internal structure. In most cases, the scheduling system is going to be a packaged
application and may not allow this type of modification. Second, even if the scheduling system is
customizable, we need to consider that we are making a change to the system based on a the
specific needs of another system. For example, if we also want to send a letter to the patient
confirming the visit we would have to change the scheduling system again to accommodate the
customer's mailing address. The integration solution would be much more maintainable if we
decouple the scheduling system from the specifics of the applications that consume the "Doctor
Visit" message.
Option B: Instead of storing the customer's information inside the scheduling system, the
scheduling system could request the SSN and carrier data from the customer care system just
before it is sending the 'Doctor Visit' message. This solves the first problem -- we no longer have
to modify the storage of the scheduling system. However, the second problem remains: the
scheduling system needs to know that the SSN and carrier information is required in order to
notify the accounting system. Therefore, the semantics of the message is more similar to 'Notify
Insurance' than 'Doctor Visit'. In a loosely coupled system we do not want one system to instruct
the next one on what to do. We rather send an Event Message and let the other systems decide
what to do. In addition, this solution couples the scheduling system more tightly to the customer
care system because the scheduling system now needs to know where to get the missing data.
This ties the scheduling system to both the accounting system and the customer care system. This
type of coupling is undesirable because it leads to brittle integration solutions.
Option C: We can avoid some of these dependencies if we send the message to the customer care
system first instead of the accounting system. The customer care system can then fetch all the
required information and send a message with all required data to the accounting system. This
decouples the scheduling system nicely from the subsequent flow of the message. However, now
we implement the business rule that a the insurance company receives a bill after the patient
visits the doctor inside the customer care system. This requires us to modify the logic inside the
customer care system. If the customer care system is a packaged application, this modification
may be difficult or impossible. Even if we can make this modification, we now make the
customer care system indirectly responsible for sending bills messages. This may not be a
problem if all the data items required by the accounting system are available inside the customer
care system. If some of the fields have to be retrieved from other systems we are in a similar
situation to where we started.
Option D (not shown): We could also modify the accounting system to only require the customer
ID and retrieve the SSN and carrier information from the customer care system. This approach
has two disadvantages. First, we now couple the accounting system to the customer care system.
Second, this option again assumes that we have control over the accounting system. In most cases,
the accounting system is going to be a packaged application with limited options for
customization.
Use a specialized transformer, a Content Enricher, to access an external data source in order to
augment a message with missing information.
The Content Enricher uses information inside the incoming message (e.g. key fields) to retrieve
data from an external source. After the Content Enricher retrieves the required data from the
resource, it appends the data to the message. The original information from the incoming
message may be carried over into the resulting message or may no longer be needed, depending
on the specific needs of the receiving application.
The additional information injected by the Content Enricher has to be available somewhere in the
system. The most common sources for the new data are:
? Computation The Content Enricher may be able to compute the missing information. In
this case, the algorithm incorporates the additional information. For example, if the receiving
system requires a state code but the incoming message only contains a ZIP code. Or, a
receiving system may require a data format that specifies the total size of the message. The
Content Enricher can add the length of all message fields and thus compute the message size.
This form of Content Enricher is very similar to the basic Message Translator because it needs
no external data source.
? Environment The Content Enricher may be able to retrieve the additional data from the
operating environment. The most common example is a time stamp. For example, the
receiving system may require each message to carry a time stamp. if the sending system does
not include this field, the Content Enricher can get the current time from the operating system
and add it to the message.
? Another System This option is the most common one. The Content Enricher has to retrieve
the missing data from another system. This data resource can take on a number of forms,
including a database, a file, an LDAP directory, a system, or a user who manually enters
missing data.
In many cases, the external resource required by the Content Enricher may be situated on another
system or even outside the enterprise. Accordingly, the communication between the Content
Enricher and the resource can occur via message channels or via any other communication
mechanism. Since the interaction between the Content Enricher and the data source is by
definition synchronous (the Content Enricher cannot send the enriched message until the data
source returns the requested data), a synchronous protocol (e.g. HTTP or an ODBC connection to
a database) may result in better performance than using asynchronous messaging. The Content
Enricher and the data source are inherently tightly coupled, so achieving loose coupling through
Message Channels is not as important.
Returning to our example, we can insert a Content Enricher to retrieve the additional data from
the customer care system (see picture). This way, the scheduling system is nicely decoupled from
having to deal with insurance information or the customer care system. All it has to do is publish
the 'Doctor Visit' message. The Content Enricher component takes care of retrieving the required
data. The accounting system also remains independent from the customer care system.
Applying the Enricher to the Patient Example
The Content Enricher is used in many occasions to resolve references contained in a message. In
order to keep message small and easy to manage, often times we choose to pass simple references
to objects rather than passing a complete object with all data elements. These references usually
take the form of keys or unique IDs. When the message needs to be processed by a system, we
need to retrieve the required data items based on the object references included in the original
message. We use a Content Enricher to perform this task. There are some apparent trade-offs
involved. Using references reduces the data volume in the original messages, but requires
additional look-ups in the resource. Whether the use of references improves performance
depends on how many components can operate simply on references versus how many
components need to use an Content Enricher to restore some of the original message content. For
example, if a message passes through a long list of intermediaries before it reaches the final
recipient, using an object reference can decrease message traffic significantly. We can insert a
Content Enricher as the last step before the final recipient to load the missing information into the
message. If the message already contains data that we we might not want to carry along the way,
we can use the Claim Check to store the data and obtain a reference to it.
A Content Enricher is also commonly used when communicating with external parties that require
messages to be compliant with a specific message standard (e.g. ebXML). Most of these standards
require large messages with a long list of data. We can usually simplify our internal operations
significantly if we keep internal messages as simple as possible and then use a Content Enricher to
add the missing fields whenever we send a message outside of the organization. Likewise, we
can use a Content Filter to strip unnecessary information from incoming messages (see picture).
##%%&&
The Content Enricher helps us in situations where a message receiver requires more - or different -
data elements than the message creator provides. There are surprisingly many situations where
the opposite effect is desired: removing data elements from a message.
How do you simplify dealing with a large message, when you are interested only in a few data
items?
Why would we want to remove valuable data elements from a message? One common reason is
security. A requestor of data may not be authorized to see all data elements that a message
contains. The service provider may not have knowledge of a security scheme and always return
all data elements regardless of user identity. We need to add a step that removes sensitive data
based on the requestor's proven identity. For example, the payroll system may expose only a
simple interface that returns all data about an employee. This data may include payroll
information, social security numbers and other sensitive information. If you are trying to build a
service that returns an employee's start date with the company you may want to eliminate all
sensitive information from the result message.
Another reason to remove data elements is to simplify message handling and to reduce network
traffic. In many instances, processes are initiated by messages received from business partners.
For obvious reasons, it is desirable to base communication with third parties on a standardized
message format. A number of standards bodies and committees define standard XML data
formats for certain industries and applications. Well-known examples are RosettaNet, ebXML,
ACORD and many more. While these XML formats are useful to conduct interaction with
external parties based on an agreed-upon standard, the documents can be very large. Many of the
documents have hundreds of fields, consisting of multiple nested levels. Such large documents
are difficult to work with for internal message exchange. For example, most visual (drag-drop
style) transformation tools become unusable if the documents to be mapped have hundreds of
element. Also, debugging becomes a major nightmare. Therefore, we want to simplify the
incoming documents to include only the elements we actually require for our internal processing
steps. In a sense, removing elements enriches the usefulness of such a message, because
redundant and irrelevant fields are removed, leaving a more meaningful message and less room
for developer mistakes.
Use a Content Filter to remove unimportant data items from a message leaving only important
items.
The Content Filter does not necessarily just remove data elements. A Content Filter is also useful to
simplify the structure of the message. Often times, messages are represented as tree structures.
Many messages originating from external systems or packaged applications contain many levels
of nested, repeating groups because they are modeled after generic, normalized database
structures. Frequently, known constraints and assumptions make this level of nesting
superfluous and a Content Filter can be used to 'flatten' the hierarchy into a simple list of elements
than can be more easily understood and processed by other systems.
Multiple Content Filters can be used as a Filtering Splitter (see Splitter) to break one complex
message into individual messages that each deal with a certain aspect of the large message.
Many integration suites provide Channel Adapters to connect to existing systems. In many cases,
these adapters publish messages whose format resembles the internal structure of the application.
For example, let's assume we connect a database adapter to a database with the following
schema:
It is very typical for a physical database schema to store related entities in separate tables that are
linked by foreign keys and relation tables (e.g. ACCOUNT_CONTACT links the ACCOUNT and
CONTACT tables). Many database adapters will translate these related tables into a hierarchical
message structure that can contain additional fields such as primary and foreign keys that may
not be relevant to the message receiver. In order to make processing a message easier, we can use
a Content Filter to flatten the message structure and extract only relevant fields. The example
shows the implementation of a Content Filter using a visual transformation tool. We can see how
we reduce the message from over a dozen fields spread across multiple levels into a simple
message with five fields. It will be much easier (and more efficient) for other components to work
with the simpler message.
A Content Filter is not the only solution to this particular problem. For example, we could
configure a view in the database that resolves the table relationships and returns a simple result
set. This may be a simple choice if we have the ability to add views to the database. In many
situations, enterprise integration aims to be as little intrusive as possible and that guideline may
include not adding views to a database.
##%%&&
The Content Enricher tells us how we can deal with situations where our message is missing
required data items. The Content Filter lets us remove uninteresting data items from a message.
Sometimes, we want to remove fields only temporarily. For example, a message may contain a
set of data items that may be needed later in the message flow, but that are not necessary for all
intermediate processing steps. We may not want to carry all this information through each
processing step because it may cause performance degradation and makes debugging harder
because we carry so much extra data.
How can we reduce the data volume of message sent across the system without sacrificing
information content?
Moving large amounts of data via messages may be inefficient. Some messaging systems even
have hard limits as to the size of messages. Other messaging systems use an XML representation
of data, which can increase the size of a message by an order of magnitude or more. So while
messaging provides the most reliable and responsive ay to transmit information, it may not be
the most efficient.
A simple Content Filter helps us reduce data volume but does no guarantee that we can restore
the message content later on. Therefore, we need to store the complete message information in a
way that we can retrieve it later.
Because we need to store data for each message, we need a key to retrieve the correct data items
associated with a message. We could use the message ID as the key, but that would not allow
subsequent components to pass the key on because the message ID changes with each message.
Store message data in a persistent store and pass a Claim Check to subsequent components.
These components can use the Claim Check to retrieve the stored information.
The Claim Check pattern consists of the following steps:
1. A message with data arrives.
2. The "Check Luggage" component generates a unique key for the information. This key
will be used later as the Claim Check
3. The Check Luggage component extracts the data from the message and stores it in a
persistent store, e.g. a file or a database. It associates the stored data with the key .
4. It removes the persisted data from the message and adds the Claim Check.
5. Another component can use a Content Enricher to retrieve the data based on the Claim
Check.
This process is analogous to a luggage check at the airport. If you do not want to carry all your
luggage with you, you simply check it with the airline counter. In return you receive a sticker on
your ticket that has a reference number that uniquely identifies each piece of luggage you
checked. Once you reach your final destination, you can retrieve your luggage.
As the picture illustrates, the data that was contained in the original message still needs to be
"moved" to the ultimate destination. So did we gain anything? Yes, because transporting data via
messaging may be less efficient than storing it in a central data store. For example, the message
may undergo multiple routing steps that do not require the large amount of data. Using a
messaging system, the message data would be marshaled and unmarshaled, possibly encrypted
and decrypted at every step. This type of operation can be very CPU intensive and would be
completely unnecessary of the data is not needed by an intermediate step but only by the final
destination. The Claim Check also works well in a scenario where a message travels through a
number of components and returns to the sender. In this case, the "Check Luggage" component
and the Content Enricher are local to the same component and the data never has to travel across
the network (see below):
How should we choose a key for the data? A number of options spring to mind:
? A business key may already be contained in the message body, e.g. a Customer ID.
? The message may contain a message ID that can be used to associate the data in the data
store with the message.
? We can generate a unique ID.
Reusing an existing business key seem like the easiest choice. If we have to stow away some
customer detail we can reference it later by the customer ID. When we pass this key to other
components we need to decide whether we want these components to be aware that the key is a
customer ID as opposed to just an abstract key. Representing the key as an abstract key has the
advantage that we can process all keys in the same way and can create a generic mechanism to
retrieve data from the data store based on an abstract key.
Using the message ID as the key may seem convenient but is generally not a good idea. Using a
message ID as a key for data retrieval results in dual semantics being attached to a single data
element and can cause conflicts. For example, let's assume we need to pass the reference on to
another message. The new message is supposed to be assigned a new, unique ID, but then we
can't use that new ID anymore to retrieve data from the data store. The use of a message ID can
be meaningful only in a circumstance where we want the data to be accessible only within the
scope of the single message. Therefore, in general it is better to assign a new element to hold the
key and avoid this bad form of 'element reuse'.
The data may be stored in the data store only temporarily. How do we remove unused data? We
can modify the semantics of the data retrieval from the data store to delete the data when it is
read. In this case we can retrieve the data only once, which may actually be desirable in some
cases for security reasons. However, it does not allow multiple components to access the same
data. Alternatively, we can attach an expiration date to the data and define a 'garbage collection'
process that periodically removes all data over a certain age. As a third option, we may not want
to remove any data. This may be the case because we use a business system as the data store (e.g.
an accounting system) and need to maintain all data in that system.
The implementation of a data store can take on various forms. A database is an obvious choice,
but a set of XML files or an in-memory message store can serve as a data store just as well.
Sometimes, we may use an application as the data store. It is important that the data store is
reachable by components in other parts of the integration solution so that these parts can
reconstruct the original message.
While the original intent of the Claim Check is to avoid sending around large volumes of data, it
can also serve other purposes. Often times we want to remove sensitive data before sending a
message to an outside party (see picture). This accomplishes that outside parties receive data only
on a 'need to know basis'. For example when we send employee data to an external party we may
prefer not to reference employees by some magic unique ID and eliminate fields such as social
security number. After the outside party has completed the required processing, we reconstruct
the complete message by merging data from the data store and the message returned from the
outside party. We may even generate special unique keys for these messages so that we restrict
the actions the outside party can take by the key it possesses. This will restrict the outside party
from maliciously feeding messages into our system. Messages containing an invalid (or expired
or already used) key will be blocked by the Content Enricher attempting to retrieve message data
using the key.
If we interact with more than one external party, a Process Manager can serve as a Claim Check. A
Process Manager creates process instances (sometimes called 'tasks' or 'jobs') when a message
arrives. The Process Manager allows additional data to be associated with each individual process
instance. In effect, the process engine now serves as the data store, storing our message data. This
allows the Process Manager to send messages to external parties that contain only the data relevant
to that party. The messages do not have to carry all the information contained in the original
message since that information is kept with the process' data store. When the Process
Manager receives a response message from an external party it re-merges the new data with the
data stored by the process instance.
##%%&&
In a business-to-business (B2B) integration scenario it is quite common for an enterprise to receive messages from
different business partners. These message may have the same meaning, but follow different formats, depending on the
partners' internal systems and preferences. For example, we built a solution for a pay-per-view provider that has to
accept and process viewership information from over 1700 (!) affiliates, most of which did not conform to a standard
format.
How do you process messages that are semantically equivalent, but arrive in a different format?
The easiest solution from a technical perspective may seem to dictate a uniform format on all participants. This may
work if the business is a large corporation and has control over the B2B exchange or the supply channel. For example,
if General Motors would like to receive order status updates from their suppliers in a common message format, we can
be pretty sure that Joe's Supplier business is likely to conform to the 'guidelines'. In many other situations, however, a
business is not going to have such a luxury. In the contrary, many business models position the message recipient as an
'aggregator' of information and part of the agreement with the individual participants is that a minimum of changes is
required to their systems infrastructure. As a result, you find the aggregator willing to process information arriving in
any data format ranging from EDI records or comma separated files to XML documents or Excel spreadsheets arriving
via e-mail.
One important consideration when dealing with a multitude of partners is the rate of change. Not only may each
participant prefer a different data format to begin with -- the preferred format may also change over time. In addition,
new participants may join while others drop off. Even if a specific partner makes changes to the data format only once
every couple of years, dealing with a few dozen partners can quickly result in monthly or weekly changes. It is
important to isolate these changes from the rest of the processing as much as possible to avoid a "ripple-effect" of
changes throughout the whole system.
To isolate the remainder of the system from the variety of incoming message formats, you need to transform the
incoming messages into a common format. Because the incoming messages are of different types, you need a different
Message Translator for each message data format. The easiest way to accomplish this is to use a collection of Datatype
Channels, one for each message type. Each Datatype Channel is then connected to a different Message Translator.
The drawback of this approach is that a large number of message formats translates into an equally large number of
message channels.
Use a Normalizer to route each message type through a custom Message Translator so that the resulting
messages match a common format.
The Normalizer uses a Message Router to route the incoming message to the correct Message Translator. This
requires the Message Router to detect the type of the incoming message. Many messaging systems equip each message
with a type specifier field in the Message Header to make this type of task simple. However, in many B2B scenarios
messages do not arrive as messages compliant with the enterprise's internal messaging system, but in diverse formats
such as comma separated files or XML document without associated schema. While it is certainly best practice to
equip any incoming data format with a type specifier we know all too well that the world is far from perfect. As a result,
we need to think of more general ways to identify the format of the incoming message. One common way for
schema-less XML documents is to use the name of the root element to assume the correct type. If multiple data
formats use the same root element, you can use XPATH expressions to determine the existence of specific sub-nodes.
Comma-separated files can require a little more creativity. Sometimes you can determine the type based on the number
of fields and the type of the data (e.g. numeric vs. string). If the data arrives as files, the easiest way may be to use the
file name or the file folder structure as a surrogate Datatype Channel. Each business partner can name the file with a
unique naming convention. The Message Router can then use the file name to route the message to the appropriate
Message Translator.
Adding a Message Router also allows the same transformation to be used for multiple business partners. That might be
useful if multiple business partners use the same format or if a transformation is generic enough to accommodate
multiple message formats. For example, x-path expressions are great at picking out elements from XML documents
even if the documents vary in format.
Since a Normalizer is a common occurrence in messaging solutions, we created a short-hand icon for it:
##%%&&
I am designing several applications to work together through Messaging. Each application has its
own internal data format.
How can you minimize dependencies when integrating applications that use different data
formats?
Independently developed applications tend to use different data formats because each format
was designed with just that application in mind. When an application is designed to send
messages to or receive messages from some unknown application, the application will naturally
use the message format that is most convenient for it. Likewise, commercial adapters used to
integrate packaged applications using publish and consume messages in a data format that
resembles the application's internal data structure.
The Message Translator resolves differences in message formats without changing the applications
or having the applications know about each other's data formats. However, if a large number of
applications communicate with each other, one Message Translator may be needed between each
pair of communicating applications (see picture).
This approach requires a large number of Message Translators, especially when considering that
each integrated application may publish or consume multiple message types. The number of
required Message Translators increases exponentially with the number of integrated applications
which quickly becomes unmanageable.
While the Message Translator provides an indirection between the message formats used by two
communicating applications it is still dependent on the message formats used by either
application. As a result, if an application's data format changes, all Message Translators between
the changing application and all other applications that it communicates with have to change.
Likewise, if a new application is added to the solution, new Message Translators have to be created
from each existing application to the new application in order to exchange messages. This
situation creates a nightmare out of having to maintain all Message Translators.
We also need to keep in mind that each additional transformation step injected into a message
flow can increase latency and reduce message throughput.
Therefore, design a Canonical Data Model that is independent from any specific application.
Require each application to produce and consume messages in this common format.
The Canonical Data Model provides an additional level of indirection between application's
individual data formats. If a new application is added to the integration solution only
transformation between the Canonical Data Model has to created, independent from the number of
applications that already participate.
The use a Canonical Data Model seems complicated if only a small number of applications
participate in the integration solution. However, the solution quickly pays off as the number of
applications increases. If we assume that each application sends and receives message to and
from each other application, a solution consisting of 2 applications would require only 2 Message
Translators if we translate between the applications' data formats directly whereas the Canonical
Data Model requires 4 Message Translators. A solution consisting of 3 applications requires 6
Message Translators with either approach. However, a solution consisting of 6 applications
requires 30(!) Message Translators without a Canonical Data Model and only 12 Message Translators
when using a Canonical Data Model.
The use of a Canonical Data Model can also be very useful if an existing application is likely to be
replaced by another application in the future. For example, is a number of applications interface
with a legacy system that is likely to be replaced by a new system in the future, the effort of
switching from one application to the other is much reduced if the concept of a Canonical Data
Model is built into the original solution.
How do you make applications conform to the common format? You have three basic choices:
? Change the applications internal data format. This may be in theory possible, but is
unlikely in a complex real-life scenario. If it was easy to just make each application to
natively use the same data format, we would be better off using Shared Database and not
Messaging.
? Implement a Messaging Mapper inside the application. Custom applications can use a
mapper to generate the desired data format.
? Use an external Message Translator. You can use an external Message Translator to
translate from the app-specific message format into the format specified by the Canonical Data
Model.
Whether to use a Messaging Mapper or an external Message Translator depends on the complexity
of the transformation and the maintainability of the application. Packaged applications usually
eliminate the use of a Messaging Mapper because the source code is not available. For custom
applications the choice depends on the complexity of the transformation. Many integration tool
suites provide visual transformation editors that allow faster construction of mapping rules.
However, these visual tools can get unwieldy if transformations are complex.
The use of a Canonical Data Model does introduce a certain amount of overhead into the message
flow. Each message now has to undergo two translation steps instead of one -- one translation
from the source application's format into the common format and from the common format into
the target application's format. For this reason, the use of a Canonical Data Model is sometimes
referred to as "double translation" (transforming directly from one application's format to the
other is called "direct translation"). Each translation step causes additional latency in the flow of
messages. Therefore, for very high throughput systems direct translation can be the only choice.
This trade-off between maintainability and performance is common. The best advice is to use the
more maintainable solution (i.e., the Canonical Data Model) unless performance requirements do
not allow it. A mitigating factor may be that fact that many translations are stateless and
therefore lend themselves to load balancing with multiple Message Translators executing in
parallel.
Designing a Canonical Data Model can be difficult. Designers should strive to make the unified
model work equally well for all applications being integrated, but in reality this ideal is difficult
to achieve. The chances of successfully definition a Canonical Data Model improve when
considering that the Canonical Data Model does not have to model the complete data model of all
applications, but only the portion that participates in messaging (see picture). This can
significantly reduce the complexity of creating the Canonical Data Model.
Using a Canonical Data Model can also have political advantages. Using a Canonical Data Model
allows developers and business users to discuss the integration solution in terms of the
company's business domain, not a specific package implementation. For example, packaged
applications may represent the common concept of a customer in many different internal formats,
such as 'account', 'payer', 'contact' etc. Defining a Canonical Data Model is often the first step to
resolving case of semantic dissonance between applications (see [Kent].
The picture in the pattern introduction showing the large number of transformers needed to
translate between each and every application looks surprisingly similar to the picture in
Introduction to Message Routing. This reminds us that dependencies between applications can exist
at multiple levels. The use of Message Channels provides a common transport layer between
applications and removes dependencies between application's individual transport protocols.
Message Routers can provide location-independence so that a sending application does not have
to depend on the location of the receiving application. The use of a common data representation
such as XML removes dependencies on any application-specific data types. Finally, the Canonical
Data Model resolves dependencies in the data formats and semantics used by the applications.
Since the canonical format is likely to change over time, it should specify a Format Indicator.
When accessing an external service from your application the service may already specify a
Canonical Data Model to be used. In the world of XML Web services, the data format is specified
by a WSDL document, the Web Services Definition Language (see [WSDL 1.1]. The WSDL
specifies the structure of request and reply messages that the service can consume and produce.
In most cases, the data format specified in the WSDL is different than the internal format of the
application providing the service. Effectively, the WSDL specifies a Canonical Data Model to be
used by both parties participating in the conversation. The double-translation consists of a
Messaging Mapper or a Gateway in the service consumer and a Remote Facade [GoF] in the service
provider.
Many EAI tool suites provide a complete set of tools to define and describe the canonical data
format. For example, the TIBCO ActiveEnterprise suite provides the TIB/Designer that allows
the user to inspect all common message definitions. Message definitions can be imported from or
exported to XML schema definitions. When implementing a Message Translator using built-in
visual tool set, the tool presents the designer with both the application specific data format and
the Canonical Data Model stored in the central data format repository.
##%%&&
An application accesses another system via Messaging.
How do you encapsulate access to the messaging system from the rest of the application?
Most custom applications access the messaging infrastructures through a vendor-supplied API.
While there are many different flavors of such API's, these libraries generally expose similar
functions, such as "open channel", "create message", "send message". While this type of API
allows the application to send any kind of message data across any kind of channel, it is
sometimes hard to tell what the intent of sending the message data is.
Messaging solutions are inherently asynchronous. This can complicate the code to access an
external function over messaging. Instead of calling a method GetCreditScore that returns the
numeric credit score, the application has to send the request message and expect the reply
message to arrive at a later time (see Request-Reply). The application developer may prefer the
simple semantics of a synchronous function as opposed dealing with incoming message events.
Loose coupling between applications provides architectural advantages, such as resilience to
minor changes in message formats (i.e., adding fields). Usually, the loose coupling is achieved by
using XML documents or other data structures that are not strongly typed like a Java or C# class.
Coding against such structures is tedious and error-prone because there is no compile-type
support to detect misspelled field names or mismatching data types. Therefore, we often gain the
flexibility in data formats at the expense of application development effort.
Sometimes, a simple logical function to be executed via messaging requires more than one
message to be sent. For example, a function to get customer information may in reality require
multiple messages, one to get the address, another to get the order history and yet another to get
personal information. Each of these messages may be processed by a different system. We would
not want to clutter the application code with all the logic required to send and receive three
separate message. We could take some of the burden off the application by using a Scatter-Gather
that receives a single message, sends three messages separate messages and aggregates them
back into a single reply message. However, not always do we have the luxury of adding this
function to the messaging middleware.
Use a Messaging Gateway, a class than wraps messaging-specific method calls and exposes
domain-specific methods to the application.
The Messaging Gateway encapsulates messaging-specific code (e.g., the code required to send or
receive a message) and separates it from the rest of the application code. This way, only the
Messaging Gateway code knows about the messaging system; the rest of the application code does
not. The Messaging Gateway exposes a business function to the rest of the application so that
instead of requiring the application to set properties like
Message.MessageReadPropertyFilter.AppSpecific, a Messaging Gateway exposes methods such as
GetCreditScore that accept strongly typed parameters just like any other method. A Messaging
Gateway is a messaging-specific version of the more general Gateway pattern [EAA].
A Gateway Eliminates Direct Dependencies Between the Application and the Messaging Systems
A Messaging Gateway sits between the application and the messaging system and provides a
domain-specific API to the application (see picture). Because the application doesn't even know
that it's using a messaging system, we can swap out the gateway with a different implementation
that uses another integration technology, such as remote procedure calls or Web services.
Many Messaging Gateways send a message to another component and expect a reply message
(Request-Reply). Such a Messaging Gateway can be implemented in two different ways:
? Blocking (Synchronous) Messaging Gateway
? Event-Driven (Asynchronous) Messaging Gateway
A blocking Messaging Gateways sends out a message and waits for the reply message to arrive
before returning control to the application. When the gateway receives the reply, it processes the
message and returns the result to the application (see sequence diagram).
Blocking (Synchronous) Messaging Gateway
A blocking Messaging Gateway encapsulates the asynchronous nature of the messaging interaction,
exposing a regular synchronous method to the application logic. Thus, the application is unaware
of any asynchronicity in the communication. For example, a blocking gateway may expose the
following method:
While this approach makes writing application code against the Messaging Gateway very simple,
it can also lead to poor performance because the application ends up spending most of its time
sitting around and waiting for reply messages while it could be performing other tasks.
An event-driven Messaging Gateway exposes the asynchronous nature of the messaging layer to
the application. When the application makes the domain-specific request to the Messaging
Gateway it provides a domain-specific callback for the reply. Control returns immediately to the
application. When the reply message arrives, the Messaging Gateway processes it and then invokes
the callback (see sequence diagram).
For example, in C# using delegates, the Messaging Gateway could expose the following public
interface:
The method RequestCreditScore accepts an additional parameter that specifies the callback
method to be invoked when the reply message arrives. The callback method has a parameter
CreditScore so that the Messaging Gateway can pass the results to the application. Depending on
the programming language or platform, the callback can be accomplished with function pointers,
object references or delegates (as shown here). Note that despite the event-driven nature of this
interface there is no dependency at all on a specific messaging technology.
Alternatively, the application can periodically poll to see whether the results arrived. This
approach makes the higher-level interface simple without introducing blocking, essentially
employing the Half-sync/Half-async pattern [POSA2]. This pattern describes the use of buffers
that store incoming messages so that the application can poll at its convenience to see whether a
message has arrived.
One of the challenges of using an event-driven Messaging Gateway is that the Messaging Gateway
requires the application to maintain state between the request method and the callback event (the
call stack takes care of this in the blocking case). When the Messaging Gateway invokes the
callback event into the application logic, the application needs to be able to correlate the reply
with the request it made earlier so that it can continue processing the correct thread of execution.
The Messaging Gateway can make it easier for the application to maintain state if it allows the
application to pass a reference to an arbitrary set of data to the request method. The Messaging
Gateway will then pass this data back to the application with the callback. This way, the
application has all necessary data available when the asynchronous callback is invoked. This type
of interaction is commonly called ACT (Asynchronous Completion Token) [POSA2].
The public interface of an event-driven Messaging Gateway that supports an ACT may look like
this:
While supporting an ACT is a very convenient feature for the application, it does introduce the
danger of a memory leak if the Messaging Gateway maintains a reference to an object but the
expected reply message never arrives.
It can be beneficial to create more than one layer of Messaging Gateways. The "lower-level"
Messaging Gateway can simply abstract the syntax of the messaging system but maintain generic
messaging semantics, e.g. "SendMessage". This Messaging Gateway can help shield the rest of the
application when the enterprise changes messaging technologies, e.g. from MSMQ to Web
Services. We wrap this basic Messaging Gateway with an additional Messaging Gateway that
translates the generic messaging API into a narrow, domain-specific API, such as GetCreditScore.
We use this configuration in the MSMQ implementation of the Loan Broker example (see
Asynchronous Implementation with MSMQ, figure below).
Besides making coding the application simpler, the intent of the Messaging Gateway is also to
eliminate dependencies of the application code on specific messaging technologies. This is easy to
do by wrapping any messaging-specific method calls behind the Messaging Gateway interface.
However, most messaging layers tend to throw messaging-specific exceptions, e.g. the
InvalidDestinationException exception raised by JMS. If we really want to make our application
code independent from the messaging library, the Messaging Gateway has to catch any messaging
specific exception and throw an application-specific (or a generic) exception instead. This code
can get a little tedious, but it is very helpful if we ever have to switch the underlying
implementations, e.g. from JMS to Web services.
In many situations, we can generate the Messaging Gateway code from metadata exposed by the
external resource. This is common on the world of Web services. Almost every vendor or
open-source platform provides a tool such as wsdl2java that connects to the Web Service
Description Language (WSDL) exposed by an external web service. The tool generates Java (or
C# or whatever language you need) classes that encapsulate all the nasty SOAP stuff and expose
a simple function call. We created a similar tool that can read message schema definitions off the
TIBCO repository and creates Java source code for a class that mimics the schema definition. This
allows application developers to send correctly-typed TIBCO ActiveEnterprise messages without
having to learn the TIBCO API.
Messaging Gateways make great testing vehicles. Because we wrapped all the nasty messaging
code behind a narrow, domain specific interface, we can easily create a dummy implementation
of this interface. We simply separate interface and implementation and provide two
implementations: one "real" implementation that accesses the messaging infrastructure and a
"fake" implementation for testing purposes (see picture). The "fake" implementations acts as a
Service Stub (see [EAA]) and allows us to test the application without any dependency on
messaging. A Service Stub can also be useful to debug an application that uses an event-driven
Messaging Gateway. For example, a simple test stub for an event-driven Messaging Gateway can
simply invoke the callback (or delegate) right from the request method, effectively executing both
the request and the response processing in one thread. This can simplify step-by-step debugging
enormously.
Gateways as a Testing Tool
This example shows a piece of the Loan Broker example introduced in the Composed Messaging
Interlude (see Asynchronous Implementation with MSMQ).
You will notice that the public method GetCreditScore and the public delegate
OnCreditReplyEvent make no references to messaging at all. This implementation allows the
calling application to pass an arbitrary object reference as an Asynchronous Completion Token.
The CreditBureauGateway stores this object reference in a dictionary indexed by the Correlation
Identifier of the request message. When the reply message arrives, the CreditBureauGateway can
retrieve the data that was associated with the outbound request message. The calling application
does not have to worry about how the messages are correlated.
##%%&&
When integrating applications using messaging, the data inside a message is often derived from
domain objects inside the integrated applications. If we use a Document Message, the message
itself may directly represent one or domain objects. If we use a Command Message, some of the
data fields associated with the command are likely to be extracted from domain objects as well.
There are some distinct differences between messages and objects. For example, most objects rely
on associations in the form of object references and inheritance relationships. Many messaging
infrastructures do not support these concepts because they have to be able to communicate with a
range of applications, some of which may not be object-oriented at all.
How do you move data between domain objects and the messaging infrastructure while
keeping the two independent of each other?
Why can't we make our messages look exactly like the domain objects and make the problem go
away? In many cases, we are not in control of the message format because it is defined by a
Canonical Data Model or a common messaging standard (e.g. ebXML). We could still publish the
message in a format that corresponds to the domain object and use a Message Translator inside the
messaging layer to make the necessary transformation to the common message format. This
approach is commonly used by adapters to third-party systems that do not allow transformation
inside the application (e.g., a database adapter).
Alternatively, the domain can create and publish a message in the required format without the
need for a separate Message Translator. This option most likely results in better performance
because we do not publish an intermediate message. Also, if our domain model contains many
small objects it may be beneficial to combine them into a single message first to simplify routing
and improve efficiency inside the messaging layer. Even if we can afford the additional
transformation step, we will run into limitations if we want to create messages that mimic
domain objects. The shortcoming of this approach is that the domain must know the message
format, which makes domain maintenance difficult if multiple formats can be used or if the
format changes.
Most messaging infrastructures support the notion of a "Message" object as part of the
application programming interface (API). This message object encapsulates the data to be sent
over a channel. In most cases, this message object can contain only scalar data types such as
strings, numbers, or dates, but does not support inheritance or object references. This is one of the
key differences between RPC-style communications (i.e. RMI) and asynchronous messaging
systems. Let's assume we send an asynchronous message containing an object reference to a
component. In order to process the message, the component would have to resolve the object
reference. It would do this by requesting the object from the message source. However,
request-reply interaction would defeat some of the motivations of using asynchronous messaging
in the first place, i.e. loose coupling between components. Worse yet, by the time the
asynchronous message is received by the subscriber, the referenced object may no longer exist in
the source system.
One attempt to resolve the issue of object references is to traverse the dependency tree of an
object and include all dependent objects in the message. For example, if an Order object references
five OrderItem objects, we would include the five objects in the message. This will ensure that the
receiver has access to all data references by the "root" object. If we use a fine-grained domain
object model in which many objects are interrelated, messages can quickly explode in size. It
would desirable to have more control over what is included in a message and what is not.
Let's assume for a moment that our domain object is self-contained and does not have any
references to other objects. We still cannot simply stick the whole domain object into a message as
most messaging infrastructures do not support objects because they have to be language
independent (the JMS interface ObjectMessage and the Message class inside .NET's
System.Messaging namespace are exceptions since these messaging systems are either language
(Java) or platform (.NET CLR) specific). We could think of serializing the object into a string and
store it into a string field called "data," which is supported by pretty much every messaging
system. However, this approach has disadvantages as well. First, a Message Router would not be
able to use object properties for routing purposes because this string field would be 'opaque' to
the messaging layer. It would also make testing and debugging difficult, because we would have
to decipher the contents of the 'data' field. Also, constructing all messages so that they just
contain a single string field would not allow us to route messages by message type because to the
infrastructure all messages look the same. It would also be difficult to verify the correct format of
the message because the messaging infrastructure would not verify anything inside the "data"
field. Lastly, we would not be able to use the serialization facilities provided by the language
run-time libraries because these presentation are usually not compatible across languages. So we
would have to write our own serialization code.
Some messaging infrastructures now support XML fields inside messages so that we could
serialize objects into XML. This can alleviate some of the disadvantages because the messages are
easier to decipher now and some messaging layers can access elements inside and XML string
directly. However, we now have to deal with quite verbose messages and limited data type
validation. Plus, we still have to create code that translates an object into XML and back.
Depending on the programming language we use, this could be quite complex, especially if we
use an older language that does not support reflection.
We are well advised to separate this mapping code from the domain object for a number of
reasons. First of all, we may not want to blend code that concerns itself with low-level language
features with application logic. In many cases, we will have a group of programmers dedicated to
working with the messaging layer, while another group focuses on the domain logic. Sticking
both pieces of code into one object will make it difficult for the teams to work in parallel.
Second, incorporating mapping code inside the domain object makes the domain object
dependent on the messaging infrastructure because the mapping code will need to make calls
into the messaging API (e.g. to instantiate the Message object). In most cases this dependency is
not desirable because it prevents the reuse of the domain objects in another context that does not
use messaging or uses another vendor's messaging infrastructure. As a result, we would
seriously impede the reusability of the domain objects.
We often see people write "abstraction layers" that wrap the messaging infrastructure API,
effectively making the code that deals with messaging independent from the messaging API.
Such a layer provides a level of indirection because it separates the messaging interface from the
messaging implementation. Therefore, we can reuse the messaging-related code even if we have
to switch to another vendor's messaging layer. All we need to do is implement a new abstraction
layer that translates the messaging interface to the new API. However, this approach does not
resolve the dependency of the domain objects on the messaging layer. The domain objects would
now contain references to the abstracted messaging interface as opposed to the vendor-specific
messaging API. But we still cannot use the domain objects in a context that does not use
messaging.
Many messages are composed of more than one domain object. Since we cannot pass object
references through the messaging infrastructure, it is likely that we need to include fields from
other objects. In some cases, we may include the whole "dependency tree" of all dependent
objects inside one message. Which class should hold the mapping code? The same object may be
part of multiple message types, combined with different objects, so there is no easy answer to this
question.
Create a separate Messaging Mapper that contains the mapping logic between the messaging
infrastructure and the domain objects. Neither the objects nor the infrastructure have
knowledge of the Messaging Mapper's existence.
The Messaging Mapper accesses one or more domain objects and converts them into a message as
required by the messaging channel. It also performs the opposite function, creating or updating
domain objects based on incoming messages. Since the Messaging Mapper is implemented as a
separate class that references the domain object(s) and the messaging layer, neither layer is aware
of the other. The layers don't even know about the Messaging Mapper.
The Messaging Mapper is a specialization of the Mapper pattern defined in [EAA]. It shares some
analogies with the Data Mapper defined in the same book. Anyone who has worked on O-R
(Object-Relational) Mapping strategies will understand the complexities of mapping data
between layers that use different paradigms. The issues inherent in the Messaging Mapper are
similarly complex and a detailed discussion of all possible aspects is beyond the scope of this
book. Many of the Data Source Architectural Patterns in [EAA] make a good read for anyone
concerned with creating a Messaging Mapper layer.
A Messaging Mapper is different from the frequently used concept of an abstraction layer wrapped
around the messaging API. In the case of an abstraction layer, the domain objects do not know
about the messaging API, but they do know about the abstraction layer (the abstraction layer
essentially performs the function of a Gateway (see [EAA]). In the case of a Messaging Mapper, the
objects have no idea whatsoever that we are dealing with messaging.
The intent of a Messaging Mapper is similar to that of a Mediator ([GoF]) which is also used to
separate elements. In the case of a Mediator, though, the elements are aware of the Mediator
whereas neither element is aware of the Messaging Mapper.
If neither the domain objects nor the messaging infrastructure know about the Messaging Mapper
how does it get invoked? In most cases, the Messaging Mapper is invoked through events
triggered either by the messaging infrastructure or the application. Since neither one is
dependent on the Messaging Mapper the event notification can either happen through a separate
piece of code or by making the Messaging Mapper an Observer pattern (see [GoF]). For example, if
we use the JMS API to interface with the messaging infrastructure we can implement the
MessageListener interface to be notified of any incoming messages. Likewise, we can use an
Observer to be notified of any relevant events inside the domain objects and to invoke the
Messaging Mapper. If we have to invoke the Messaging Mapper directly from the application, we
should define a Messaging Mapper interface so that the application does at least not depend on the
Messaging Mapper implementation.
Some Messaging Mapper implementations may contain a lot of repetitive code: get a field from the
domain object and store it in a message object. Go on to the next field and repeat until all fields
are done. This can be pretty tedious and also smells suspiciously like code duplication. We have a
number of tools to help us avoid this tedium. First, we can write generic a Messaging Mapper that
uses reflection to extract fields from a domain object in a generic way. For example, it could
traverse the list of all fields inside the domain object and store it in a field of the same name in the
message object. Obviously, this works only if the field names match. According to our previous
discussions, we need to come up with some way to resolve object references since we cannot
store those in the message object. The alternative is to use a configurable code generator to
generate the Messaging Mapper code. This allows us some more flexibility in the field naming (the
message field name and the domain object field name do not have to match and we can device
clever ways to deal with object references. The downside of code generators is that they can be
difficult to test and debug, but if we make it generic enough we have to write it only once.
Some frameworks, such as Microsoft .NET, feature built-in object serialization of objects into
XML and vice versa and take away of lot of the grunt work involved in object serialization. Even
if the framework does some of the legwork of converting an object into a message, this
conversion is limited to the 'syntactic' level of translation. It might be tempting to just let the
framework do all the work, but it will just create messages that correspond to the domain objects
one-to-one. As we explained earlier, this may not be desirable because the constraints and design
criteria for messages are quite different from those for domain objects. It may make sense to
define a set of 'interface objects' that correspond to the desired messages structure and let the
framework do the conversion between the messages and these objects. The Messaging Mapper
layer will then manage the translation between the true domain objects and the interface objects.
These interface objects bear some resemblance to Data Transfer Objects ([EAA]) even though the
motivations are slightly different.
Even if we use a Messaging Mapper it still makes sense to use a Message Translator to translate the
messages generated by the Messaging Mapper into messages compliant with the Canonical Data
Model. This gives us an additional level of indirection. We can use the Messaging Mapper to
resolve issues such as object references and data type conversions and leave structural mappings
to a Message Translator inside the messaging layer. The price we pay for this additional
decoupling is the creation of an additional component and a small performance penalty. Also,
sometimes it is easier to perform complex transformations inside the application's programming
language as opposed to the drag-and-drop 'doodleware' supplied by the integration vendor.
If we use both a Messaging Mapper and a Message Translator we gain an additional level of
indirection between the canonical data format and the domain objects. It has been said that
computer science is the area where every problem can be solved by adding just one more level of
indirection, so does this rule hold true here? The additional indirection gives us the ability to
compensate for changes in the canonical model inside the messaging layer without having to
touch application code. It also allows us to simplify the mapping logic inside the application by
leaving tedious field mappings and data type changes (e.g., a numeric 'ZIP_Code' field to an
alphanumeric 'Postal_Code' field) to the mapping tools of messaging layer that are optimized for
this kind of work. The Messaging Mapper would then primarily deal with the resolution of object
references and the elimination of unnecessary domain object detail. The apparent downside of
the extra level of indirection is that a change in the domain object may now require changes to
both the Messaging Mapper and the Message Translator. If we did manage to generate the
Messaging Mapper code, this issue largely goes away.
Combining Mapper and Message Translator
The AuctionAggregate class in the Aggregator JMS example acts as a Messaging Mapper between
the JMS messaging system and the Bid class. The methods addMessage and getResultMessage
convert between JMS messages and Bid objects. Neither the messaging system nor the Bid class
have any knowledge of this interaction.
##%%&&
A messaging system, by necessity, uses transactional behavior internally. It may be useful for an
external client to be able to control the scope of the transactions that impact its behavior.
How can a client control its transactions with the messaging system?
A messaging system must use transactions internally. A single Message Channel can have multiple
senders and multiple receivers, so the messaging system must coordinate the messages to make
sure senders don¡¯t overwrite each other¡¯s Messages, multiple Point-to-Point Channel receivers don¡¯t
receive the same message, multiple Publish-Subscribe Channel receivers each receive one copy of
each message, etc. To manage all of this, messaging systems internally use transactions to make
sure a message gets added to the channel or not, and gets read from the channel or not.
Messaging systems also have to employ transactions¡ªpreferably two-phase, distributed
transactions¡ªto copy a message from the sender¡¯s computer to the receiver¡¯s computer, such that
at any given time, the message is ¡°really¡± only on one computer or the other.
Message Endpoints sending and receiving messages are transactional, even if they don¡¯t realize it.
The send method that adds a message to a channel does so within a transaction to isolate that
message from any other messages simultaneously being added to or removed from that channel.
Likewise, a receive method also uses a transaction, which prevents other point-to-point receivers
from getting the same message, and even assures that a publish-subscribe receiver won¡¯t read the
same message twice. (Transactions are often described as being ACID: atomic, consistent, isolated,
and durable. Only transactions for Guaranteed Delivery are durable, and a message by definition is
atomic. But all messaging transactions have to be consistent and isolated. A message can¡¯t be sort
of in the channel, it either is or isn¡¯t. And an application¡¯s sending and receiving of messages has
to be isolated from whatever other sending and receiving other threads and applications might
be doing.)
The messaging system¡¯s internal transactions are sufficient and convenient for a client that
simply wants to send or receive a single message. However, an application may need a broader
transaction to coordinate several messages or to coordinate messaging with other resources.
Common scenarios like this include:
? Send-Receive Message Pairs ¨C Receive one message and send another, such as a
Request-Reply scenario or when implementing a message filter such as a Message Router or
Message Translator.
? Message Groups ¨C Send or receive a group of related messages, such as a Message
Sequence.
? Message/Database Coordination ¨C Combine sending or receiving a message with
updating a database, such as with a Channel Adapter. For example, when an application
receives and processes a message for ordering a product, the application will also need to
update the product inventory database. Likewise, the sender of a Document Message may
wish to delete a persisted document, but only when it is sent successfully; the receiver
may want to persist the document before the message is truly considered to be consumed.
? Message/Workflow Coordination ¨C Use a pair of Request-Reply messages to perform a
work item, and use transactions to ensure that the work item isn¡¯t acquired unless the
request is also sent, and the work item isn¡¯t completed or aborted unless the reply is also
received.
Scenarios like these require a larger atomic transaction that involve more than just a single
message and may involve other transactional stores besides the messaging system. A transaction
is required so that if part of the scenario works (receiving the message, for example) but another
part does not (such as updating the database, or sending another message), all parts can be rolled
back as if they never happened, and then the application can try again.
Yet a messaging system¡¯s internal transaction model is insufficient to allow an application to
coordinate handling a message with other messages or other resources. What is needed is a way
for the application to externally control the messaging system¡¯s transactions and combine them
with other transactions in the messaging system or elsewhere.
Use a Transactional Client¡ªmake the client¡¯s session with the messaging system transactional
so that the client can specify transaction boundaries.
Both a sender and a receiver can be transactional. With a sender, the message isn¡¯t ¡°really¡± added
to the channel until the sender commits the transaction. With a receiver, the message isn¡¯t
¡°really¡± removed from the channel until the receiver commits the transaction. A sender that uses
explicit transactions can be used with a receiver that uses implicit transactions, and vise versa. A
single channel might have a combination of implicitly and explicitly transactional senders; it
could also have a combination of receivers.
With a transactional receiver, an application can receive a message without actually removing the
message from the queue. At this point, if the application crashed, when it recovered, the message
would still be on the queue; the message would not be lost. Having received the message, the
application can then process it. Once the application is finished with the message and is certain it
wants to consume it, the application commits the transaction, which (if successful) removes the
message from the channel. At this point, if the application crashed, when it recovered, the
message would no longer be on the channel, so the application had better truly be finished with
the message.
How does controlling a messaging system¡¯s transactions externally help an application
coordinate several tasks? Here¡¯s what the application would do in the scenarios described above:
Send-Receive Message Pairs
? What to do: Start a transaction, receive and process the first message, create and send the
second message, then commit.
? What this does: This keeps the first message from being removed from its channel until
the second message is successfully added to its channel.
? Transaction type: If the two messages are sent via channels in the same messaging
system, the transaction encompassing the two channels is a simple one. However, if the
two channels are managed by two separate messaging systems, such as with a Messaging
Bridge, the transaction will be a distributed one coordinating the two messaging systems.
? Warning: A single transaction only works for the receiver of a request sending a reply.
The sender of a request cannot use a single transaction to send a request and wait for its
reply. If it tries to do this, the request will never really be sent¡ªbecause the send
transaction isn¡¯t committed¡ªso the reply will never be received.
Message Groups
? What to do: Start a transaction, send or receive all of the messages in the group (such as a
Message Sequence), then commit.
? What this does: When sending, none of the messages in the group will be added to the
channel until they are all successfully sent. When receiving, none of the messages will be
removed from the channel until all are received.
? Transaction type: Since all of the messages are being sent to or received from a single
channel, that channel will be managed by a single messaging system, so the transaction
will be a simple one. Also, in many messaging system implementations, sending a group
of messages in a single transaction ensures that they will be received on the other end of
the channel in the order they were sent.
Message/Database Coordination
? What to do: Start a transaction, receive a message, update the database, then commit. Or
update the database and send a message to report the update to others, then commit.
(This behavior is often implemented by a Channel Adapter.)
? What this does: The message will not be removed unless the database is updated (or the
database change will not stick if the message cannot be sent).
? Transaction type: Since the messaging system and the database each has its own
transaction manager, the transaction to coordinate them will be a distributed one.
Message/Workflow Coordination
? What to do: Use a pair of Request-Reply messages to perform a work item. Start a
transaction, acquire the work item, send the request message, then commit. Or start
another transaction, receive the reply message, complete or abort the work item, then
commit.
? What this does: The work item will not be committed unless the request is sent; the reply
will not be removed unless the work item is updated.
? Transaction type: Since the messaging system and the workflow engine each has its own
transaction manager, the transaction to coordinate them will be a distributed one.
In this way, the application can assure that it will not lose the messages it receives, nor forget to
send a message that it should. If something goes wrong in the middle, the application can roll
back the transaction and try again.
Transactional clients using Event-Driven Consumers may not work as expected. The consumer
typically must commit the transaction for receiving the message before passing the message to
the application. Then if the application examines the message and decides it does not want to
consume it, or if the application encounters an error and wants to rollback the consume action, it
cannot because it does not have access to the transaction. So an event-driven consumer tends to
work the same whether or not its client is transactional.
Messaging systems are capable of participating in a distributed transaction, although some
implementations may not support it. In JMS, a provider can act as an XA resource and participate
in Java Transaction API [JTA] transactions. This behavior is defined by the ¡°XA¡± classes in the
javax.jms package, particularly javax.jms.XASession, and by the javax.transaction.xa package.
The JMS spec recommends that JMS clients not try to handle distributed transactions directly, so
an application should use the distributed transaction support provided by a J2EE application
server. MSMQ can also participate in an XA transaction; this behavior is exposed in .NET by the
MessageQueue.Transactional property and the MessageQueueTransaction class.
As discussed earlier, transactional clients can be useful as part of other patterns like Request-Reply,
message filters in Pipes and Filters, Message Sequence, and Channel Adapter. Likewise, the receiver
of an Event Message may want to complete processing the event before removing its message
from the channel completely. On the other hand, Transactional Clients do not work well with
Event-Driven Consumers nor Message Dispatchers, can cause problems for Competing Consumers, but
work well with a single Polling Consumer.
In JMS, a client makes itself transactional when it creates its session. [JMS11, p.64], [Hapner,
pp.64-66]
Setting the first createSession parameter to true makes the session transactional.
When a client is using a transactional session, it must explicitly commit sends and receives to
make them real.
At this point, the message has only been consumed in the consumer¡¯s transactional view. But to
other consumers with their own transactional views, the message is still available.
Now, assuming that the commit message does not throw any exceptions, the consumer¡¯s
transactional view becomes the message system¡¯s, which now considers the message consumed.
In .NET, queues are not transactional by default. So to use a transactional client, the queue must
be made transactional when it is created:
Once a queue is transactional, each client action (send or receive) on the queue can be
transactional or non-transactional. A transactional receive looks like this:
Although the client had received the message, the messaging system did not make the message
unavailable on the queue until the client committed the transaction successfully. [SysMsg]
The following example enhances the basic filter component introduced in Pipes and Filters to use
transactions. This example implements the Send-Receive Message Pair scenario, receiving and
sending a message inside the same transaction. We really have to add only a few lines of code to
make the filter transactional. We use a variable of type MessageQueueTransaction to manage the
transaction. We open a transaction before we consume the input message and commit after we
publish the output message. If any exception occurs we abort the transaction which rolls back all
message consumption and publication actions and returns the input message to the queue to be
available to other queue consumers.
How do we verify that our Transactional Client works as intended? We subclass the basic
TransactionalFilter with the aptly named class RandomlyFailingFilter. For each consumed
message, this filter draws a random number between 0 and 10. If the number is less than 3, it
throws an an arbitrary exception (ArgumentNullException seemed convenient enough for an
example). If we implemented this filter on top of our basic, non-transactional filter described in
Pipes and Filters, we would lose about one in three messages.
To make sure that we do in fact not lose any messages with the transactional version we rigged
up a really simple test harness that publishes a sequence of messages to the input queue and
makes sure that it can receive all messages in the correct order from the output queue. It is
important to remember that the output messages remain in sequence only if we run a single
instance of the transactional filter. If we run multiple filters in parallel, messages can (and will)
get out of order (see Resequencer.
##%%&&
An application needs to consume Messages, but it wants to control when it consumes each
message.
How can an application consume a message when the application is ready?
Message consumers exit for one reason¡ªto consume messages. The messages represent work
that needs to be done, so the consumer needs to consume those messages and do the work.
But how does the consumer know when a new message is available? The easiest approach is for
the consumer to repeatedly check the channel to see if a message is available. When a message is
available, it consumes the message, and then goes back to checking for the next one. This process
is called polling.
The beauty of polling is that the consumer can request the next message when it is ready for
another message. So it consumes messages at the rate it wants to, rather than at the rate they
arrive in the channel.
The application should use a Polling Consumer, one that explicitly makes a call when it wants to
receive a message.
This is also known as a synchronous receiver, because the receiver thread blocks until a message
is received. We call it a Polling Consumer because the receiver polls for a message, processes it,
then polls for another. As a convenience, messaging API¡¯s usually provide a receive method that
blocks until a message is delivered, in addition to methods like receiveNoWait() and Receive(0)
that return immediately if no message is available. This difference is only apparent when the
receiver is polling faster than messages are arriving.
A Polling Consumer is an object that an application uses to receive messages by explicitly
requesting them. When the application is ready for another message, it polls the consumer, which
in turn gets a message from the messaging system and returns it. (How the consumer gets the
message from the messaging system is implementation-specific and may or may not involve
polling. All the application knows is that it doesn¡¯t get the message until it explicitly asks for
one.)
When the application polls for a message, the consumer blocks until it gets a message to return
(or until some other condition is met, such as a time limit). Once the application receives the
message, it can then process it. Once it is through processing the message and wishes to receive
another, the application can poll again.
By using polling consumers, an application can control how many messages are consumed
concurrently by limiting the number of threads that are polling. This can help keep the receiving
application from being overwhelmed by too many requests; extra messages queue up until the
receiver can process them.
A receiver-application typically uses one thread (at least) per channel that it wishes to monitor,
but it can also use a single thread to monitor multiple channels, to help conserve threads when
monitoring channels that are frequently empty. To poll a single channel, assuming that the thread
has nothing to do until a message arrives, use a version of receive that blocks until a message
arrives. To poll multiple channels with a single thread, or to perform other work while waiting
for a message to arrive, use a version of receive with a timeout or receiveNoWait() so that if one
channel is empty, the thread goes on to check another channel or perform other work.
A consumer that is polling too much or blocking threads for too long can be inefficient, in which
case an Event-Driven Consumer may be more efficient. Multiple Polling Consumers can be
Competing Consumers. A Message Dispatcher can be implemented as a Polling Consumer. A Polling
Consumer can be a Selective Consumer; it can also be a Durable Subscriber. A Polling Consumer can
also be a Transactional Client so that the consumer can control when the message is actually
removed from the channel.
In JMS, a message consumer uses MessageConsumer.receive to consume a message synchronously.
[JMS11, p.69], [Hapner, p.21]
MessageConsumer has three different receive methods:
1. receive() ¨C Blocks until a message is available, and then returns it.
2. receiveNoWait() ¨C Checks once for a message, and returns it or null.
3. receive(long) ¨C Blocks either until a message is available and returns it, or until the time-out expires and
returns null.
For example, the code to create a consumer and receive a message is very simple:
In .NET, a consumer uses MessageQueue.Receive to consume a message synchronously. [SysMsg]
A MessageQueue client has several variations of receive. The two simplest are:
1. Receive() ¨C Blocks until a message is available, and then returns it.
2. Receive(TimeSpan) ¨C Blocks either until a message is available and returns it, or until the time-out expires
and throws MessageQueueException.
The code to receive a message from an existing queue is quite simple:
##%%&&
An application needs to consume Messages as soon as they¡¯re delivered.
How can an application automatically consume messages as they become available?
The problem with Polling Consumers is that when the channel is empty, the consumer blocks
threads and/or consumes process time while polling for messages that are not there. Polling
enables the client to control the rate of consumption, but wastes resources when there¡¯s nothing
to consume.
Rather than continuously asking the channel if it has messages to consume, it would be better if
the channel could tell the client when a message is available. For that matter, instead of making
the consumer poll for the message to get the message, just give the message to the consumer as
soon as the message becomes available.
The application should use an Event-Driven Consumer, one that is automatically handed
messages as they¡¯re delivered on the channel.
This is also known as an asynchronous receiver, because the receiver does not have a running
thread until a callback thread delivers a message. We call it an Event-Driven Consumer because the
receiver acts like the message delivery is an event that triggers the receiver into action.
An Event-Driven Consumer is an object that is invoked by the messaging system when a message
arrives on the consumer¡¯s channel. The consumer passes the message to the application through a
callback in the application¡¯s API. (How the messaging system gets the message is
implementation-specific and may or may not be event-driven. All the consumer knows is that it
can sit dormant with no active threads until it gets invoked by the messaging system passing it a
message.)
An Event-Driven Consumer is invoked by the messaging system, yet it invokes an
application-specific callback. To bridge this gap, the consumer has an application-specific
implementation that conforms to a known API defined by the messaging system.
The code for an Event-Driven Consumer consists of two parts:
1. Initialization ¨C The application creates an application-specific consumer and associates it
with a particular Message Channel. After this code is run once, the consumer is ready to
receive a series of messages.
2. Consumption ¨C The consumer receives a message, which it and the application process.
This code is run once per message being consumed.
Event-Driven Consumer Sequence
The application creates its custom consumer and associates it with the channel. Once the
consumer is initialized, it (and the application) can then go dormant, with no running threads,
waiting to be invoked when a message arrives.
When a message is delivered, the messaging system calls the consumer¡¯s message-received-event
method and passes in the message as a parameter. The consumer passes the message to the
application using the application¡¯s callback API. The application now has the message and can
process it. Once the application finishes processing the message, it and the consumer can then go
dormant again until the next message arrives. Typically, a messaging system will not run
multiple threads through a single consumer, so the consumer can only process one message at a
time.
Event-Driven Consumers automatically consume messages as they become available. For more
fine-grained control of the consumption rate, use a Polling Consumer. Event-Driven Consumers can
be Competing Consumers. A Message Dispatcher can be implemented as a Event-Driven Consumer. A
Event-Driven Consumer can be a Selective Consumer; it can also be a Durable Subscriber. Transactional
Clients may not work as well with event-driven consumers as they do with polling consumers
(see the JMS example).
In JMS, an Event-Driven Consumer is a class that implements the MessageListener interface.
[Hapner, p.22] This interface declares a single method, onMessage(Message). The consumer
implements onMessage to process the message. Here is an example of a JMS performer:
The initializer part of an Event-Driven Consumer creates the desired performer object (which is a
MessageListener instance) and associates it with a message consumer for the desired channel:
Now, when a message is delivered to the destination, the JMS provider will call
MyEventDrivenConsumer.onMessage with the message as a parameter.
Note that in JMS, an Event-Driven Consumer that is also a Transactional Client will not work as
expected. Normally a transaction is rolled back when the code in the transaction throws an
exception, but the MessageListener.onMessage signiture does not provide for an exception being
thrown (such as JMSException), and a runtime exception is considered programmer error. If a
runtime exception occurs, the JMS provider responds by delivering the next message, so the
message that caused the exception is lost. [JMS11, p.69], [Hapner, p.22] To successfully achieve
transaction, event-driven behavior, use a message-driven EJB. [EJB20, pp.311-326], [Hapner,
pp.69-71]
With .NET, the performer part of an Event-Driven Consumer implements a method that is a
ReceiveCompletedEventHandler delegate. This delegate method must accept two parameters: an
Object that is the MessageQueue, and a ReceiveCompletedEventArgs that is the arguments from the
ReceiveCompleted event. [SysMsg] The method uses the arguments to get the message from the
queue and process it. Here is an example of a .NET performer:
The initializer part of an event-driven client specifies that the queue should run the delegate
method to handle a ReceiveCompleted event:
Now, when a message is delivered to the queue, the queue will issue a ReceiveCompleted event,
which will run the MyEventDrivenConsumer method.
##%%&&
An application is using Messaging. However, it cannot process messages as fast as they¡¯re being
added to the channel.
How can a messaging client process multiple messages concurrently?
Messages arrive through a Message Channel sequentially, so the natural inclination of a consumer
is to process them sequentially. However, sequential consumption may be too slow and messages
may pile up on the channel, which makes the messaging system a bottleneck and hurts overall
throughput of the application. This can happen either because of multiple senders on the channel,
because a network outage causes a backlog of messages which are then delivered all at once,
because a receiver outage causes a backlog, or because each message takes significantly more
effort to consume and perform than it does to create and send.
The application could use multiple channels, but one channel might become a bottleneck while
another sits empty, and a sender would not know which one of equivalent channels to use.
Multiple channels would have the advantage, however, of enabling multiple consumers (one per
channel), processing messages concurrently. Even if this worked, though, the number of channels
the application defined would still limit the throughput.
What is needed is a way for a channel to have multiple consumers.
Create multiple Competing Consumers on a single channel so that the consumers can process
multiple messages concurrently.
Competing Consumers are multiple consumers that are all created to receive messages from a
single Point-to-Point Channel. When the channel delivers a message, any of the consumers could
potentially receive it. The messaging system's implementation determines which consumer
actually receives the message, but in effect the consumers compete with each other to be the
receiver. Once a consumer receives a message, it can delegate to the rest of its application to help
process the message. (This solution only works with Point-to-Point Channels; multiple consumers
on a Publish-Subscribe Channel just create more copies of each message.)
Each of the Competing Consumers runs in its own thread so that they all can consume messages
concurrently. When the channel delivers a message, the messaging system's transactional
controls ensure that only one of the consumers successfully receives the message. While that
consumer is processing the message, the channel can deliver other messages, which other
consumers can concurrently consume and process. The channel coordinates the consumers,
making sure that they each receives a different message; the consumers do not have to coordinate
with each other.
Each consumer processes a different message concurrently, so the bottleneck becomes how
quickly the channel can feed messages to the consumers instead of how long it takes a consumer
to process a message. A limited number of consumers may still be a bottleneck, but increasing the
number of consumers can alleviate that constraint as long as there are available computing
resources.
To run concurrently, each consumer must run in its own thread. For Polling Consumers, this
means that each consumer must have its own thread to perform the polling concurrently. For
Event-Driven Consumers, the messaging system must use a thread per concurrent consumer; that
thread will be used to hand the message to the consumer, and will be used by the consumer to
process the message.
A sophisticated messaging system will detect competing consumers on a channel and internally
provide a Message Dispatcher that ensures that each message is only delivered to a single
consumer. This helps avoid conflicts that would arise if multiple consumers each thought they
were the consumer of a single message. A less-sophisticated messaging system will allow
multiple consumers to attempt to consume the same message. When this happens, whichever
consumer commits its transaction first wins; then the other consumers will not be able to commit
successfully and will have to roll back their transactions.
A messaging system that allows multiple consumers to attempt consuming the same message can
make a Transactional Client very inefficient. The client thinks it has a message, consumes it,
spends effort processing the message, then tries to commit and cannot (because the message has
already been consumed by a competitor). Frequently performing work just to roll it back hurts
throughput, whereas the point of this solution is to increase throughput. Thus the performance of
competing transactional consumers should be measured carefully; it could vary significantly on
different messaging system implementations and configurations.
Not only can Competing Consumers be used to spread load across multiple consumer threads in a
single application; they can also spread the consumption load across multiple applications. This
way, if one application cannot consume messages fast enough, multiple consumer
applications--perhaps with each employing multiple consumer threads--can attack the problem.
The ability to have multiple applications running on multiple computers using multiple threads
to consume messages provides virtually unlimited message processing capacity, where the only
limit is the messaging system's ability to deliver messages from the channel to the consumers.
The coordination of competing consumers depends on each messaging system¡¯s implementation.
If the client wants to implement this coordination itself, it should use a Message Dispatcher.
Competing Consumers can be Polling Consumers, Event-Driven Consumers, or a combination thereof.
Competing Transactional Clients can waste significant effort processing messages whose receive
operations do not commit successfully and have to be rolled back.
This is a simple example of how to implement a competing consumer in Java. An external
driver/manager object (not shown) runs a couple of them. It runs each one in its own thread and
calls stopRunning() to make it stop.
A JMS session must be single-threaded. [JMS11, pp.26-27], [Hapner, p.19] A single session
serializes the order of message consumption. [JMS11, p.60], [Hapner, p.19] So for each competing
consumer to work properly in its own thread, and for the consumers to be able to consume
messages in parallel, each consumer must have its own Session (and therefore its own
MessageConsumer). The JMS specification does not specify the semantics of how concurrent
QueueReceivers (e.g., Competing Consumers) should work, or even require that this approach work
at all. Thus applications which use this technique are not assumed to be portable and may work
differently with different JMS providers. [JMS11, p.65], [Hapner, p.347]
The consumer class implements Runnable so that it can run in its own thread; this allows the
consumers to run concurrently. All of the consumers share the same Connection but each creates
its own Session, which is important since each session can only support a single thread. Each
consumer repeatedly receives a message from the queue and processes it.
So implementing a simple competing consumer is easy. The main trick is to make the consumer a
Runnable and run it in its own thread.
##%%&&
An application is using Messaging. The application needs multiple consumers on a single Message
Channel to work in a coordinated fashion.
How can multiple consumers on a single channel coordinate their message processing?
Multiple consumers on a single Point-to-Point Channel act as Competing Consumers. That's fine
when the consumers are interchangeable, but it does not allow for specializing the consumers so
that certain consumers are better able to consume certain messages.
Multiple consumers on a single Publish-Subscribe Channel won't work as intended. Rather than
distribute the message load, these consumers will duplicate the effort.
Selective Consumers can be used as specialized consumers. However, not all messaging systems
support this feature. Even amongst those that do, they may not support selection based on values
in the body of the message. Their selector value expressions may be too simple to adequately
distinguish amongst the messages, or the performance of repeatedly evaluating those expressions
may be slow. There may be numerous expressions which need to be carefully designed in a
coordinated fashion so that they do not overlap but also do not leave any selector values
unhandled. They may need to implement a default case for selector values that are not handled
by other consumers or that are unexpected.
Datatype Channels can be used to keep different types of messages separate and to enable
consumers to specialize for those message types. But the type system may be too large and varied
to justify creating a separate channel for each type. Or the types may be based on dynamically
changing criteria which are difficult to handle with a static set of channels. The enterprise may
already require a huge number of channels, taxing the messaging system, and multiplying many
of those channels for distinct message types may simply require too many channels.
Each of these problems could be solved if consumers could work together. They could avoid
duplicating work by being aware if another consumer had already processed that work. They
could be specialized; if a consumer got the wrong kind of message for its speciality, it could hand
off the message to another consumer with the right speciality. If an application had too many
channels coming in, it could save channels by having all of its consumers share a single channel;
they'd coordinate to make sure that the right messages went to the right consumers.
Alas, consumers are very independent objects that are difficult to coordinate. Making specialized
consumers general enough to handle any message and hand it off would add a lot of design and
processing overhead to each consumer. They would all have to know about each other so they
could hand off work, and they would all need to know which of the others were busy so as not to
give a consumer a message to process while it's already processing another. Making consumers
work together would radically change the typical consumer design.
The Mediator pattern [GoF] offers some help. A Mediator coordinates a group of objects so that
they don't need to know how to coordinate with each other. What we need for messaging is a
mediator that coordinates the consumers for a channel. Then each consumer could focus on
processing a particular kind of message, and the coordinator could make sure the right message
gets to the right consumer.
Create a Message Dispatcher on a channel that will consume messages from a channel and
distribute them to performers.
A Message Dispatcher consists of two parts:
1. Dispatcher ¨C The object that consumes messages from a channel and distributes each
message to a performer.
2. Performer ¨C The object that is given the message by the dispatcher and processes it.
When a Message Dispatcher receives a message, it obtains a performer and dispatches the message
to the performer to process it. A performer can delegate to the rest of its application to help
process its message. The performer could be newly created by the dispatcher, or could be selected
from a pool of available performers. Each performer can run in its own thread to process
messages concurrently. All performers may be appropriate for all messages, or the dispatcher
may match a message to a specialized performer based on properties of the message.
Message Dispatcher Sequence
When the dispatcher receives a message, it delegates the message to an available performer to
process it. If the performer processes the message using the dispatcher's thread, then the
dispatcher blocks until the performer is finished processing the message. On the other hand, if
the performer processes the message in its own thread, then once the dispatcher starts that thread,
it can immediately start receiving other messages and delegating them to other performers, so
that the messages are processed concurrently. This way, messages can be consumed as fast as the
dispatcher can receive and delegate them, regardless of how long each message takes to process.
A dispatcher acts as a one-to-many connection between a single channel and a group of
performers. The performers do most of the work; the dispatcher just acts as a matchmaker,
matching each message with an available performer, and does not block as long as the
performers run in their own threads. The dispatcher receives the message and then sends it to a
performer to process it. Because the dispatcher does relatively little work and does not block, it
potentially can dispatch messages as fast as the messaging system can feed them and thus avoids
becoming a bottleneck.
This pattern is a simpler, messaging-specific version of the Reactor pattern [POSA2], where the
message dispatcher is a Reactor and the message performers are Concrete Event Handlers. The
Message Channel acts as the Synchronous Event Demultiplexer, making the messages available to
the dispatcher one at a time. The messages themselves are like Handles, but much simpler. A true
handle tends to be a reference to a resource's data, whereas a message usually contains the data
directly. (However, the message does not have to store the data directly. If a message's data is
stored externally and the message is a Claim Check, then the message contains a reference to the
data, which is more like a Reactor handle.) Different types of handles select different types of
concrete event handlers, whereas a Message Channel is a Datatype Channel, so all of the messages
(handles) are of the same type, so there is typically only one type of concrete event handler.
On the other hand, whereas Datatype Channel designs a channel so that all messages are of the
same type and all consumers process messages of that type, the Reactor pattern points out an
opportunity to use Message Dispatcher to support multiple data types on the same channel and
process them with type-specific performers. Each message must specify its type; the dispatcher
detects the message's type and dispatches it to a type-specific performer for processing. In this
way, a dispatcher with specialized performers can act as an alternative to Datatype Channels and
as a specialized implementation of Selective Consumers.
One difference between Message Dispatcher and Competing Consumers is the ability to distribute
across multiple applications. Whereas a set of competing consumers may be distributed amongst
multiple applications, a set of performers typically all run in the same application as the
dispatcher (even if they run in different threads). If a performer were running in a different
application from its dispatcher, the dispatcher would have to communicate with the performer in
a distributed, Remote Procedure Invocation manner, which is exactly what Messaging intends to
avoid in the first place.
Since a dispatcher is a single consumer, it works fine with both Point-to-Point Channels and
Publish-Subscribe Channels. With point-to-point messaging, a dispatcher can be a suitable
alternative to Competing Consumers; this alternative may be preferable if the messaging system
handles multiple consumers badly or if handling of multiple consumers across different
messaging system implementations is inconsistent.
A dispatcher makes the performers work much like Event-Driven Consumers, even though the
dispatcher itself could be event-driven or a Polling Consumer. As such, implementing a dispatcher
as part of a Transactional Client can be difficult. If the client is transactional, ideally the dispatcher
should allow the performer to process a message before completing the transaction. Then, only if
the performer is successful should the dispatcher commit the transaction. If the performer fails to
process the message, the dispatcher should rollback the transaction. Since each performer may
need to rollback its individual message, the dispatcher needs a session for each performer and
must use that performer¡¯s session to receive the performer¡¯s message and complete its transaction.
Since event-driven consumers often do not work well with transactional clients, the dispatcher
should not be an event-driven consumer, but rather should be a polling consumer.
It can be helpful to implement performers as Event-Driven Consumers. In JMS, this means
implementing the performer as a MessageListener. A message listener has one method,
onMessage(Message); it accepts a message and performs whatever processing necessary. This
forms a clean separation between the dispatcher and the performer. Likewise, in .NET, the
performer should be a ReceiveCompletedEventHandler delegate, even though the dispatcher will
not really issue ReceiveCompleted events. However, these event-driven API's may not be
compatible with the API necessary to run a performer in its own thread.
To avoid the effort of implementing your own Message Dispatcher, consider instead using
Competing Consumers on a Datatype Channel, or using Selective Consumers. A Message Dispatcher can
be a Polling Consumer or an Event-Driven Consumer. Message Dispatchers do not make very good
Transactional Clients.
Usually, a Message Dispatcher dispatches messages to the performers (see the Java example). .NET
provides another option: The dispatcher can use Peek to detect a message and get its message ID,
then dispatch the message ID (not the full message) to the performer. The performer then uses
ReceiveById to consume the particular message it has been assigned. In this way, each performer
can take responsibility not just for processing the message, but for consuming it as well, which
can help with concurrency issues, especially when the consumers are Transactional Clients.
This is a simple example of how to implement a dispatcher and performer in Java. A more
sophisticated dispatcher implementation might pool several performers, keep track of which ones
are currently available to process messages, and make use of a thread pool. This simple example
skips those details, but does run each performer in its own thread so that they can run
concurrently.
The driver/manager that controls the dispatcher (not shown) will run receiveSync() repeatedly.
Each time, the dispatcher will receive() the next message, instantiate a new Performer instance to
process the message, then start the performer in its own thread.
The Performer must implement Runnable so that it can run in its own thread. The runnable's run()
method simply calls processMessage(). When this is complete, the performer becomes eligible for
garbage collection.
So implementing a simple dispatcher and performer is easy. The main trick is to make the
performer a Runnable and run it in its own thread.
##%%&&
An application is using Messaging. It consumes Messages from a Message Channel, but it does not
necessarily want to consume all of the messages on that channel, just some of them.
How can a message consumer select which messages it wishes to receive?
By default, if a Message Channel has only one consumer, all Messages on that channel will be
delivered to that consumer. Likewise, if there are multiple Competing Consumers on the channel,
any message can potentially go to any consumer, and every message will go to some consumer.
A consumer normally does not get to choose which messages it consumes; it always gets
whatever message is next.
This behavior is fine as long as the consumer wants to receive any and all messages on the
channel, which is normally the case. This is a problem, however, when a consumer only wants to
consume certain messages, because a consumer normally has no control over which messages on
a channel it receives. Why would a consumer want to receive certain messages only? Consider an
application processing loan request messages; it may want to process loans for up to $100,000
differently from those over $100,000. One approach would be for the application to have two
different kinds of consumers, one for small loans and another for big loans. Yet, since any
consumer can receive any message, how can the application make sure that the right messages go
to the right consumer?
The simplest approach might be for each to consume whatever messages it gets. If it gets the
wrong kind of message, it could somehow hand that message to the appropriate kind of
consumer. That¡¯s going to be difficult, though; consumer instances usually don¡¯t know about
each other, and finding one that isn¡¯t already busy processing another message can be difficult.
Perhaps when the consumer realizes it doesn¡¯t want the message, it could put the message back
on the channel. But then it¡¯s likely to just consume the message yet again. Perhaps every
consumer could get a copy of every message, and just discard the ones it doesn¡¯t want. This will
work, but will cause a lot of message duplication and a lot of wasted processing on messages that
are ultimately discarded.
Perhaps the messaging system could define separate channels for each type of message. Then the
sender could make sure to send each message on the proper channel, and the receivers could be
sure that the messages they receive off of a particular channel are the kind desired. However, this
solution is not very dynamic. The receivers may change their selection criteria while the system is
running, which would require defining new channels and redistributing the messages already on
the channels. It also means that the senders must know what the receiver's selection criteria are
and when those criteria change. The criteria need to be a property of the receivers, not of the
channels, and the messages on the channel need to specify what criteria they meet.
What is needed is a way for messages fitting a variety of criteria to all be sent on the same
channel, for the consumers to be able to specify what criteria they're interested in, and for each
consumer to only receive the messages that meet its criteria.
Make the consumer a Selective Consumer, one that filters the messages delivered by its
channel so that it only receives the ones that match its criteria.
There are three parts to this filtering process:
1. Specifying Producer ¡ª Specifies the message¡¯s selection value before sending it.
2. Selection Value ¡ª One or more values specified in the message that allow a consumer to
decide whether to select the message.
3. Selective Consumer ¡ª Only receives messages that meet its selection criteria.
The message sender specifies each message¡¯s selection value before sending it. When a message
arrives, a Selective Consumer tests the message¡¯s selection value to see if the value meets the
consumer¡¯s selection criteria. If so, the consumer receives the message and passes it to the
application for processing.
Selective Consumer Sequence
When the sender creates the message, it also sets the message¡¯s selection value; then it sends the
message. When the messaging system delivers the message, the Selective Consumer tests the
message¡¯s selection value to determine whether to select the message. If the message passes, the
consumer receives the message and passes the message to the application using a callback.
Selective Consumers are often used in groups¡ªone consumer filters for one set of criteria, while
another filters for a different set, and so on. For the loan processing example, one consumer
would select ¡°amount <= $100,000¡± while another would select ¡°amount > $100,000¡±. Then each
consumer would only get the kinds of loans it is interested in.
When multiple Selective Consumers are used with a Point-to-Point Channel, they effectively become
Competing Consumers that are also selective. If two consumer¡¯s criteria overlap, and a message¡¯s
selection value meets both of their criteria, either consumer can consume the message.
Consumers should be designed to ensure that at least one of them is eligible to consume every
valid selection value. Otherwise, a message with an unmatched selection value will never be
consumed and will clutter the channel forever (or at least until Message Expiration occurs).
When multiple Selective Consumers are used with a Publish-Subscribe Channel, each message will
be delivered to each subscriber, but a subscriber will simply ignore its copy of a message that
does not fit its criteria. Once a consumer decides to ignore a message, the messaging system can
discard the message since it has been successfully delivered and will never be consumed. A
messaging system can optimize this process by not even delivering a message it knows the
consumer will ignore, thereby decreasing the number of copies of a message that must be
produced and transmitted. This behavior of discarding ignored messages is independent of
whatever Guaranteed Delivery, Durable Subscriber, and/or Message Expiration settings are used.
Selective Consumers make a single channel act like multiple Datatype Channels. Different types of
messages can have different selection values, so that a consumer that is specialized for a
particular type will only receive messages of that type. This can facilitate sending a large number
of types using a small number of channels. This approach can also conserve channels in an
enterprise which requires more channels than a messaging system can support.
Competing, Selective Consumers
Using Selective Consumers to emulate Datatype Channels is not a good approach when trying to
hide messages of a certain type from certain consumer applications. Whereas a messaging system
can ensure only authorized applications successfully receive messages from a channel, they
usually do not authorize a consumer¡¯s selection criteria, so a malicious consumer authorized to
access the channel can gain access to unauthorized messages by changing its criteria. Separate
datatype channels are needed to securely lock out applications.
An alternative to using Selective Consumers is to use a Message Dispatcher. The selection criteria are
built into the dispatcher, which then uses them to determine the performer for each message. If a
message does not meet any of the performer¡¯s criteria, rather than leave it cluttering the channel
or discarding it, the dispatcher can reroute the unmatched message to the Invalid Message Channel.
As with the tradeoff between Message Dispatcher and Competing Consumers, the question is really
whether you wish to let the messaging system do the dispatching or whether you want to
implement it yourself. If a messaging system does not support Selective Consumer as a feature,
you have no choice but to implement it yourself using a Message Dispatcher.
As mentioned earlier, if none of the Selective Consumers on a channel match the message¡¯s selector
value, the message will be ignored as if the channel has no receivers. A similar problem in
procedural programming is a case statement where none of the cases matches the value being
tested. Thus a case statement can have a default case which matches values that aren¡¯t matched
by any other case. Applying this approach to messaging, it may seem tempting to create some
sort of default consumer for messages that otherwise have no matching consumer. Yet such a
default consumer will not work as desired because it would require an expression that matches
all selector values, so it would compete with all other consumers. Instead, to implement a default
consumer, use a Message Dispatcher that implements a case statement with a default option for
unhandled cases that uses the default consumer.
Another alternative to Selective Consumers is Message Filter. They accomplish much the same goal,
but in different ways. With a Selective Consumer, all of the messages are delivered to the receivers,
but each receiver ignores unwanted messages. A Message Filter sits between a channel from the
sender and a channel to the receiver and only transfers desired messages from the sender¡¯s
channel to the receiver¡¯s. Thus unwanted messages are never even delivered to the receiver¡¯s
channel, so the receiver has nothing to ignore. Message Filter is useful for getting rid of messages
that no receiver wants. Selective Consumers are useful when one receiver wants to ignore certain
messages but other receivers want to receive those messages.
Another alternative to consider is Content-Based Router. This type of router, like a filter, makes
sure that a channel only gets the messages that the receivers want, which can increase security
and increase the performance of the consumers. Selective Consumer is more flexible, however,
because each filtering option simply requires a new consumer (which are easy to create while the
system is running), whereas each new option with a Content-Based Router requires a new output
channel (which is not so easy to create and use while the system is running) as well as a new
consumer for the new channel. Consider a requirements change where you want to process
medium-size loans ($50K-$150K) differently from small and large loans. With Content-Based
Router, you need to create a new channel for medium loans, as well as a consumer on that new
channel, and adjust the way the router separates loans. You also need to worry about what
happens when the change takes effect, because some messages that have already been routed
onto the original channels may not have been consumed yet and may now be on the wrong
channel. With Selective Consumer, you just replace the two types of consumers (less-than-$100K
and greater-than-$100K) with three types (less-than-$50K, $50K-$150K, and greater-than-$150K).
Content-Based Router is a much more static approach, whereas Selective Consumer can be much
more dynamic.
Ideally, a message¡¯s selection value should be specified in its header, not its body, so that a
Selective Consumer can process the value without having to parse (and know how to parse) the
message¡¯s body.
Selective Consumers make a single channel act like multiple Datatype Channels. They allow
messages to be available for other receivers whereas Message Filter prevents unwanted messages
from being delivered to any receiver, and can be used more dynamically than a Content-Based
Router. A Selective Consumer can be implemented as a Polling Consumer or Event-Driven Consumer,
and can be part of a Transactional Client. To implement the filtering behavior yourself, use a
Message Dispatcher.
For example, a stock trading system with a limited number of channels might need to use one
channel for both quotes and trades. The receiver for performing a quote is very different from
that for trading, so the right receiver needs to be sure to consume the right message. So the
sender would set the selector value on a quote message to QUOTE, and the Selective Consumer for
quotes would only consume messages with that selector value. Trade messages would have their
own TRADE selector value that their senders and receivers would use. In this way, two message
types can successfully share a single channel.
In JMS, a MessageConsumer (QueueReceiver or TopicSubscriber) can be created with a message
selector string that filters messages based on their property values. [JMS11, p.41], [Hapner, p.23]
First, a sender would set the value of a property in the message that the receiver could filter by:
Second, a receiver set its message selector to filter for that value:
This receiver will ignore all messages whose request type property is not set to ¡°quote¡± as if those
messages were never delivered to the destination at all.
In .NET, MessageQueue.Receive does not support JMS-style message selectors per se. Rather, what
a receiver can do is use MessageQueue.Peek to look at a message. If it meets the desired criteria,
then it can use MessageQueue.Receive to read it from the queue. This may not work very reliably,
though, since the message returned by the Receive call may not necessarily be the same message
that was Peeked. Thus use ReceiveById, whereby the consumer specifies the Id property value of
the message it wishes to receive, instead of Receive to ensure getting the same message that was
Peeked.
Another option in .NET is the ReceiveByCorrelationId method, with which the consumer
specifies the CorrelationId property value of the message it wants to receive. A sender of a
particular request message can use ReceiveByCorrelationId to receive the reply message specific
to that request (see Request-Reply and Correlation Identifier).
##%%&&
An application is receiving messages on a Publish-Subscribe Channel.
How can a subscriber avoid missing messages while it¡¯s not listening for them?
Why is this even an issue? Once a message is added to a channel, it stays there until it is either
consumed, it expires (see Message Expiration), or the system crashes (unless you¡¯re using
Guaranteed Delivery). This is true for a message on a Point-to-Point Channel, but a Publish-Subscribe
Channel works somewhat differently.
When a message is published on a Publish-Subscribe Channel, the messaging system must deliver
the message to each subscriber. How it does this is implementation specific: It can keep the
message until the list of subscribers that have not received it is empty, or it might duplicate and
deliver the message to each subscriber. Whatever the case, which subscribers receive the message
is completely dependent upon who is subscribed to the channel when the message is published.
If a receiver is not subscribed when the message is published, even if the receiver subscribes an
instant later, it will not receive that message. (There is also a timing issue of what happens when
a subscriber subscribes and a message is published on the same channel at ¡°about¡± the same time.
Does the subscriber receive the message? How this issue is resolved depends on the messaging
system¡¯s implementation. To be safe, subscribers should be sure to subscribe before messages of
interest are published.)
As a practical matter, a subscriber unsubscribes from a channel by closing its connection to the
channel. Thus no explicit unsubscribe action is necessary; the subscriber just closes its connection.
Often, an application prefers to ignore messages published after it disconnects, because being
disconnected means that the application is uninterested in whatever may be published. For
example, a B2B/C application selling bricks may subscribe to a channel where buyers can request
bricks. If the application stops selling bricks, or is temporarily out of bricks, it may decide to
disconnect from the channel to avoid receiving requests it cannot fulfill anyway.
Yet this behavior can be disadvantageous, because the ¡°you snooze, you lose¡± approach can
cause an application to miss messages it needs. If an application crashes, or must be stopped for
maintenance, it may want to know what messages it missed while it wasn¡¯t running. The whole
idea of messaging is to make communication reliable even if the sender and receiver applications
and network aren¡¯t all working at the same time.
So, sometimes applications disconnect because they don¡¯t want messages from that channel
anymore. But sometimes applications have to disconnect for a short time, but when they
reconnect, they want to have access to all of the messages that were published during the
connection lapse. A subscriber is normally either connected (subscribed) or disconnected
(unsubscribed), but a third possible state is inactive, the state of a subscriber that is disconnected but
still subscribed because it wants to receive messages published while it is disconnected.
If a subscriber was connected to a Publish-Subscribe Channel, but is disconnected when a message
is published, how does the messaging system know whether to save the message for the
subscriber so that it can deliver the message when the subscriber reconnects? That is, how does
the messaging system know whether a disconnected subscriber is inactive or unsubscribed?
There needs to be two kinds of subscriptions, those that end when the subscriber disconnects and
those that survive even when the application disconnects and are only broken when the
application explicitly unsubscribes.
By default, a subscription only lasts as long as its connection. So what is needed it another type of
subscription that survives disconnects by becoming inactive.
Use a Durable Subscriber to make the messaging system save messages published while the
subscriber is disconnected.
A durable subscription saves messages for an inactive subscriber and delivers these saved
messages when the subscriber reconnects. In this way, a subscriber will not lose any messages
even though it disconnected. A durable subscription has no effect on the behavior of the
subscriber or the messaging system while the subscriber is active (e.g., connected). A connected
subscriber acts the same whether its subscription is durable or non-durable. The difference is in
how the messaging system behaves when the subscriber is disconnected.
A Durable Subscriber is simply a subscriber on a Publish-Subscribe Channel. However, when the
subscriber disconnects from the messaging system, it becomes inactive and the messaging system
will save any messages published on its channel until it becomes active again. Meanwhile, other
subscribers to the same channel may not be durable; they¡¯re non-durable subscribers.
Durable Subscription Sequence
To be a subscriber, the Durable Subscriber must establish its subscription to the channel. Once it
has, when it closes its connection, it becomes inactive. While the subscriber is inactive, the
publisher publishes a message. If the subscriber were non-durable, it would miss this message;
but because it is durable, the messaging system saves this message for this subscriber. When the
subscriber resubscribes, becoming active once more, the messaging system delivers the queued
message (and any others saved for this subscriber). The subscriber receives the message and
process it (perhaps delegating the message to the application). Once the subscriber is through
processing messages, if it does not wish to receive any more messages, it closes its connection,
becoming inactive again. Since it does not want the messaging system to save messages for it
anymore, it also unsubscribes.
An interesting consequence is: What would happen if a durable subscriber never unsubscribed?
The inactive durable subscription would continue to retain messages¡ªthat is, the messaging
system will save all of the published messages until the subscriber reconnects. But if the
subscriber does not reconnect for a long time, the number of saved messages can become
excessive. Message Expiration can help alleviate this problem. The messaging system may also
wish to limit the number of messages that can be saved for an inactive subscription.
A stock trading system might use a Publish-Subscribe Channel to broadcast changes in stocks
prices; each time a stocks¡¯ price changes, a message is published. One subscriber might be a GUI
that displays the current prices for certain stocks. Another subscriber might be a database that
stores the day¡¯s trading range for certain stocks.
Both applications should be subscribers to the price-change channel so that they¡¯re notified when
a stock¡¯s price changes. The GUI¡¯s subscription can be non-durable because it is displaying the
current price. If the GUI crashes a loses its connection to the channel, there is no point in saving
price changes the GUI cannot display. On the other hand, the price range database should use a
Durable Subscriber. While it is running, it can display the range thus far. If it loses its connection,
when it reconnects, it can process the price changes that occurred and update the range as
necessary.
JMS supports durable subscriptions for TopicSubscribers. [JMS11, pp.80-81], [Hapner, pp.61-63]
One challenge with durable subscriptions is differentiating between an old subscriber that is
reconnecting vs. a completely new subscriber. In JMS, a durable subscription is identified by
three criteria:
1. the topic being subscribed to
2. the connection¡¯s client ID
3. and the subscriber¡¯s subscription name
The connection¡¯s client ID is a property of its connection factory, which is set when the
connection factory is created using the messaging system¡¯s administration tool. The subscription
name has to be unique for each subscriber (for a particular topic and client ID).
A Durable Subscriber is created using the Session.createDurableSubscriber method:
This subscriber is now active. It will receive messages as they are published to the topic (just like
a non-durable subscriber). To make it inactive, close it, like this:
The subscriber is now disconnected and therefore inactive. Any messages published to its topic
will be saved for this subscriber and delivered when it reconnects.
To make the subscription active again, you must create a new durable subscriber with the same
topic, client ID, and subscription name. The code is the same as before, except that the connection
factory, topic, and subscription name must be the same as before.
Because the code is the same to establish a durable subscription and to reconnect to it, only the
messaging system knows whether this durable subscription had already been established or is a
new one. One interesting consequence is that the application re-connecting to a subscription may
not be the same application that disconnected earlier. As long as the new application uses the
same topic, the same connection factory (and so the same client ID), and the same subscription
name as the old application, the messaging system cannot distinguish between the two
applications and will proceed to deliver all messages to the new application that weren¡¯t
delivered to the old application before it disconnected.
Once an application has a durable subscription on a topic, it will have the opportunity to receive
all messages published to that topic, even if the subscriber closes its connection (or if it crashes
and the messaging system closes the subscriber¡¯s connection for it). To stop the messaging system
from queuing messages for this inactive subscriber, the application must explicitly unsubscribe
its durable subscription.
Once the subscriber is unsubscribed, the subscription is removed from the topic, and messages
will no longer be delivered to this subscriber.
##%%&&
Even when a sender application only sends a message once, the receiver application may receive
the message more than once.
How can a message receiver deal with duplicate messages?
The Channel Patterns discuss how to make messaging channels reliable by using Guaranteed
Delivery. However, some reliable messaging implementations can produce duplicate messages. In
other scenarios, Guaranteed Delivery may not be available because the communication relies on
inherently unreliable protocols. This is the case in many B2B (business-to-business) integration
scenarios where messages have to be sent over the Internet using the HTTP protocol. In these
cases, message delivery can generally only be guaranteed by resending the message until an
acknowledgment is returned from the recipient. However, if the acknowledgment is lost due to
an unreliable connection, the sender may resend a message that the receiver had already received
(see diagram).
Many messaging systems incorporate built-in mechanisms to eliminate duplicate messages so
that the application does not have to worry about duplicates. Eliminating duplicates inside the
messaging infrastructure causes additional overhead. If the receiver is inherently resilient against
duplicate messages, for example, a stateless receiver that processes query-style Command
Messages, messaging throughput can be increased if duplicates are allowed. For this reason, some
messaging systems provide only "at least once" delivery and let the application deal with
duplicate messages. Others allow the application to specify whether it deals with duplicates or
not (for example, the JMS spec defines a DUPS_OK_ACKNOWLEDGE mode).
Another scenario that can produce duplicate messages is a failed distributed transaction. Many
packaged applications that are connected to the messaging infrastructure through commercial
adapters cannot properly participate in a distributed two-phase commit. When a message is sent
to multiple applications and the message causes one or more of these applications to fail it may
be difficult to recover from this inconsistent state. If receivers are designed to ignore duplicate
messages, the sender can simply re-send the message to all recipients. Those recipients that had
already received and processed the original message will simply ignore the resend. Those
applications that were not able to properly consume the original message will apply the message
that was resent.
Therefore:
Design a receiver to be an Idempotent Receiver--one that can safely receive the same message
multiple times.
The term idempotent is used in mathematics to describe a function that produces the same result if
it is applied to itself, i.e. f(x) = f(f(x)). In Messaging this concepts translates into the a message that
has the same effect whether it is received once or multiple times. This means that a message can
safely be resent without causing any problems even if the receiver receives duplicates of the same
message.
Idempotency can be achieved through two primary means:
1. Explicit "de-duping", i.e. the removal of duplicate messages.
2. Defining the message semantics to support idempotency.
The recipient can explicitly de-dup messages (let's assume this is a proper English word) by
keeping track of messages that it already received. A unique message identifier simplifies this
task and helps detect those cases where two legitimate messages with the same message content
arrive. By using a separate field, the message identifier, we do not tie the semantics of a duplicate
message to the message content. We then assign a unique message identifier to each message.
Many messaging systems, such as JMS-compliant messaging tools, automatically assign unique
message identifiers to each message without the applications having to worry about them.
In order to detect and eliminate duplicate messages based on the message identifier, the message
recipient has to keep a buffer of already received message identifiers. One of the key design
decisions is how long to keep this history of messages and whether to persist the history to
permanent storage such as disk. This decision depends primarily on the contract between the
sender and the receiver. In the simplest case, the sender sends one message at a time, awaiting
the receiver's acknowledgment after every message. In this scenario, it is sufficient for the
receiver to compare the message identifier of any incoming message to the identifier of the
previous message. It will then ignore the new message if the identifiers are identical. Effectively,
the receiver keeps a history of a single message. In practice, this style of communication can be
very inefficient, especially if the latency (the time for the message to travel from the sender to the
receiver) is significant relative to the desired message throughput. In these situations, the sender
may want to send a whole set of messages without awaiting acknowledgment for each one. This
implies, though, that the receiver has to keep a longer history of identifiers for already received
messages. The size of the receiver's "memory" depends on the number of message the sender can
send without having gotten an acknowledgment from the receiver. This problem resembles the
considerations presented in the Resequencer.
Eliminating duplicate messages is another example where we can learn quite a bit by having a
closer look at the low-level TCP/IP protocol. When IP network packets are routed across the
network, duplicate packets can be generated. The TCP/IP protocol ensures elimination of
duplicate packets by attaching a unique identifier to each packet. Sender and receiver negotiate a
"window size" that the recipient allocates in order to detect duplicates. For a thorough discussion
of how TCP/IP implements this mechanism, see [Stevens].
In some cases, it may be tempting to use a business key as the message identifier and let the
persistence layer handle the de-duping. For example, let's assume that an application persists
incoming orders into a database. If each order contains a unique order number, and we configure
the database to use a unique key on the order number field, the insert operation into the database
would fail if a duplicate order message is received. This solution appears elegant because we
delegated the checking of duplicates to the database systems which is very efficient at detecting
duplicate keys. But we have to be cautious because we associated dual semantics to a single field.
Specifically, we tied infrastructure-related semantics (a duplicate message) to a business field
(order number). Imagine that the business requirements change so that customers can amend
existing orders by sending another message with the same order number (this is quite common).
We would now have to make changes to our message structure since we tied the unique message
identifier to a business field. Therefore, it is best to avoid overloading a single field with dual
semantics.
Using a database to force de-duping is sometimes used with database adapters that are provided
by the messaging infrastructure vendors. In many cases, these adapters are not capable of
eliminating duplicates so that this function has to be delegated to the database.
An alternative approach to achieve idempotency is to define the semantics of a message such that
resending the message does not impact the system. For example, rather than defining a message
as "Add $10 to account 12345", we could change the message to "Set the balance of account 12345
to $110". Both messages achieve the same result if the current account balance is $100. The second
message is idempotent because receiving it twice will not have any effect. Admittedly, this
example ignores concurrency situations, for example the case where another message "Set the
balance of account 12345 to $150" arrives between the original and the duplicate message.
The Microsoft Interface Definition Language (MIDL) supports the concept of idempotency as part
of the remote call semantics. A remote procedure can be declared as idempotent by using the
[idempotent] attribute. The MIDL specification states that the "[idempotent] attribute specifies
that an operation does not modify state information and returns the same results each time it is
performed. Performing the routine more than once has the same effect as performing it once".
##%%&&
An application has a service that it would like to make available to other applications.
How can an application design a service to be invoked both via various messaging
technologies and via non-messaging techniques?
An application may not want to choose whether a service (an operation in a Service Layer [EAA])
can be invoked synchronously or asynchronously, it may want to support both approaches for
the same service. Yet technologies can seem to force the choice. For example, an application
implemented using EJB (Enterprise JavaBeans) may need to use a session bean to support
synchronous clients, but a message-driven bean to support messaging clients. (Thanks to Mark
Weitzel for this example.)
Developers designing an application to work with other applications, such as a B2B
(business-to-business) application, may not know what other applications they¡¯re communicating
with and how the various communication will work. There are too many different messaging
technologies and data formats to try to support every one just in case it¡¯s needed. (Thanks to
Luke Hohmann for this example.)
Receiving and processing a message involves a number of steps; separating these steps can be
difficult and unnecessarily complex. Yet Message Endpoint code that mixes together these
tasks¡ªreceiving the message, extracting its contents, and acting on those contents to perform
work¡ªcan be difficult to reuse.
When designing clients for multiple styles of communication, it may well seem necessary to
reimplement the service for each style. This makes supporting each new style cumbersome and
creates the risk that each style may not produce quite the same behavior. What is needed is a way
for a single service to support multiple styles of communication.
Design a Service Activator that connects the messages on the channel to the service being
accessed.
A Service Activator can be one-way (request only) or two-way (Request-Reply). The service can be
as simple as a method call¡ªsynchronous and non-remote¡ªperhaps part of a Service Layer
[EAA]. The activator can be hard-coded to always invoke the same service, or can use reflection
to invoke the service indicated by the message. The activator handles all of the messaging details
and invokes the service like any other client, such that the service doesn¡¯t even know it¡¯s being
invoked through messaging.
The Service Activator handles receiving the request message (either as a Polling Consumer or as an
Event-Driven Consumer). It knows the message¡¯s format and processes the message to extract the
information necessary to know what service to invoke and what parameter values to pass in. The
activator then invokes the service just like any other client of the service, and blocks while the
service executes. When the service completes and returns a value, the activator can optionally
create a reply message containing the value and return it to the requestor. (The reply makes the
service invocation an example of Request-Reply messaging.)
A Service Activator enables a service to be written as though it¡¯s always going to be invoked
synchronously. The activator receives the asynchronous message, determines what service to
invoke and what data to pass it, and then invokes the service synchronously. The service is
designed to work without messaging, yet the activator enables it to easily be invoked via
messaging.
If the Service Activator cannot process the message successfully, the message is invalid and should
be moved to an Invalid Message Channel. If the message can be processed and the service is
invoked successfully, then any errors that occur as part of executing the service are semantic
errors in the application and should be handled by the application.
Developers still may not be able to predict every way partners might wish to access their services,
but they do at least know what services their application will provide and can implement those.
Then implementing new activators for different technologies and formats as needed is relatively
easy.
This Service Activator pattern is also documented in [CoreJ2EE], which is where the pattern was
originally named. That version of the pattern is somewhat different from this one¡ªit assumes the
activator is an Event-Driven Consumer, and assumes that the service already exists so that the
activator can be added to the service¡ªbut both versions propose the same solution to the same
problem in a very similar fashion. Service Activator is related to the Half-Sync/Half-Async pattern
[POSA2], which separates service processing into synchronous and asynchronous layers.
A Service Activator usually receives Command Messages, which describe what service to invoke. A
Service Activator serves as a Messaging Gateway, separating the messaging details from the service.
The activator can be a Polling Consumer or an Event-Driven Consumer. If the service is transactional,
the activator should be a Transactional Client so that the message consumption can participate in
the same transaction as the service invocation. Multiple activators can be Competing Consumers or
coordinated by a Message Dispatcher. If a Service Activator cannot process a message successfully, it
should send the message to an Invalid Message Channel.
Consider, for example, Enterprise JavaBeans (EJBs) [EJB20] in J2EE: Encapsulate the service as a
session bean, and then implement message-driven beans for various messaging scenarios: One
for a JMS destination using messages of one format; another for a different destination using
another format; another for a web service/SOAP message; and so on. Each message-driven bean
that processes the message by invoking the service is a Service Activator. Clients that wish to
invoke the service synchronously can access the session bean directly.
##%%&&
Naturally, enterprise integration systems are distributed. In fact, one of the defining qualities of
an enterprise messaging system is to enable communication between disparate systems.
Messaging systems allow information to be routed and transformed so that data can be
exchanged between these systems. In most cases, these applications are spread across multiple
networks, buildings, cities or continents.
How can we effectively administer a messaging system that is distributed across multiple
platforms and a wide geographic area?
A distributed, loosely coupled architecture allows for flexibility and scalability. At the same time
it poses serious challenges for administration and control of such a system. For example, how can
you tell whether all components are up and running? A simple process status won't suffice
because processes are distributed across many machines. Also, if you cannot obtain status from a
remote machine does it mean that the remote machine is not functioning or may the
communication with the remote machine be disturbed?
Besides just knowing whether a system or a component is up and running you also need to
monitor the dynamic behavior of the system. What is the message throughput? Are there any
unusual delays? Are channels filling up? Some of this information requires tracking of message
travel times between components or through components. This requires the collection and
combination of information from more than one machine.
Also, just reading information from components may not be sufficient. Often times, you need to
make adjustments or change configuration settings while the system is running. For example,
you may need to turn logging features on or off while the system is running. Many applications
use property files and error logs to read configuration information and report error conditions.
This approach tends to work well as long as the application consists of a single machine, or
possibly a small number of machines. In a large, distributed solution, property files would have
to be copied to remote machines using some file transfer mechanism, which requires the file
system on every machine to be accessible remotely. This can pose security risks and can be
challenging if the machines are connected over the Internet or a wide-area network that may
not support file mapping protocols. Also, the versions of the local property files would have to be
managed carefully -- a management nightmare waiting to happen.
It seems natural to try to leverage the messaging infrastructure to perform some of these tasks.
For example, we could send a message to a component to change its configuration. This control
message could be transported and routed just like a regular message. This would solve most of
the communication problems but also poses new challenges. Configuration messages should be
subject to stricter security policies than regular application messages. For example, one wrongly
formatted control message could easily bring a component down. Also, what if messages are
queued up on a message channel because a component is malfunctioning? If we send a control
message to reset the component, this control message would get queued up with all the other
messages and not reach the component in distress. Some messaging systems support message
priorities that can help move control messages to the front of the queue. However, not all systems
provide this ability, and the priority may not help if a queue is filled to the limit and refuses to
accept another message. Likewise, some control messages are of a lower priority than application
messages. If we have components publish periodic status messages, delaying or losing a 'I am
alive' control message may be a lot less troublesome and delaying or losing the 'Order for $1
Million' message.
Use a Control Bus to manage an enterprise integration system. The Control Bus uses the same
messaging mechanism used by the application data, but uses separate channels to transmit
data that is relevant to the management of components involved in the message flow.
Each component in the system is now connected to two messaging subsystems:
? The Application Message Flow
? The Control Bus
The application message flow transports all application-related messages. The components
subscribe and publish to these channels just like they would in an unmanaged scenario. In
addition, each component also sends and receives messages from the channels that make up the
Control Bus. These channels connect to a central management component.
The Control Bus is well suited to carry the following types of messages:
? Configuration - each component involved in the message flow should have configurable
parameters that can be changed as required. These parameters include channel addresses,
message data formats, timeouts etc. Components use the Control Bus to retrieve this
information from a central repository rather than using property files, allowing a central
point of configuration and the reconfiguration of the integration solution at run-time. For
example, the routing table inside a Content-Based Router may need to be updated
dynamically based on system conditions, such as overload or component failure.
? Heartbeat - each component may send a periodic 'heartbeat' message on the Control Bus
at specified intervals so that a central console application can verify that the component is
functioning properly. This 'heartbeat' may also include metrics about the component,
such as number of messages processed, the amount of available memory on the machine
etc.
? Test Messages - heartbeat messages tell the Control Bus that a component is still alive, but
may provide limited information on ability of the component to correctly process
messages. In addition to having components publish periodic heartbeat messages to the
Control Bus, we can also inject test messages into the message stream that will be
processed by the components. We will then extract the message later to see whether the
component processed the message correctly. As this blurs the definition of the control bus
and the message bus, I decided to define a separate pattern for it (see Test Message).
? Exceptions - each component can channel exception conditions to the Control Bus to be
evaluated. Severe exceptions may cause an operator to be alerted. The rules to define
exception handling should be specified in a central handler.
? Statistics - Each component can collect statistics about the number of messages processed,
average throughput, average time to process a message, etc. Some of this data may be
split out by message type, so we can determine whether messages of a certain type are
flooding the system. Since this message tends to be lower priority than other messages, it
is likely that the Control Bus uses non-guaranteed or lower-priority channels for this type
of data.
? Live Console - most of the functions mentioned here can be aggregated for display in a
central console. From here, operators can assess the health of the messaging system and
take corrective action if needed.
Many of the functions that a Control Bus supports resemble traditional network management
functions that are used to monitor and maintain any networked solution. A Control Bus allows us
to implement equivalent management functions at the messaging system level -- essentially
elevating them from the low-level IP network level to the richer messaging level. Providing this
functionality is vital to the successful operation of a messaging infrastructure, but the absence of
management standards for messaging solutions makes it difficult to build enterprise-wide,
reusable management solutions for messaging systems.
When we design message processing components, we architect the core processor around three
interfaces (see Figure). The inbound data interface receives incoming messages from the message
channel. The outbound data interface sends processed messages to the outbound channel. The
control interface sends and receives control messages from and to the Control Bus.
At the end of this chapter, we show how to use a Control Bus to instrument the Asynchronous
Implementation with MSMQ of the Loan Broker example. This implementation includes a simple
management console that displays the status of components in real-time (see Loan Broker System
Management).
##%%&&
A Wire Tap is useful to inspect messages that travel across a channel. Sometimes, though, we
need to modify or reroute the messages instead of simply inspecting them.
How can you route a message through intermediate steps to perform validation, testing or
debugging functions?
Performing validations on messages that travel between components can be a very useful
debugging tool. However, these extra steps may not always be required and would slow down
the system if they are always executed.
Being able to include or skip these steps based on a central setting can be a very effective
debugging or performance tuning tool. For example, while we test a system we may want to pass
messages through additional validation steps. Bypassing these steps during production may
improve performance. We can compare these validations to assert statements in code that are
executed in the debug configuration but not in the release configuration of the executable.
Likewise, during trouble-shooting it may be useful to route messages through additional steps
for logging or monitoring purposes. Being able to turn these logging steps on and off allows us to
maximize message throughput under normal circumstances.
Construct a Detour with a context-based router controlled via the Control Bus. In one state the
router routes incoming messages through additional steps while in the other it routes
messages directly to the destination channel.
The Detour uses a simple context-based router with two output channels. One output channel
passes the unmodified message to the original destination. When instructed by the Control Bus,
the Detour routes messages to a different channel. This channel sends the message to additional
components that can inspect and/or modify the message. Ultimately, these components route the
message to the same destination.
If the detour route contains only a single component, it may be more efficient to combine the
Detour switch and the component into a single filter. However, this solution assumes that the
component in the detour path can be modified to include the Control Bus-controlled bypass logic.
The strength of controlling the Detour over the Control Bus is that multiple Detours can be
activated or deactivated simultaneously with a single command on the Control Bus using a
Publish-Subscribe Channel from the control console to all Detours.
##%%&&
Point-to-Point Channels are often used for Document Messages because they ensure that exactly one
consumer will consume each message. However, for testing, monitoring or troubleshooting, it
may be useful to be able to inspect all messages that travel across the channel.
How do you inspect messages that travel on a point-to-point channel?
It can be very useful to see which messages traverse a channel. For example, for simple
debugging purposes or to store messages in a Message Store.
You can't just add another listener to the Point-to-Point Channel because it would consume
messages off the channel and prevent the intended recipient from being able to consume the
message.
Alternatively, you could make the sender or the receiver responsible to publish the message to a
separate channel for inspection. However, this would force us to modify a potentially large set of
components. Additionally, if we are dealing with packaged applications, we may not even be
able to modify the application.
You could also consider changing the channel to a Publish-Subscribe Channel. This would allow
additional listeners to inspect messages without disturbing the flow of messages. However, a
Publish-Subscribe Channel change the semantics of the channel. For example, multiple Competing
Consumers may be consuming messages off the channel, relying on the fact that only one
consumer can receive a specific message. Changing the channel to the a Publish-Subscribe Channel
would cause each consumer to receive each message. This could be very undesirable, for example
if the incoming messages represent orders that now get processed multiple times. Even if only a
single consumer listens on the channel, using a Publish-Subscribe Channel may be less efficient or
less reliable than a Point-to-Point Channel.
Many messaging systems provide a peek method that allows a component to inspect a messages
inside a Point-to-Point Channel without consuming the message. This approach has one important
limitation though: once the intended consumer consumes the message, the peek method can no
longer see the message. Therefore, this approach does not allow us to analyze messages after they
have been consumed.
You could insert a component into the channel (a form of "interceptor') that performs any
necessary inspection. The component would consume a message off the incoming channel,
inspect the message and pass the unmodified message to the output channel. However,
frequently the type of inspection depends on messages from more than one channel (e.g. to
measure message run-time) so that this function cannot be implemented by a single filter inside a
single channel.
Insert a simple Recipient List into the channel that publishes each incoming message to the
main channel and a secondary channel.
The Wire Tap is a fixed Recipient List with two output channels. It consumes messages off the
input channel and publishes the unmodified message to both output channels. To insert the Wire
Tap into a channel, you need to create an additional channel and change the destination receiver
to consume of the second channel. Because the analysis logic is located inside a second
component, we can insert a generic Wire Tap into any channel without any danger of modifying
the primary channel behavior. This improves reuse and reduces the risk of instrumenting an
existing solution.
It might be useful to make the Wire Tap programmable over the Control Bus so that the secondary
channel (the "tap") can be turned on or off. This way, the Wire Tap can be instructed to publish
messages to the secondary channel only during testing or debugging cycles.
The main disadvantage of the Wire Tap is the additional latency incurred by consuming and
republishing a message. Many integration tool suites automatically decode a message even if it is
published to another channel without modification. Also, the new message will receive a new
message id and new time stamps. These operations can add up to additional overhead.
Because the Wire Tap publishes two messages, it is important not to correlate messages by their
message ID. Even though the primary and the secondary channel receive identical messages,
most messaging systems automatically assign a new message ID to each message in the system.
This means that the original message and the "duplicate" message have different message IDs.
A Message Broker can easily be turned into a Wire Tap because all messages already pass through
this central component.
An important limitation of the Wire Tap is that it cannot alter the messages flowing across the
channel. If you need to be able to manipulate messages, use a Detour instead.
At the end of this chapter we enhance the loan broker to include a Wire Tap on the request
channel to the Credit Bureau to keep a log of all requests made to this external service (see Loan
Broker System Management).
One of the strengths of the Wire Tap is that we can combine multiple Wire Taps to send copies of
messages to a central component for analysis. That component can be a Message Store or another
component that analyses relationships between messages, e.g. the time interval between two
related messages. (see picture)
##%%&&
One of key benefits of a message-based systems is the loose coupling between participants; the
message sender and recipient make no (or few) assumptions about each other's identity. If a
message recipient retrieves a message from a message channel, it generally does not know nor
care which application put the message on the channel. The message is by definition
self-contained and is not associated with a specific sender. This is one of the architectural
strengths of message-based systems.
However, the same property can make debugging and analyzing dependencies very difficult. If
we are not sure where a message goes, how can we assess the impact of a change in the message
format? Likewise, if we don't know which application published a message it is difficult to
correct a problem with the message.
How can we effectively analyze and debug the flow of messages in a loosely coupled system?
The Control Bus monitors the state of each component that processes messages, but it does not
concern itself with the route that an individual message takes. You could modify each component
to publish the unique message identifier of each message that passes through it to the Control Bus.
This information can then be collected in a common database, a Message Store. This approach
requires a significant amount of infrastructure, including a separate data store. Also, if a
component needs to examine the history of a message, it would have to execute a query against a
central database, running the risk of turning the database into a bottle neck.
Tracking the flow of a message through a system is not as simple as it appears. It would seem
natural to use the unique message ID associated with each message. However when a component
(e.g. a Message Router processes a message and publishes it to the output channel, the resulting
message will receive a new message identifier that is not associated with the message that the
component consumed. Therefore, we would need to identify a new key that is copied from the
incoming message to outgoing message to that the two message can be associated later. This can
work reasonable well if the component publishes exactly one message for every message it
consumes. However, this is not the case for many components, e.g. a Recipient List, an Aggregator,
or a Process Manager.
Instead of identifying the path of each message by tagging the messages, the message itself could
collect a list of components that it traversed. If each component in the messaging system carries a
unique identifier, each component could add its identifier to each message it publishes.
Therefore, attach a Message History to the message. The Message History is a list of all
applications that the message passed through since its origination.
The Message History maintains a list of all components that the message passed through. Every
component that processes the message (including the originator) adds one entry to the list. The
Message History should be part of the message header because it contains system-specific control
information. Keeping this information in the header separates it from the message body that
contains application specific data.
Not every message that a component publishes is the result of a single message. For example, an
Aggregator publishes a single message that carries information collected from multiple messages,
each of which could have its own history. If we want to represent this scenario in the Message
History, we have two choices. If we want to track the complete history we can enhance the
Message History to be stored as a hierarchical tree structure. Because of the recursive nature of a
tree structure we can store multiple message histories under a single 'node'. Alternatively, we can
keep a simple list and only keep the history of one incoming message. This can work well if one
incoming message is more important to the result than other, auxiliary messages.
The Message History is most useful if a series of messages flows through a number of filters to
perform a specific business function or process. If it is important to manage the path that a
message takes, a Process Manager can be useful. The Process Manager creates one process instance
for each incoming trigger message. The flow of the message through various components is now
managed centrally, alleviating the need to tag each message with its history.
Equipping a message with its history has also another important benefit when using
Publish-Subscribe Channels to propagate events. Assume we implement a system that propagates
address changes to multiple systems via Publish-Subscribe Channels. Each address change is
broadcast to all interested systems so that they may update their records. This approach is very
flexible towards the addition of new systems -- the new system will automatically receive the
broadcast message without requiring any changes to the existing messaging system. Assume the
customer care system is one of the systems that stores addresses in the application database. Each
change to the database fields causes a message to be triggered to notify all systems of the change.
By nature of the publish-subscribe paradigm, all systems subscribing to the 'address changed'
channel will receive the event. The customer care system itself has to subscribe to this channel, in
order to receive updates made in other systems, e.g. through a self-service Web site. This means
that the customer care system will receive the message that it just published. This received
message would result in a database update, which will in turn trigger another 'address changed'
message. We could end up in an infinite loop of 'address changed' messages. To avoid such an
infinite loop, the subscribing applications can inspect the Message History to determine whether
the message originated from the very same system and ignore the incoming message if this is the
case.
Many EAI integration suites include support for a Message History. For example, the message
header of every ActiveEnterprise message includes a tracking field that maintains a list of all
components though which the message has passed. In this context it is important to note that a
TIBCO ActiveEnterprise component assigns outgoing messages the same message ID as the
consumed message. This makes tracking of messages through multiple component easier, but it
also means that the message ID is not a system-wide unique property because multiple
individual messages share the same ID. For example, when implementing a Recipient List TIBCO
ActiveEnterprise transfers the ID of the consumed message to each outbound message.
The following example shows the dump of a message that passed through multiple components,
including two Integration Manager processes, OrderProcess and VerifyCustomerStub.
##%%&&
As the Message History describes, the architectural principle of loose coupling allows for flexibility
in the solution, but can make it difficult to gain insight into the dynamic behavior of the
integration solution.
How can we report against message information without disturbing the loosely coupled and
transient nature of a messaging system?
The very properties that make messaging powerful can also make it difficult to manage.
Asynchronous messaging guarantees delivery, but does not guarantee when the message will be
delivered. For many practical applications, though the response time of a system may be critical.
It may also be important to know how many messages passed through the system within a
certain time interval.
The Message History pattern illustrates the usefulness of being able to tell the "source" of a
message. From this data, we can derive interesting message throughput and runtime statistics.
The only downside is that the information is contained within each individual message. There is
no easy way to report against this information since it is spread across many messages. Also, the
lifetime of a message can be very short. Once the message is consumed, the Message History may
no longer be available.
In order to perform meaningful reporting, we need to store message data persistently and in a
central location.
Use a Message Store to capture information about each message in a central location.
When using a Message Store, we can take advantage of the asynchronous nature of a messaging
infrastructure. When we send a message to a channel, we send a duplicate of the message to a
special channel to be collected by the Message Store. This can be performed by the component
itself or we can insert a Wire Tap into the channel. We can consider the secondary channel that
carries a copy of the message as part of the Control Bus. Sending a second message in a
'fire-and-forget' mode will not slow down the flow of the main application messages. It does,
however, increase network traffic. That's why we may not store the complete message, but just a
few key fields that are required for later analysis, such as a message ID, or the channel on which
the message was sent and a timestamp.
How much detail to store is actually an important consideration. Obviously, the more data we
have about each message, the better reporting abilities we have. The counter-forces are network
traffic and storage capacity of the Message Store. Even if we store all message data, our reporting
abilities may still be limited. Messages typically share the same message header structure, but the
message content is structured differently. Therefore, we may not be able to easily report against
the data elements contained in a message.
Since the message data is different for each type of message, we need to consider different
storage options. If we create a separate storage schema (e.g. tables) for each message type that
matches that message type's data structure, we can apply indexes and perform complex searches
on the message content. However, this assumes that we have a separate storage structure for each
message type. This could become a maintenance burden. We could also store the message data as
unstructured data in XML format in a long character field. This allows us a generic storage
schema. We could still query against header fields, but would not be able to report against fields
in the message body. However, once we identified a message, we can recreate the message
content based on the XML document stored in the Message Store.
The Message Store may get very large, so most likely we will need to consider introducing a
purging mechanism. This mechanism could move older message logs to a back-up database or
delete it altogether.
Some enterprise integration tools supply a Message Store. For example, MSMQ allows queues to
automatically store sent or received messages in a Journal Queue. Microsoft BizTalk optionally
stores all documents (messages) in a SQL Server database for later analysis.
##%%&&
A pair of Wire Taps can be used to track messages that flow through a component. However, this
approach assumes that the component publishes messages to a fixed output channel. However,
many service-style components publish reply messages to the channel specified by the Return
Address included in the request message.
How can you track messages on a service that publishes reply messages to the Return Address
specified by the requestor?
In order to track messages flowing through a service, we need to capture both request and reply
messages. Intercepting a request message using a Wire Tap is easy enough. Intercepting reply
messages is the tough part because the service publishes the reply message to different channels
based on the requestor's preferred Return Address.
The support of a Return Address is required for most Request-Reply services so that a requestor can
specify the channel that the reply message should be sent to. Changing the service to post reply
messages to a fixed channel would make it hard for each requestor to extract the correct reply
messages. Some messaging systems allow consumers to "peek" for specific messages inside a
single reply queue but that approach is implementation specific and does not work in those
instances where the reply message does not go back to the requestor but to a third party.
As discussed in the Wire Tap, modifying the component to inspect messages is not always feasible
or practical. If we are dealing with a custom application we may not be able to modify the
application code and have to implement a solution that is external to the application. Likewise,
we may not want to require each application to implement inspection logic, especially because
the nature of the logic may vary depending on whether we operate in test mode or production
mode. Keeping the inspection functions in a separate self-contained component improves
flexibility, reuse and testability.
Use a Smart Proxy to store the Return Address supplied by the original requestor and replace
it with the address of the Smart Proxy. When the service sends the reply message route it to
the original Return Address.
The Smart Proxy intercepts messages sent on the request channel to the Request-Reply service. For
each incoming message, the Smart Proxy stores the Return Address specified by the original sender.
It then replaces the Return Address in the message with the channel the reply channel that the
Smart Proxy is listening on. When a reply message comes in on that channel, the Smart Proxy
retrieves the stored Return Address and uses a Message Router to forward the unmodified reply
address to that channel.
The Smart Proxy is also useful in cases where an external service does not support a Return
Address but instead replies to a fixed reply channel. We can proxy such a service with a Smart
Proxy to provide support for a Return Address. In this case the Smart Proxy performs no analytical
functions, but simply forwards the reply message to the correct channel.
The Smart Proxy needs to store the Return Address supplied by the original requestor in such a
way that it can correlate incoming reply messages with the Return Address and forward the reply
message to the correct channel. The Smart Proxy can store this data in two places:
? Inside the Message
? Inside the Smart Proxy
To store the Return Address inside the message, the Smart Proxy can add a new message field with
the Return Address to the message. The Request-Reply service is required to copy this field the
reply message. All the Smart Proxy has to do is to extract the special message field from the reply
message, remove the field from the message and forward the message to the channel specified by
the field. This solution keeps the Smart Proxy simple but it requires collaboration by the
Request-Reply service. If the Request-Reply service is a non-modifiable component, this option may
not be available.
Alternatively, the Smart Proxy can store the Return Address in dedicated storage, for example in a
memory structure or a relational database. Because the purpose of the Smart Proxy is to track
messages between the request and reply message the Smart Proxy usually has to store data from
the request message anyway to correlate it to the reply message and to analyze both messages in
unison. This approach requires the Smart Proxy to be able to correlate the reply message to the
response message. Most Request-Reply services support a Correlation Identifier that the service
copies from the request message to the reply message. If the Smart Proxy cannot modify the
original message format, it can (ab-)use this field to correlate request and reply messages. The
Smart Proxy should construct its own Correlation Identifier because not all requestors will specify a
Correlation Identifier and also because the Correlation Identifier needs to be unique only across
requests made by a single requestor and not across multiple requestors. Because the service reply
queue now carries messages from multiple requestors using the original Correlation Identifier is
not reliable. Therefore, the Smart Proxy stores the original Correlation Identifier together with the
original Return Address and replaces the original Correlation Identifier with its own Correlation
Identifier so that it can retrieve the original Correlation Identifier and Return Address when the reply
message arrives. Many services us the Message ID of the request message as the Correlation
Identifier for the reply message. This introduces another problem. The service will now copy the
Message ID of the request message it received from the Smart Proxy to the reply message to the
Smart Proxy. The Smart Proxy needs to replace this Correlation Identifier in the reply message with
the Message ID of the original request message so that the requestor can properly correlate
request and reply messages. The following picture illustrates this process:
Implementing a Smart Proxy is not as complicated as it sounds. The following code implements a
solution scenario consisting of two requestors, a Smart Proxy and a simple service. The Smart
Proxy passes the message processing-time to the control bus for display in the console. We want
to allow the requestors to correlate by either the message Id or the numeric AppSpecific property
provided by the Message object.
For our coding convenience, we define a base class MessageConsumer that encapsulates a the code
that is required to create an event-driven message consumer. Inheriting classes can simply
overload the virtual method ProcessMessage to perform any necessary message handling and do
not have to worry about the configuration of the message queue or the event-driven processing.
Separating this code into a common base class makes it easy to create test clients and a dummy
request-reply service with just a few lines of code.
With the MessageConsumer class as a starting point, we can create a Smart Proxy. A Smart Proxy
contains two MessageConsumers, one for the request messages coming from the requestors and one
for the reply messages returned by the request-reply service. The Smart Proxy also defines a
Hashtable to store message data between request and reply message.
The SmartProxyRequestConsumer is relatively simple. It stores relevant information from the
request message (message ID, the Return Address, the AppSpecific property, and the current time)
in the hashtable, indexed by the message Id of the new request message sent to the actual service.
The request-reply service copies this message ID to the CorrelationID field of the service reply
message so that the Smart Proxy can retrieve the stored message data. The
SmartProxyRequestConsumer also replaces the Return Address (the ResponseQueue property) with
the queue that the Smart Proxy listens on for reply messages. We included a virtual method
AnalyzeMessage in this class so that subclasses can perform any desired analysis.
The SmartProxyReplyConsumer listens on the service reply channel. The ProcessMessage method
retrieves the message data for the associated request message stored by the
SmartProxyRequestConsumer and calls the AnalyzeMessage template method. It then copies the
CorrelationID and the AppSpecific properties to the new reply message and routes it to the
Return Address specified in the original request message.
In order to collect metrics and send them to the control bus we subclass both the generic
SmartProxy and SmartProxyReplyConsumer classes. The new MetricsSmartProxy instantiates the
SmartProxyReplyConsumerMetrics as the consumer, which includes a simple implementation of
the AnalyzeMessage method. The method computes the message runtime between request and
response and sends this data together with the number of outstanding messages to the control
bus queue. We could easily enhance this method to perform more complex computations. The
control bus queue is connected to a simple file writer that writes each incoming message to a file.
The class diagram for the solution looks as follows:
Smart Proxy Example Class Diagram
To test the proxy, we created a dummy request-reply service that does nothing but wait for a
random interval between 0 and 200 ms. We feed the Smart Proxy from two requestors, each of
which publishes 30 messages in 100ms intervals. For demonstration purposes we loaded the
resulting control bus file into a Microsoft Excel spreadsheet and created a nice looking chart:
We can see that the queue size and the response time increase steadily until 13 messages are
queued up. At that time, the requestors do no longer send new messages and the queue size
decreases steadily. The response time decreases as well, but remains around 1 second because the
message that are now being processed have been sitting in the request queue for that long.
##%%&&
The Control Bus describes a number of approaches to monitor the health of the message
processing system. Each component in the system can publish periodic "heartbeat" messages to
the control bus in order to keep the monitoring mechanism informed that the component is still
active. The heartbeat messages can contain vital stats of the component as well, such as the
number of messages processed, the average time required to process a message or the percentage
of CPU utilization on the machine.
What happens, though, if a component is actively processing messages, but garbles outgoing
messages due to an internal fault?
A simple heartbeat mechanism will not detect this error condition because it operates only at a
component level and is not aware of application message formats.
Therefore, use Test Message to assure the health of message processing components (see
Figure).
The Test Message pattern relies on the following components:
? The Test Data Generator creates messages to be sent to the component for testing. Test
data may be constant, driven by a test data file or generated randomly.
? The Test Message Injector inserts test data into the regular stream of data messages sent
to the component. The main role of the injector is to tag messages in order to differentiate
'real' application messages from test messages. This can be accomplished by inserting a
special header field. If we have no control over the message structure, we can try to use
special values to indicate test messages (e.g. OrderID = 999999). This changes the
semantics of application data by using the same field to represent application data (the
actual order number) and control information (this is a test message). Therefore, this
approach should be used only as a last resort.
? The Test Message Separator extracts the results of test messages from the output stream.
This can usually be accomplished by using a Content-Based Router.
? The Test Data Verifier compares actual results with expected results and flags an
exception if a discrepancy is discovered. Depending on the nature of the test data, the
verifier may need access to the original test data.
An explicit Test Message Separator may not needed if the component under test supports a
Return Address. In this case the test Data Generator can include a special test channel as the Return
Address so that test messages are not passed through the remainder of the system. Effectively, the
Return Address acts as the tag for text messages.
Test Message is considered an active monitoring mechanism. Unlike passive mechanisms, active
mechanisms do not rely on information generated by the components (e.g. log files or heartbeat
messages), but actively probe the component. The advantage is that active monitoring usually
achieves a deeper level of testing since data is routed through the same processing steps as the
application messages. It also works well with components that were not designed to support
passive monitoring.
One possible disadvantage of active monitoring is the additional load placed on the processor.
We need to find a balance between frequency of test and minimizing the performance impact.
Active monitoring may also incur cost if we are being charged for the use of a component on a
pay-per-use basis. This is the case for many external components, e.g. if we request credit reports
for our customers from an external credit scoring agency.
Active monitoring does not work with all components. Stateful components may not be able to
distinguish test data from real data and may create database entries for test data. We may not
want to have test orders included in our annual revenue report!
In the example at the end of the chapter, we use a Test Message to actively monitor the external
Credit Bureau (see Loan Broker System Management).
##%%&&
When Bobby and I worked on the simple JMS Request/Reply Example, we ran into a simple, but
interesting problem. The example consists of a Requestor which sends a message to a Replier and
waits for the response. The example uses two Point-to-Point Channels, RequestQueue and
ReplyQueue (see picture).
We had plastered println statements all over the code so we can see what is going on. We started
the Replier first, then the Requestor. Then a very odd thing happened. The Requestor console
window claimed to have gotten a response before the Replier ever acknowledged receiving a
request. A delay in the console output? Lacking any great ideas, we decided to shut the Replier
down and re-ran the Requestor. Odd enough, we still received a response to our request! Magic?
No, just a side effect of persistent messaging. A superfluous message was present on the
ReplyQueue. When we started the Requestor it placed a new message on the RequestQueue and
then immediately retrieved the extraneous reply message that was sitting on the ReplyQueue. We
never noticed that this was not the reply to the request the Requestor just made! Once the Replier
received the request message, it placed a new reply message on the ReplyQueue so that the 'magic'
repeated during the next test. It can be amazing (or amazingly frustrating) how persistent,
asynchronous messaging can play tricks on you in even the most simple scenarios!
How can you keep 'left-over' messages on a channel from disturbing tests or running systems?
Message Channels are designed to deliver messages reliably, even if the receiving component is
unavailable. In order to do so, the channel has to persist messages along the way. This useful
feature can cause confusing situations during testing or if one of the components misbehaves
(and does not use transactional message consumption and production). We can quickly end up
with extraneous messages stuck on channels as described above. These messages make it
impossible to pass test data into the system until the pending messages have been consumed. If
the pending messages are orders worth a few million Dollars, this is a good thing. If we are
testing or debugging a system and have a channel full of query messages or of reply messages it
can cause us a fair amount of headache.
In our simple example, some of our debugging pain could have been eased if we had used a
Correlation Identifier. Using the identifier, the Requestor would have recognized that the incoming
message is actually not the response to the request it just sent. It could then discard the 'old' reply
message or route it to an Invalid Message Channel, which would effectively remove the 'stuck'
message. In other scenarios, it is not as easy to detect duplicate or unwanted messages. For
example, if a specific message is malformed and causes the message recipient to fail, the recipient
cannot restart until the 'bad' message is removed, because it would just fail right away again. Of
course, this example requires the defect in the recipient to be corrected (no malformed message
should cause a component failure), but removing the message can get the system up and running
quickly until the defect is corrected.
Another way to avoid left-over messages in channels is to use temporary channels (e.g., JMS
provides the method createTemporaryQueue for this purpose). These channels are intended for
request-reply applications and lose all messages once the application closes its connection to the
messaging system. But again, this approach is limited to a simple request-reply example and
does not protect against other messages being left over on other channels that need to be
permanent.
It may be tempting to assume that transaction management can eliminate the 'extra message'
scenario because message consumption, message processing and message publication are
covered in a transaction. So if a component aborts in the middle of processing a message, the
message would not be considered consumed. Likewise, a reply message would not be published
until the component signals the final 'commit' to send the message. We need to keep in mind
though, that transactions do not protect us against programming errors. In our simple
request-reply example, a programmer error may have caused the Requestor to not read a
response from the ReplyQueue channel. As a result, despite potential transactionality, a message is
stuck on that channel causing the symptoms described above.
Use a Channel Purger to remove unwanted messages from a channel.
A simple Channel Purger simply remove all the messages from a channel. This may be sufficient
for test scenarios where we want to reset the system into a consistent state. If we are debugging a
production system we may need to remove individual message or a set of messages based on
specific criteria, such as the message ID or the values of specific message fields.
In many cases it is alright for the Channel Purger to simply delete the message from the channel.
In other cases, we may need the Channel Purger to store the removed messages for later inspection
or replay. This is useful if the messages on a channel cause a system to malfunction, so that we
need to remove them to continue operation. However, once the problems are corrected, we want
to re-inject the message(s) so that the system does not lose the contents of the message. This may
also include the requirement to edit message contents before re-injecting the message. This type
of function combines some of the features of a Message Store and a Channel Purger.
This example shows a simple Channel Purger implemented in Java. This example simply removes
all messages on a channel. The ChannelPurger class references two external classes:
? JMSEndpoint - The base class for any JMS particpant. Provides pre-initialized instance variables for a
Connection and Session instance.
? JNDIUtil - Implements helper functions to encapsulate the lookup of JMS objects via JNDI.
Both classes are described in more detail in JMS Request/Reply Example.
##%%&&
Load data on demand into a cache from a data store. This pattern can improve performance and also helps to
maintain consistency between data held in the cache and the data in the underlying data store.
Applications use a cache to optimize repeated access to information held in a data store. However, it is usually impractical to expect that cached data will always be completely consistent with the data in the data
store. Applications should implement a strategy that helps to ensure that the data in the cache is up to date
as far as possible, but can also detect and handle situations that arise when the data in the cache has become
stale.
Many commercial caching systems provide read-through and write-through/write-behind operations. In these
systems, an application retrieves data by referencing the cache. If the data is not in the cache, it is transparently retrieved from the data store and added to the cache. Any modifications to data held in the cache are
automatically written back to the data store as well.
For caches that do not provide this functionality, it is the responsibility of the applications that use the cache
to maintain the data in the cache.
An application can emulate the functionality of read-through caching by implementing the cache-aside strategy. This strategy effectively loads data into the cache on demand. Figure 1 summarizes the steps in this
process.
If an application updates information, it can emulate the write-through strategy as follows:
1. Make the modification to the data store
2. Invalidate the corresponding item in the cache.
When the item is next required, using the cache-aside strategy will cause the updated data to be retrieved
from the data store and added back into the cache.
Consider the following points when deciding how to implement this pattern:
? Lifetime of Cached Data. Many caches implement an expiration policy that causes data to be invalidated and removed from the cache if it is not accessed for a specified period. For cache-aside to be
effective, ensure that the expiration policy matches the pattern of access for applications that use the
data. Do not make the expiration period too short because this can cause applications to continually
retrieve data from the data store and add it to the cache. Similarly, do not make the expiration period
so long that the cached data is likely to become stale. Remember that caching is most effective for relatively static data, or data that is read frequently.
? Evicting Data. Most caches have only a limited size compared to the data store from where the data
originates, and they will evict data if necessary. Most caches adopt a least-recently-used policy for selecting items to evict, but this may be customizable. Configure the global expiration property and other properties of the cache, and the expiration property of each cached item, to help ensure that the
cache is cost effective. It may not always be appropriate to apply a global eviction policy to every item
in the cache. For example, if a cached item is very expensive to retrieve from the data store, it may be
beneficial to retain this item in cache at the expense of more frequently accessed but less costly items.
? Priming the Cache. Many solutions prepopulate the cache with the data that an application is likely to
need as part of the startup processing. The Cache-Aside pattern may still be useful if some of this data
expires or is evicted.
? Consistency. Implementing the Cache-Aside pattern does not guarantee consistency between the
data store and the cache. An item in the data store may be changed at any time by an external process,
and this change might not be re?ected in the cache until the next time the item is loaded into the
cache. In a system that replicates data across data stores, this problem may become especially acute if
synchronization occurs very frequently.
? Local (In-Memory) Caching. A cache could be local to an application instance and stored in-memory.
Cache-aside can be useful in this environment if an application repeatedly accesses the same data.
However, a local cache is private and so different application instances could each have a copy of the
same cached data. This data could quickly become inconsistent between caches, so it may be necessary to expire data held in a private cache and refresh it more frequently. In these scenarios it may be
appropriate to investigate the use of a shared or a distributed caching mechanism.
Use this pattern when:
? A cache does not provide native read-through and write-through operations.
? Resource demand is unpredictable. This pattern enables applications to load data on demand. It makes
no assumptions about which data an application will require in advance.
This pattern might not be suitable:
? When the cached data set is static. If the data will ft into the available cache space, prime the cache
with the data on startup and apply a policy that prevents the data from expiring.
? For caching session state information in a web application hosted in a web farm. In this environment,
you should avoid introducing dependencies based on client-server affinity.
In Windows Azure you can use Windows Azure Cache to create a distributed cache that can be shared by
multiple instances of an application. The GetMyEntityAsync method in the following code example shows
an implementation of the Cache-aside pattern based on Windows Azure Cache. This method retrieves an
object from the cache using the read-though approach.
An object is identified by using an integer ID as the key. The GetMyEntityAsync method generates a string
value based on this key (the Windows Azure Cache API uses strings for key values) and attempts to retrieve
an item with this key from the cache. If a matching item is found, it is returned. If there is no match in the
cache, the GetMyEntityAsync method retrieves the object from a data store, adds it to the cache, and then
returns it (the code that actually retrieves the data from the data store has been omitted because it is data
store dependent). Note that the cached item is configured to expire in order to prevent it from becoming
stale if it is updated elsewhere.
The examples use the Windows Azure Cache API to access the store and retrieve information from the
cache. For more information about the Windows Azure Cache API, see Using Windows Azure Cache on
MSDN.
The UpdateEntityAsync method shown below demonstrates how to invalidate an object in the cache when
the value is changed by the application. This is an example of a write-through approach. The code updates
the original data store and then removes the cached item from the cache by calling the Remove method,
specifying the key (the code for this part of the functionality has been omitted as it will be data store dependent).
The order of the steps in this sequence is important. If the item is removed before the cache is updated,
there is a small window of opportunity for a client application to fetch the data (because it is not found
in the cache) before the item in the data store has been changed, resulting in the cache containing stale
data.
The following patterns and guidance may also be relevant when implementing this pattern:
? Caching Guidance. This guidance provides additional information on how you can cache data in a
cloud solution, and the issues that you should consider when you implement a cache.
? Data Consistency Primer. Cloud applications typically use data that is dispersed across data stores.
Managing and maintaining data consistency in this environment can become a critical aspect of the
system, particularly in terms of the concurrency and availability issues that can arise. This primer describes the issues surrounding consistency across distributed data, and summarizes how an application
can implement eventual consistency to maintain the availability of data.
##%%&&
Handle faults that may take a variable amount of time to rectify when connecting to a remote service or resource. This pattern can improve the stability and resiliency of an application.
In a distributed environment such as the cloud, where an application performs operations that access remote
resources and services, it is possible for these operations to fail due to transient faults such as slow network
connections, timeouts, or the resources being overcommitted or temporarily unavailable. These faults typically correct themselves after a short period of time, and a robust cloud application should be prepared to
handle them by using a strategy such as that described by the Retry Pattern.
However, there may also be situations where faults are due to unexpected events that are less easily anticipated, and that may take much longer to rectify. These faults can range in severity from a partial loss of
connectivity to the complete failure of a service. In these situations it may be pointless for an application to
continually retry performing an operation that is unlikely to succeed, and instead the application should
quickly accept that the operation has failed and handle this failure accordingly.
Additionally, if a service is very busy, failure in one part of the system may lead to cascading failures. For ex ample, an operation that invokes a service could be configured to implement a timeout, and reply with a
failure message if the service fails to respond within this period. However, this strategy could cause many
concurrent requests to the same operation to be blocked until the timeout period expires. These blocked
requests might hold critical system resources such as memory, threads, database connections, and so on.
Consequently, these resources could become exhausted, causing failure of other possibly unrelated parts of
the system that need to use the same resources. In these situations, it would be preferable for the operation
to fail immediately, and only attempt to invoke the service if it is likely to succeed. Note that setting a
shorter timeout may help to resolve this problem, but the timeout should not be so short that the operation
fails most of the time, even if the request to the service would eventually succeed.
The Circuit Breaker pattern can prevent an application repeatedly trying to execute an operation that is
likely to fail, allowing it to continue without waiting for the fault to be rectified or wasting CPU cycles while
it determines that the fault is long lasting. The Circuit Breaker pattern also enables an application to detect
whether the fault has been resolved. If the problem appears to have been rectified, the application can attempt to invoke the operation.
The purpose of the Circuit Breaker pattern is different from that of the Retry Pattern. The Retry
Pattern enables an application to retry an operation in the expectation that it will succeed. The Circuit
Breaker pattern prevents an application from performing an operation that is likely to fail. An
application may combine these two patterns by using the Retry pattern to invoke an operation through
a circuit breaker. However, the retry logic should be sensitive to any exceptions returned by the circuit
breaker and abandon retry attempts if the circuit breaker indicates that a fault is not transient.
A circuit breaker acts as a proxy for operations that may fail. The proxy should monitor the number of recent
failures that have occurred, and then use this information to decide whether to allow the operation to proceed, or simply return an exception immediately.
The proxy can be implemented as a state machine with the following states that mimic the functionality of
an electrical circuit breaker:
? Closed: The request from the application is routed through to the operation. The proxy maintains a
count of the number of recent failures, and if the call to the operation is unsuccessful the proxy increments this count. If the number of recent failures exceeds a specified threshold within a given time period, the proxy is placed into the Open state. At this point the proxy starts a timeout timer, and when
this timer expires the proxy is placed into the Half-Open state.
The purpose of the timeout timer is to give the system time to rectify the problem that caused the
failure before allowing the application to attempt to perform the operation again.
? Open: The request from the application fails immediately and an exception is returned to the application.
? Half-Open: A limited number of requests from the application are allowed to pass through and invoke
the operation. If these requests are successful, it is assumed that the fault that was previously causing
the failure has been fixed and the circuit breaker switches to the Closed state (the failure counter is
reset). If any request fails, the circuit breaker assumes that the fault is still present so it reverts back to
the Open state and restarts the timeout timer to give the system a further period of time to recover
from the failure.
The Half-Open state is useful to prevent a recovering service from suddenly being inundated with
requests. As a service recovers, it may be able to support a limited volume of requests until the
recovery is complete, but while recovery is in progress a flood of work may cause the service to time
out or fail again.
Figure 1 illustrates the states for one possible implementation of a circuit breaker.
Note that, in Figure 1, the failure counter used by the Closed state is time-based. It is automatically reset at
periodic intervals. This helps to prevent the circuit breaker from entering the Open state if it experiences
occasional failures; the failure threshold that trips the circuit breaker into the Open state is only reached
when a specified number of failures have occurred during a specified interval. The success counter used by
the Half-Open state records the number of successful attempts to invoke the operation. The circuit breaker
reverts to the Closed state after a specified number of consecutive operation invocations have been successful. If any invocation fails, the circuit breaker enters the Open state immediately and the success counter will
be reset the next time it enters the Half-Open state.
How the system recovers is handled externally, possibly by restoring or restarting a failed component or
repairing a network connection.
Implementing the circuit breaker pattern adds stability and resiliency to a system, offering stability while the
system recovers from a failure and minimizing the impact of this failure on performance. It can help to
maintain the response time of the system by quickly rejecting a request for an operation that is likely to fail,
rather than waiting for the operation to time out (or never return). If the circuit breaker raises an event each
time it changes state, this information can be used to monitor the health of the part of the system protected
by the circuit breaker, or to alert an administrator when a circuit breaker trips to the Open state.
The pattern is customizable and can be adapted according to the nature of the possible failure. For example,
you can apply an increasing timeout timer to a circuit breaker. You could place the circuit breaker in the Open
state for a few seconds initially, and then if the failure has not been resolved increase the timeout to a few
minutes, and so on. In some cases, rather than the Open state returning failure and raising an exception, it
could be useful to return a default value that is meaningful to the application.
You should consider the following points when deciding how to implement this pattern:
? Exception Handling. An application invoking an operation through a circuit breaker must be prepared
to handle the exceptions that could be raised if the operation is unavailable. The way in which such
exceptions are handled will be application specific. For example, an application could temporarily degrade its functionality, invoke an alternative operation to try to perform the same task or obtain the
same data, or report the exception to the user and ask them to try again later.
? Types of Exceptions. A request may fail for a variety of reasons, some of which may indicate a more
severe type of failure than others. For example, a request may fail because a remote service has crashed
and may take several minutes to recover, or failure could be caused by a timeout due to the service being temporarily overloaded. A circuit breaker may be able to examine the types of exceptions that occur and adjust its strategy depending on the nature of these exceptions. For example, it may require a
larger number of timeout exceptions to trip the circuit breaker to the Open state compared to the
number of failures due to the service being completely unavailable.
? Logging. A circuit breaker should log all failed requests (and possibly successful requests) to enable an
administrator to monitor the health of the operation that it encapsulates.
? Recoverability. You should configure the circuit breaker to match the likely recovery pattern of the
operation it is protecting. For example, if the circuit breaker remains in the Open state for a long period, it could raise exceptions even if the reason for the failure has long since been resolved. Similarly, a
circuit breaker could oscillate and reduce the response times of applications if it switches from the
Open state to the Half-Open state too quickly.
? Testing Failed Operations. In the Open state, rather than using a timer to determine when to switch
to the Half-Open state, a circuit breaker may instead periodically ping the remote service or resource
to determine whether it has become available again. This ping could take the form of an attempt to
invoke an operation that had previously failed, or it could use a special operation provided by the remote service specifically for testing the health of the service, as described by the Health Endpoint
Monitoring Pattern.
? Manual Override. In a system where the recovery time for a failing operation is extremely variable, it
may be beneficial to provide a manual reset option that enables an administrator to forcibly close a circuit breaker (and reset the failure counter). Similarly, an administrator could force a circuit breaker into
the Open state (and restart the timeout timer) if the operation protected by the circuit breaker is temporarily unavailable.
? Concurrency. The same circuit breaker could be accessed by a large number of concurrent instances of
an application. The implementation should not block concurrent requests or add excessive overhead to
each call to an operation.
? Resource Differentiation. Be careful when using a single circuit breaker for one type of resource if
there might be multiple underlying independent providers. For example, in a data store that comprises
multiple shards, one shard may be fully accessible while another is experiencing a temporary issue. If
the error responses in these scenarios are con?ated, an application may attempt to access some shards
even when failure is highly likely, while access to other shards may be blocked even though it is likely
to succeed.
? Accelerated Circuit Breaking. Sometimes a failure response can contain enough information for the
circuit breaker implementation to know it should trip immediately and stay tripped for a minimum
amount of time. For example, the error response from a shared resource that is overloaded could indicate that an immediate retry is not recommended and that the application should instead try again in a
few minutes time.
The HTTP protocol defines the ¡°HTTP 503 Service Unavailable¡± response that can be returned if a
requested service is not currently available on a particular web server. This response can include
additional information, such as the anticipated duration of the delay.
? Replaying Failed Requests. In the Open state, rather than simply failing quickly, a circuit breaker could
also record the details of each request to a journal and arrange for these requests to be replayed when
the remote resource or service becomes available.
? Inappropriate Timeouts on External Services. A circuit breaker may not be able to fully protect applications from operations that fail in external services that are configured with a lengthy timeout period. If the timeout is too long, a thread running a circuit breaker may be blocked for an extended period before the circuit breaker indicates that the operation has failed. In this time, many other
application instances may also attempt to invoke the service through the circuit breaker and tie up a
significant number of threads before they all fail.
Use this pattern:
? To prevent an application from attempting to invoke a remote service or access a shared resource if
this operation is highly likely to fail.
This pattern might not be suitable:
? For handling access to local private resources in an application, such as in-memory data structure. In
this environment, using a circuit breaker would simply add overhead to your system.
? As a substitute for handling exceptions in the business logic of your applications.
In a web application, several of the pages are populated with data retrieved from an external service. If the
system implements minimal caching, most hits to each of these pages will cause a round trip to the service.
Connections from the web application to the service could be configured with a timeout period (typically 60
seconds), and if the service does not respond in this time the logic in each web page will assume that the
service is unavailable and throw an exception.
However, if the service fails and the system is very busy, users could be forced to wait for up to 60 seconds
before an exception occurs. Eventually resources such as memory, connections, and threads could be exhausted, preventing other users from connecting to the system¡ªeven if they are not accessing pages that
retrieve data from the service.
Scaling the system by adding further web servers and implementing load balancing may delay the point at
which resources become exhausted, but it will not resolve the issue because user requests will still be unresponsive and all web servers could still eventually run out of resources.
Wrapping the logic that connects to the service and retrieves the data in a circuit breaker could help to alleviate the effects of this problem and handle the service failure more elegantly. User requests will still fail,
but they will fail more quickly and the resources will not be blocked.
The CircuitBreaker class maintains state information about a circuit breaker in an object that implements the
ICircuitBreakerStateStore interface shown in the following code.
The State property indicates the current state of the circuit breaker, and will be one of the values Open,
HalfOpen, or Closed as defined by the CircuitBreakerStateEnum enumeration. The IsClosed property
should be true if the circuit breaker is closed, but false if it is open or half-open. The Trip method switches
the state of the circuit breaker to the open state and records the exception that caused the change in state,
together with the date and time that the exception occurred. The LastException and the LastStateChanged DateUtc properties return this information. The Reset method closes the circuit breaker, and the HalfOpen
method sets the circuit breaker to half-open.
The InMemoryCircuitBreakerStateStore class in the example contains an implementation of the ICircuitBreakerStateStore interface. The CircuitBreaker class creates an instance of this class to hold the state of
the circuit breaker.
The ExecuteAction method in the CircuitBreaker class wraps an operation (in the form of an Action delegate) that could fail. When this method runs, it first checks the state of the circuit breaker. If it is closed (the
local IsOpen property, which returns true if the circuit breaker is open or half-open, is false) the ExecuteAction method attempts to invoke the Action delegate. If this operation fails, an exception handler executes
the TrackException method, which sets the state of the circuit breaker to open by calling the Trip method
of the InMemoryCircuitBreakerStateStore object. The following code example highlights this flow.
The following example shows the code (omitted from the previous example) that is executed if the circuit
breaker is not closed. It first checks if the circuit breaker has been open for a period longer than the time
specified by the local OpenToHalfOpenWaitTime field in the CircuitBreaker class. If this is the case, the
ExecuteAction method sets the circuit breaker to half-open, then attempts to perform the operation specified by the Action delegate.
If the operation is successful, the circuit breaker is reset to the closed state. If the operation fails, it is tripped
back to the open state and the time at which the exception occurred is updated so that the circuit breaker
will wait for a further period before attempting to perform the operation again.
If the circuit breaker has only been open for a short time, less than the OpenToHalfOpenWaitTime value,
the ExecuteAction method simply throws a CircuitBreakerOpenException exception and returns the error
that caused the circuit breaker to transition to the open state.
Additionally, to prevent the circuit breaker from attempting to perform concurrent calls to the operation
while it is half-open, it uses a lock. A concurrent attempt to invoke the operation will be handled as if the
circuit breaker was open, and it will fail with an exception as described later.
To use a CircuitBreaker object to protect an operation, an application creates an instance of the Circuit Breaker class and invokes the ExecuteAction method, specifying the operation to be performed as the parameter. The application should be prepared to catch the CircuitBreakerOpenException exception if the
operation fails because the circuit breaker is open. The following code shows an example:
The following patterns may also be relevant when implementing this pattern:
? Retry Pattern. The Retry pattern is a useful adjunct to the Circuit Breaker pattern. It describes how an
application can handle anticipated temporary failures when it attempts to connect to a service or net work resource by transparently retrying an operation that has previously failed in the expectation that
the cause of the failure is transient.
? Health Endpoint Monitoring Pattern. A circuit breaker may be able to test the health of a service by
sending a request to an endpoint exposed by the service. The service should return information indicating its status.
##%%&&
Undo the work performed by a series of steps, which together define an eventually consistent operation, if
one or more of the steps fail. Operations that follow the eventual consistency model are commonly found in
cloud-hosted applications that implement complex business processes and workflows.
Applications running in the cloud frequently modify data. This data may be spread across an assortment of
data sources held in a variety of geographic locations. To avoid contention and improve performance in a
distributed environment such as this, an application should not attempt to provide strong transactional consistency. Rather, the application should implement eventual consistency. In this model, a typical business operation consists of a series of autonomous steps. While these steps are being performed the overall view of
the system state may be inconsistent, but when the operation has completed and all of the steps have been
executed the system should become consistent again.
The Data Consistency Primer provides more information about why distributed transactions do not
scale well, and the principles that underpin the eventual consistency model.
A significant challenge in the eventual consistency model is how to handle a step that has failed irrecoverably.
In this case it may be necessary to undo all of the work completed by the previous steps in the operation.
However, the data cannot simply be rolled back because other concurrent instances of the application may
have since changed it. Even in cases where the data has not been changed by a concurrent instance, undoing
a step might not simply be a matter of restoring the original state. It may be necessary to apply various business-specific rules (see the travel website described in the Example section).
If an operation that implements eventual consistency spans several heterogeneous data stores, undoing the
steps in such an operation will require visiting each data store in turn. The work performed in every data store
must be undone reliably to prevent the system from remaining inconsistent.
Not all data affected by an operation that implements eventual consistency might be held in a database. In a
Service Oriented Architecture (SOA) environment an operation may invoke an action in a service, and cause
a change in the state held by that service. To undo the operation, this state change must also be undone. This
may involve invoking the service again and performing another action that reverses the effects of the first.
Implement a compensating transaction. The steps in a compensating transaction must undo the effects of
the steps in the original operation. A compensating transaction might not be able to simply replace the current
state with the state the system was in at the start of the operation because this approach could overwrite
changes made by other concurrent instances of an application. Rather, it must be an intelligent process that
takes into account any work done by concurrent instances. This process will usually be application-specific,
driven by the nature of the work performed by the original operation.
A common approach to implementing an eventually consistent operation that requires compensation is to
use a workflow. As the original operation proceeds, the system records information about each step and how
the work performed by that step can be undone. If the operation fails at any point, the workflow rewinds
back through the steps it has completed and performs the work that reverses each step. Note that a compensating transaction might not have to undo the work in the exact mirror-opposite order of the original operation, and it may be possible to perform some of the undo steps in parallel.
This approach is similar to the Sagas strategy. A description of this strategy is available online in Clemens
Vasters¡¯ blog.
A compensating transaction is itself an eventually consistent operation and it could also fail. The system
should be able to resume the compensating transaction at the point of failure and continue. It may be necessary to repeat a step that has failed, so the steps in a compensating transaction should be defined as idempotent commands. For more information about idempotency, see Idempotency Patterns on Jonathan Oliver¡¯s blog.
In some cases it may not be possible to recover from a step that has failed except through manual intervention. In these situations the system should raise an alert and provide as much information as possible about
the reason for the failure.
Consider the following points when deciding how to implement this pattern:
? It might not be easy to determine when a step in an operation that implements eventual consistency
has failed. A step might not fail immediately, but instead it could block. It may be necessary to implement some form of time-out mechanism.
? Compensation logic is not easily generalized. A compensating transaction is application-specific; it relies on the application having sufficient information to be able to undo the effects of each step in a
failed operation.
? You should define the steps in a compensating transaction as idempotent commands. This enables the
steps to be repeated if the compensating transaction itself fails.
? The infrastructure that handles the steps in the original operation, and the compensating transaction,
must be resilient. It must not lose the information required to compensate for a failing step, and it
must be able to reliably monitor the progress of the compensation logic.
? A compensating transaction does not necessarily return the data in the system to the state it was in at
the start of the original operation. Instead, it compensates for the work performed by the steps that
completed successfully before the operation failed.
? The order of the steps in the compensating transaction does not necessarily have to be the mirror opposite of the steps in the original operation. For example, one data store may be more sensitive to inconsistencies than another, and so the steps in the compensating transaction that undo the changes to
this store should occur first.
? Placing a short-term timeout-based lock on each resource that is required to complete an operation,
and obtaining these resources in advance, can help increase the likelihood that the overall activity will
succeed. The work should be performed only after all the resources have been acquired. All actions
must be finalized before the locks expire.
? Consider using retry logic that is more forgiving than usual to minimize failures that trigger a compensating transaction. If a step in an operation that implements eventual consistency fails, try handling the
failure as a transient exception and repeat the step. Only abort the operation and initiate a compensating transaction if a step fails repeatedly or irrecoverably.
Many of the challenges and issues of implementing a compensating transaction are the same as those
concerned with implementing eventual consistency. See the section ¡°Considerations for Implementing
Eventual Consistency¡± in the Data Consistency Primer for more information.
Use this pattern only for operations that must be undone if they fail. If possible, design solutions to avoid the
complexity of requiring compensating transactions (for more information, see the Data Consistency Primer).
A travel website enables customers to book itineraries. A single itinerary may comprise a series of flights and
hotels. A customer traveling from Seattle to London and then on to Paris could perform the following steps
when creating an itinerary:
1. Book a seat on flight F1 from Seattle to London.
2. Book a seat on flight F2 from London to Paris.
3. Book a seat on flight F3 from Paris to Seattle.
4. Reserve a room at hotel H1 in London.
5. Reserve a room at hotel H2 in Paris.
These steps constitute an eventually consistent operation, although each step is essentially a separate atomic
action in its own right. Therefore, as well as performing these steps, the system must also record the counter
operations necessary to undo each step in case the customer decides to cancel the itinerary. The steps necessary to perform the counter operations can then run as a compensating transaction if necessary.
Notice that the steps in the compensating transaction might not be the exact opposite of the original steps,
and the logic in each step in the compensating transaction must take into account any business-specific rules.
For example, ¡°unbooking¡± a seat on a flight might not entitle the customer to a complete refund of any
money paid.
It may be possible for the steps in the compensating transaction to be performed in parallel, depending
on how you have designed the compensating logic for each step.
In many business solutions, failure of a single step does not always necessitate rolling the system back by
using a compensating transaction. For example, if¡ªafter having booked flights F1, F2, and F3 in the travel
website scenario¡ªthe customer is unable to reserve a room at hotel H1, it is preferable to offer the customer a room at a different hotel in the same city rather than cancelling the flights. The customer may still
elect to cancel (in which case the compensating transaction runs and undoes the bookings made on flights
F1, F2, and F3), but this decision should be made by the customer rather than by the system.
The following patterns and guidance may also be relevant when implementing this pattern:
? Data Consistency Primer. The Compensating Transaction pattern is frequently used to undo operations that implement the eventual consistency model. This primer provides more information on the
benefits and tradeoffs of eventual consistency.
? Scheduler-Agent-Supervisor Pattern. This pattern describes how to implement resilient systems that
perform business operations that utilize distributed services and resources. In some circumstances, it
may be necessary to undo the work performed by an operation by using a compensating transaction.
? Retry Pattern. Compensating transactions can be expensive to perform, and it may be possible to minimize their use by implementing an effective policy of retrying failing operations by following the Retry Pattern.
##%%&&
Enable multiple concurrent consumers to process messages received on the same messaging channel. This
pattern enables a system to process multiple messages concurrently to optimize throughput, to improve
scalability and availability, and to balance the workload.
An application running in the cloud may be expected to handle a large number of requests. Rather than
process each request synchronously, a common technique is for the application to pass them through a messaging system to another service (a consumer service) that handles them asynchronously. This strategy helps
to ensure that the business logic in the application is not blocked while the requests are being processed.
The number of requests could vary significantly over time for many reasons. A sudden burst in user activity or
aggregated requests coming from multiple tenants may cause unpredictable workload. At peak hours a system
might need to process many hundreds of requests per second, while at other times the number could be very
small. Additionally, the nature of the work performed to handle these requests might be highly variable. Using
a single instance of the consumer service might cause that instance to become flooded with requests or the
messaging system may be overloaded by an influx of messages coming from the application. To handle this
fluctuating workload, the system can run multiple instances of the consumer service. However these consumers must be coordinated to ensure that each message is only delivered to a single consumer. The workload also
needs to be load balanced across consumers to prevent an instance from becoming a bottleneck.
Use a message queue to implement the communication channel between the application and the instances
of the consumer service. The application posts requests in the form of messages to the queue, and the consumer service instances receive messages from the queue and process them. This approach enables the same
pool of consumer service instances to handle messages from any instance of the application. Figure 1 illustrates this architecture.
This solution offers the following benefits:
? It enables an inherently load-leveled system that can handle wide variations in the volume of requests
sent by application instances. The queue acts as a buffer between the application instances and the
consumer service instances, which can help to minimize the impact on availability and responsiveness
for both the application and the service instances (as described by the Queue-based Load Leveling Pat tern). Handling a message that requires some long-running processing to be performed does not prevent other messages from being handled concurrently by other instances of the consumer service.
? It improves reliability. If a producer communicates directly with a consumer instead of using this pat tern, but does not monitor the consumer, there is a high probability that messages could be lost or fail
to be processed if the consumer fails. In this pattern messages are not sent to a specific service instance, a failed service instance will not block a producer, and messages can be processed by any working service instance.
? It does not require complex coordination between the consumers, or between the producer and the
consumer instances. The message queue ensures that each message is delivered at least once.
? It is scalable. The system can dynamically increase or decrease the number of instances of the consumer service as the volume of messages fluctuates.
? It can improve resiliency if the message queue provides transactional read operations. If a consumer
service instance reads and processes the message as part of a transactional operation, and if this consumer service instance subsequently fails, this pattern can ensure that the message will be returned to
the queue to be picked up and handled by another instance of the consumer service.
Consider the following points when deciding how to implement this pattern:
? Message Ordering. The order in which consumer service instances receive messages is not guaranteed,
and does not necessarily reflect the order in which the messages were created. Design the system to
ensure that message processing is idempotent because this will help to eliminate any dependency on
the order in which messages are handled. For more information about idempotency, see Idempotency
Patterns on Jonathon Oliver¡¯s blog.
Windows Azure Service Bus Queues can implement guaranteed first-in-first-out ordering of messages
by using message sessions. For more information, see Messaging Patterns Using Sessions on MSDN.
? Designing Services for Resiliency. If the system is designed to detect and restart failed service instances, it may be necessary to implement the processing performed by the service instances as idempotent operations to minimize the effects of a single message being retrieved and processed more than
once.
? Detecting Poison Messages. A malformed message, or a task that requires access to resources that are
not available, may cause a service instance to fail. The system should prevent such messages being returned to the queue, and instead capture and store the details of these messages elsewhere so that
they can be analyzed if necessary.
? Handling Results. The service instance handling a message is fully decoupled from the application logic that generates the message, and they may not be able to communicate directly. If the service instance generates results that must be passed back to the application logic, this information must be
stored in a location that is accessible to both and the system must provide some indication of when
processing has completed to prevent the application logic from retrieving incomplete data.
If you are using Windows Azure, a worker process may be able to pass results back to the application
logic by using a dedicated message reply queue. The application logic must be able to correlate these
results with the original message. This scenario is described in more detail in the Asynchronous
Messaging Primer.
? Scaling the Messaging System. In a large-scale solution, a single message queue could be over whelmed by the number of messages and become a bottleneck in the system. In this situation, consider
partitioning the messaging system to direct messages from specific producers to a particular queue, or
use load balancing to distribute messages across multiple message queues.
? Ensuring Reliability of the Messaging System. A reliable messaging system is needed to guarantee
that, once the application enqueues a message, it will not be lost. This is essential for ensuring that all
messages are delivered at least once.
Use this pattern when:
? The workload for an application is divided into tasks that can run asynchronously.
? Tasks are independent and can run in parallel.
? The volume of work is highly variable, requiring a scalable solution.
? The solution must provide high availability, and must be resilient if the processing for a task fails.
This pattern may not be suitable when:
? It is not easy to separate the application workload into discrete tasks, or there is a high degree of dependence between tasks.
? Tasks must be performed synchronously, and the application logic must wait for a task to complete
before continuing.
? Tasks must be performed in a specific sequence.
Some messaging systems support sessions that enable a producer to group messages together and
ensure that they are all handled by the same consumer. This mechanism can be used with prioritized
messages (if they are supported) to implement a form of message ordering that delivers messages in
sequence from a producer to a single consumer.
Windows Azure provides storage queues and Service Bus queues that can act as a suitable mechanism for
implementing this pattern. The application logic can post messages to a queue, and consumers implemented
as tasks in one or more roles can retrieve messages from this queue and process them. For resiliency, a Service
Bus queue enables a consumer to use PeekLock mode when it retrieves a message from the queue. This mode
does not actually remove the message, but simply hides it from other consumers. The original consumer can
delete the message when it has finished processing it. If the consumer should fail, the peek lock will time out
and the message will become visible again, allowing another consumer to retrieve it.
For detailed information on using Windows Azure Service Bus queues, see Service Bus Queues, Topics,
and Subscriptions on MSDN. For information on using Windows Azure storage queues, see How to use
the Queue Storage Service on MSDN.
The following code shows from the QueueManager class in CompetingConsumers solution of the examples
available for download for this guidance shows how you can create a queue by using a QueueClient instance
in the Start event handler in a web or worker role.
The next code snippet shows how an application can create and send a batch of messages to the queue.
The following code shows how a consumer service instance can receive messages from the queue by following an event-driven approach. The processMessageTask parameter to the ReceiveMessages method is a
delegate that references the code to run when a message is received. This code is run asynchronously.
Note that autoscaling features, such as those available in Windows Azure, can be used to start and stop role
instances as the queue length fluctuates. For more information, see Autoscaling Guidance. In addition, it is
not necessary to maintain a one-to-one correspondence between role instances and worker processes¡ªa
single role instance can implement multiple worker processes. For more information, see the Compute Resource Consolidation Pattern.
The following patterns and guidance may be relevant when implementing this pattern:
? Asynchronous Messaging Primer. Message queues are an inherently asynchronous communications
mechanism. If a consumer service needs to send a reply to an application, it may be necessary to implement some form of response messaging. The Asynchronous Messaging Primer provides information on
how to implement request/reply messaging by using message queues.
? Autoscaling Guidance. It may be possible to start and stop instances of a consumer service as the
length of the queue to which applications post messages varies. Autoscaling can help to maintain
throughput during times of peak processing.
? Compute Resource Consolidation Pattern. It may be possible to consolidate multiple instances of a
consumer service into a single process to reduce costs and management overhead. The Compute Resource Consolidation Pattern describes the benefits and tradeoffs of following this approach.
? Queue-based Load Leveling Pattern. Introducing a message queue can add resiliency to the system,
enabling service instances to handle widely varying volumes of requests from application instances.
The message queue effectively acts as a buffer which levels the load. The Queue-based Load Leveling
Pattern describes this scenario in more detail.
##%%&&
Consolidate multiple tasks or operations into a single computational unit. This pattern can increase compute
resource utilization, and reduce the costs and management overhead associated with performing compute
processing in cloud-hosted applications.
A cloud application frequently implements a variety of operations. In some solutions it may make sense initially to follow the design principle of separation of concerns, and divide these operations into discrete
computational units that are hosted and deployed individually (for example, as separate roles in a Windows
Azure Cloud Service, separate Windows Azure Web Sites, or separate Virtual Machines). However, although
this strategy can help to simplify the logical design of the solution, deploying a large number of computational units as part of the same application can increase runtime hosting costs and make management of the
system more complex.
As an example, Figure 1 shows the simplified structure of a cloud-hosted solution that is implemented using
more than one computational unit. Each computational unit runs in its own virtual environment. Each function has been implemented as a separate task (labeled Task A through Task E) running in its own computational unit.
Each computational unit consumes chargeable resources, even when it is idle or lightly used. Therefore, this
approach may not always be the most cost-effective solution.
In Windows Azure, this concern applies to roles in a Cloud Service, Web Sites, and Virtual Machines. These
items execute in their own virtual environment. Running a collection of separate roles, web sites, or virtual
machines that are designed to perform a set of well-defined operations, but that need to communicate and
cooperate as part of a single solution, may be an inefficient use of resources.
To help reduce costs, increase utilization, improve communication speed, and ease the management effort it
may be possible to consolidate multiple tasks or operations into a single computational unit.
Tasks can be grouped according to a variety of criteria based on the features provided by the environment,
and the costs associated with these features. A common approach is to look for tasks that have a similar
profile concerning their scalability, lifetime, and processing requirements. Grouping these items together al lows them to scale as a unit. The elasticity provided by many cloud environments enables additional in stances of a computational unit to be started and stopped according to the workload. For example, Windows
Azure provides autoscaling that you can apply to roles in a Cloud Service, Web Sites, and Virtual Machines.
For more information, see Autoscaling Guidance.
As a counter example to show how scalability can be used to determine which operations should probably
not be grouped together, consider the following two tasks:
? Task 1 polls for infrequent, time-insensitive messages sent to a queue.
? Task 2 handles high-volume bursts of network traffic.
The second task requires elasticity that may involve starting and stopping a large number of instances of the
computational unit. Applying the same scaling to the first task would simply result in more tasks listening for
infrequent messages on the same queue, and is a waste of resources.
In many cloud environments it is possible to specify the resources available to a computational unit in terms
of the number of CPU cores, memory, disk space, and so on. Generally, the more resources specified, the
greater the cost. For financial efficiency, it is important to maximize the amount of work an expensive computational unit performs, and not let it become inactive for an extended period.
If there are tasks that require a great deal of CPU power in short bursts, consider consolidating these into a
single computational unit that provides the necessary power. However, it is important to balance this need
to keep expensive resources busy against the contention that could occur if they are over-stressed. Long running, compute-intensive tasks should probably not share the same computational unit, for example.
Consider the following points when implementing this pattern:
? Scalability and Elasticity. Many cloud solutions implement scalability and elasticity at the level of the
computational unit by starting and stopping instances of units. Avoid grouping tasks that have conflicting scalability requirements in the same computational unit.
? Lifetime. The cloud infrastructure may periodically recycle the virtual environment that hosts a computational unit. When executing many long-running tasks inside a computational unit, it may be necessary to configure the unit to prevent it from being recycled until these tasks have finished. Alternatively, design the tasks by using a check-pointing approach that enables them to stop cleanly, and continue
at the point at which they were interrupted when the computational unit is restarted.
? Release Cadence. If the implementation or configuration of a task changes frequently, it may be necessary to stop the computational unit hosting the updated code, reconfigure and redeploy the unit, and
then restart it. This process will also require that all other tasks within the same computational unit are
stopped, redeployed, and restarted.
? Security. Tasks in the same computational unit may share the same security context and be able to access the same resources. There must be a high degree of trust between the tasks, and confidence that
that one task is not going to corrupt or adversely affect another. Additionally, increasing the number of
tasks running in a computational unit may increase the attack surface of the computational unit; each
task is only as secure as the one with the most vulnerabilities.
? Fault Tolerance. If one task in a computational unit fails or behaves abnormally, it can affect the other
tasks running within the same unit. For example, if one task fails to start correctly it may cause the en tire startup logic for the computational unit to fail, and prevent other tasks in the same unit from running.
? Contention. Avoid introducing contention between tasks that compete for resources in the same
computational unit. Ideally, tasks that share the same computational unit should exhibit different re source utilization characteristics. For example, two compute-intensive tasks should probably not reside
in the same computational unit, and neither should two tasks that consume large amounts of memory.
However, mixing a compute intensive task with a task that requires a large amount of memory may be
a viable combination.
You should consider consolidating compute resources only for a system that has been in
production for a period of time so that operators and developers can monitor the system and
create a heat map that identifies how each task utilizes differing resources. This map can be used
to determine which tasks are good candidates for sharing compute resources.
? Complexity. Combining multiple tasks into a single computational unit adds complexity to the code in
the unit, possibly making it more difficult to test, debug, and maintain.
? Stable Logical Architecture. Design and implement the code in each task so that it should not need
to change, even if the physical environment in which task runs does change.
? Other Strategies. Consolidating compute resources is only one way to help reduce costs associated
with running multiple tasks concurrently. It requires careful planning and monitoring to ensure that it
remains an effective approach. Other strategies may be more appropriate, depending on the nature of
the work being performed and the location of the users on whose behalf these tasks are running. For
example, functional decomposition of the workload (as described by the Compute Partitioning Guidance) may be a better option.
Use this pattern for tasks that are not cost effective if they run in their own computational units. If a task
spends much of its time idle, running this task in a dedicated unit can be expensive.
This pattern might not be suitable for tasks that perform critical fault-tolerant operations, or tasks that
process highly-sensitive or private data and require their own security context. These tasks should run in their
own isolated environment, in a separate computational unit.
When building a cloud service on Windows Azure, it¡¯s possible to consolidate the processing performed by
multiple tasks into a single role. Typically this is a worker role that performs background or asynchronous
processing tasks.
In some cases it may be possible to include background or asynchronous processing tasks in the web
role. This technique can help to reduce costs and simplify deployment, although it can impact the
scalability and responsiveness of the public-facing interface provided by the web role. The article
Combining Multiple Azure Worker Roles into an Azure Web Role contains a detailed description of
implementing background or asynchronous processing tasks in a web role.
The role is responsible for starting and stopping the tasks. When the Windows Azure fabric controller loads
a role, it raises the Start event for the role. You can override the OnStart method of the WebRole or Worker Role class to handle this event, perhaps to initialize the data and other resources on which the tasks in this
method depend.
When the OnStart method completes, the role can start responding to requests. You can find more information and guidance about using the OnStart and Run methods in a role in the Application Startup Processes
section in the patterns & practices guide Moving Applications to the Cloud.
Keep the code in the OnStart method as concise as possible. Windows Azure does not impose any
limit on the time taken for this method to complete, but the role will not be able to start responding to
network requests sent to it until this method completes.
When the OnStart method has finished, the role executes the Run method. At this point, the fabric controller can start sending requests to the role.
Place the code that actually creates the tasks in the Run method. Note that the Run method effectively
defines the lifetime of the role instance. When this method completes, the fabric controller will arrange for
the role to be shut down.
When a role shuts down or is recycled, the fabric controller prevents any more incoming requests being received from the load balancer and raises the Stop event. You can capture this event by overriding the OnStop
method of the role and perform any tidying up required before the role terminates.
Any actions performed in the OnStop method must be completed within five minutes (or 30 seconds if
you are using the Windows Azure emulator on a local computer); otherwise the Windows Azure fabric
controller assumes that the role has stalled and will force it to stop.
Figure 2 illustrates the lifecycle of a role, and the tasks and resources that it hosts. The tasks are started by
the Run method, which then waits for the tasks to complete. The tasks themselves, which implement the
business logic of the cloud service, can respond to messages posted to the role through the Windows Azure
load balancer.
The WorkerRole.cs fle in the ComputeResourceConsolidation.Worker project shows an example of how you
might implement this pattern in a Windows Azure cloud service.
The ComputeResourceConsolidation.Worker project is part of the ComputeResourceConsolidation
solution that is available for download with this guidance.
In the worker role, code that runs when the role is initialized creates the required cancellation token and a
list of tasks to run.
The MyWorkerTask1 and the MyWorkerTask2 methods are provided to illustrate how to perform different
tasks within the same worker role. The following code shows MyWorkerTask1. This is a simple task that
sleeps for 30 seconds and then outputs a trace message. It repeats this process indefinitely until the task is
cancelled. The code in MyWorkerTask2 is very similar.
The approach shown by the sample code is a common implementation of a background process. In a
real world application you can follow this same structure, except that you should place your own
processing logic in the body of the loop that waits for the cancellation request.
After the worker role has initialized the resources it uses, the Run method starts the two tasks concurrently,
as shown here.
In this example, the Run method waits for tasks to be completed. If a task is canceled, the Run method assumes that the role is being shut down and waits for the remaining tasks to be canceled before finishing (it
waits for a maximum of five minutes before terminating). If a task fails due to an expected exception, the Run
method cancels the task.
Note that you could implement more comprehensive monitoring and exception handling strategies in
the Run method such as restarting tasks that have failed, or including code that enables the role to stop
and start individual tasks.
Compute Resource Consolidation Pattern 41
The Stop method shown in the following code is called when the fabric controller shuts down the role in stance (it is invoked from the OnStop method). The code stops each task gracefully by cancelling it. If any
task takes more than five minutes to complete, the cancellation processing in the Stop method ceases waiting
and the role is terminated.
The following patterns and guidance may also be relevant when implementing this pattern:
? Autoscaling Guidance. Autoscaling can be used to start and stop instances of service hosting computational resources, depending on the anticipated demand for processing.
? Compute Partitioning Guidance. This guidance describes how to allocate the services and components in a cloud service in a way that helps to minimize running costs while maintaining the scalability,
performance, availability, and security of the service.
##%%&&
Segregate operations that read data from operations that update data by using separate interfaces. This
pattern can maximize performance, scalability, and security; support evolution of the system over time
through higher flexibility; and prevent update commands from causing merge conflicts at the domain level.
In traditional data management systems, both commands (updates to the data) and queries (requests for data)
are executed against the same set of entities in a single data repository. These entities may be a subset of the
rows in one or more tables in a relational database such as SQL Server.
Typically, in these systems, all create, read, update, and delete (CRUD) operations are applied to the same
representation of the entity. For example, a data transfer object (DTO) representing a customer is retrieved
from the data store by the data access layer (DAL) and displayed on the screen. A user updates some fields of
the DTO (perhaps through data binding) and the DTO is then saved back in the data store by the DAL. The
same DTO is used for both the read and write operations, as shown in Figure 1.
Traditional CRUD designs work well when there is only limited business logic applied to the data operations.
Scaffold mechanisms provided by development tools can create data access code very quickly, which can then
be customized as required.
However, the traditional CRUD approach has some disadvantages:
? It often means that there is a mismatch between the read and write representations of the data, such
as additional columns or properties that must be updated correctly even though they are not required
as part of an operation.
? It risks encountering data contention in a collaborative domain (where multiple actors operate in parallel on the same set of data) when records are locked in the data store, or update conflicts caused by
concurrent updates when optimistic locking is used. These risks increase as the complexity and
throughput of the system grows. In addition, the traditional approach can also have a negative effect
on performance due to load on the data store and data access layer, and the complexity of queries required to retrieve information.
? It can make managing security and permissions more cumbersome because each entity is subject to
both read and write operations, which might inadvertently expose data in the wrong context.
For a deeper understanding of the limits of the CRUD approach see ¡°CRUD, Only When You Can
Afford It¡± on MSDN.
Command and Query Responsibility Segregation (CQRS) is a pattern that segregates the operations that read
data (Queries) from the operations that update data (Commands) by using separate interfaces. This implies
that the data models used for querying and updates are different. The models can then be isolated, as shown
in Figure 2, although this is not an absolute requirement.
Compared to the single model of the data (from which developers build their own conceptual models) that
is inherent in CRUD-based systems, the use of separate query and update models for the data in CQRS-based
systems considerably simplifies design and implementation. However, one disadvantage is that, unlike CRUD
designs, CQRS code cannot automatically be generated by using scaffold mechanisms.
The query model for reading data and the update model for writing data may access the same physical store,
perhaps by using SQL views or by generating projections on the fly. However, it is common to separate the
data into different physical stores to maximize performance, scalability, and security; as shown in Figure 3.
The read store can be a read-only replica of the write store, or the read and write stores may have a different
structure altogether. Using multiple read-only replicas of the read store can considerably increase query
performance and application UI responsiveness, especially in distributed scenarios where read-only replicas
are located close to the application instances. Some database systems, such as SQL Server, provide additional features such as failover replicas to maximize availability.
Separation of the read and write stores also allows each to be scaled appropriately to match the load. For
example, read stores typically encounter a much higher load that write stores.
When the query/read model contains denormalized information (see Materialized View Pattern), performance
is maximized when reading data for each of the views in an application or when querying the data in the system.
For more information about the CQRS pattern and its implementation, see the following resources:
? The patterns & practices guide CQRS Journey on MSDN. In particular you should read the chapter
Introducing the Command Query Responsibility Segregation Pattern for a full exploration of the pattern
and when it is useful, and the chapter Epilogue: Lessons Learned to understand some of the issues that
can arise when using this pattern.
? The post CQRS by Martin Fowler, which explains the basics of the pattern and links to several other
useful resources.
? Greg Young¡¯s posts on the Code Better website, which explore many aspects of the CQRS pattern.
Consider the following points when deciding how to implement this pattern:
? Dividing the data store into separate physical stores for read and write operations can increase the
performance and security of a system, but it can add considerable complexity in terms of resiliency and
eventual consistency. The read model store must be updated to reflect changes to the write model
store, and it may be difficult to detect when a user has issued a request based on stale read data¡ª
meaning that the operation cannot be completed.
For a description of eventual consistency see the Data Consistency Primer.
? Consider applying CQRS to limited sections of your system where it will be most valuable, and learn
from the experience.
? A typical approach to embracing eventual consistency is to use event sourcing in conjunction with
CQRS so that the write model is an append-only stream of events driven by execution of commands.
These events are used to update materialized views that act as the read model. For more information
see the section ¡°Event Sourcing and CQRS¡± below.
This pattern is ideally suited to:
? Collaborative domains where multiple operations are performed in parallel on the same data. CQRS
allows you to define commands with a sufficient granularity to minimize merge conflicts at the domain
level (or any conflicts that do arise can be merged by the command), even when updating what appears
to be the same type of data.
? Use with task-based user interfaces (where users are guided through a complex process as a series of
steps), with complex domain models, and for teams already familiar with domain-driven design (DDD)
techniques. The write model has a full command-processing stack with business logic, input validation,
and business validation to ensure that everything is always consistent for each of the aggregates (each
cluster of associated objects that are treated as a unit for the purpose of data changes) in the write
model. The read model has no business logic or validation stack and just returns a DTO for use in a
view model. The read model is eventually consistent with the write model.
? Scenarios where performance of data reads must be fine-tuned separately from performance of data
writes, especially when the read/write ratio is very high, and when horizontal scaling is required. For
example, in many systems the number of read operations is orders of magnitude greater that the number of write operations. To accommodate this, consider scaling out the read model, but running the
write model on only one or a few instances. A small number of write model instances also helps to
minimize the occurrence of merge conflicts.
? Scenarios where one team of developers can focus on the complex domain model that is part of the
write model, and another less experienced team can focus on the read model and the user interfaces.
? Scenarios where the system is expected to evolve over time and may contain multiple versions of the
model, or where business rules change regularly.
? Integration with other systems, especially in combination with Event Sourcing, where the temporal
failure of one subsystem should not affect the availability of the others.
This pattern might not be suitable in the following situations:
? Where the domain or the business rules are simple.
? Where a simple CRUD-style user interface and the related data access operations are sufficient.
? For implementation across the whole system. There are specific components of an overall data management scenario where CQRS can be useful, but it can add considerable and often unnecessary complexity where it is not actually required.
The CQRS pattern is often used in conjunction with the Event Sourcing pattern. CQRS-based systems use
separate read and write data models, each tailored to relevant tasks and often located in physically separate
stores. When used with Event Sourcing, the store of events is the write model, and this is the authoritative
source of information. The read model of a CQRS-based system provides materialized views of the data,
typically as highly denormalized views. These views are tailored to the interfaces and display requirements of
the application, which helps to maximize both display and query performance.
Using the stream of events as the write store, rather than the actual data at a point in time, avoids update
conflicts on a single aggregate and maximizes performance and scalability. The events can be used to asynchronously generate materialized views of the data that are used to populate the read store.
Because the event store is the authoritative source of information, it is possible to delete the materialized
views and replay all past events to create a new representation of the current state when the system evolves,
or when the read model must change. The materialized views are effectively a durable read-only cache of the
data.
When using CQRS combined with the Event Sourcing pattern, consider the following:
? As with any system where the write and read stores are separate, systems based on this pattern are
only eventually consistent. There will be some delay between the event being generated and the data
store that holds the results of operations initiated by these events being updated.
? The pattern introduces additional complexity because code must be created to initiate and handle
events, and assemble or update the appropriate views or objects required by queries or a read model.
The inherent complexity of the CQRS pattern when used in conjunction with Event Sourcing can
make a successful implementation more difficult, and requires relearning of some concepts and a different approach to designing systems. However, Event Sourcing can make it easier to model the domain, and makes it easier to rebuild views or create new ones because the intent of the changes in the
data is preserved.
? Generating materialized views for use in the read model or projections of the data by replaying and
handling the events for specific entities or collections of entities may require considerable processing
time and resource usage, especially if it requires summation or analysis of values over long time periods,
because all the associated events may need to be examined. This may be partially resolved by implementing snapshots of the data at scheduled intervals, such as a total count of the number of a specific
action that have occurred, or the current state of an entity.
For more information see Event Sourcing Pattern and Materialized View Pattern, and the patterns
& practices guide CQRS Journey on MSDN. In particular you should read the chapter Introducing
Event Sourcing for a full exploration of the pattern and how it is useful with CQRS, and the
chapter A CQRS and ES Deep Dive to understand more¡ªincluding how aggregate partitioning can
be used with CQRS in Windows Azure.
The following code shows some extracts from an example of a CQRS implementation, which uses different
definitions for the read and the write models. The model interfaces do not dictate any features of the underlying data stores, and they can evolve and be fine-tuned independently because these interfaces are separated.
The following code shows the read model definition.
The system allows users to rate products. The application code does this by using the RateProduct command
shown in the following code.
The system uses the ProductsCommandHandler class to handle commands sent by the application. Clients
typically send commands to the domain through a messaging system such as a queue. The command handler accepts these commands and invokes methods of the domain interface. The granularity of each command is designed
to mitigate the chance of conflicting requests. The following code shows an outline of the ProductsCommand Handler class.
The following code shows the ProductsDomain interface from the write model.
Also notice how the ProductsDomain interface contains methods that have a meaning in the domain. Typically, in a CRUD environment these methods would have generic names such as Save or Update, and have a
DTO as the only argument. The CQRS approach can be better tailored to suit the way that this organization
carries out business and inventory management.
The following patterns and guidance may also be relevant when implementing this pattern:
? Data Consistency Primer. This guidance explains the issues that are typically encountered due to eventual consistency between the read and write data stores when using the CQRS pattern, and how these
issues can be resolved.
? Data Partitioning Guidance. This guidance describes how the read and write data stores used in the
CQRS pattern can be divided into separate partitions that can be managed and accessed separately to
improve scalability, reduce contention, and optimize performance.
? Event Sourcing Pattern. This pattern describes in more detail how Event Sourcing can be used with the
CQRS pattern to simplify tasks in complex domains; improve performance, scalability, and responsiveness; provide consistency for transactional data; and maintain full audit trails and history that may en able compensating actions.
? Materialized View Pattern. The read model of a CQRS implementation may contain materialized views
of the write model data, or the read model may be used to generate materialized views.
##%%&&
Use an append-only store to record the full series of events that describe actions taken on data in a domain,
rather than storing just the current state, so that the store can be used to materialize the domain objects. This
pattern can simplify tasks in complex domains by avoiding the requirement to synchronize the data model
and the business domain; improve performance, scalability, and responsiveness; provide consistency for transactional data; and maintain full audit trails and history that may enable compensating actions.
Most applications work with data, and the typical approach is for the application to maintain the current state
of the data by updating it as users work with the data. For example, in the traditional create, read, update, and
delete (CRUD) model a typical data process will be to read data from the store, make some modifications to it,
and update the current state of the data with the new values¡ªoften by using transactions that lock the data.
The CRUD approach has some limitations:
? The fact that CRUD systems perform update operations directly against a data store may hinder performance and responsiveness, and limit scalability, due to the processing overhead it requires.
? In a collaborative domain with many concurrent users, data update conflicts are more likely to occur
because the update operations take place on a single item of data.
? Unless there is an additional auditing mechanism, which records the details of each operation in a separate log, history is lost.
For a deeper understanding of the limits of the CRUD approach see ¡°CRUD, Only When You Can
Afford It¡± on MSDN.
The Event Sourcing pattern defines an approach to handling operations on data that is driven by a sequence
of events, each of which is recorded in an append-only store. Application code sends a series of events that
imperatively describe each action that has occurred on the data to the event store, where they are persisted.
Each event represents a set of changes to the data (such as AddedItemToOrder).
The events are persisted in an event store that acts as the source of truth or system of record (the authoritative
data source for a given data element or piece of information) about the current state of the data. The event
store typically publishes these events so that consumers can be notified and can handle them if needed.
Consumers could, for example, initiate tasks that apply the operations in the events to other systems, or
perform any other associated action that is required to complete the operation. Notice that the application
code that generates the events is decoupled from the systems that subscribe to the events.
Typical uses of the events published by the event store are to maintain materialized views of entities as actions
in the application change them, and for integration with external systems. For example, a system may maintain
a materialized view of all customer orders that is used to populate parts of the UI. As the application adds
new orders, adds or removes items on the order, and adds shipping information, the events that describe these
changes can be handled and used to update the materialized view.
See the Materialized View Pattern for more information.
In addition, at any point in time it is possible for applications to read the history of events, and use it to materialize the current state of an entity by effectively ¡°playing back¡± and consuming all the events related to
that entity. This may occur on demand in order to materialize a domain object when handling a request, or
through a scheduled task so that the state of the entity can be stored as a materialized view to support the
presentation layer.
Figure 1 shows a logical overview of the pattern, including some of the options for using the event stream
such as creating a materialized view, integrating events with external applications and systems, and replaying
events to create projections of the current state of specific entities.
The Event Sourcing pattern provides many advantages, including the following:
? Events are immutable and so can be stored using an append-only operation. The user interface, work?ow, or process that initiated the action that produced the events can continue, and the tasks that
handle the events can run in the background. This, combined with the fact that there is no contention
during the execution of transactions, can vastly improve performance and scalability for applications,
especially for the presentation level or user interface.
? Events are simple objects that describe some action that occurred, together with any associated data
required to describe the action represented by the event. Events do not directly update a data store;
they are simply recorded for handling at the appropriate time. These factors can simplify implementation and management.
? Events typically have meaning for a domain expert, whereas the complexity of the object-relational
impedance mismatch might mean that a database table may not be clearly understood by the domain
expert. Tables are artificial constructs that represent the current state of the system, not the events
that occurred.
? Event sourcing can help to prevent concurrent updates from causing conflicts because it avoids the
requirement to directly update objects in the data store. However, the domain model must still be designed to protect itself from requests that might result in an inconsistent state.
? The append-only storage of events provides an audit trail that can be used to monitor actions taken
against a data store, regenerate the current state as materialized views or projections by replaying the
events at any time, and assist in testing and debugging the system. In addition, the requirement to use
compensating events to cancel changes provides a history of changes that were reversed, which would
not be the case if the model simply stored the current state. The list of events can also be used to analyze application performance and detect user behavior trends, or to obtain other useful business information.
? The decoupling of the events from any tasks that perform operations in response to each event raised
by the event store provides flexibility and extensibility. For example, the tasks that handle events
raised by the event store are aware only of the nature of the event and the data it contains. The way
that the task is executed is decoupled from the operation that triggered the event. In addition, multiple
tasks can handle each event. This may enable easy integration with other services and systems that
need only listen for new events raised by the event store. However, the event sourcing events tend to
be very low level, and it may be necessary to generate specific integration events instead.
Event sourcing is commonly combined with the CQRS pattern by performing the data
management tasks in response to the events, and by materializing views from the stored events.
Consider the following points when deciding how to implement this pattern:
? The system will only be eventually consistent when creating materialized views or generating projections of data by replaying events. There is some delay between an application adding events to the
event store as the result of handling a request, the events being published, and consumers of the
events handling them. During this period, new events that describe further changes to entities may
have arrived at the event store.
See the Data Consistency Primer for information about eventual consistency.
? The event store is the immutable source of information, and so the event data should never be updated. The only way to update an entity in order to undo a change is to add a compensating event to the
event store, much as you would use a negative transaction in accounting. If the format (rather than the
data) of the persisted events needs to change, perhaps during a migration, it can be difficult to combine existing events in the store with the new version. It may be necessary to iterate through all the
events making changes so that they are compliant with the new format, or add new events that use the
new format. Consider using a version stamp on each version of the event schema in order to maintain
both the old and the new event formats.
? Multi-threaded applications and multiple instances of applications may be storing events in the event
store. The consistency of events in the event store is vital, as is the order of events that affect a specific entity (the order in which changes to an entity occur affects its current state). Adding a timestamp to every event is one option that can help to avoid issues. Another common practice is to an notate each event that results from a request with an incremental identifier. If two actions attempt to
add events for the same entity at the same time, the event store can reject an event that matches an
existing entity identifier and event identifier.
? There is no standard approach, or ready-built mechanisms such as SQL queries, for reading the events to
obtain information. The only data that can be extracted is a stream of events using an event identifier as
the criteria. The event ID typically maps to individual entities. The current state of an entity can be determined only by replaying all of the events that relate to it against the original state of that entity.
? The length of each event stream can have consequences on managing and updating the system. If the
streams are large, consider creating snapshots at specific intervals such as a specified number of events.
The current state of the entity can be obtained from the snapshot and by replaying any events that
occurred after that point in time.
For more information about creating snapshots of data, see Snapshot on Martin Fowler¡¯s
Enterprise Application Architecture website and Master-Subordinate Snapshot Replication on
MSDN.
? Even though event sourcing minimizes the chance of conflicting updates to the data, the application
must still be able to deal with inconsistencies that may arise through eventual consistency and the lack
of transactions. For example, an event that indicates a reduction in stock inventory might arrive in the
data store while an order for that item is being placed, resulting in a requirement to reconcile the two
operations; probably by advising the customer or creating a back order.
? Event publication may be ¡°at least once,¡± and so consumers of the events must be idempotent. They
must not reapply the update described in an event if the event is handled more than once. For example,
if multiple instances of a consumer maintain an aggregate of a property of some entity, such as the total number of orders placed, only one must succeed in incrementing the aggregate when an ¡°order
placed¡± event occurs. While this is not an intrinsic characteristic of event sourcing, it is the usual implementation decision.
This pattern is ideally suited to the following scenarios:
? When you want to capture ¡°intent,¡± ¡°purpose,¡± or ¡°reason¡± in the data. For example, changes to a customer entity may be captured as a series of specific event types such as Moved home, Closed account,
or Deceased.
? When it is vital to minimize or completely avoid the occurrence of conflicting updates to data.
? When you want to record events that occur, and be able to replay them to restore the state of a system; use them to roll back changes to a system; or simply as a history and audit log. For example, when
a task involves multiple steps you may need to execute actions to revert updates and then replay some
steps to bring the data back into a consistent state.
? When using events is a natural feature of the operation of the application, and requires little additional
development or implementation effort.
? When you need to decouple the process of inputting or updating data from the tasks required to apply these actions. This may be to improve UI performance, or to distribute events to other listeners
such as other applications or services that must take some action when the events occur. An example
would be integrating a payroll system with an expenses submission website so that events raised by
the event store in response to data updates made in the expenses submission website are consumed by
both the website and the payroll system.
? When you want flexibility to be able to change the format of materialized models and entity data if
requirements change, or¡ªwhen used in conjunction with CQRS¡ªyou need to adapt a read model or
the views that expose the data.
? When used in conjunction with CQRS, and eventual consistency is acceptable while a read model is
updated or, alternatively, the performance impact incurred in rehydrating entities and data from an
event stream is acceptable.
This pattern might not be suitable in the following situations:
? Small or simple domains, systems that have little or no business logic, or non-domain systems that naturally work well with traditional CRUD data management mechanisms.
? Systems where consistency and real-time updates to the views of the data are required.
? Systems where audit trails, history, and capabilities to roll back and replay actions are not required.
? Systems where there is only a very low occurrence of conflicting updates to the underlying data. For
example, systems that predominantly add data rather than updating it.
A conference management system needs to track the number of completed bookings for a conference so that
it can check whether there are seats still available when a potential attendee tries to make a new booking.
The system could store the total number of bookings for a conference in at least two ways:
? The system could store the information about the total number of bookings as a separate entity in a
database that holds booking information. As bookings are made or cancelled, the system could increment or decrement this number as appropriate. This approach is simple in theory, but can cause scalability issues if a large number of attendees are attempting to book seats during a short period of time.
For example, in the last day or so prior to the booking period closing.
? The system could store information about bookings and cancellations as events held in an event store.
It could then calculate the number of seats available by replaying these events. This approach can be
more scalable due to the immutability of events. The system only needs to be able to read data from
the event store, or to append data to the event store. Event information about bookings and cancellations is never modified.
Figure 2 shows how the seat reservation sub-system of the conference management system might be implemented by using event sourcing.
The sequence of actions for reserving two seats is as follows:
1. The user interface issues a command to reserve seats for two attendees. The command is handled by a
separate command handler (a piece of logic that is decoupled from the user interface and is responsible
2. An aggregate containing information about all reservations for the conference is constructed by
querying the events that describe bookings and cancellations. This aggregate is called SeatAvailability,
and is contained within a domain model that exposes methods for querying and modifying the data in
the aggregate.
Some optimizations to consider are using snapshots (so that you don¡¯t need to query and replay
the full list of events to obtain the current state of the aggregate), and maintaining a cached copy
of the aggregate in memory.
3. The command handler invokes a method exposed by the domain model to make the reservations.
4. The SeatAvailability aggregate records an event containing the number of seats that were reserved.
The next time the aggregate applies events, all the reservations will be used to compute how many
seats remain.
5. The system appends the new event to the list of events in the event store.
If a user wishes to cancel a seat, the system follows a similar process except that the command handler issues
a command that generates a seat cancellation event and appends it to the event store
As well as providing more scope for scalability, using an event store also provides a complete history, or audit
trail, of the bookings and cancellations for a conference. The events recorded in the event store are the definitive and only source of truth. There is no need to persist aggregates in any other way because the system
can easily replay the events and restore the state to any point in time.
You can find more information about this example in the chapter Introducing Event Sourcing in the
patterns & practices guide CQRS Journey on MSDN.
The following patterns and guidance may also be relevant when implementing this pattern:
? Command and Query Responsibility Segregation (CQRS) Pattern. The write store that provides the
immutable source of information for a CQRS implementation is often based on an implementation of
the Event Sourcing pattern. The Command and Query Responsibility Segregation pattern describes
how to segregate the operations that read data in an application from the operations that update data
by using separate interfaces.
? Materialized View Pattern. The data store used in a system based on event sourcing is typically not
well suited to efficient querying. Instead, a common approach is to generate pre-populated views of
the data at regular intervals, or when the data changes. The Materialized View pattern shows how this
can be achieved.
? Compensating Transaction Pattern. The existing data in an event sourcing store is not updated; instead
new entries are added that transition the state of entities to the new values. To reverse a change, compensating entries are used because it is not possible to simply reverse the previous change. The Compensating Transaction pattern describes how to undo the work that was performed by a previous operation.
? Data Consistency Primer. When using event sourcing with a separate read store or materialized views,
the read data will not be immediately consistent; instead it will be only eventually consistent. The Data
Consistency Primer summarizes the issues surrounding maintaining consistency over distributed data.
? Data Partitioning Guidance. Data is often partitioned when using event sourcing in order to improve
scalability, reduce contention, and optimize performance. The Data Partitioning Guidance describes
how to divide data into discrete partitions, and the issues that can arise.
##%%&&
Move configuration information out of the application deployment package to a centralized location. This
pattern can provide opportunities for easier management and control of configuration data, and for sharing
configuration data across applications and application instances.
The majority of application runtime environments include configuration information that is held in files deployed with the application, located within the application folders. In some cases it is possible to edit these
files to change the behavior of the application after it has been deployed. However, in many cases, changes
to the configuration require the application to be redeployed, resulting in unacceptable downtime and additional administrative overhead.
Local configuration files also limit the configuration to a single application, whereas in some scenarios it
would be useful to share configuration settings across multiple applications. Examples include database connection strings, UI theme information, or the URLs of queues and storage used by a related set of applications.
Managing changes to local configurations across multiple running instances of the application, especially in a
cloud-hosted scenario, may also be challenging. It may result in instances using different configuration settings
while the update is being deployed.
In addition, updates to applications and components may require changes to configuration schemas. Many
configuration systems do not support different versions of configuration information.
Store the configuration information in external storage, and provide an interface that can be used to quickly
and efficiently read and update configuration settings. The type of external store depends on the hosting and
runtime environment of the application. In a cloud-hosted scenario it is typically a cloud-based storage service, but could be a hosted database or other system.
The backing store chosen for configuration information should be fronted by a suitable interface that provides
consistent and easy to use access in a controlled way that enables reuse. Ideally, it should expose the information in a correctly typed and structured format. The implementation may also need to authorize users¡¯ access
in order to protect configuration data, and be flexible enough to allow multiple versions of the configuration
(such as development, staging, or production, and multiple release versions of each one) to be stored.
Many built-in configuration systems read the data when the application starts up, and cache the data in
memory to provide fast access and to minimize the impact on application performance. Depending on
the type of backing store used, and the latency of this store, it might be advantageous to implement a
caching mechanism within the external configuration store. For more information about implementing
caching, see the Caching Guidance.
Figure 1 shows an overview of this pattern.
Consider the following points when deciding how to implement this pattern:
? Choose a backing store that offers acceptable performance, high availability, robustness, and can be
backed up as part of the application maintenance and administration process. In a cloud-hosted application, using a cloud storage mechanism is usually a good choice to meet these requirements.
? Design the schema of the backing store to allow flexibility in the types of information it can hold. Ensure that it provides for all configuration requirements such as typed data, collections of settings, multiple versions of settings, and any other features that the applications using it may require. The schema
should be easy to extend as requirements change in order to support additional settings.
? Consider the physical capabilities of the backing store, how it relates to the way that configuration information is stored, and the effects on performance. For example, storing an XML document containing configuration information will require either the configuration interface or the application to parse
the document in order to read individual settings, and will make updating a setting more complicated,
though caching the settings can help to offset slower read performance.
? Consider how the configuration interface will permit control of the scope and inheritance of configuration settings. For example, it may be a requirement to scope configuration settings at the organization, application, and the machine level; to support delegation of control over access to different
scopes; and to prevent or allow individual applications to override settings.
? Ensure that the configuration interface can expose the configuration data in the required formats such
as typed values, collections, key/value pairs, or property bags. However, consider the balance between
capabilities and complexity of the API in order to make it useful and yet as easy to use as possible.
? Consider how the configuration store interface will behave when settings contain errors, or do not exist in the backing store. It may be appropriate to return default settings and log errors. Also consider
aspects such as the case sensitivity of configuration setting keys or names, the storage and handling of
binary data, and the ways that null or empty values are handled.
? Consider how you will protect the configuration data to allow access only to the appropriate users and
applications. This is likely to be a feature of the configuration store interface, but it is also necessary to
ensure that the data in the backing store cannot be accessed directly without the appropriate permission. Ensure strict separation between the permissions required to read and to write configuration
data. Also consider whether you need to encrypt some or all of the configuration settings, and how
this will be implemented within the configuration store interface.
? Keep in mind that centrally stored configurations, which change application behavior during runtime,
are critically important and should be deployed, updated, and managed using the same mechanisms as
deploying application code. For example, changes that can affect more than one application must be
carried out using a full test and staged deployment approach to ensure that the change is appropriate
for all applications that use this configuration. If an administrator simply edits a setting to update one
application, it could adversely impact other applications that use the same setting.
? If an application caches configuration information, the application may need to be alerted if the configuration changes. It may be possible to implement an expiration policy over cached configuration
data so that this information is automatically refreshed periodically and any changes picked up (and
actioned). The Runtime Reconfiguration Pattern described elsewhere in this guide may be relevant to
your scenario.
This pattern is ideally suited for:
? Configuration settings that are shared between multiple applications and application instances, or
where a standard configuration must be enforced across multiple applications and application instances.
? Where the standard configuration system does not support all of the required configuration settings,
such as storing images or complex data types.
? As a complementary store for some of the settings for applications, perhaps allowing applications to
override some or all of the centrally-stored settings.
? As a mechanism for simplifying administration of multiple applications, and optionally for monitoring
use of configuration settings by logging some or all types of access to the configuration store.
In a Windows Azure hosted application, a typical choice for storing configuration information externally is
to use Windows Azure storage. This is resilient, offers high performance, and is replicated three times with
automatic failover to offer high availability. Windows Azure tables provide a key/value store with the capability to use a flexible schema for the values. Windows Azure blob storage provides a hierarchical container based store that can hold any type of data in individually named blobs.
The following example shows how a configuration store can be implemented over Windows Azure blob storage to store and expose configuration information. The BlobSettingsStore class abstracts blob storage for
holding configuration information, and implements the ISettingsStore interface shown in the following code.
This code is provided in the ExternalConfgurationStore.Cloud project in the
ExternalConfgurationStore solution. This solution is available for download with this guidance.
This interface defines methods for retrieving and updating configuration settings held in the configuration
store, and includes a version number that can be used to detect whether any configuration settings have been
modified recently. When a configuration setting is updated, the version number changes. The BlobSettings Store class uses the ETag property of the blob to implement versioning. The ETag property of a blob is up dated automatically each time the blob is written.
Note that, by design, this simple solution exposes all configuration settings as string values rather than
typed values.
The ExternalConfgurationManager class provides a wrapper around a BlobSettingsStore object. An application can use this class to store and retrieve configuration information. This class uses the Microsoft Reactive Extensions library to expose any changes made to the configuration through an implementation of the
IObservable interface. If a setting is modified by calling the SetAppSetting method, the Changed event is
raised and all subscribers to this event will be notified.
Note that all settings are also cached in a Dictionary object inside the ExternalConfgurationManager class
for fast access. The SetAppSetting method updates this cache, and the GetSetting method that an application can use to retrieve a configuration setting reads the data from the cache (if the setting is not found in
the cache, it is retrieved from the BlobSettingsStore object instead).
The GetSettings method invokes the CheckForConfgurationChanges method to detect whether the configuration information in blob storage has changed by examining the version number and comparing it with
the current version number held by the ExternalConfgurationManager object. If one or more changes have
occurred, the Changed event is raised and the configuration settings cached in the Dictionary object are
refreshed. This is an application of the Cache-Aside Pattern.
The following code sample shows how the Changed event, the SetAppSettings method, the GetSettings
method, and the CheckForConfgurationChanges method are implemented.
The ExternalConfgurationManager class also provides a property named Environment. The purpose
of this property is to support varying configurations for an application running in different
environments, such as staging and production.
An ExternalConfgurationManager object can also query the BlobSettingsStore object periodically for any
changes (by using a timer). The StartMonitor and StopMonitor methods illustrated in the code sample below
start and stop the timer. The OnTimerElapsed method runs when the timer expires and invokes the Check ForConfgurationChanges method to detect any changes and raise the Changed event, as described earlier.
The ExternalConfgurationManager class is instantiated as a singleton instance by the ExternalConfguration
class shown below.
The following code is taken from the WorkerRole class in the ExternalConfgurationStore.Cloud project. It
shows how the application uses the ExternalConfguration class to read and update a setting.
The following code, also from the WorkerRole class, shows how the application subscribes to configuration
events.
The following pattern may also be relevant when implementing this pattern:
? Runtime Reconfiguration Pattern. In addition to storing configuration externally, it is useful to be
able to update configuration settings and have the changes applied without restarting the application.
The Runtime Reconfiguration pattern describes how to design an application so that it can be reconfigured without requiring redeployment or restarting.
##%%&&
Delegate authentication to an external identity provider. This pattern can simplify development, minimize the
requirement for user administration, and improve the user experience of the application.
Users typically need to work with multiple applications provided by, and hosted by different organizations
with which they have a business relationship. However, these users may be forced to use specific (and different) credentials for each one. This can:
? Cause a disjointed user experience. Users often forget sign-in credentials when they have many different ones.
? Expose security vulnerabilities. When a user leaves the company the account must immediately be
deprovisioned. It is easy to overlook this in large organizations.
? Complicate user management. Administrators must manage credentials for all of the users, and perform additional tasks such as providing password reminders.
Users will, instead, typically expect to use the same credentials for these applications.
Implement an authentication mechanism that can use federated identity. Separating user authentication from
the application code, and delegating authentication to a trusted identity provider, can considerably simplify
development and allow users to authenticate using a wider range of identity providers (IdPs) while minimizing
the administrative overhead. It also allows you to clearly decouple authentication from authorization.
The trusted identity providers may include corporate directories, on-premises federation services, other security token services (STSs) provided by business partners, or social identity providers that can authenticate
users who have, for example, a Microsoft, Google, Yahoo!, or Facebook account.
Figure 1 illustrates the principles of the federated identity pattern when a client application needs to access
a service that requires authentication. The authentication is performed by an identity provider (IdP), which
works in concert with a security token service (STS). The IdP issues security tokens that assert information
about the authenticated user. This information, referred to as claims, includes the user¡¯s identity, and may also
include other information such as role membership and more granular access rights.
This model is often referred to as claims-based access control. Applications and services authorize access to
features and functionality based on the claims contained in the token. The service that requires authentication must trust the IdP. The client application contacts the IdP that performs the authentication. If the authentication is successful, the IdP returns a token containing the claims that identify the user to the STS (note
that the IdP and STS may be the same service). The STS can transform and augment the claims in the token
based on predefined rules, before returning it to the client. The client application can then pass this token to
the service as proof of its identity.
In some scenarios there may be additional STSs in the chain of trust. For example, in the Windows
Azure scenario described later, an on-premises STS trusts another STS that is responsible for accessing
an identity provider to authenticate the user. This approach is common in enterprise scenarios where
there is an on-premises STS and directory.
Federated authentication provides a standards-based solution to the issue of trusting identities across diverse
domains, and can support single sign on. It is becoming more common across all types of applications, especially cloud-hosted applications, because it supports single sign on without requiring a direct network connection to identity providers. The user does not have to enter credentials for every application. This increases security because it prevents the proliferation of credentials required to access many different applications, and it also hides the user¡¯s credentials from all but the original identity provider. Applications see just
the authenticated identity information contained within the token.
Federated identity also has the major advantage that management of the identity and credentials is the responsibility of the identity provider. The application or service does not need to provide identity management
features. In addition, in corporate scenarios, the corporate directory does not need to know about the user
(providing it trusts the identity provider), which removes all the administrative overhead of managing the user
identity within the directory.
Consider the following when designing applications that implement federated authentication:
? Authentication can be a single point of failure. If you deploy your application to multiple datacenters,
consider deploying your identity management mechanism to the same datacenters in order to maintain
application reliability and availability.
? Authentication mechanisms may provide facilities to configure access control based on role claims contained in the authentication token. This is often referred to as role-based access control (RBAC), and it
may allow a more granular level of control over access to features and resources.
? Unlike a corporate directory, claims-based authentication using social identity providers does not usually provide information about the authenticated user other than an email address, and perhaps a name.
Some social identity providers, such as a Microsoft account, provide only a unique identifier. The application will usually need to maintain some information on registered users, and be able to match this
information to the identifier contained in the claims in the token. Typically this is done through a registration process when the user first accesses the application, and information is then injected into the
token as additional claims after each authentication.
? If there is more than one identity provider configured for the STS, it must detect which identity provider the user should be redirected to for authentication. This process is referred to as home realm discovery. The STS may be able to do this automatically based on an email address or user name that the
user provides, a subdomain of the application that the user is accessing, the user¡¯s IP address scope, or
on the contents of a cookie stored in the user¡¯s browser. For example, if the user entered an email address in the Microsoft domain, such as user@live.com, the STS will redirect the user to the Microsoft
account sign-in page. On subsequent visits, the STS could use a cookie to indicate that the last sign in
was with a Microsoft account. If automatic discovery cannot determine the home realm, the STS will
display a home realm discovery (HRD) page that lists the trusted identity providers, and the user must
select the one they want to use.
This pattern is ideally suited for a range of scenarios, such as:
? Single sign on in the enterprise. In this scenario you need to authenticate employees for corporate
applications that are hosted in the cloud outside the corporate security boundary, without requiring
them to sign on every time they visit an application. The user experience is the same as when using on premises applications where they are initially authenticated when signing on to a corporate network,
and from then on have access to all relevant applications without needing to sign on again.
? Federated identity with multiple partners. In this scenario you need to authenticate both corporate
employees and business partners who do not have accounts in the corporate directory. This is common
in business-to-business (B2B) applications, applications that integrate with third party services, and
where companies with disparate IT systems have merged or share resources.
? Federated identity in SaaS applications. In this scenario independent software vendors (ISVs) provide
a ready to use service for multiple clients or tenants. Each tenant will want to authenticate using a
suitable identity provider. For example, business users will want to us their corporate credentials, while
consumers and clients of the tenant may want to use their social identity credentials.
This pattern might not be suitable in the following situations:
? All users of the application can be authenticated by one identity provider, and there is no requirement
to authenticate using any other identity provider. This is typical in business applications that use only a
corporate directory for authentication, and access to this directory is available in the application directly, by using a VPN, or (in a cloud-hosted scenario) through a virtual network connection between
the on-premises directory and the application.
? The application was originally built using a different authentication mechanism, perhaps with custom
user stores, or does not have the capability to handle the negotiation standards used by claims-based
technologies. Retrofitting claims-based authentication and access control into existing applications can
be complex, and may not be cost effective.
An organization hosts a multi-tenant Software as a Service (SaaS) application in Windows Azure. The application includes a website that tenants can use to manage the application for their own users. The application
allows tenants to access the tenant¡¯s website by using a federated identity that is generated by Active Directory Federation Services (ADFS) when a user is authenticated by that organization¡¯s own Active Directory.
Figure 2 shows an overview of this process.
In the scenario shown in Figure 2, tenants authenticate with their own identity provider (step 1), in this case
ADFS. After successfully authenticating a tenant, ADFS issues a token. The client browser forwards this token
to the SaaS application¡¯s federation provider, which trusts tokens issued by the tenant¡¯s ADFS, in order to get
back a token that is valid for the SaaS federation provider (step 2). If necessary, the SaaS federation provider
performs a transformation on the claims in the token into claims that the application recognizes (step 3) before returning the new token to the client browser. The application trusts tokens issued by the SaaS federation provider and uses the claims in the token to apply authorization rules (step 4).
Tenants will not need to remember separate credentials to access the application, and an administrator at the
tenant¡¯s company will be able to configure in its own ADFS the list of users that can access the application.
##%%&&
Protect applications and services by using a dedicated host instance that acts as a broker between clients and
the application or service, validates and sanitizes requests, and passes requests and data between them. This
can provide an additional layer of security, and limit the attack surface of the system.
Applications expose their functionality to clients by accepting and processing requests. In cloud-hosted scenarios, applications expose endpoints to which clients connect, and typically include the code to handle the
requests from clients. This code may perform authentication and validation, some or all request processing,
and is likely to accesses storage and other services on behalf of the client.
If a malicious user is able to compromise the system and gain access to application¡¯s hosting environment, the
security mechanisms it uses such as credentials and storage keys, and the services and data it accesses, are
exposed. As a result, the malicious user may be able to gain unrestrained access to sensitive information and
other services.
To minimize the risk of clients gaining access to sensitive information and services, decouple hosts or tasks
that expose public endpoints from the code that processes requests and accesses storage. This can be achieved
by using a fa?ade or a dedicated task that interacts with clients and then hands off the request (perhaps
through a decoupled interface) to the hosts or tasks that will handle the request. Figure 1 shows a high-level
view of this approach.
The gatekeeper pattern may be used simply to protect storage, or it may be used as a more comprehensive
fa?ade to protect all of the functions of the application. The important factors are:
? Controlled validation. The Gatekeeper validates all requests, and rejects those that do not meet validation requirements.
? Limited risk and exposure. The Gatekeeper does not have access to the credentials or keys used by
the trusted host to access storage and services. If the Gatekeeper is compromised, the attacker does
not obtain access to these credentials or keys.
? Appropriate security. The Gatekeeper runs in a limited privilege mode, whereas the remainder of the
application runs in the full trust mode required to access storage and services. If the Gatekeeper is
compromised, it cannot directly access the application services or data.
This pattern effectively acts like a firewall in a typical network topography. It allows the Gatekeeper to examine requests and make a decision about whether to pass the request on to the trusted host (sometimes
called the Keymaster) that performs the required tasks. This decision will typically require the Gatekeeper to
validate and sanitize the request content before passing it on to the trusted host.
Consider the following points when deciding how to implement this pattern:
? Ensure that the trusted hosts to which the Gatekeeper passes requests expose only internal or protected endpoints, and connect only to the Gatekeeper. The trusted hosts should not expose any external endpoints or interfaces.
? The Gatekeeper must run in a limited privilege mode. Typically this means running the Gatekeeper and
the trusted host in separate hosted services or virtual machines.
? The Gatekeeper should not perform any processing related to the application or services, or access any
data. Its function is purely to validate and sanitize requests. The trusted hosts may need to perform
additional validation of requests, but the core validation should be performed by the Gatekeeper.
? Use a secure communication channel (HTTPS, SSL, or TLS) between the Gatekeeper and the trusted
hosts or tasks where this is possible. However, some hosting environments may not support HTTPS on
internal endpoints.
? Adding the extra layer to the application to implement the Gatekeeper pattern is likely to have some
impact on performance of the application due to the additional processing and network communication it requires.
? The Gatekeeper instance could be a single point of failure. To minimize the impact of a failure, consider
deploying additional instances and using an autoscaling mechanism to ensure sufficient capacity to
maintain availability.
This pattern is ideally suited for:
? Applications that handle sensitive information, expose services that must have high a degree of protection from malicious attacks, or perform mission-critical operations that must not be disrupted.
? Distributed applications where it is necessary to perform request validation separately from the main
tasks, or to centralize this validation to simplify maintenance and administration.
In a cloud-hosted scenario, this pattern can be implemented by decoupling the Gatekeeper role or virtual
machine from the trusted roles and services in an application by using an internal endpoint, a queue, or storage as an intermediate communication mechanism. Figure 2 shows the basic principle when using an internal
endpoint.
The following pattern may also be relevant when implementing this pattern:
? Valet Key Pattern. When communicating between the Gatekeeper and trusted roles it is good practice
to enhance security by using keys or tokens that limit permissions for accessing resources. The Valet
Key pattern describes how to use a token or key that provides clients with restricted direct access to a
specific resource or service.
##%%&&
Implement functional checks within an application that external tools can access through exposed endpoints
at regular intervals. This pattern can help to verify that applications and services are performing correctly
It is good practice¡ªand often a business requirement¡ªto monitor web applications, and middle-tier and
shared services, to ensure that they are available and performing correctly. However, it is more difficult to
monitor services running in the cloud than it is to monitor on-premises services. For example, you do not have
full control of the hosting environment, and the services typically depend on other services provided by
platform vendors and others.
There are also many factors that affect cloud-hosted applications such as network latency, the performance
and availability of the underlying compute and storage systems, and the network bandwidth between them.
The service may fail entirely or partially due to any of these factors. Therefore, you must verify at regular
intervals that the service is performing correctly to ensure the required level of availability¡ªwhich might be
part of your Service Level Agreement (SLA).
Implement health monitoring by sending requests to an endpoint on the application. The application should
perform the necessary checks, and return an indication of its status.
A health monitoring check typically combines two factors: the checks (if any) performed by the application
or service in response to the request to the health verification endpoint, and analysis of the result by the tool
or framework that is performing the health verification check. The response code indicates the status of the
application and, optionally, any components or services it uses. The latency or response time check is performed by the monitoring tool or framework. Figure 1 shows an overview of the implementation of this
pattern.
Additional checks that might be carried out by the health monitoring code in the application include:
? Checking cloud storage or a database for availability and response time.
? Checking other resources or services located within the application, or located elsewhere but used by
the application.
Several existing services and tools are available for monitoring web applications by submitting a request to a
configurable set of endpoints, and evaluating the results against a set of configurable rules. It is relatively easy
to create a service endpoint whose sole purpose is to perform some functional tests on the system.
Typical checks that can be performed by the monitoring tools include:
? Validating the response code. For example, an HTTP Response of 200 (OK) indicates that the application responded without error. The monitoring system might also check for other response codes to
give a more comprehensive indication of the result.
? Checking the content of the response to detect errors, even when a 200 (OK) status code is returned.
This can detect errors that affect only a section of the returned web page or service response. For example, checking the title of a page or looking for a specific phrase that indicates the correct page was
returned.
? Measuring the response time, which indicates a combination of the network latency and the time that
the application took to execute the request. An increasing value may indicate an emerging problem
with the application or network.
? Checking resources or services located outside the application, such as a content delivery network
used by the application to deliver content from global caches.
? Checking for expiration of SSL certificates.
? Measuring the response time of a DNS lookup for the URL of the application in order to measure DNS
latency and DNS failures.
? Validating the URL returned by the DNS lookup to ensure correct entries. This can help to avoid malicious request redirection through a successful attack on the DNS server.
It is also useful, where possible, to run these checks from different on-premises or hosted locations to mea sure and compare response times from different places. Ideally you should monitor applications from locations that are close to customers in order to get an accurate view of the performance from each location. In
addition to providing a more robust checking mechanism, the results may influence the choice of deployment
location for the application¡ªand whether to deploy it in more than one datacenter.
Tests should also be run against all the service instances that customers use to ensure the application is
working correctly for all customers. For example, if customer storage is spread across more than one storage
account, the monitoring process must check all of these.
Consider the following points when deciding how to implement this pattern:
? How to validate the response. For example, is just a single a 200 (OK) status code sufficient to verify
the application is working correctly? While this provides the most basic measure of application avail ability, and is the minimum implementation of this pattern, it provides little information about the operations, trends, and possible upcoming issues in the application.
Make sure that the application does correctly return a 200 status code only when the target
resource is found and processed. In some scenarios, such as when using a master page to host the
target web page, the server may send back a 200 OK status code instead of a 404 Not Found
code, even when the target content page was not found.
? The number of endpoints to expose for an application. One approach is to expose at least one end point for the core services the application uses and another for ancillary or lower priority services, al lowing different levels of importance to be assigned to each monitoring result. Also consider exposing
more endpoints, such as one for each core service, to provide additional monitoring granularity. For
example, a health verification check might check the database, storage, and an external geocoding service an application uses; each requiring a different level of uptime and response time. The application
may still be healthy if the geocoding service, or some other background task, is unavailable for a few
minutes.
? Whether to use the same endpoint for monitoring as is used for general access, but to a specific path
designed for health verification checks; for example, /HealthCheck/{GUID}/ on the general access end point. This allows some functional tests within the application to be executed by the monitoring tools,
such as adding a new user registration, signing in, and placing a test order, while also verifying that the
general access endpoint is available.
? The type of information to collect in the service in response to monitoring requests, and how to return
this information. Most existing tools and frameworks look only at the HTTP status code that the end point returns. To return and validate additional information it may be necessary to create a custom
monitoring utility or service.
? How much information to collect. Performing excessive processing during the check may overload the
application and impact other users, and the time it takes may exceed the timeout of the monitoring
system so that it marks the application as unavailable. Most applications include instrumentation such
as error handlers and performance counters that log performance and detailed error information, and
this may be sufficient instead of returning additional information from a health verification check.
? How to configure security for the monitoring endpoints to protect them from public access; which might
expose the application to malicious attacks, risk the exposure of sensitive information, or attract denial of
service (DoS) attacks. Typically this should be done in the application configuration so that it can be updated easily without restarting the application. Consider using one or more of the following techniques:
? Secure the endpoint by requiring authentication. This may be achieved by using an authentication security key in the request header or by passing credentials with the request, provided that
the monitoring service or tool supports authentication.
? Use an obscure or hidden endpoint. For example, expose the endpoint on a different IP address
to that used by the default application URL, configure the endpoint on a non-standard HTTP
port, and/or use a complex path to the test page. It is usually possible to specify additional endpoint addresses and ports in the application configuration, and add entries for these endpoints to
the DNS server if required to avoid having to specify the IP address directly.
? Expose a method on an endpoint that accepts a parameter such as a key value or an operation
mode value. Depending on the value supplied for this parameter when a request is received the
code can perform a specific test or set of tests, or return a 404 (Not Found) error if the parameter value is not recognized. The recognized parameter values could be set in the application
configuration.
DoS attacks are likely to have less impact on a separate endpoint that performs basic functional
tests without compromising the operation of the application. Ideally, avoid using a test that might
expose sensitive information. If you must return any information that might be useful to an
attacker, consider how you will protect the endpoint and the data from unauthorized access. In
this case just relying on obscurity is not sufficient. You should also consider using an HTTPS
connection and encrypting any sensitive data, although this will increase the load on the server.
? How to access an endpoint that is secured using authentication. Not all tools and frameworks can be
configured to include credentials with the health verification request. For example, Windows Azure
built-in health verification features cannot provide authentication credentials. Some third party alternatives that can are Pingdom, Panopta, NewRelic, and Statuscake.
? How to ensure that the monitoring agent is performing correctly. One approach is to expose an endpoint that simply returns a value from the application configuration or a random value that can be used
to test the agent.
Also ensure that the monitoring system performs checks on itself, such as a self-test and built-in
test, to avoid it issuing false positive results.
This pattern is ideally suited for:
? Monitoring websites and web applications to verify availability.
? Monitoring websites and web applications to check for correct operation.
? Monitoring middle-tier or shared services to detect and isolate a failure that could disrupt other applications.
? To complement existing instrumentation within the application, such as performance counters and error handlers. Health verification checking does not replace the requirement for logging and auditing in
the application. Instrumentation can provide valuable information for an existing framework that monitors counters and error logs to detect failures or other issues. However, it cannot provide information
if the application is unavailable.
The following code examples, taken from the HealthCheckController class in the HealthEndpointMonitoring.
Web project that is included in the samples you can download for this guide, demonstrates exposing an endpoint
for performing a range of health checks.
The CoreServices method, shown below, performs a series of checks on services used in the application. If
all of the tests execute without error, the method returns a 200 (OK) status code. If any of the tests raises an
exception, the method returns a 500 (Internal Error) status code. The method could optionally return additional information when an error occurs, if the monitoring tool or framework is able to make use of it.
The ObscurePath method shows how you can read a path from the application configuration and use it as
the endpoint for tests. This example also shows how you can accept an ID as a parameter and use it to check
for valid requests.
The TestResponseFromConfig method shows how you can expose an endpoint that performs a check for a
specified configuration setting value.
Some options for monitoring endpoints in Windows Azure applications are:
? Use the built-in features of Windows Azure, such as the Management Services or Traffic Manager.
? Use a third party service or a framework such as Microsoft System Center Operations Manager.
? Create a custom utility or a service that runs on your own or on a hosted server.
Even though Windows Azure provides a reasonably comprehensive set of monitoring options, you
may decide to use additional services and tools to provide extra information.
Windows Azure Management Services provides a comprehensive built-in monitoring mechanism built around
alert rules. The Alerts section of the Management Services page in the Windows Azure management portal
allows you to configure up to ten alert rules per subscription for your services. These rules specify a condition
and a threshold value for a service such as CPU load, or the number of requests or errors per second, and the
service can automatically send email notifications to addresses you define in each rule.
The conditions you can monitor vary depending on the hosting mechanism you choose for your application
(such as Web Sites, Cloud Services, Virtual Machines, or Mobile Services), but all of these include the capability to create an alert rule that uses a web endpoint you specify in the settings for your service. This endpoint
should respond in a timely way so that the alert system can detect that the application is operating correctly.
For more information about creating monitoring alerts, see Management Services on MSDN.
If you host your application in Windows Azure Cloud Services web and worker roles or Virtual Machines, you
can take advantage of one of the built-in services in Windows Azure called Traffic Manager. Traffic Manager
is a routing and load-balancing service that can distribute requests to specific instances of your Cloud Services hosted application based on a range of rules and settings.
In addition to routing requests, Traffic Manager pings a URL, port, and relative path you specify on a regular
basis to determine which instances of the application defined in its rules are active and are responding to
requests. If it detects a status code 200 (OK) it marks the application as available, any other status code
causes Traffic Manager to mark the application as offline. You can view the status in the Traffic Manager
console, and configure the rule to reroute requests to other instances of the application that are responding.
However, keep in mind that Traffic Manager will only wait ten seconds to receive a response from the monitoring URL. Therefore, you should ensure that your health verification code executes within this timescale,
allowing for network latency for the round trip from Traffic Manager to your application and back again.
For more information about using Windows Traffic Manager to monitor your applications, see Windows
Azure Traffic Manager on MSDN. Traffic Manager is also discussed in Multiple Datacenter Deployment
Guidance.
The following guidance may also be relevant when implementing this pattern:
? Instrumentation and Telemetry Guidance. Checking the health of services and components is typically done by probing, but it is also useful to have the appropriate information in place to monitor application performance and detect events that occur at runtime. This data can be transmitted back to
monitoring tools to provide an additional feature for health monitoring. The Instrumentation and Telemetry guidance explores the process of gathering remote diagnostics information that is collected by
instrumentation in applications.
##%%&&
Create indexes over the fields in data stores that are frequently referenced by query criteria. This pattern can
improve query performance by allowing applications to more quickly locate the data to retrieve from a data
store.
Many data stores organize the data for a collection of entities by using the primary key. An application can
use this key to locate and retrieve data. Figure 1 shows an example of a data store holding customer information. The primary key is the Customer ID.
While the primary key is valuable for queries that fetch data based on the value of this key, an application
might not be able to use the primary key if it needs to retrieve data based on some other field. In the Customers example, an application cannot use the Customer ID primary key to retrieve customers if it queries data
solely by specifying criteria that reference the value of some other attribute, such as the town in which the
customer is located. To perform a query such as this may require the application to fetch and examine every
customer record, and this could be a slow process.
Many relational database management systems support secondary indexes. A secondary index is a separate
data structure that is organized by one or more non-primary (secondary) key fields, and it indicates where the
data for each indexed value is stored. The items in a secondary index are typically sorted by the value of the
secondary keys to enable fast lookup of data. These indexes are usually maintained automatically by the database management system.
You can create as many secondary indexes as are required to support the different queries that your application performs. For example, in a Customers table in a relational database where the customer ID is the primary key, it may be beneficial to add a secondary index over the town field if the application frequently looks
up customers by the town in which they reside.
However, although secondary indexes are a common feature of relational systems, most NoSQL data stores
used by cloud applications do not provide an equivalent feature.
If the data store does not support secondary indexes, you can emulate them manually by creating your own
index tables. An index table organizes the data by a specified key. Three strategies are commonly used for
structuring an index table, depending on the number of secondary indexes that are required and the nature
of the queries that an application performs:
? Duplicate the data in each index table but organize it by different keys (complete denormalization).
Figure 2 shows index tables that organize the same customer information by Town and LastName:
This strategy may be appropriate if the data is relatively static compared to the number of times it is
queried by using each key. If the data is more dynamic, the processing overhead of maintaining each
index table may become too great for this approach to be useful. Additionally, if the volume of data is
very large, the amount of space required to store the duplicate data will be significant.
? Create normalized index tables organized by different keys and reference the original data by using the
primary key rather than duplicating it, as shown in Figure 3. The original data is referred to as a fact table:
This technique saves space and reduces the overhead of maintaining duplicate data. The disadvantage
is that an application has to perform two lookup operations to find data by using a secondary key (find
the primary key for the data in the index table, and then look up the data in the fact table by using the
primary key).
? Create partially normalized index tables organized by different keys that duplicate frequently retrieved
fields. Reference the original data to access less frequently accessed fields. Figure 4 shows this structure.
Using this technique, you can strike a balance between the first two approaches. The data for common
queries can be retrieved quickly by using a single lookup, while the space and maintenance overhead is
not as great as duplicating the entire data set.
If an application frequently queries data by specifying a combination of values (for example, ¡°Find all customers that live in Redmond and that have a last name of Smith¡±), you could implement the keys to the items in
the index table as a concatenation of the Town attribute and the LastName attribute, as shown in Figure 5.
The keys are sorted by Town, and then by LastName for records that have the same value for Town.
Index tables can speed up query operations over sharded data, and are especially useful where the shard key
is hashed. Figure 6 shows an example where the shard key is a hash of the Customer ID. The index table can
organize data by the non-hashed value (Town and LastName), and provide the hashed shard key as the
lookup data. This can save the application from repeatedly calculating hash keys (which may be an expensive
operation) if it needs to retrieve data that falls within a range, or it needs to fetch data in order of the non hashed key. For example, a query such as ¡°Find all customers that live in Redmond¡± can be quickly resolved
by locating the matching items in the index table (which are all stored in a contiguous block), and then following the references to the customer data by using the shard keys stored in the index table.
Consider the following points when deciding how to implement this pattern:
? The overhead of maintaining secondary indexes can be significant. You must analyze and understand
the queries that your application uses. Only create index tables where they are likely to be used regularly. Do not create speculative index tables to support queries that an application does not perform,
or that an application performs only very occasionally.
? Duplicating data in an index table can add a significant overhead in terms of storage costs and the effort required to maintain multiple copies of data.
? Implementing an index table as a normalized structure that references the original data may require an
application to perform two lookup operations to find data. The first operation searches the index table to retrieve the primary key, and the second uses the primary key to fetch the data.
? If a system incorporates a number of index tables over very large data sets, it can be difficult to maintain consistency between index tables and the original data. It might be possible to design the application around the eventual consistency model. For example, to insert, update, or delete data, an application could post a message to a queue and let a separate task perform the operation and maintain the
index tables that reference this data asynchronously. For more information about implementing eventual consistency, see the Data Consistency Primer.
Windows Azure storage tables support transactional updates for changes made to data held in the
same partition (referred to as entity group transactions). If you can store the data for a fact table
and one or more index tables in the same partition, you may be able to use this feature to help
ensure consistency.
? Index tables may themselves be partitioned or sharded.
Use this pattern to improve query performance when an application frequently needs to retrieve data by
using a key other than the primary (or shard) key.
This pattern might not be suitable when:
? Data is volatile. An index table may become out of date very quickly, rendering it ineffective or making
the overhead of maintaining the index table greater than any savings made by using it.
? A field selected as the secondary key for an index table is very non-discriminating and can only have a
small set of values (for example, gender).
? The balance of the data values for a field selected as the secondary key for an index table are highly
skewed. For example, if 90% of the records contain the same value in a field, then creating and maintaining an index table to look up data based on this field may exert more overhead than scanning sequentially through the data. However, if queries very frequently target values that lie in the remaining
10%, this index may be useful. You must understand the queries that your application is performing,
and how frequently they are performed.
Windows Azure storage tables provide a highly scalable key/value data store for applications running in the
cloud. Applications store and retrieve data values by specifying a key. The data values can contain multiple
fields, but the structure of a data item is opaque to table storage, which simply handles a data item as an array
of bytes.
Index Table Pattern 87
Windows Azure storage tables also support sharding. The sharding key comprises two elements, a partition
key and a row key. Items that have the same partition key are stored in the same partition (shard), and the
items are stored in row key order within a shard. Table storage is optimized for performing range queries that
fetch data falling within a contiguous range of row key values within a partition. If you are building cloud
applications that store information in Windows Azure tables, you should structure your data with this feature
in mind.
For example, consider an application that stores information about movies. The application frequently queries
movies by genre (Action, Documentary, Historical, Comedy, Drama, and so on). You could create a Windows
Azure table with partitions for each genre by using the genre as the partition key, and specifying the movie
name as the row key, as shown in Figure 7.
This approach is less effective if the application also needs to query movies by starring actor. In this case, you
can create a separate Windows Azure table that acts as an index table. The partition key is the actor and the
row key is the movie name. The data for each actor will be stored in separate partitions. If a movie stars more
than one actor, the same movie will occur in multiple partitions.
You can duplicate the movie data in the values held by each partition by adopting the first approach described
in the Solution section above. However, it is likely that each movie will be replicated several times (once for
each actor), so it may be more efficient to partially denormalize the data to support the most common queries
(such as the names of the other actors) and enable an application to retrieve any remaining details by including the partition key necessary to find the complete information in the genre partitions. This approach is
described by the third option in the Solution section. Figure 8 depicts this approach.
The following patterns and guidance may also be relevant when implementing this pattern:
? Data Consistency Primer. An index table must be maintained as the data that it indexes changes. In
the cloud, it may not be possible or appropriate to perform operations that update an index as part of
the same transaction that modifies the data¡ªan eventually consistent approach may be more suitable.
This primer provides information on the issues surrounding eventual consistency.
? Sharding Pattern. The Index Table pattern is frequently used in conjunction with data partitioned by
using shards. The Sharding pattern provides more information on how to divide a data store into a set
of shards.
? Materialized View Pattern. Instead of indexing data to support queries that summarize data, it may be
more appropriate to create a materialized view of the data. This pattern describes how to support efficient summary queries by generating pre-populated views over data.
##%%&&
Coordinate the actions performed by a collection of collaborating task instances in a distributed application
by electing one instance as the leader that assumes responsibility for managing the other instances. This
pattern can help to ensure that task instances do not conflict with each other, cause contention for shared
resources, or inadvertently interfere with the work that other task instances are performing.
A typical cloud application consists of many tasks acting in a coordinated manner. These tasks could all be
instances running the same code and requiring access to the same resources, or they might be working together in parallel to perform the individual parts of a complex calculation.
The task instances might run autonomously for much of the time, but it may also be necessary to coordinate
the actions of each instance to ensure that they don¡¯t conflict, cause contention for shared resources, or in advertently interfere with the work that other task instances are performing. For example:
? In a cloud-based system that implements horizontal scaling, multiple instances of the same task could
be running simultaneously with each instance servicing a different user. If these instances write to a
shared resource, it may be necessary to coordinate their actions to prevent each instance from blindly
overwriting the changes made by the others.
? If the tasks are performing individual elements of a complex calculation in parallel, the results will need
to be aggregated when they all complete.
Because the task instances are all peers, there is no natural leader that can act as the coordinator or aggregator.
A single task instance should be elected to act as the leader, and this instance should coordinate the actions
of the other subordinate task instances. If all of the task instances are running the same code, they could all
be capable of acting as the leader. Therefore, the election process must be managed carefully to prevent two
or more instances taking over the leader role at the same time.
The system must provide a robust mechanism for selecting the leader. This mechanism must be able to cope
with events such as network outages or process failures. In many solutions, the subordinate task instances
monitor the leader through some type of heartbeat mechanism, or by polling. If the designated leader terminates unexpectedly, or a network failure renders the leader inaccessible by the subordinate task instances, it
will be necessary for them to elect a new leader.
There are several strategies available for electing a leader amongst a set of tasks in a distributed environment,
including:
? Selecting the task instance with the lowest-ranked instance or process ID.
? Racing to obtain a shared distributed mutex. The first task instance that acquires the mutex is the leader. However, the system must ensure that, if the leader terminates or becomes disconnected from the
rest of the system, the mutex is released to allow another task instance to become the leader.
? Implementing one of the common leader election algorithms such as the Bully Algorithm or the Ring
Algorithm. These algorithms are relatively straightforward, but there are also a number of more sophisticated techniques available. These algorithms assume that each candidate participating in the election
has a unique ID, and that they can communicate with the other candidates in a reliable manner.
Consider the following points when deciding how to implement this pattern:
? The process of electing a leader should be resilient to transient and persistent failures.
? It must be possible to detect when the leader has failed or has become otherwise unavailable (perhaps
due to a communications failure). The speed at which such detection is required will be system dependent. Some systems may be able to function for a short while without a leader, during which time a
transient fault that caused the leader to become unavailable may have been rectified. In other cases, it
may be necessary to detect leader failure immediately and trigger a new election.
? In a system that implements horizontal autoscaling, the leader could be terminated if the system scales
back and shuts down some of the computing resources.
? Using a shared distributed mutex introduces a dependency on the availability of the external service
that provides the mutex. This service may constitute a single point of failure. If this service should be come unavailable for any reason, the system will not be able to elect a leader.
? Using a single dedicated process as the leader is a relatively straightforward approach. However, if the
process fails there may be a significant delay while it is restarted, and the resultant latency may affect
the performance and response times of other processes if they are waiting for the leader to coordinate
an operation.
? Implementing one of the leader election algorithms manually provides the greatest flexibility for tuning
and optimizing the code.
Use this pattern when the tasks in a distributed application, such as a cloud-hosted solution, require careful
coordination and there is no natural leader.
Avoid making the leader a bottleneck in the system. The purpose of the leader is to coordinate the
work performed by the subordinate tasks, and it does not necessarily have to participate in this work
itself¡ªalthough it should be capable of doing so if the task is not elected as the leader.
This pattern might not be suitable:
? If there is a natural leader or dedicated process that can always act as the leader. For example, it may
be possible to implement a singleton process that coordinates the task instances. If this process fails or
becomes unhealthy, the system can shut it down and restart it.
? If the coordination between tasks can be easily achieved by using a more lightweight mechanism. For
example, if several task instances simply require coordinated access to a shared resource, a preferable
solution might be to use optimistic or pessimistic locking to control access to that resource.
? If a third-party solution is more appropriate. For example, the Windows Azure HDInsight service
(based on Apache Hadoop) uses the services provided by Apache Zookeeper to coordinate the map/
reduce tasks that aggregate and summarize data. It¡¯s also possible to install and configure Zookeeper
Leader Election Pattern 91
on a Windows Azure Virtual Machine and integrate it into your own solutions, or use the Zookeeper
prebuilt virtual machine image available from Microsoft Open Technologies. For more information, see
Apache Zookeeper on Windows Azure on the Microsoft Open Technologies website.
The DistributedMutex project in the LeaderElection solution included in the sample code available for this
guide shows how to use a lease on a Windows Azure storage blob to provide a mechanism for implementing
a shared distributed mutex. This mutex can be used to elect a leader amongst a group of role instances in a
Windows Azure cloud service. The first role instance to acquire the lease is elected the leader, and remains
the leader until it releases the lease or until it is unable to renew the lease. Other role instances can continue
to monitor the blob lease in the event that the leader is no longer available.
A blob lease is an exclusive write lock over a blob. A single blob can be the subject of a maximum of
one lease at any one point in time. A role instance can request a lease over a specified blob, and it will
be granted the lease if no other lease over the same blob is currently held by this or any other role
instance, otherwise the request will throw an exception.
To reduce the possibility that a faulted role instance retains the lease indefinitely, specify a lifetime for
the lease. When this expires, the lease becomes available. However, while a role instance holds the lease
it can request that the lease is renewed, and it will be granted the lease for a further period of time. The
role instance can continually repeat this process if it wishes to retain the lease.
For more information on how to lease a blob, see Lease Blob (REST API) on MSDN.
The BlobDistributedMutex class in the example contains the RunTaskWhenMutexAquired method that enables a role instance to attempt to obtain a lease over a specified blob. The details of the blob (the name, container, and storage account) are passed to the constructor in a BlobSettings object when the BlobDistributedMutex object is created (this object is a simple struct that is included in the sample code). The constructor also
accepts a Task that references the code that the role instance should run if it successfully acquires the lease over
the blob and is elected the leader. Note that the code that handles the low-level details of obtaining the lease is
implemented in a separate helper class named BlobLeaseManager.
The RunTaskWhenMutexAquired method in the code sample above invokes the RunTaskWhenBlobLease Acquired method shown in the following code sample to actually acquire the lease. The RunTaskWhen BlobLeaseAcquired method runs asynchronously. If the lease is successfully acquired, the role instance has
been elected the leader. The purpose of the taskToRunWhenLeaseAcquired delegate is to perform the work
that coordinates the other role instances. If the lease is not acquired, another role instance has been elected
as the leader and the current role instance remains a subordinate. Note that the TryAcquireLeaseOrWait
method is a helper method that uses the BlobLeaseManager object to obtain the lease.
The task started by the leader also executes asynchronously. While this task is running, the RunTaskWhen BlobLeaseAquired method shown in the following code sample periodically attempts to renew the lease.
This action helps to ensure that the role instance remains the leader. In the sample solution, the delay between
renewal requests is less than the time specified for the duration of the lease in order to prevent another role
instance from being elected the leader. If the renewal fails for any reason, the task is cancelled.
If the lease fails to be renewed or the task is cancelled (possibly as a result of the role instance shutting down),
the lease is released. At this point, this or another role instance might be elected as the leader. The code extract below shows this part of the process.
The KeepRenewingLease method is another helper method that uses the BlobLeaseManager object to re new the lease. The CancelAllWhenAnyCompletes method cancels the tasks specified as the first two parameters.
The following code example shows how to use the BlobDistributedMutex class in a worker role. This code
obtains a lease over a blob named MyLeaderCoordinatorTask in the leases container in development storage,
and specifies that the code defined in the MyLeaderCoordinatorTask method should run if the role instance
is elected the leader.
Note the following points about the sample solution:
? The blob is a potential single point of failure. If the blob service becomes unavailable, or the blob is inaccessible, the leader will be unable to renew the lease and no other role instance will be able to obtain
the lease. In this case, no role instance will be able to act as the leader. However, the blob service is designed to be resilient, so complete failure of the blob service is considered to be extremely unlikely.
? If the task being performed by the leader stalls, the leader might continue to renew the lease, preventing any other role instance from obtaining the lease and taking over the leader role in order to coordinate tasks. In the real world, the health of the leader should be checked at frequent intervals.
? The election process is non-deterministic. You cannot make any assumptions about which role in stance will obtain the blob lease and become the leader.
? The blob used as the target of the blob lease should not be used for any other purpose. If a role in stance attempts to store data in this blob, this data will not be accessible unless the role instance is the
leader and holds the blob lease.
The following guidance may also be relevant when implementing this pattern:
? Autoscaling Guidance. It may be possible to start and stop instances of the task hosts as the load on
the application varies. Autoscaling can help to maintain throughput and performance during times of
peak processing.
? Compute Partitioning Guidance. This guidance describes how to allocate tasks to hosts in a cloud
service in a way that helps to minimize running costs while maintaining the scalability, performance,
availability, and security of the service.
##%%&&
Generate prepopulated views over the data in one or more data stores when the data is formatted in a way
that does not favor the required query operations. This pattern can help to support efficient querying and
data extraction, and improve application performance.
When storing data, the priority for developers and data administrators is often focused on how the data is
stored, as opposed to how it is read. The chosen storage format is usually closely related to the format of the
data, requirements for managing data size and data integrity, and the kind of store in use. For example, when
using NoSQL Document store, the data is often represented as a series of aggregates, each of which contains
all of the information for that entity.
However, this may have a negative effect on queries. When a query requires only a subset of the data from
some entities, such as a summary of orders for several customers without all of the order details, it must extract all of the data for the relevant entities in order to obtain the required information.
To support efficient querying, a common solution is to generate, in advance, a view that materializes the data
in a format most suited to the required results set. The Materialized View pattern describes generating pre populated views of data in environments where the source data is not in a format that is suitable for querying,
where generating a suitable query is difficult, or where query performance is poor due to the nature of the
data or the data store.
These materialized views, which contain only data required by a query, allow applications to quickly obtain
the information they need. In addition to joining tables or combining data entities, materialized views may
include the current values of calculated columns or data items, the results of combining values or executing
transformations on the data items, and values specified as part of the query. A materialized view may even be
optimized for just a single query.
A key point is that a materialized view and the data it contains is completely disposable because it can be
entirely rebuilt from the source data stores. A materialized view is never updated directly by an application,
and so it is effectively a specialized cache.
When the source data for the view changes, the view must be updated to include the new information. This
may occur automatically on an appropriate schedule, or when the system detects a change to the original data.
In other cases it may be necessary to regenerate the view manually.
Consider the following points when deciding how to implement this pattern:
? Consider how and when the view will be updated. Ideally it will be regenerated in response to an event
indicating a change to the source data, although in some circumstances this may lead to excessive over heads if the source data changes rapidly. Alternatively, consider using a scheduled task, an external trigger, or a manual action to initiate regeneration of the view.
? In some systems, such as when using the Event Sourcing Pattern to maintain a store of only the events
that modified the data, materialized views may be necessary. Prepopulating views by examining all
events to determine the current state may be the only way to obtain information from the event store.
In cases other than when using Event Sourcing it is necessary to gauge the advantages that a materialized view may offer. Materialized views tend to be specifically tailored to one, or a small number of
queries. If many queries must be used, maintaining materialized views may result in unacceptable storage capacity requirements and storage cost.
? Consider the impact on data consistency when generating the view, and when updating the view if this
occurs on a schedule. If the source data is changing at the point when the view is generated, the copy
of the data in the view may not be fully consistent with the original data.
? Consider where you will store the view. The view does not have to be located in the same store or partition as the original data. It could be a subsets from a few different partitions combined.
? If the view is transient and is used only to improve query performance by reflecting the current state
of the data, or to improve scalability, it may be stored in cache or in a less reliable location. It can be
rebuilt if lost.
? When defining a materialized view, maximize its value by adding data items or columns to the view
based on computation or transformation of existing data items, on values passed in the query, or on
combinations of these values where this is appropriate.
? Where the storage mechanism supports it, consider indexing the materialized view to further maximize
performance. Most relational databases support indexing for views, as do Big Data solutions based on
Apache Hadoop.
This pattern is ideally suited for:
? Creating materialized views over data that is difficult to query directly, or where queries must be very
complex in order to extract data that is stored in a normalized, semi-structured, or unstructured way.
? Creating temporary views that can dramatically improve query performance, or can act directly as
source views or data transfer objects (DTOs) for the UI, for reporting, or for display.
? Supporting occasionally connected or disconnected scenarios where connection to the data store is
not always available. The view may be cached locally in this case.
? Simplifying queries and exposing data for experimentation in a way that does not require knowledge
of the source data format. For example, by joining different tables in one or more databases, or one or
more domains in NoSQL stores, and then formatting the data to suit its eventual use.
? Providing access to specific subsets of the source data that, for security or privacy reasons, should not
be generally accessible, open to modification, or fully exposed to users.
? Bridging the disjoint when using different data stores based on their individual capabilities. For example, by using a cloud store that is efficient for writing as the reference data store, and a relational data base that offers good query and read performance to hold the materialized views.
This pattern might not be suitable in the following situations:
? The source data is simple and easy to query.
? The source data changes very quickly, or can be accessed without using a view. The processing overhead of creating views may be avoidable in these cases.
? Consistency is a high priority. The views may not always be fully consistent with the original data.
Figure 2 shows an example of using the Materialized View pattern. Data in the Order, OrderItem, and Customer tables in separate partitions in a Windows Azure storage account are combined to generate a view
containing the total sales value for each product in the Electronics category, together with a count of the
number of customers who made purchases of each item.
Creating this materialized view requires complex queries. However, by exposing the query result as materialized view, users can easily obtain the results and use them directly or incorporate them in another query. The
view is likely to be used in a reporting system or dashboard, and so can be updated on a scheduled basis such
as weekly.
Although this example utilizes Windows Azure table storage, many relational database management
systems also provide native support for materialized views.
The following patterns and guidance may also be relevant when implementing this pattern:
? Data Consistency Primer. It is necessary to maintain the summary information held in a materialized
view so that it reflects the underlying data values. As the data values change, it may not be feasible to
update the summary data in real time, and instead an eventually consistent approach must be adopted.
The Data Consistency Primer summarizes the issues surrounding maintaining consistency over distributed data, and describes the benefits and tradeoffs of different consistency models.
? Command and Query Responsibility Segregation (CQRS) Pattern. You may be able to use this pattern to update the information in a materialized view by responding to events that occur when the underlying data values change.
? Event Sourcing Pattern. You can use this pattern in conjunction with the CQRS pattern to maintain
the information in a materialized view. When the data values on which a materialized view is based are
modified, the system can raise events that describe these modifications and save them in an event
store.
? Index Table Pattern. The data in a materialized view is typically organized by a primary key, but queries may need to retrieve information from this view by examining data in other fields. You can use the
Index Table Pattern to create secondary indexes over data sets for data stores that do not support native secondary indexes.
##%%&&
Decompose a task that performs complex processing into a series of discrete elements that can be reused.
This pattern can improve performance, scalability, and reusability by allowing task elements that perform the
processing to be deployed and scaled independently.
An application may be required to perform a variety of tasks of varying complexity on the information that it
processes. A straightforward but inflexible approach to implementing this application could be to perform this
processing as monolithic module. However, this approach is likely to reduce the opportunities for refactoring the
code, optimizing it, or reusing it if parts of the same processing are required elsewhere within the application.
Figure 1 illustrates the issues with processing data by using the monolithic approach. An application receives
and processes data from two sources. The data from each source is processed by a separate module that performs a series of tasks to transform this data, before passing the result to the business logic of the application.
Some of the tasks that the monolithic modules perform are functionally very similar, but the modules have
been designed separately. The code that implements the tasks is closely coupled within a module, and this
code has been developed with little or no thought given to reuse or scalability.
However, the processing tasks performed by each module, or the deployment requirements for each task,
could change as business requirements are amended. Some tasks might be compute-intensive and could
benefit from running on powerful hardware, while others might not require such expensive resources. Furthermore, additional processing might be required in the future, or the order in which the tasks performed by
the processing could change. A solution is required that addresses these issues, and increases the possibilities
for code reuse.
Decompose the processing required for each stream into a set of discrete components (or filters), each of
which performs a single task. By standardizing the format of the data that each component receives and emits,
these filters can be combined together into a pipeline. This helps to avoid duplicating code, and makes it easy
to remove, replace, or integrate additional components if the processing requirements change. Figure 2 shows
an example of this structure.
The time taken to process a single request depends on the speed of the slowest filter in the pipeline. It is possible that one or more filters could prove to be a bottleneck, especially if a large number of requests appear in
a stream from a particular data source. A key advantage of the pipeline structure is that it provides opportunities
for running parallel instances of slow filters, enabling the system to spread the load and improve throughput.
The filters that comprise a pipeline can run on different machines, enabling them to be scaled independently
and can take advantage of the elasticity that many cloud environments provide. A filter that is computationally
intensive can run on high performance hardware, while other less demanding filters can be hosted on commodity (cheaper) hardware. The filters do not even have to be in the same data center or geographical location,
which allows each element in a pipeline to run in an environment that is close to the resources it requires.
Figure 3 shows an example applied to the pipeline for the data from Source 1.
If the input and output of a filter are structured as a stream, it may be possible to perform the processing for
each filter in parallel. The first filter in the pipeline can commence its work and start to emit its results, which
are passed directly on to the next filter in the sequence before the first filter has completed its work.
Another benefit is the resiliency that this model can provide. If a filter fails or the machine it is running on is
no longer available, the pipeline may be able to reschedule the work the filter was performing and direct this
work to another instance of the component. Failure of a single filter does not necessarily result in failure of
the entire pipeline.
Using the Pipes and Filters pattern in conjunction with the Compensating Transaction Pattern can provide an
alternative approach to implementing distributed transactions. A distributed transaction can be broken down
into separate compensable tasks, each of which can be implemented by using a filter that also implements
the Compensating Transaction pattern. The filters in a pipeline can be implemented as separate hosted tasks
running close to the data that they maintain.
You should consider the following points when deciding how to implement this pattern:
? Complexity. The increased flexibility that this pattern provides can also introduce complexity, especially if the filters in a pipeline are distributed across different servers.
? Reliability. Use an infrastructure that ensures data flowing between filters in a pipeline will not be
lost.
? Idempotency. If a filter in a pipeline fails after receiving a message and the work is rescheduled to another instance of the filter, part of the work may have already been completed. If this work updates
some aspect of the global state (such as information stored in a database), the same update could be
repeated. A similar issue might arise if a filter fails after posting its results to the next filter in the pipe line, but before indicating that it has completed its work successfully. In these cases, the same work
could be repeated by another instance of the filter, causing the same results to be posted twice. This
could result in subsequent filters in the pipeline processing the same data twice. Therefore filters in a
pipeline should be designed to be idempotent. For more information see Idempotency Patterns on Jonathan Oliver¡¯s blog.
? Repeated messages. If a filter in a pipeline fails after posting a message to the next stage of the pipeline, another instance of the filter may be run (as described by the idempotency consideration above),
and it will post a copy of the same message to the pipeline. This could cause two instances of the same
message to be passed to the next filter. To avoid this, the pipeline should detect and eliminate duplicate messages.
If you are implementing the pipeline by using message queues (such as Windows Azure Service
Bus queues), the message queuing infrastructure may provide automatic duplicate message
detection and removal.
? Context and state. In a pipeline, each filter essentially runs in isolation and should not make any assumptions about how it was invoked. This means that each filter must be provided with sufficient context with
which it can perform its work. This context may comprise a considerable amount of state information.
Use this pattern when:
? The processing required by an application can easily be decomposed into a set of discrete, independent
steps.
? The processing steps performed by an application have different scalability requirements.
It may be possible to group filters that should scale together in the same process. For more
information, see the Compute Resource Consolidation Pattern.
? Flexibility is required to allow reordering of the processing steps performed by an application, or the
capability to add and remove steps.
? The system can benefit from distributing the processing for steps across different servers.
? A reliable solution is required that minimizes the effects of failure in a step while data is being processed.
This pattern might not be suitable when:
? The processing steps performed by an application are not independent, or they must be performed
together as part of the same transaction.
? The amount of context or state information required by a step makes this approach inefficient. It may
be possible to persist state information to a database instead, but do not use this strategy if the additional load on the database causes excessive contention.
You can use a sequence of message queues to provide the infrastructure required to implement a pipeline. An
initial message queue receives unprocessed messages. A component implemented as a filter task listens for a
message on this queue, performs its work, and then posts the transformed message to the next queue in the
sequence. Another filter task can listen for messages on this queue, process them, post the results to another
queue, and so on until the fully transformed data appears in the final message in the queue.
If you are building a solution on Windows Azure you can use Service Bus queues to provide a reliable and
scalable queuing mechanism. The ServiceBusPipeFilter class shown below provides an example. It demonstrates how you can implement a filter that receives input messages from a queue, processes these messages,
and posts the results to another queue.
The ServiceBusPipeFilter class is defined in the PipesAndFilters.Shared project in the PipesAndFilters
solution. This sample code is available is available for download with this guidance.
The Start method in the ServiceBusPipeFilter class connects to a pair of input and output queues, and the
Close method disconnects from the input queue. The OnPipeFilterMessageAsync method performs the
actual processing of messages; the asyncFilterTask parameter to this method specifies the processing to be
performed. The OnPipeFilterMessageAsync method waits for incoming messages on the input queue, runs
the code specified by the asyncFilterTask parameter over each messages as it arrives, and posts the results
to the output queue. The queues themselves are specified by the constructor.
The sample solution implements filters in a set of worker roles. Each worker role can be scaled independently, depending on the complexity of the business processing that it performs or the resources that it re quires to perform this processing. Additionally, multiple instances of each worker role can be run in parallel
to improve throughput.
The following code shows a Windows Azure worker role named PipeFilterARoleEntry, which is defined in
the PipeFilterA project in the sample solution.
This role contains a ServiceBusPipeFilter object. The OnStart method in the role connects to the queues for
receiving input messages and posting output messages (the names of the queues are defined in the Constants
class). The Run method invokes the OnPipeFilterMessagesAsync method to perform some processing on
each message that is received (in this example, the processing is simulated by waiting for a short period of time).
When processing is complete, a new message is constructed containing the results (in this case, the input
message is simply augmented with a custom property), and this message is posted to the output queue.
The sample code contains another worker role named PipeFilterBRoleEntry in the PipeFilterB project. This
role is similar to PipeFilterARoleEntry except that it performs different processing in the Run method. In
the example solution, these two roles are combined to construct a pipeline; the output queue for the Pipe FilterARoleEntry role is the input queue for the PipeFilterBRoleEntry role.
The sample solution also provides two further roles named InitialSenderRoleEntry (in the InitialSender
project) and FinalReceiverRoleEntry (in the FinalReceiver project). The InitialSenderRoleEntry role provides the initial message in the pipeline. The OnStart method connects to a single queue and the Run
method posts a method to this queue. This queue is the input queue used by the PipeFilterARoleEntry role,
so posting a message to this queue causes the message to be received and processed by the PipeFilterARoleEntry role. The processed message then passes through the PipeFilterBRoleEntry role.
The input queue for the FinalReceiveRoleEntry role is the output queue for the PipeFilterBRoleEntry role.
The Run method in the FinalReceiveRoleEntry role, shown below, receives the message and performs some
final processing. Then it writes the values of the custom properties added by the filters in the pipeline to the
trace output.
The following patterns and guidance may also be relevant when implementing this pattern:
? Competing Consumers Pattern. A pipeline can contain multiple instances of one or more filters. This
approach is useful for running parallel instances of slow filters, enabling the system to spread the load
and improve throughput. Each instance of a filter will compete for input with the other instances; two
instances of a filter should not be able to process the same data. The Competing Consumers pattern
provides more information on this approach.
? Compute Resource Consolidation Pattern. It may be possible to group filters that should scale together into the same process. The Compute Resource Consolidation pattern provides more information about the benefits and tradeoffs of this strategy.
? Compensating Transaction Pattern. A filter can be implemented as an operation that can be reversed,
or that has a compensating operation that restores the state to a previous version in the event of a failure. The Compensating Transaction pattern explains how this type of operation may be implemented
in order to maintain or achieve eventual consistency.
##%%&&
Prioritize requests sent to services so that requests with a higher priority are received and processed more
quickly than those of a lower priority. This pattern is useful in applications that offer different service level
guarantees to individual clients.
Applications may delegate specific tasks to other services; for example, to perform background processing or
to integrate with other applications or services. In the cloud, a message queue is typically used to delegate
tasks to background processing. In many cases the order in which requests are received by a service is not
important. However, in some cases it may be necessary to prioritize specific requests. These requests should
be processed earlier than others of a lower priority that may have been sent previously by the application.
A queue is usually a first-in, first-out (FIFO) structure, and consumers typically receive messages in the same
order that they were posted to the queue. However, some message queues support priority messaging; the
application posting a message can assign a priority to a message and the messages in the queue are automatically reordered so that messages with a higher priority will be received before those of a lower priority.
Figure 1 illustrates a queue that provides priority messaging.
Most message queue implementations support multiple consumers (following the Competing
Consumers Pattern), and the number of consumer processes can be scaled up or down as demand
dictates.
In systems that do not support priority-based message queues, an alternative solution is to maintain a separate
queue for each priority. The application is responsible for posting messages to the appropriate queue. Each
queue can have a separate pool of consumers. Higher priority queues can have a larger pool of consumers
running on faster hardware than lower priority queues. Figure 2 shows this approach.
A variation on this strategy is to have a single pool of consumers that check for messages on high priority
queues first, and then only start to fetch messages from lower priority queues if no higher priority messages
are waiting. There are some semantic differences between a solution that uses a single pool of consumer
processes (either with a single queue that supports messages with different priorities or with multiple queues
that each handle messages of a single priority), and a solution that uses multiple queues with a separate pool
for each queue.
In the single pool approach, higher priority messages will always be received and processed before lower
priority messages. In theory, messages that have a very low priority may be continually superseded and might
never be processed. In the multiple pool approach, lower priority messages will always be processed, just not
as quickly as those of a higher priority (depending on the relative size of the pools and the resources that they
have available).
Using a priority queuing mechanism can provide the following advantages:
? It allows applications to meet business requirements that necessitate prioritization of availability or
performance, such as offering different levels of service to specific groups of customers.
? It can help to minimize operational costs. In the single queue approach, you can scale back the number
of consumers if necessary. High priority messages will still be processed first (although possibly more
slowly), and lower priority messages may be delayed for longer. If you have implemented the multiple
message queue approach with separate pools of consumers for each queue, you can reduce the pool of
consumers for lower priority queues, or even suspend processing for some very low priority queues by
halting all the consumers that listen for messages on those queues.
? The multiple message queue approach can help to maximize application performance and scalability by
partitioning messages based on processing requirements. For example, vital tasks can be prioritized to
be handled by receivers that run immediately while less important background tasks can be handled by
receivers that are scheduled to run at less busy periods.
Consider the following points when deciding how to implement this pattern:
? Define the priorities in the context of the solution. For example, ¡°high priority¡± could mean that messages should be processed within ten seconds. Identify the requirements for handling high priority
items, and what other resources must be allocated to meet these criteria.
? Decide if all high priority items must be processed before any lower priority items. If the messages are
being processed by a single pool of consumers, it may be necessary to provide a mechanism that can
preempt and suspend a task that is handling a low priority message if a higher priority message becomes available.
? In the multiple queue approach, when using a single pool of consumer processes that listen on all
queues rather than a dedicated consumer pool for each queue, the consumer must apply an algorithm
that ensures it always services messages from higher priority queues before those from lower priority
queues.
? Monitor the speed of processing on high and low priority queues to ensure that messages in these
queues are processed at the expected rates.
? If you need to guarantee that low priority messages will be processed, it may be necessary to implement the multiple message queue approach with multiple pools of consumers. Alternatively, in a queue
that supports message prioritization, it may be possible to dynamically increase the priority of a
queued message as it ages. However, this approach depends on the message queue providing this feature.
? Using a separate queue for each message priority works best for systems that have a small number of
well-defined priorities.
? Message priorities may be determined logically by the system. For example, rather than having explicit
high and low priority messages, they could be designated as ¡°fee paying customer¡±, or ¡°non-fee paying
customer.¡± Depending on your business model, your system might allocate more resources to processing messages from fee paying customers than non-fee paying ones.
? There may be a financial and processing cost associated with checking a queue for a message (some
commercial messaging systems charge a small fee each time a message is posted or retrieved, and each
time a queue is queried for messages). This cost will be increased when checking multiple queues.
? It may be possible to dynamically adjust the size of a pool of consumers based on the length of the
queue that the pool is servicing. For more information, see the Autoscaling Guidance.
This pattern is ideally suited to scenarios where:
? The system must handle multiple tasks that might have different priorities.
? Different users or tenants should be served with different priority.
Windows Azure does not provide a queuing mechanism that natively support automatic prioritization of
messages through sorting. However, it does provide Windows Azure Service Bus topics and subscriptions,
which support a queuing mechanism that provides message filtering, together with a wide range of flexible
capabilities that make it ideal for use in almost all priority queue implementations.
A Windows Azure solution can implement a Service Bus topic to which an application can post messages, in
the same way as a queue. Messages can contain metadata in the form of application-defined custom properties. Service Bus subscriptions can be associated with the topic, and these subscriptions can filter messages
based on their properties. When an application sends a message to a topic, the message is directed to the
appropriate subscription from where it can be read by a consumer. Consumer processes can retrieve messages from a subscription using the same semantics as a message queue (a subscription is a logical queue).
Figure 3 illustrates a solution using Windows Azure Service Bus topics and subscriptions.
In Figure 3 the application creates several messages and assigns a custom property called Priority in each
message with a value, either High or Low. The application posts these messages to a topic. The topic has two
associated subscriptions, which both filter messages by examining the Priority property. One subscription
accepts messages where the Priority property is set to High, and the other accepts messages where the
Priority property is set to Low. A pool of consumers reads messages from each subscription. The high priority subscription has a larger pool, and these consumers might be running on more powerful (and expensive)
computers with more resources available than the consumers in the low priority pool.
Note that there is nothing special about the designation of high and low priority messages in this example.
These are simply labels specified as properties in each message, and are used to direct messages to a specific
subscription. If additional priorities are required, it is relatively easy to create further subscriptions and pools
of consumer processes to handle these priorities.
The PriorityQueue solution in the code available with this guidance contains an implementation of this approach. This solution contains two worker roles projects named PriorityQueue.High and PriorityQueue.Low.
These two worker roles inherit from a class called PriorityWorkerRole which contains the functionality for
connecting to a specified subscription in the OnStart method.
The PriorityQueue.High and PriorityQueue.Low worker roles connect to different subscriptions, defined by
their configuration settings. An administrator can configure different numbers of each role to be run; typically there will be more instances of the PriorityQueue.High worker role than the PriorityQueue.Low worker
role.
The Run method in the PriorityWorkerRole class arranges for the virtual ProcessMessage method (also defined in the PriorityWorkerRole class) to be executed for each message received on the queue. The following
code shows the Run and ProcessMessage methods. The QueueManager class, defined in the PriorityQueue.
Shared project, provides helper methods for using Windows Azure Service Bus queues.
The PriorityQueue.High and PriorityQueue.Low worker roles both override the default functionality of the
ProcessMessage method. The code below shows the ProcessMessage method for the PriorityQueue.High
worker role.
When an application posts messages to the topic associated with the subscriptions used by the PriorityQueue.High and PriorityQueue.Low worker roles, it specifies the priority by using the Priority custom property, as shown in the following code example. This code (which is implemented in the WorkerRole class in
the PriorityQueue.Sender project), uses the SendBatchAsync helper method of the QueueManager class to
post messages to a topic in batches.
The following patterns and guidance may also be relevant when implementing this pattern:
? Asynchronous Messaging Primer. A consumer service processing a request may need to send a reply
to the instance of the application that posted the request. The Asynchronous Messaging Primer provides more information on the strategies that can be used to implement request/response messaging.
? Competing Consumers Pattern. To increase the throughput of the queues, it¡¯s possible to have multiple consumers that listen on the same queue, and process the tasks in parallel. These consumers will
compete for messages, but only one should be able to process each message. The Competing Consumers pattern provides more information on the benefits and tradeoffs of implementing this approach.
? Throttling Pattern. You can implement throttling by using queues. Priority messaging can be used to
ensure that requests from critical applications, or applications being run by high-value customers, are
given precedence over requests from less important applications.
? Autoscaling Guidance. It may be possible to scale the size of the pool of consumer processes handling
a queue depending on the length of the queue. This strategy can help to improve performance, especially for pools handling high priority messages.
##%%&&
Use a queue that acts as a buffer between a task and a service that it invokes in order to smooth intermittent
heavy loads that may otherwise cause the service to fail or the task to time out. This pattern can help to
minimize the impact of peaks in demand on availability and responsiveness for both the task and the service.
Many solutions in the cloud involve running tasks that invoke services. In this environment, if a service is
subjected to intermittent heavy loads, it can cause performance or reliability issues
A service could be a component that is part of the same solution as the tasks that utilize it, or it could be a
third-party service providing access to frequently used resources such as a cache or a storage service. If the
same service is utilized by a number of tasks running concurrently, it can be difficult to predict the volume of
requests to which the service might be subjected at any given point in time.
It is possible that a service might experience peaks in demand that cause it to become overloaded and unable
to respond to requests in a timely manner. Flooding a service with a large number of concurrent requests may
also result in the service failing if it is unable to handle the contention that these requests could cause.
Refactor the solution and introduce a queue between the task and the service. The task and the service run
asynchronously. The task posts a message containing the data required by the service to a queue. The queue
acts as a buffer, storing the message until it is retrieved by the service. The service retrieves the messages from
the queue and processes them. Requests from a number of tasks, which can be generated at a highly variable
rate, can be passed to the service through the same message queue. Figure 1 shows this structure.
The queue effectively decouples the tasks from the service, and the service can handle the messages at its
own pace irrespective of the volume of requests from concurrent tasks. Additionally, there is no delay to a
task if the service is not available at the time it posts a message to the queue.
This pattern provides the following benefits:
? It can help to maximize availability because delays arising in services will not have an immediate and
direct impact on the application, which can continue to post messages to the queue even when the
service is not available or is not currently processing messages.
? It can help to maximize scalability because both the number of queues and the number of services can
be varied to meet demand.
? It can help to control costs because the number of service instances deployed needs only to be sufficient to meet average load rather than the peak load.
Some services may implement throttling if demand reaches a threshold beyond which the system
could fail. Throttling may reduce the functionality available. You might be able to implement load
leveling with these services to ensure that this threshold is not reached.
Consider the following points when deciding how to implement this pattern:
? It is necessary to implement application logic that controls the rate at which services handle messages
to avoid overwhelming the target resource. Avoid passing spikes in demand to the next stage of the
system. Test the system under load to ensure that it provides the required leveling, and adjust the number of queues and the number of service instances that handle messages to achieve this.
? Message queues are a one-way communication mechanism. If a task expects a reply from a service, it
may be necessary to implement a mechanism that the service can use to send a response. For more information, see the Asynchronous Messaging Primer.
? You must be careful if you apply autoscaling to services that are listening for requests on the queue
because this may result in increased contention for any resources that these services share, and diminish the effectiveness of using the queue to level the load.
This pattern is ideally suited to any type of application that uses services that may be subject to overloading.
This pattern might not be suitable if the application expects a response from the service with minimal latency.
A Windows Azure web role stores data by using a separate storage service. If a large number of instances of
the web role run concurrently, it is possible that the storage service could be overwhelmed and be unable to
respond to requests quickly enough to prevent these requests from timing out or failing. Figure 2 highlights
this issue.
To resolve this issue, you can use a queue to level the load between the web role instances and the storage
service. However, the storage service is designed to accept synchronous requests and cannot be easily modified to read messages and manage throughput. Therefore, you can introduce a worker role to act as a proxy
service that receives requests from the queue and forwards them to the storage service. The application
logic in the worker role can control the rate at which it passes requests to the storage service to prevent the
storage service from being overwhelmed. Figure 3 shows this solution.
The following patterns and guidance may also be relevant when implementing this pattern:
? Asynchronous Messaging Primer. Message queues are an inherently asynchronous communications
mechanism. It may be necessary to redesign the application logic in a task if it is adapted from communicating directly with a service to using a message queue. Similarly, it may be necessary to refactor
a service to accept requests from a message queue (alternatively, it may be possible to implement a
proxy service, as described in the example).
? Competing Consumers Pattern. It may be possible to run multiple instances of a service, each of
which act as a message consumer from the load-leveling queue. You can use this approach to adjust
the rate at which messages are received and passed to a service.
? Throttling Pattern. A simple way to implement throttling with a service is to use queue-based load leveling and route all requests to a service through a message queue. The service can process requests
at a rate that ensures resources required by the service are not exhausted, and to reduce the amount
of contention that could occur.
##%%&&
Enable an application to handle anticipated, temporary failures when it attempts to connect to a service or
network resource by transparently retrying an operation that has previously failed in the expectation that the
cause of the failure is transient. This pattern can improve the stability of the application.
An application that communicates with elements running in the cloud must be sensitive to the transient faults
that can occur in this environment. Such faults include the momentary loss of network connectivity to components and services, the temporary unavailability of a service, or timeouts that arise when a service is busy.
These faults are typically self-correcting, and if the action that triggered a fault is repeated after a suitable
delay it is likely to be successful. For example, a database service that is processing a large number of concurrent requests may implement a throttling strategy that temporarily rejects any further requests until its
workload has eased. An application attempting to access the database may fail to connect, but if it tries again
after a suitable delay it may succeed.
In the cloud, transient faults are not uncommon and an application should be designed to handle them elegantly and transparently, minimizing the effects that such faults might have on the business tasks that the
application is performing.
If an application detects a failure when it attempts to send a request to a remote service, it can handle the
failure by using the following strategies:
? If the fault indicates that the failure is not transient or is unlikely to be successful if repeated (for example, an authentication failure caused by providing invalid credentials is unlikely to succeed no matter
how many times it is attempted), the application should abort the operation and report a suitable exception.
? If the specific fault reported is unusual or rare, it may have been caused by freak circumstances such as
a network packet becoming corrupted while it was being transmitted. In this case, the application
could retry the failing request again immediately because the same failure is unlikely to be repeated
and the request will probably be successful.
? If the fault is caused by one of the more commonplace connectivity or ¡°busy¡± failures, the network or
service may require a short period while the connectivity issues are rectified or the backlog of work is
cleared. The application should wait for a suitable time before retrying the request.
For the more common transient failures, the period between retries should be chosen so as to spread requests
from multiple instances of the application as evenly as possible. This can reduce the chance of a busy service
continuing to be overloaded. If many instances of an application are continually bombarding a service with
retry requests, it may take the service longer to recover.
If the request still fails, the application can wait for a further period and make another attempt. If necessary,
this process can be repeated with increasing delays between retry attempts until some maximum number of
requests have been attempted and failed. The delay time can be increased incrementally, or a timing strategy
such as exponential back-off can be used, depending on the nature of the failure and the likelihood that it
will be corrected during this time.
Figure 1 illustrates this pattern. If the request is unsuccessful after a predefined number of attempts, the
application should treat the fault as an exception and handle it accordingly.
The application should wrap all attempts to access a remote service in code that implements a retry policy
matching one of the strategies listed above. Requests sent to different services can be subject to different
policies, and some vendors provide libraries that encapsulate this approach. These libraries typically implement policies that are parameterized, and the application developer can specify values for items such as the
number of retries and the time between retry attempts.
The code in an application that detects faults and retries failing operations should log the details of these
failures. This information may be useful to operators. If a service is frequently reported as unavailable or busy,
it is often because the service has exhausted its resources. You may be able to reduce the frequency with
which these faults occur by scaling out the service. For example, if a database service is continually overloaded, it may be beneficial to partition the database and spread the load across multiple servers.
Windows Azure provides extensive support for the Retry pattern. The patterns & practices Transient
Fault Handling Block enables an application to handle transient faults in many Windows Azure services
using a range of retry strategies. The Microsoft Entity Framework version 6 provides facilities for retrying
database operations. Additionally, many of the Windows Azure Service Bus and Windows Azure
Storage APIs implement retry logic transparently.
You should consider the following points when deciding how to implement this pattern:
? The retry policy should be tuned to match the business requirements of the application and the nature
of the failure. It may be better for some noncritical operations to fail fast rather than retry several
times and impact the throughput of the application. For example, in an interactive web application
that attempts to access a remote service, it may be better to fail after a smaller number of retries with
only a short delay between retry attempts, and display a suitable message to the user (for example,
¡°please try again later¡±) to prevent the application from becoming unresponsive. For a batch application, it may be more appropriate to increase the number of retry attempts with an exponentially increasing delay between attempts.
? A highly aggressive retry policy with minimal delay between attempts, and a large number of retries,
could further degrade a busy service that is running close to or at capacity. This retry policy could also
affect the responsiveness of the application if it is continually attempting to perform a failing operation rather than doing useful work.
? If a request still fails after a significant number of retries, it may be better for the application to prevent further requests going to the same resource for a period and simply report a failure immediately.
When the period expires, the application may tentatively allow one or more requests through to see
whether they are successful. For more details of this strategy, see the Circuit Breaker Pattern.
? The operations in a service that are invoked by an application that implements a retry policy may need
to be idempotent. For example, a request sent to a service may be received and processed successfully
but, due to a transient fault, it may be unable to send a response indicating that the processing has
completed. The retry logic in the application might then attempt to repeat the request on the assumption that the first request was not received.
? A request to a service may fail for a variety of reasons and raise different exceptions, depending on the
nature of the failure. Some exceptions may indicate a failure that could be resolved very quickly, while
others may indicate that the failure is longer lasting. It may be beneficial for the retry policy to adjust
the time between retry attempts based on the type of the exception.
? Consider how retrying an operation that is part of a transaction will affect the overall transaction consistency. It may be useful to fine tune the retry policy for transactional operations to maximize the
chance of success and reduce the need to undo all the transaction steps.
? Ensure that all retry code is fully tested against a variety of failure conditions. Check that it does not
severely impact the performance or reliability of the application, cause excessive load on services and
resources, or generate race conditions or bottlenecks.
? Implement retry logic only where the full context of a failing operation is understood. For example, if a
task that contains a retry policy invokes another task that also contains a retry policy, this extra layer
of retries can add long delays to the processing. It may be better to configure the lower-level task to
fail fast and report the reason for the failure back to the task that invoked it. This higher-level task can
then decide how to handle the failure based on its own policy.
? It is important to log all connectivity failures that prompt a retry so that underlying problems with the
application, services, or resources can be identified.
? Investigate the faults that are most likely to occur for a service or a resource to discover if they are
likely to be long lasting or terminal. If this is the case, it may be better to handle the fault as an exception. The application can report or log the exception, and then attempt to continue either by invoking
an alternative service (if there is one available), or by offering degraded functionality. For more information on how to detect and handle long-lasting faults, see the Circuit Breaker Pattern.
Use this pattern:
? When an application could experience transient faults as it interacts with a remote service or accesses
a remote resource. These faults are expected to be short lived, and repeating a request that has previously failed could succeed on a subsequent attempt.
This pattern might not be suitable:
? When a fault is likely to be long lasting, because this can affect the responsiveness of an application.
The application may simply be wasting time and resources attempting to repeat a request that is most
likely to fail.
? For handling failures that are not due to transient faults, such as internal exceptions caused by errors in
the business logic of an application.
? As an alternative to addressing scalability issues in a system. If an application experiences frequent
¡°busy¡± faults, it is often an indication that the service or resource being accessed should be scaled up.
This example illustrates an implementation of the Retry pattern. The OperationWithBasicRetryAsync
method, shown below, invokes an external service asynchronously through the TransientOperationAsync
method (the details of this method will be specific to the service and are omitted from the sample code).
The statement that invokes this method is encapsulated within a try/catch block wrapped in a for loop. The
for loop exits if the call to the TransientOperationAsync method succeeds without throwing an exception.
If the TransientOperationAsync method fails, the catch block examines the reason for the failure, and if it
is deemed to be a transient error the code waits for a short delay before retrying the operation.
The for loop also tracks the number of times that the operation has been attempted, and if the code fails
three times the exception is assumed to be more long lasting. If the exception is not transient or it is long-lasting, the catch handler throws an exception. This exception exits the for loop and should be caught by the
code that invokes the OperationWithBasicRetryAsync method.
The IsTransient method, shown below, checks for a specific set of exceptions that are relevant to the environment in which the code is run. The definition of a transient exception may vary according to the resources
being accessed and the environment in which the operation is being performed.
The following pattern may also be relevant when implementing this pattern:
? Circuit Breaker Pattern. The Retry Pattern is ideally suited to handling transient faults. If a failure is
expected to be more long lasting, it may be more appropriate to implement the Circuit Breaker Pat tern. The Retry Pattern can also be used in conjunction with a circuit breaker to provide a comprehensive approach to handling faults.
##%%&&
Design an application so that it can be reconfigured without requiring redeployment or restarting the application. This helps to maintain availability and minimize downtime.
A primary aim for important applications such as commercial and business websites is to minimize downtime
and the consequent interruption to customers and users. However, at times it is necessary to reconfigure the
application to change specific behavior or settings while it is deployed and in use. Therefore, it is an advantage
for the application to be designed in such a way as to allow these configuration changes to be applied while
it is running, and for the components of the application to detect the changes and apply them as soon as
possible.
Examples of the kinds of configuration changes to be applied might be adjusting the granularity of logging to
assist in debugging a problem with the application, swapping connection strings to use a different data store,
or turning on or off specific sections or functionality of the application.
The solution for implementing this pattern depends on the features available in the application hosting environment. Typically, the application code will respond to one or more events that are raised by the hosting
infrastructure when it detects a change to the application configuration. This is usually the result of uploading
a new configuration file, or in response to changes in the configuration through the administration portal or
by accessing an API.
Code that handles the configuration change events can examine the changes and apply them to the components of the application. It is necessary for these components to detect and react to the changes, and so the
values they use will usually be exposed as writable properties or methods that the code in the event handler
can set to new values or execute. From this point, the components should use the new values so that the
required changes to the application behavior occur.
If it is not possible for the components to apply the changes at runtime, it will be necessary to restart the
application so that these changes are applied when the application starts up again. In some hosting environments it may be possible to detect these types of changes, and indicate to the environment that the application must be restarted. In other cases it may be necessary to implement code that analyses the setting
changes and forces an application restart when necessary.
Most environments expose events raised in response to configuration changes. In those that do not, a polling
mechanism that regularly checks for changes to the configuration and applies these changes will be necessary.
It may also be necessary to restart the application if the changes cannot be applied at runtime. For example,
it may be possible to compare the date and time of a configuration file at preset intervals, and run code to
apply the changes when a newer version is found. Another approach would be to incorporate a control in the
administration UI of the application, or expose a secured endpoint that can be accessed from outside the
application, that executes code that reads and applies the updated configuration.
Alternatively, the application could react to some other change in the environment. For example, occurrences
of a specific runtime error might change the logging configuration to automatically collect additional information, or the code could use the current date to read and apply a theme that reflects the season or a special event.
Consider the following points when deciding how to implement this pattern:
? The configuration settings must be stored outside of the deployed application so that they can be up dated without requiring the entire package to be redeployed. Typically the settings are stored in a configuration file, or in an external repository such as a database or online storage. Access to the runtime
configuration mechanism should be strictly controlled, as well as strictly audited when used.
? If the hosting infrastructure does not automatically detect configuration change events, and expose
these events to the application code, you must implement an alternative mechanism to detect and apply the changes. This may be through a polling mechanism, or by exposing an interactive control or
endpoint that initiates the update process.
? If you need to implement a polling mechanism, consider how often checks for updates to the configuration should take place. A long polling interval will mean that changes might not be applied for some time.
A short interval might adversely affect operation by absorbing available compute and I/O resources.
? If there is more than one instance of the application, additional factors should be considered, depending on how changes are detected. If changes are detected automatically through events raised by the
hosting infrastructure, these changes may not be detected by all instances of the application at the
same time. This means that some instances will be using the original configuration for a period while
others will use the new settings. If the update is detected through a polling mechanism, this must communicate the change to all instances in order to maintain consistency.
? Some configuration changes may require the application to be restarted, or even require the hosting
server to be rebooted. You must identify these types of configuration settings and perform the appropriate action for each one. For example, a change that requires the application to be restarted might do
this automatically, or it might be the responsibility of the administrator to initiate the restart at a suitable time when the application is not under excessive load and other instances of the application can
handle the load.
? Plan for a staged rollout of updates and confirm they are successful, and that the updated application
instances are performing correctly, before applying the update to all instances. This can prevent a total
outage of the application should an error occur. Where the update requires a restart or a reboot of the
application, particularly where the application has a significant start up or warm up time, use a staged
rollout approach to prevent multiple instances being offline at the same time.
? Consider how you will roll back configuration changes that cause issues, or that result in failure of the
application. For example, it should be possible to roll back a change immediately instead of waiting for
a polling interval to detect the change.
? Consider how the location of the configuration settings might affect application performance. For example, you should handle the error that will occur if the external store you use is unavailable when the
application starts, or when configuration changes are to be applied¡ªperhaps by using a default configuration or by caching the settings locally on the server and reusing these values while retrying access
to the remote data store.
? Caching can help to reduce delays if a component needs to repeatedly access configuration settings.
However, when the configuration changes, the application code will need to invalidate the cached settings, and the component must use the updated settings.
This pattern is ideally suited for:
? Applications for which you must avoid all unnecessary downtime, while still being able to apply changes to the application configuration.
? Environments that expose events raised automatically when the main configuration changes. Typically
this is when a new configuration file is detected, or when changes are made to an existing configuration file.
? Applications where the configuration changes often and the changes can be applied to components
without requiring the application to be restarted, or without requiring the hosting server to be rebooted.
This pattern might not be suitable if the runtime components are designed so they can be configured only at
initialization time, and the effort of updating those components cannot be justified in comparison to restarting the application and enduring a short downtime.
Windows Azure Cloud Services roles detect and expose two events that are raised when the hosting environment detects a change to the ServiceConfguration.cscfg files:
? RoleEnvironment.Changing. This event is raised after a configuration change is detected, but before
it is applied to the application. You can handle the event to query the changes and to cancel the
runtime reconfiguration. If you cancel the change, the web or worker role will be restarted automatically so that the new configuration is used by the application.
? RoleEnvironment.Changed. This event is raised after the application configuration has been applied.
You can handle the event to query the changes that were applied.
When you cancel a change in the RoleEnvironment.Changing event you are indicating to Windows Azure
that a new setting cannot be applied while the application is running, and that it must be restarted in order
to use the new value. Effectively you will cancel a change only if your application or component cannot react
to the change at runtime, and requires a restart in order to use the new value.
For more information see RoleEnvironment.Changing Event and Use the RoleEnvironment.Changing Event
on MSDN.
To handle the RoleEnvironment.Changing and RoleEnvironment.Changed events you will typically add a
custom handler to the event. For example, the following code from the Global.asax.cs class in the Runtime
Reconfguration solution of the examples you can download for this guide shows how to add a custom
function named RoleEnvironment_Changed to the event hander chain. This is from the Global.asax.cs file
of the example.
The examples for this pattern are in the RuntimeReconfguration.Web project of the
RuntimeReconfguration solution.
In a web or worker role you can use similar code in the OnStart event handler of the role to handle the
RoleEnvironment.Changing event. This is from the WebRole.cs fle of the example.
Be aware that, in the case of web roles, the OnStart event handler runs in a separate process from the web
application process itself. This is why you will typically handle the RoleEnvironment.Changed event handler
in the Global.asax file so that you can update the runtime configuration of your web application, and the
RoleEnvironment.Changing event in the role itself. In the case of a worker role, you can subscribe to both
the RoleEnvironment.Changing and RoleEnvironment.Changed events within the OnStart event handler.
You can store custom configuration settings in the service configuration file, in a custom configuration
file, in a database such as Windows Azure SQL Database or SQL Server in a Virtual Machine, or in
Windows Azure blob or table storage. You will need to create code that can access the custom
configuration settings and apply these to the application¡ªtypically by setting the properties of
components within the application.
For example, the following custom function reads the value of a setting, whose name is passed as a parameter,
from the Windows Azure service configuration file and then applies it to the current instance of a runtime
component named SomeRuntimeComponent. This is from the Global.asax.cs file of the example
Some configuration settings, such as those for Windows Identity Framework, cannot be stored in the
Windows Azure service configuration file and must be in the App.confg or Web.confg file.
In Windows Azure, some configuration changes are detected and applied automatically. This includes the
configuration of the Widows Azure diagnostics system in the Diagnostics.wadcfg file, which specifies the
types of information to collect and how to persist the log files. Therefore, it is only necessary to write code
that handles the custom settings you add to the service configuration file. Your code should either:
? Apply the custom settings from an updated configuration to the appropriate components of your application at runtime so that their behavior reflects the new configuration.
? Cancel the change to indicate to Windows Azure that the new value cannot be applied at runtime, and
that the application must be restarted in order for the change to be applied.
For example, the following code from the WebRole.cs class in the Runtime Reconfiguration solution of the
examples you can download for this guide shows how you can use the RoleEnvironment.Changing event to
cancel the update for all settings except the ones that can be applied at runtime without requiring a restart.
This example allows a change to the settings named ¡°CustomSetting¡± to be applied at runtime without restarting the application (the component that uses this setting will be able to read the new value and change
its behavior accordingly at runtime). Any other change to the configuration will automatically cause the web
or worker role to restart.
This approach demonstrates good practice because it ensures that a change to any setting that the
application code is not aware of (and so cannot be sure that it can be applied at runtime) will cause a
restart. If any one of the changes is cancelled, the role will be restarted.
Updates that are not cancelled in the RoleEnvironment.Changing event handler can then be detected and
applied to the application components after the new configuration has been accepted by the Windows Azure
framework. For example, the following code in the Global.asax file of the example solution handles the
RoleEnvironment.Changed event. It examines each configuration setting and, when it finds the setting
named ¡°CustomSetting¡±, calls a function (shown earlier) that applies the new setting to the appropriate
component in the application.
Note that if you fail to cancel a configuration change, but do not apply the new value to your application
component, then the change will not take effect until the next time that the application is restarted. This may
lead to unpredictable behavior, particularly if the hosting role instance is restarted automatically by Windows
Azure as part of its regular maintenance operations¡ªat which point the new setting value will be applied.
The following pattern may also be relevant when implementing this pattern:
? External Configuration Store Pattern. Moving configuration information out of the application deployment package to a centralized location can provide opportunities for easier management and control of configuration data, and sharing configuration data across applications and application instances.
The External Configuration Store pattern explains how you can do this.
##%%&&
Coordinate a set of actions across a distributed set of services and other remote resources, attempt to transparently handle faults if any of these actions fail, or undo the effects of the work performed if the system
cannot recover from a fault. This pattern can add resiliency to a distributed system by enabling it to recover
and retry actions that fail due to transient exceptions, long-lasting faults, and process failures.
An application performs tasks that comprise a number of steps, some of which may invoke remote services
or access remote resources. The individual steps may be independent of each other, but they are orchestrated
by the application logic that implements the task.
Whenever possible, the application should ensure that the task runs to completion and resolve any failures that
might occur when accessing remote services or resources. These failures could occur for a variety of reasons.
For example, the network might be down, communications could be interrupted, a remote service may be unresponsive or in an unstable state, or a remote resource might be temporarily inaccessible¡ªperhaps due to resource constraints. In many cases these failures may be transient and can be handled by using the Retry Pattern.
If the application detects a more permanent fault from which it cannot easily recover, it must be able to restore the system to a consistent state and ensure integrity of the entire end-to-end operation.
The Scheduler Agent Supervisor pattern defines the following actors. These actors orchestrate the steps
(individual items of work) to be performed as part of the task (the overall process):
? The Scheduler arranges for the individual steps that comprise the overall task to be executed and orchestrates their operation. These steps can be combined into a pipeline or workflow, and the Scheduler
is responsible for ensuring that the steps in this workflow are performed in the appropriate order. The
Scheduler maintains information about the state of the workflow as each step is performed (such as
¡°step not yet started,¡± ¡°step running,¡± or ¡°step completed¡±) and records information about this state.
This state information should also include an upper limit of the time allowed for the step to finish (referred to as the Complete By time). If a step requires access to a remote service or resource, the Scheduler invokes the appropriate Agent, passing it the details of the work to be performed. The Scheduler
typically communicates with an Agent by using asynchronous request/response messaging. This can be
implemented by using queues, although other distributed messaging technologies could be used instead.
The Scheduler performs a similar function to the Process Manager in the Process Manager pattern.
The actual workflow is typically defined and implemented by a workflow engine that is controlled
by the Scheduler. This approach decouples the business logic in the workflow from the Scheduler.
? The Agent contains logic that encapsulates a call to a remote service, or access to a remote resource referenced by a step in a task. Each Agent typically wraps calls to a single service or resource, implementing
the appropriate error handling and retry logic (subject to a timeout constraint, described later). If the
steps in the workflow being run by the Scheduler utilize several services and resources across different
steps, each step might reference a different Agent (this is an implementation detail of the pattern).
? The Supervisor monitors the status of the steps in the task being performed by the Scheduler. It runs
periodically (the frequency will be system-specific), examines the status of steps as maintained by the
Scheduler. If it detects any that have timed out or failed, it arranges for the appropriate Agent to recover the step or execute the appropriate remedial action (this may involve modifying the status of a
step). Note that the recovery or remedial actions are typically implemented by the Scheduler and
Agents. The Supervisor should simply request that these actions be performed.
The Scheduler, Agent, and Supervisor are logical components and their physical implementation depends on the
technology being used. For example, several logical agents may be implemented as part of a single web service.
The Scheduler maintains information about the progress of the task and the state of each step in a durable data
store, referred to as the State Store. The Supervisor can use this information to help determine whether a step has
failed. Figure 1 illustrates the relationship between the Scheduler, the Agents, the Supervisor, and the State Store.
This diagram shows a simplified illustration of the pattern. In a real implementation, there may be many
instances of the Scheduler running concurrently, each a subset of tasks. Similarly, the system could run
multiple instances of each Agent, or even multiple Supervisors. In this case, Supervisors must
coordinate their work with each other carefully to ensure that they don¡¯t compete to recover the same
failed steps and tasks. The Leader Election Pattern provides one possible solution to this problem.
When an application wishes to run a task, it submits a request to the Scheduler. The Scheduler records initial
state information about the task and its steps (for example, ¡°step not yet started¡±) in the State Store and then
commences performing the operations defined by the workflow. As the Scheduler starts each step, it updates
the information about the state of that step in the State Store (for example, ¡°step running¡±).
If a step references a remote service or resource, the Scheduler sends a message to the appropriate Agent.
The message may contain the information that the Agent needs to pass to the service or access the resource,
in addition to the Complete By time for the operation. If the Agent completes its operation successfully, it
returns a response to the Scheduler. The Scheduler can then update the state information in the State Store
(for example, ¡°step completed¡±) and perform the next step. This process continues until the entire task is
complete.
An Agent can implement any retry logic that is necessary to perform its work. However, if the Agent does not
complete its work before the Complete By period expires the Scheduler will assume that the operation has
failed. In this case, the Agent should stop its work and not attempt to return anything to the Scheduler (not
even an error message), or attempt any form of recovery. The reason for this restriction is that, after a step
has timed out or failed, another instance of the Agent may be scheduled to run the failing step (this process
is described later).
If the Agent itself fails, the Scheduler will not receive a response. The pattern may not make a distinction
between a step that has timed out and one that has genuinely failed.
If a step times out or fails, the State Store will contain a record that indicates that the step is running (¡°step
running¡±), but the Complete By time will have passed. The Supervisor looks for steps such as this and attempts
to recover them. One possible strategy is for the Supervisor to update the Complete By value to extend the
time available to complete the step, and then send a message to the Scheduler identifying the step that has
timed out . The Scheduler can then attempt to repeat this step. However, such a design requires the tasks to
be idempotent.
It may be necessary for the Supervisor to prevent the same step from being retried if it continually fails or
times out. To achieve this, the Supervisor could maintain a retry count for each step, along with the state
information, in the State Store. If this count exceeds a predefined threshold the Supervisor can adopt a
strategy such as waiting for an extended period before notifying the Scheduler that it should retry the step,
in the expectation that the fault will be resolved during this period. Alternatively, the Supervisor can send a
message to the Scheduler to request the entire task be undone by implementing a Compensating Transaction
(this approach will depend on the Scheduler and Agents providing the information necessary to implement
the compensating operations for each step that completed successfully).
It is not the purpose of the Supervisor to monitor the Scheduler and Agents, and restart them if they
fail. This aspect of the system should be handled by the infrastructure in which these components are
running. Similarly, the Supervisor should not have knowledge of the actual business operations that the
tasks being performed by the Scheduler are running (including how to compensate should these tasks
fail). This is the purpose of the workflow logic implemented by the Scheduler. The sole responsibility of
the Supervisor is to determine whether a step has failed and arrange either for it to be repeated or for
the entire task containing the failed step to be undone.
If the Scheduler is restarted after a failure, or the workflow being performed by the Scheduler terminates
unexpectedly, the Scheduler should be able to determine the status of any in-flight task that it was handling
when it failed, and be prepared to resume this task from the point at which it failed. The implementation details of this process are likely to be system specific. If the task cannot be recovered, it may be necessary to
undo the work already performed by the task. This may also require implementing a Compensating Transaction.
The key advantage of this pattern is that the system is resilient in the event of unexpected temporary or
unrecoverable failures. The system can be constructed to be self-healing. For example, if an Agent or the
Scheduler crashes, a new one can be started and the Supervisor can arrange for a task to be resumed. If the
Supervisor fails, another instance can be started and can take over from where the failure occurred. If the
Supervisor is scheduled to run periodically, a new instance may be automatically started after a predefined
interval. The State Store may be replicated to achieve an even greater degree of resiliency.
You should consider the following points when deciding how to implement this pattern:
? This pattern may be nontrivial to implement and requires thorough testing of each possible failure
mode of the system.
? The recovery/retry logic implemented by the Scheduler may be complex and dependent on state information held in the State Store. It may also be necessary to record the information required to implement a Compensating Transaction in a durable data store.
? The frequency with which the Supervisor runs will be important. It should run frequently enough to
prevent any failed steps from blocking an application for an extended period, but it should not run so
frequently that it becomes an overhead.
? The steps performed by an Agent could be run more than once. The logic that implements these steps
should be idempotent.
Use this pattern when a process that runs in a distributed environment such as the cloud must be resilient to
communications failure and/or operational failure.
This pattern might not be suitable for tasks that do not invoke remote services or access remote resources.
A web application that implements an ecommerce system has been deployed on Windows Azure. Users can
run this application to browse the products available from an organization, and place orders for these products. The user interface runs as a web role, and the order processing elements of the application are implemented as a set of worker roles. Part of the order processing logic involves accessing a remote service, and
this aspect of the system could be prone to transient or more long-lasting faults. For this reason, the designers used the Scheduler Agent Supervisor pattern to implement the order processing elements of the system.
When a customer places an order, the application constructs a message that describes the order and posts
this message to a queue. A separate Submission process, running in a worker role, retrieves this message, inserts the details of the order into the Orders database, and creates a record for the order process in the State
Store. Note that the inserts into the Orders database and the State Store are performed as part of the same
operation. The Submission process is designed to ensure that both inserts complete together.
The state information that the Submission process creates for the order includes:
? OrderID: The ID of the order in the Orders database.
? LockedBy: The instance ID of the worker role handling the order. There may be multiple current instances of the worker role running the Scheduler, but each order should only be handled by a single
instance.
? CompleteBy: The time by which the order should be processed.
? ProcessState: The current state of the task handling the order. The possible states are:
? Pending. The order has been created but processing has not yet been initiated.
? Processing. The order is currently being processed.
? Processed. The order has been processed successfully.
? Error. The order processing has failed.
? FailureCount: The number of times that processing has been attempted for the order.
In this state information, the OrderID field is copied from the order ID of the new order. The LockedBy and
CompleteBy fields are set to null, the ProcessState field is set to Pending, and the FailureCount field is set
to 0.
In this example, the order handling logic is relatively simple and only comprises a single step that
invokes a remote service. In a more complex multi-step scenario, the Submission process would likely
involve several steps, and so several records would be created in the State Store¡ªeach one describing
the state of an individual step.
The Scheduler also runs as part of a worker role and implements the business logic that handles the order. An
instance of the Scheduler polling for new orders examines the State Store for records where the LockedBy
field is null and the ProcessState field is Pending. When the Scheduler finds a new order, it immediately
populates the LockedBy field with its own instance ID, sets the CompleteBy field to an appropriate time,
and sets the ProcessState field to Processing. The code that does this is designed to be exclusive and
atomic to ensure that two concurrent instances of the Scheduler cannot attempt to handle the same order
simultaneously.
The Scheduler then runs the business workflow to process the order asynchronously, passing it the value in
the OrderID field from the State Store. The workflow handling the order retrieves the details of the order
from the Orders database and performs its work. When a step in the order processing workflow needs to
invoke the remote service, it uses an Agent. The workflow step communicates with the Agent by using a pair
of Windows Azure Service Bus message queues acting as a request/response channel. Figure 2 shows a high level view of the solution.
The message sent to the Agent from a workflow step describes the order and includes the CompleteBy time.
If the Agent receives a response from the remote service before the CompleteBy time expires, it constructs
a reply message that it posts on the Service Bus queue on which the workflow is listening. When the workflow step receives the valid reply message, it completes its processing and the Scheduler sets the ProcessState
field of the order state to Processed. At this point, the order processing has completed successfully.
If the CompleteBy time expires before the Agent receives a response from the remote service, the Agent
simply halts its processing and terminates handling the order. Similarly, if the workflow handling the order
exceeds the CompleteBy time, it also terminates. In both of these cases, the state of the order in the State
Store remains set to Processing, but the CompleteBy time indicates that the time for processing the order
has passed and the process is deemed to have failed. Note that if the Agent that is accessing the remote
service, or the workflow that is handling the order (or both) terminate unexpectedly, the information in the
State Store will again remain set to Processing and eventually will have an expired CompleteBy value.
If the Agent detects an unrecoverable non-transient fault while it is attempting to contact the remote service,
it can send an error response back to the workflow. The Scheduler can set the status of the order to Error
and raise an event that alerts an operator. The operator can then attempt to resolve the reason for the failure
manually and resubmit the failed processing step.
The Supervisor periodically examines the State Store looking for orders with an expired CompleteBy value.
If the Supervisor finds such a record, it increments the FailureCount field. If the FailureCount value is below
a specified threshold value, the Supervisor resets the LockedBy field to null, updates the CompleteBy field
with a new expiration time, and sets the ProcessState field to Pending. An instance of the Scheduler can pick
up this order and perform its processing as before. If the FailureCount value exceeds a specified threshold,
the reason for the failure is assumed to be non-transient. The Supervisor sets the status of the order to Error
and raises an event that alerts an operator, as previously described.
In this example, the Supervisor is implemented in a separate worker role. You can utilize a variety of
strategies to arrange for the Supervisor task to be run, including using the Windows Azure Scheduler
service (not to be confused with the Scheduler component in this pattern). For more information about
the Windows Azure Scheduler service, visit the Scheduler page.
Although it is not shown in this example, the Scheduler may need to keep the application that submitted the
order in the first place informed about the progress and status of the order. The application and the Scheduler are isolated from each other to eliminate any dependencies between them. The application has no
knowledge of which instance of the Scheduler is handling the order, and the Scheduler is unaware of which
specific application instance posted the order.
To enable the order status to be reported, the application could use its own private response queue. The details
of this response queue would be included as part of the request sent to the Submission process, which would
include this information in the State Store. The Scheduler would then post messages to this queue indicating
the status of the order (¡°request received,¡± ¡°order completed,¡± ¡°order failed,¡± and so on). It should include the
Order ID in these messages so that they can be correlated with the original request by the application.
The following patterns and guidance may also be relevant when implementing this pattern:
? Retry Pattern. An Agent can use this pattern to transparently retry an operation that accesses a remote service or resource, and that has previously failed, in the expectation that the cause of the failure
is transient and may be corrected.
? Circuit Breaker Pattern. An Agent can use this pattern to handle faults that may take a variable
amount of time to rectify when connecting to a remote service or resource.
? Compensating Transaction Pattern. If the workflow being performed by a Scheduler cannot be completed successfully, it may be necessary to undo any work it has previously performed. The Compensating Transaction pattern describes how this can be achieved for operations that follow the eventual
consistency model. These are the types of operations that are commonly implemented by a Scheduler
that performs complex business processes and workflows.
? Asynchronous Messaging Primer. The components in the Scheduler Agent Supervisor pattern typically run decoupled from each other and communicate asynchronously. The Asynchronous Messaging
primer describes some of the approaches that can be used to implement asynchronous communication
based on message queues.
? Leader Election Pattern. It may be necessary to coordinate the actions of multiple instances of a Supervisor to prevent them from attempting to recover the same failed process. The Leader Election pattern describes how this coordination can be achieved.
##%%&&
Divide a data store into a set of horizontal partitions or shards. This pattern can improve scalability when
storing and accessing large volumes of data.
A data store hosted by a single server may be subject to the following limitations:
? Storage space. A data store for a large-scale cloud application may be expected to contain a huge volume of data that could increase significantly over time. A server typically provides only a finite amount
of disk storage, but it may be possible to replace existing disks with larger ones, or add further disks to
a machine as data volumes grow. However, the system will eventually reach a hard limit whereby it is
not possible to easily increase the storage capacity on a given server.
? Computing resources. A cloud application may be required to support a large number of concurrent
users, each of which run queries that retrieve information from the data store. A single server hosting
the data store may not be able to provide the necessary computing power to support this load, resulting in extended response times for users and frequent failures as applications attempting to store and
retrieve data time out. It may be possible to add memory or upgrade processors, but the system will
reach a limit when it is not possible to increase the compute resources any further.
? Network bandwidth. Ultimately, the performance of a data store running on a single server is governed by the rate at which the server can receive requests and send replies. It is possible that the volume of network traffic might exceed the capacity of the network used to connect to the server, resulting in failed requests.
? Geography. It may be necessary to store data generated by specific users in the same region as those
users for legal, compliance, or performance reasons, or to reduce latency of data access. If the users are
dispersed across different countries or regions, it may not be possible to store the entire data for the
application in a single data store.
Scaling vertically by adding more disk capacity, processing power, memory, and network connections may
postpone the effects of some of these limitations, but it is likely to be only a temporary solution. A commercial cloud application capable of supporting large numbers of users and high volumes of data must be able
to scale almost indefinitely, so vertical scaling is not necessarily the best solution.
Divide the data store into horizontal partitions or shards. Each shard has the same schema, but holds its own
distinct subset of the data. A shard is a data store in its own right (it can contain the data for many entities
of different types), running on a server acting as a storage node.
This pattern offers the following benefits:
? You can scale the system out by adding further shards running on additional storage nodes.
? A system can use off the shelf commodity hardware rather than specialized (and expensive) computers
for each storage node.
? You can reduce contention and improved performance by balancing the workload across shards.
? In the cloud, shards can be located physically close to the users that will access the data.
When dividing a data store up into shards, decide which data should be placed in each shard. A shard typically contains items that fall within a specified range determined by one or more attributes of the data. These
attributes form the shard key (sometimes referred to as the partition key). The shard key should be static. It
should not be based on data that might change.
Sharding physically organizes the data. When an application stores and retrieves data, the sharding logic directs the application to the appropriate shard. This sharding logic may be implemented as part of the data
access code in the application, or it could be implemented by the data storage system if it transparently
supports sharding.
Abstracting the physical location of the data in the sharding logic provides a high level of control over which
shards contain which data, and enables data to migrate between shards without reworking the business logic
of an application should the data in the shards need to be redistributed later (for example, if the shards become unbalanced). The tradeoff is the additional data access overhead required in determining the location
of each data item as it is retrieved.
To ensure optimal performance and scalability, it is important to split the data in a way that is appropriate for
the types of queries the application performs. In many cases, it is unlikely that the sharding scheme will exactly match the requirements of every query. For example, in a multi-tenant system an application may need
to retrieve tenant data by using the tenant ID, but it may also need to look up this data based on some other
attribute such as the tenant¡¯s name or location. To handle these situations, implement a sharding strategy with
a shard key that supports the most commonly performed queries.
If queries regularly retrieve data by using a combination of attribute values, it may be possible to define a
composite shard key by concatenating attributes together. Alternatively, use a pattern such as Index Table to
provide fast lookup to data based on attributes that are not covered by the shard key.
Three strategies are commonly used when selecting the shard key and deciding how to distribute data across
shards. Note that there does not have to be a one-to-one correspondence between shards and the servers
that host them¡ªa single server can host multiple shards. The strategies are:
? The Lookup strategy. In this strategy the sharding logic implements a map that routes a request for
data to the shard that contains that data by using the shard key. In a multi-tenant application all the
data for a tenant might be stored together in a shard by using the tenant ID as the shard key. Multiple
tenants might share the same shard, but the data for a single tenant will not be spread across multiple
shards. Figure 1 shows an example of this strategy.
The mapping between the shard key and the physical storage may be based on physical shards where
each shard key maps to a physical partition. Alternatively, a technique that provides more flexibility
when rebalancing shards is to use a virtual partitioning approach where shard keys map to the same
number of virtual shards, which in turn map to fewer physical partitions. In this approach, an application locates data by using a shard key that refers to a virtual shard, and the system transparently maps
virtual shards to physical partitions. The mapping between a virtual shard and a physical partition can
change without requiring the application code to be modified to use a different set of shard keys.
? The Range strategy. This strategy groups related items together in the same shard, and orders them by
shard key¡ªthe shard keys are sequential. It is useful for applications that frequently retrieve sets of
items by using range queries (queries that return a set of data items for a shard key that falls within a
given range). For example, if an application regularly needs to find all orders placed in a given month,
this data can be retrieved more quickly if all orders for a month are stored in date and time order in the
same shard. If each order was stored in a different shard, they would have to be fetched individually by
performing a large number of point queries (queries that return a single data item). Figure 2 shows an
example of this strategy.
In this example, the shard key is a composite key comprising the order month as the most significant
element, followed by the order day and the time. The data for orders is naturally sorted when new orders are created and appended to a shard. Some data stores support two-part shard keys comprising a
partition key element that identifies the shard and a row key that uniquely identifies an item within the
shard. Data is usually held in row key order within the shard. Items that are subject to range queries
and need to be grouped together can use a shard key that has the same value for the partition key but a
unique value for the row key.
? The Hash strategy. The purpose of this strategy is to reduce the chance of hotspots in the data. It
aims to distribute the data across the shards in a way that achieves a balance between the size of each
shard and the average load that each shard will encounter. The sharding logic computes the shard in
which to store an item based on a hash of one or more attributes of the data. The chosen hashing
function should distribute data evenly across the shards, possibly by introducing some random element
into the computation. Figure 2 shows an example of this strategy.
To understand the advantage of the Hash strategy over other sharding strategies, consider how a multi-tenant
application that enrolls new tenants sequentially might assign the tenants to shards in the data store. When
using the Range strategy, the data for tenants 1 to n will all be stored in shard A, the data for tenants n+1 to
m will all be stored in shard B, and so on. If the most recently registered tenants are also the most active, most
data activity will occur in a small number of shards¡ªwhich could cause hotspots. In contrast, the Hash
strategy allocates tenants to shards based on a hash of their tenant ID. This means that sequential tenants are
most likely to be allocated to different shards, as shown in Figure 3 for tenants 55 and 56, which will distribute the load across these shards.
The following table lists the main advantages and considerations for these three sharding strategies.
Most common sharding schemes implement one of the approaches described above, but you should also
consider the business requirements of your applications and their patterns of data usage. For example, in a
multi-tenant application:
? You can shard data based on workload. You could segregate the data for highly volatile tenants in separate shards. The speed of data access for other tenants may be improved as a result.
? You can shard data based on the location of tenants. It may be possible to take the data for tenants in
a specific geographic region offline for backup and maintenance during off-peak hours in that region,
while the data for tenants in other regions remains online and accessible during their business hours.
? High-value tenants could be assigned their own private high-performing, lightly loaded shards, whereas
lower-value tenants might be expected to share more densely-packed, busy shards.
? The data for tenants that require a high degree of data isolation and privacy could be stored on a completely separate server.
Each of the sharding strategies implies different capabilities and levels of complexity for managing scale in,
scale out, data movement, and maintaining state.
The Lookup strategy permits scaling and data movement operations to be carried out at the user level, either
online or offline. The technique is to suspend some or all user activity (perhaps during off-peak periods), move
the data to the new virtual partition or physical shard, change the mappings, invalidate or refresh any caches
that hold this data, and then allow user activity to resume. Often this type of operation can be centrally
managed. The Lookup strategy requires state to be highly cacheable and replica friendly.
The Range strategy imposes some limitations on scaling and data movement operations, which must typically be carried out when a part or all of the data store is offline because the data must be split and merged
across the shards. Moving the data to rebalance shards may not resolve the problem of uneven load if the
majority of activity is for adjacent shard keys or data identifiers that are within the same range. The Range
strategy may also require some state to be maintained in order to map ranges to the physical partitions.
The Hash strategy makes scaling and data movement operations more complex because the partition keys
are hashes of the shard keys or data identifiers. The new location of each shard must be determined from the
hash function, or the function modified to provide the correct mappings. However, the Hash strategy does
not require maintenance of state.
Consider the following points when deciding how to implement this pattern:
? Sharding is complementary to other forms of partitioning, such as vertical partitioning and functional
partitioning. For example, a single shard may contain entities that have been partitioned vertically, and
a functional partition may be implemented as multiple shards. For more information about partitioning, see the Data Partitioning Guidance.
? Keep shards balanced so that they all handle a similar volume of I/O. As data is inserted and deleted, it
may be necessary to periodically rebalance the shards to guarantee an even distribution and to reduce
the chance of hotspots. Rebalancing can be an expensive operation. To reduce the frequency with
which rebalancing becomes necessary you should plan for growth by ensuring that each shard contains
sufficient free space to handle the expected volume of changes. You should also develop strategies and
scripts that you can use to quickly rebalance shards should this become necessary.
? Use stable data for the shard key. If the shard key changes, the corresponding data item may have to
move between shards, increasing the amount of work performed by update operations. For this reason,
avoid basing the shard key on potentially volatile information. Instead, look for attributes that are invariant or that naturally form a key.
? Ensure that shard keys are unique. For example, avoid using auto-incrementing fields as the shard key.
Is some systems, auto-incremented fields may not be coordinated across shards, possibly resulting in
items in different shards having the same shard key.
Auto-incremented values in fields that do not comprise the shard key can also cause problems. For
example, if you use auto-incremented fields to generate unique IDs, then two different items
located in different shards may be assigned the same ID.
? It may not be possible to design a shard key that matches the requirements of every possible query
against the data. Shard the data to support the most frequently performed queries, and if necessary
create secondary index tables to support queries that retrieve data by using criteria based on attributes
that are not part of the shard key. For more information, see the Index Table Pattern.
? Queries that access only a single shard will be more efficient than those that retrieve data from multiple shards, so avoid implementing a sharding scheme that results in applications performing large
numbers of queries that join data held in different shards. Remember that a single shard can contain
the data for multiple types of entities. Consider denormalizing your data to keep related entities that
are commonly queried together (such as the details of customers and the orders that they have placed)
in the same shard to reduce the number of separate reads that an application performs.
If an entity in one shard references an entity stored in another shard, include the shard key for the
second entity as part of the schema for the first entity. This can help to improve the performance
of queries that reference related data across shards.
? If an application must perform queries that retrieve data from multiple shards, it may be possible to
fetch this data by using parallel tasks. Examples include fan-out queries, where data from multiple
shards is retrieved in parallel and then aggregated into a single result. However, this approach inevitably
adds some complexity to the data access logic of a solution.
? For many applications, creating a larger number of small shards can be more efficient than having a
small number of large shards because they can offer increased opportunities for load balancing. This
approach can also be useful if you anticipate the need to migrate shards from one physical location to
another. Moving a small shard is quicker than moving a large one.
? Make sure that the resources available to each shard storage node are sufficient to handle the scalability requirements in terms of data size and throughput. For more information, see the section ¡°Designing Partitions for Scalability¡± in the Data Partitioning Guidance.
? Consider replicating reference data to all shards. If an operation that retrieves data from a shard also
references static or slow-moving data as part of the same query, add this data to the shard. The application can then fetch all of the data for the query easily, without having to make an additional round
trip to a separate data store.
If reference data held in multiple shards changes, the system must synchronize these changes
across all shards. The system may experience a degree of inconsistency while this synchronization
occurs. If you follow this approach, you should design your applications to be able to handle this
inconsistency.
? It can be difficult to maintain referential integrity and consistency between shards, so you should minimize operations that affect data in multiple shards. If an application must modify data across shards,
evaluate whether complete data consistency is actually a requirement. Instead, a common approach in
the cloud is to implement eventual consistency. The data in each partition is updated separately, and
the application logic must take responsibility for ensuring that the updates all complete successfully, as
well as handling the inconsistencies that can arise from querying data while an eventually consistent
operation is running. For more information about implementing eventual consistency, see the Data
Consistency Primer.
? Configuring and managing a large number of shards can be a challenge. Tasks such as monitoring, backing up, checking for consistency, and logging or auditing must be accomplished on multiple shards and
servers, possibly held in multiple locations. These tasks are likely to be implemented by using scripts or
other automation solutions, but scripting and automation might not be able to completely eliminate
the additional administrative requirements.
? Shards can be geo-located so that the data that they contain is close to the instances of an application
that use it. This approach can considerably improve performance, but requires additional consideration
for tasks that must access multiple shards in different locations.
Use this pattern:
? When a data store is likely to need to scale beyond the limits of the resources available to a single storage node.
? To improve performance by reducing contention in a data store.
The primary focus of sharding is to improve the performance and scalability of a system, but as a
by-product it can also improve availability by virtue of the way in which the data is divided into
separate partitions. A failure in one partition does not necessarily prevent an application from
accessing data held in other partitions, and an operator can perform maintenance or recovery of
one or more partitions without making the entire data for an application inaccessible. For more
information, see the Data Partitioning Guidance.
The following example uses a set of SQL Server databases acting as shards. Each database holds a subset of
the data used by an application. The application retrieves data that is distributed across the shards by using
its own sharding logic (this is an example of a fan-out query). The details of the data that is located in each
shard is returned by a method called GetShards. This method returns an enumerable list of ShardInformation
objects, where the ShardInformation type contains an identifier for each shard and the SQL Server connection string that an application should use to connect to the shard (the connection strings are not shown in
the code example).
The code below shows how the application uses the list of ShardInformation objects to perform a query
that fetches data from each shard in parallel. The details of the query are not shown, but in this example the
data that is retrieved comprises a string which could hold information such as the name of a customer if the
shards contain the details of customers. The results are aggregated into a ConcurrentBag collection for
processing by the application.
The following patterns and guidance may also be relevant when implementing this pattern:
? Data Consistency Primer. It may be necessary to maintain consistency for data distributed across different shards. The Data Consistency Primer summarizes the issues surrounding maintaining consistency
over distributed data, and describes the benefits and tradeoffs of different consistency models.
? Data Partitioning Guidance. Sharding a data store can introduce a range of additional issues. The Data
Partitioning Guidance describes these issues in relation to partitioning data stores in the cloud to improve scalability, reduce contention, and optimize performance.
? Index Table Pattern. Sometimes it is not possible to completely support queries just through the design of the shard key. The Index Table pattern enables an application to quickly retrieve data from a
large data store by specifying a key other than the shard key.
? Materialized View Pattern. To maintain the performance of some query operations, it may be beneficial to create materialized views that aggregate and summarize data, especially if this summary data is
based on information that is distributed across shards. The Materialized View pattern describes how to
generate and populate these views.
##%%&&
Deploy static content to a cloud-based storage service that can deliver these directly to the client. This pattern can reduce the requirement for potentially expensive compute instances.
Web applications typically include some elements of static content. This static content may include HTML
pages and other resources such as images and documents that are available to the client, either as part of an
HTML page (such as inline images, style sheets, and client-side JavaScript files) or as separate downloads (such
as PDF documents).
Although web servers are well tuned to optimize requests through efficient dynamic page code execution
and output caching, they must still handle requests to download static content. This absorbs processing cycles
that could often be put to better use.
In most cloud hosting environments it is possible to minimize the requirement for compute instances (for
example, to use a smaller instance or fewer instances), by locating some of an application¡¯s resources and
static pages in a storage service. The cost for cloud-hosted storage is typically much less than for compute
instances.
When hosting some parts of an application in a storage service, the main considerations are related to deployment of the application and to securing resources that are not intended to be available to anonymous users.
Consider the following points when deciding how to implement this pattern:
? The hosted storage service must expose an HTTP endpoint that users can access to download the static resources. Some storage services also support HTTPS, which means that it is possible to host resources in storage service that require the use of SSL.
? For maximum performance and availability, consider using a content delivery network (where available)
to cache the contents of the storage container in multiple datacenters around the world. However, this
will incur additional cost for the use of the content delivery network.
? Storage accounts are often geo-replicated by default to provide resiliency against events that might
impact a datacenter. This means that the IP address may change, but the URL will remain the same.
? When some content is located in a storage account and other content is in a hosted compute instance
it becomes more challenging to deploy an application and to update it. It may be necessary to perform
separate deployments, and version the application and content in order to manage it more easily¡ªespecially when the static content includes script files or UI components. However, if only static resources are to be updated they can simply be uploaded to the storage account without needing to redeploy
the application package.
? Storage services may not support the use of custom domain names. In this case it is necessary to specify the full URL of the resources in links because they will be in a different domain from the dynamically generated content containing the links.
? The storage containers must be configured for public read access, but it is vital to ensure that they are
not configured for public write access to prevent users being able to upload content. Consider using a
valet key or token to control access to resources that should not be available anonymously¡ªsee Valet
Key Pattern for more information.
This pattern is ideally suited for:
? Minimizing the hosting cost for websites and applications that contain some static resources.
? Minimizing the hosting cost for websites that consist of only static content and resources. Depending
on the capabilities of the hosting provider¡¯s storage system, it might be possible to host a fully static
website in its entirety within a storage account.
? Exposing static resources and content for applications running in other hosting environments or on premises servers.
? Locating content in more than one geographical area by using a content delivery network that caches
the contents of the storage account in multiple datacenters around the world.
? Monitoring costs and bandwidth usage. Using a separate storage account for some or all of the static
content allows the costs to be more easily distinguished from hosting and runtime costs.
This pattern might not be suitable in the following situations:
? The application needs to perform some processing on the static content before delivering it to the client. For example, it may be necessary to add a timestamp to a document.
? The volume of static content is very small. The overhead of retrieving this content from separate storage may outweigh the cost benefit of separating it out from the compute resources.
It is sometimes possible to store a complete website that contains only static content such as
HTML pages, images, style sheets, client-side JavaScript files, and downloadable documents such
as PDF files in a cloud-hosted storage. For more information see An efficient way of deploying a
static web site on Windows Azure on the Infosys blog.
Static content located in Windows Azure blob storage can be accessed directly by a web browser. Windows
Azure provides an HTTP-based interface over storage that can be publicly exposed to clients. For example,
content in a Windows Azure blob storage container is exposed using a URL of the form:
http://[storage-account-name].blob.core.windows.net/[container-name]/[fle-name]
When uploading the content for the application it is necessary to create one or more blob containers to hold
the files and documents. Note that the default permission for a new container is Private, and you must change
this to Public to allow clients to access the contents. If it is necessary to protect the content from anonymous
access, you can implement the Valet Key Pattern so users must present a valid token in order to download
the resources.
The page Blob Service Concepts on the Windows Azure website contains information about blob
storage, and the ways that you can access it and use it.
The links in each page will specify the URL of the resource and the client will access this resource directly
from the storage service. Figure 1 shows this approach.
The links in the pages delivered to the client must specify the full URL of the blob container and resource.
For example, a page that contains a link to an image in a public container might contain the following.
If the resources are protected by using a valet key, such as a Windows Azure Shared Access Signature
(SAS), this signature must be included in the URLs in the links.
The examples available for this guide contain a solution named StaticContentHosting that demonstrates
using external storage for static resources. The StaticContentHosting.Cloud project contains configuration
files that specify the storage account and container that holds the static content.
The Settings class in the file Settings.cs of the StaticContentHosting.Web project contains methods to extract these values and build a string value containing the cloud storage account container URL.
The StaticContentUrlHtmlHelper class in the file StaticContentUrlHtmlHelper.cs exposes a method named
StaticContentUrl that generates a URL containing the path to the cloud storage account if the URL passed
to it starts with the ASP.NET root path character (~).
The file Index.cshtml in the Views\Home folder contains an image element that uses the StaticContentUrl
method to create the URL for its src attribute.
The following pattern may also be relevant when implementing this pattern:
? Valet Key Pattern. If the target resources are not supposed to be available to anonymous users it is
necessary to implement security over the store that holds the static content. The Valet Key pattern
describes how to use a token or key that provides clients with restricted direct access to a specific resource or service such as a cloud-hosted storage service.
##%%&&
Control the consumption of resources used by an instance of an application, an individual tenant, or an entire
service. This pattern can allow the system to continue to function and meet service level agreements, even
when an increase in demand places an extreme load on resources.
The load on a cloud application typically varies over time based on the number of active users or the types
of activities they are performing. For example, more users are likely to be active during business hours, or the
system may be required to perform computationally expensive analytics at the end of each month. There may
also be sudden and unanticipated bursts in activity. If the processing requirements of the system exceed the
capacity of the resources that are available, it will suffer from poor performance and may even fail. The system
may be obliged to meet an agreed level of service, and such failure could be unacceptable.
There are many strategies available for handling varying load in the cloud, depending on the business goals for
the application. One strategy is to use autoscaling to match the provisioned resources to the user needs at
any given time. This has the potential to consistently meet user demand, while optimizing running costs.
However, while autoscaling may trigger the provisioning of additional resources, this provisioning is not instantaneous. If demand grows quickly, there may be a window of time where there is a resource deficit.
An alternative strategy to autoscaling is to allow applications to use resources only up to some soft limit, and
then throttle them when this limit is reached. The system should monitor how it is using resources so that,
when usage exceeds some system-defined threshold, it can throttle requests from one or more users to enable
the system to continue functioning and meet any service level agreements (SLAs) that are in place. For more
information on monitoring resource usage, see the Instrumentation and Telemetry Guidance.
The system could implement several throttling strategies, including:
? Rejecting requests from an individual user who has already accessed system APIs more than n times per
second over a given period of time. This requires that the system meters the use of resources for each
tenant or user running an application. For more information, see the Service Metering Guidance.
? Disabling or degrading the functionality of selected nonessential services so that essential services can
run unimpeded with sufficient resources. For example, if the application is streaming video output, it
could switch to a lower resolution.
? Using load leveling to smooth the volume of activity (this approach is covered in more detail by the
Queue-based Load Leveling Pattern). In a multitenant environment, this approach will reduce the performance for every tenant. If the system must support a mix of tenants with different SLAs, the work
for high-value tenants might be performed immediately. Requests for other tenants can be held back,
and handled when the backlog has eased. The Priority Queue Pattern could be used to help implement
this approach.
? Deferring operations being performed on behalf of lower priority applications or tenants. These operations can be suspended or curtailed, with an exception generated to inform the tenant that the system
is busy and that the operation should be retried later.
Figure 1 shows an area graph for resource utilization (a combination of memory, CPU, bandwidth, and other
factors) against time for applications that are making use of three features. A feature is an area of functionality, such as a component that performs a specific set of tasks, a piece of code that performs a complex calculation, or an element that provides a service such as an in-memory cache. These features are labeled A, B,
and C.
The area immediately below the line for a feature indicates the resources used by applications when
they invoke this feature. For example, the area below the line for Feature A shows the resources used
by applications that are making use of Feature A, and the area between the lines for Feature A and
Feature B indicates the resources by used by applications invoking Feature B. Aggregating the areas for
each feature shows the total resource utilization of the system.
The graph in Figure 1 illustrates the effects of deferring operations. Just prior to time T1, the total resources
allocated to all applications using these features reach a threshold (the soft limit of resource utilization). At
this point, the applications are in danger of exhausting the resources available. In this system, Feature B is less
critical than Feature A or Feature C, so it is temporarily disabled and the resources that it was using are released. Between times T1 and T2, the applications using Feature A and Feature C continue running as normal.
Eventually, the resource use of these two features diminishes to the point when, at time T2, there is sufficient
capacity to enable Feature B again.
The autoscaling and throttling approaches can also be combined to help keep the applications responsive and
within SLAs. If the demand is expected to remain high, throttling may provide a temporary solution while the
system scales out. At this point, the full functionality of the system can be restored.
Figure 2 shows an area graph of the overall resource utilization by all applications running in a system against
time, and illustrates how throttling can be combined with autoscaling.
At time T1, the threshold specifying the soft limit of resource utilization is reached. At this point, the system
can start to scale out. However, if the new resources do not become available sufficiently quickly then the
existing resources may be exhausted and the system could fail. To prevent this from occurring, the system is
temporarily throttled, as described earlier. When autoscaling has completed and the additional resources are
available, throttling can be relaxed.
You should consider the following points when deciding how to implement this pattern:
? Throttling an application, and the strategy to use, is an architectural decision that impacts the entire
design of a system. Throttling should be considered early on in the application design because it is not
easy to add it once a system has been implemented.
? Throttling must be performed quickly. The system must be capable of detecting an increase in activity
and react accordingly. The system must also be able to revert back to its original state quickly after the
load has eased. This requires that the appropriate performance data is continually captured and monitored.
? If a service needs to temporarily deny a user request, it should return a specific error code so that the
client application understands that the reason for the refusal to perform an operation is due to throttling. The client application can wait for a period before retrying the request.
? Throttling can be used as an interim measure while a system autoscales. In some cases it may be better
to simply throttle, rather than to scale, if a burst in activity is sudden and is not expected to be long
lived because scaling can add considerably to running costs.
? If throttling is being used as a temporary measure while a system autoscales, and if resource demands
grow very quickly, the system might not be able to continue functioning¡ªeven when operating in a
throttled mode. If this is not acceptable, consider maintaining larger reserves of capacity and configuring more aggressive autoscaling.
Use this pattern:
? To ensure that a system continues to meet service level agreements.
? To prevent a single tenant from monopolizing the resources provided by an application.
? To handle bursts in activity.
? To help cost-optimize a system by limiting the maximum resource levels needed to keep it functioning.
Figure 3 illustrates how throttling can be implemented in a multi-tenant system. Users from each of the tenant
organizations access a cloud-hosted application where they fill out and submit surveys. The application contains instrumentation that monitors the rate at which these users are submitting requests to the application.
In order to prevent the users from one tenant affecting the responsiveness and availability of the application
for all other users, a limit is applied to the number of requests per second that the users from any one tenant
can submit. The application blocks requests that exceed this limit.
The following patterns and guidance may also be relevant when implementing this pattern:
? Instrumentation and Telemetry Guidance. Throttling depends on gathering information on how
heavily a service is being used. The Instrumentation and Telemetry Guidance describes how to generate and capture custom monitoring information.
? Service Metering Guidance. This guidance describes how to meter the use of services in order to gain
an understanding of how they are used. This information can be useful in determining how to throttle
a service.
? Autoscaling Guidance. Throttling can be used as an interim measure while a system autoscales, or to
remove the need for a system to autoscale. The Autoscaling Guidance contains more information on
autoscaling strategies.
? Queue-based Load Leveling Pattern. Queue-based load leveling is a commonly used mechanism for
implementing throttling. A queue can act as a buffer that helps to even out the rate at which requests
sent by an application are delivered to a service.
? Priority Queue Pattern. A system can use priority queuing as part of its throttling strategy to maintain performance for critical or higher value applications, while reducing the performance of less important applications.
##%%&&
Use a token or key that provides clients with restricted direct access to a specific resource or service in order
to offload data transfer operations from the application code. This pattern is particularly useful in applications that use cloud-hosted storage systems or queues, and can minimize cost and maximize scalability and
performance.
Client programs and web browsers often need to read and write files or data streams to and from an application¡¯s storage. Typically, the application will handle the movement of the data¡ªeither by fetching it from
storage and streaming it to the client, or by reading the uploaded stream from the client and storing it in the
data store. However, this approach absorbs valuable resources such as compute, memory, and bandwidth.
Data stores have the capability to handle upload and download of data directly, without requiring the application to perform any processing to move this data, but this typically requires the client to have access to the
security credentials for the store. While this can be a useful technique to minimize data transfer costs and the
requirement to scale out the application, and to maximize performance, it means that the application is no
longer able to manage the security of the data. Once the client has a connection to the data store for direct
access, the application cannot act as the gatekeeper. It is no longer in control of the process and cannot
prevent subsequent uploads or downloads from the data store.
This is not a realistic approach in modern distributed systems that may need to serve untrusted clients. Instead, applications must be able to securely control access to data in a granular way, but still reduce the load
on the server by setting up this connection and then allowing the client to communicate directly with the
data store to perform the required read or write operations.
To resolve the problem of controlling access to a data store where the store itself cannot manage authentication and authorization of clients, one typical solution is to restrict access to the data store¡¯s public connection
and provide the client with a key or token that the data store itself can validate.
This key or token is usually referred to as a valet key. It provides time-limited access to specific resources and
allows only predefined operations such as reading and writing to storage or queues, or uploading and downloading in a web browser. Applications can create and issue valet keys to client devices and web browsers
quickly and easily, allowing clients to perform the required operations without requiring the application to
directly handle the data transfer. This removes the processing overhead, and the consequent impact on performance and scalability, from the application and the server.
The client uses this token to access a specific resource in the data store for only a specific period, and with
specific restrictions on access permissions, as shown in Figure 1. After the specified period, the key becomes
invalid and will not allow subsequent access to the resource.
It is also possible to configure a key that has other dependencies, such as the scope of the location of the
data. For example, depending on the data store capabilities, the key may specify a complete table in a data
store, or only specific rows in a table. In cloud storage systems the key may specify a container, or just a
specific item within a container.
The key can also be invalidated by the application. This is a useful approach if the client notifies the server
that the data transfer operation is complete. The server can then invalidate that key to prevent its use for any
subsequent access to the data store.
Using this pattern can simplify managing access to resources because there is no requirement to create and
authenticate a user, grant permissions, and then remove the user again. It also makes it easy to constrain the
location, the permission, and the validity period¡ªall by simply generating a suitable key at runtime. The important factors are to limit the validity period, and especially the location of the resource, as tightly as possible so that the recipient can use it for only the intended purpose.
Consider the following points when deciding how to implement this pattern:
? Manage the validity status and period of the key. The key is a bearer instrument that, if leaked or
compromised, effectively unlocks the target item and makes it available for malicious use during the
validity period. A key can usually be revoked or disabled, depending on how it was issued. Server-side
policies can be changed or, in the ultimate case, the server key it was signed with can be invalidated.
Specify a short validity period to minimize the risk of allowing subsequent unwarranted operations to
take place against the data store. However, if the validity period is too short, the client may not be able
to complete the operation before the key expires. Allow authorized users to renew the key before the
validity period expires if multiple accesses to the protected resource are required.
? Control the level of access the key will provide. Typically, the key should allow the user to perform
only the actions necessary to complete the operation, such as read-only access if the client should not
be able to upload data to the data store. For file uploads, it is common to specify a key that provides
write-only permission, as well as the location and the validity period. It is vital to accurately specify the
resource or the set of resources to which the key applies.
? Consider how to control users¡¯ behavior. Implementing this pattern means some loss of control over
the resources to which users are granted access. The level of control that can be exerted is limited by
the capabilities of the policies and permissions available for the service or the target data store. For
example, it is usually not possible to create a key that limits the size of the data to be written to storage, or the number of times the key can be used to access a file. This can result in huge unexpected
costs for data transfer, even when used by the intended client, and might be caused by an error in the
code that causes repeated upload or download. To limit the number of times a file can be uploaded or
downloaded it may be necessary, where possible, to force the client to notify the application when
one operation has completed. For example, some data stores raise events the application code can use
to monitor operations and control user behavior. However, it may be hard to enforce quotas for individual users in a multi-tenant scenario where the same key is used by all the users from one tenant.
? Validate, and optionally sanitize, all uploaded data. A malicious user that gains access to the key
could upload data aimed at further compromising the system. Alternatively, authorized users might up load data that is invalid and, when processed, could result in an error or system failure. To protect
against this, ensure that all uploaded data is validated and checked for malicious content before use.
? Audit all operations. Many key-based mechanisms can log operations such as uploads, downloads, and
failures. These logs can usually be incorporated into an audit process, and also used for billing if the
user is charged based on file size or data volume. Use the logs to detect authentication failures that
might be caused by issues with the key provider, or inadvertent removal of a stored access policy.
? Deliver the key securely. It may be embedded in a URL that the user activates in a web page, or it may
be used in a server redirection operation so that the download occurs automatically. Always use
HTTPS to deliver the key over a secure channel.
? Protect sensitive data in transit. Sensitive data delivered through the application will usually take
place using SSL or TLS, and this should be enforced for clients accessing the data store directly.
Other issues to be aware of when implementing this pattern are:
? If the client does not, or cannot notify the server of completion of the operation, and the only limit is
the expiry period of the key, the application will not be able to perform auditing operations such as
counting the number of uploads or downloads, or preventing multiple uploads or downloads.
? The flexibility of key policies that can be generated may be limited. For example, some mechanisms
may allow only the use of a timed expiry period. Others may not be able to specify a sufficient granularity of read/write permissions.
? If the start time for the key or token validity period is specified, ensure that it is a little earlier than the
current server time to allow for client clocks that might be slightly out of synchronization. The default
if not specified is usually the current server time.
? The URL containing the key will be recorded in server log files. While the key will typically have expired before the log files are used for analysis, ensure that you limit access to them. If log data is transmitted to a monitoring system or stored in another location, consider implementing a delay to prevent
leakage of keys until after their validity period has expired.
? If the client code runs in a web browser, the browser may need to support cross-origin resource sharing (CORS) to enable code that executes within the web browser to access data in a different domain
from the originating domain that served the page. Some older browsers and some data stores do not
support CORS, and code that runs in these browsers may not be able to use a valet key to provide access to data in a different domain, such as a cloud storage account.
This pattern is ideally suited for the following situations:
? To minimize resource loading and maximize performance and scalability. Using a valet key does not require the resource to be locked, no remote server call is required, there is no limit on the number of
valet keys that can be issued, and it avoids a single point of failure that would arise from performing
the data transfer through the application code. Creating a valet key is typically a simple cryptographic
operation of signing a string with a key.
? To minimize operational cost. Enabling direct access to stores and queues is resource and cost efficient, can result in fewer network round trips, and may allow for a reduction in the number of compute resources required.
? When clients regularly upload or download data, particularly where there is a large volume or when
each operation involves large files.
? When the application has limited compute resources available, either due to hosting limitations or cost
considerations. In this scenario, the pattern is even more advantageous if there are many concurrent
data uploads or downloads because it relieves the application from handling the data transfer.
? When the data is stored in a remote data store or a different datacenter. If the application was required to act as a gatekeeper, there may be a charge for the additional bandwidth of transferring the
data between datacenters, or across public or private networks between the client and the application,
and then between the application and the data store.
This pattern might not be suitable in the following situations:
? If the application must perform some task on the data before it is stored or before it is sent to the client. For example, the application may need to perform validation, log access success, or execute a
transformation on the data. However, some data stores and clients are able to negotiate and carry out
simple transformations such as compression and decompression (for example, a web browser can usually handle GZip formats).
? If the design and implementation of an existing application makes it difficult and costly to implement.
Using this pattern typically requires a different architectural approach for delivering and receiving data.
? If it is necessary to maintain audit trails or control the number of times a data transfer operation is executed, and the valet key mechanism in use does not support notifications that the server can use to
manage these operations.
? If it is necessary to limit the size of the data, especially during upload operations. The only solution to
this is for the application to check the data size after the operation is complete, or check the size of
uploads after a specified period or on a scheduled basis.
Windows Azure supports Shared Access Signatures (SAS) on Windows Azure storage for granular access
control to data in blobs, tables, and queues, and for Service Bus queues and topics. An SAS token can be
configured to provide specific access rights such as read, write, update, and delete to a specific table; a key
range within a table; a queue; a blob; or a blob container. The validity can be a specified time period or with
no time limit.
Windows Azure SAS also supports server-stored access policies that can be associated with a specific resource such as a table or blob. This feature provides additional control and flexibility compared to application generated SAS tokens, and should be used whenever possible. Settings defined in a server-stored policy can
be changed and are reflected in the token without requiring a new token to be issued, but settings defined
in the token itself cannot be changed without issuing a new token. This approach also makes it possible to
revoke a valid SAS token before it has expired.
For more information see Introducing Table SAS (Shared Access Signature), Queue SAS and update to Blob
SAS in the Windows Azure Storage Team blog and Shared Access Signatures, Part 1: Understanding the
SAS Model on MSDN.
The following code demonstrates how to create a SAS that is valid for five minutes. The GetSharedAccess ReferenceForUpload method returns a SAS that can be used to upload a file to Windows Azure Blob Storage.
The complete sample containing this code is available in the ValetKey solution available for download
with this guidance. The ValetKey.Web project in this solution contains a web application that includes
the ValuesController class shown above. A sample client application that uses this web application to
retrieve a SAS key and upload a file to blob storage is available in the ValetKey.Client project.
The following patterns and guidance may also be relevant when implementing this pattern:
? Gatekeeper Pattern. This pattern can be used in conjunction with the Valet Key pattern to protect
applications and services by using a dedicated host instance that acts as a broker between clients and
the application or service. The gatekeeper validates and sanitizes requests, and passes requests and
data between the client and the application. This pattern can provide an additional layer of security,
and reduce the attack surface of the system.
? Static Content Hosting Pattern. This pattern describes how to deploy static resources to a cloud based storage service that can deliver these resources directly to the client in order to reduce the requirement for expensive compute instances. Where the resources are not intended to be publicly available, the Valet Key pattern can be used to secure them.
##%%&&
The presentation-tier request handling mechanism receives many different types of
requests, which require varied types of processing. Some requests are simply
forwarded to the appropriate handler component, while other requests must be
modified, audited, or uncompressed before being further processed.
Preprocessing and post-processing of a client Web request and response are
required.
When a request enters a Web application, it often must pass several entrance tests
prior to the main processing stage. For example,
? Has the client been authenticated?
? Does the client have a valid session?
? Is the client's IP address from a trusted network?
? Does the request path violate any constraints?
? What encoding does the client use to send the data?
? Do we support the browser type of the client?
Some of these checks are tests, resulting in a yes or no answer that determines
whether processing will continue. Other checks manipulate the incoming data
stream into a form suitable for processing.
The classic solution consists of a series of conditional checks, with any failed check
aborting the request. Nested if/else statements are a standard strategy, but this
solution leads to code fragility and a copy-and-paste style of programming, because
the flow of the filtering and the action of the filters is compiled into the application.
The key to solving this problem in a flexible and unobtrusive manner is to have a
simple mechanism for adding and removing processing components, in which each
component completes a specific filtering action.
? Common processing, such as checking the data-encoding scheme or logging
information about each request, completes per request.
? Centralization of common logic is desired.
? Services should be easy to add or remove unobtrusively without affecting
existing components, so that they can be used in a variety of combinations,
such as
o Logging and authentication
o Debugging and transformation of output for a specific client
o Uncompressing and converting encoding scheme of input
Create pluggable filters to process common services in a standard manner
without requiring changes to core request processing code. The filters
intercept incoming requests and outgoing responses, allowing
preprocessing and post-processing. We are able to add and remove these
filters unobtrusively, without requiring changes to our existing code.
We are able, in effect, to decorate our main processing with a variety of common
services, such as security, logging, debugging, and so forth. These filters are
components that are independent of the main application code, and they may be
added or removed declaratively. For example, a deployment configuration file may
be modified to set up a chain of filters. The same configuration file might include a
mapping of specific URLs to this filter chain. When a client requests a resource that
matches this configured URL mapping, the filters in the chain are each processed in
order before the requested target resource is invoked.
FilterManager
The FilterManager manages filter processing. It creates the FilterChain with the
appropriate filters, in the correct order, and initiates processing.
FilterChain
The FilterChain is an ordered collection of independent filters.
FilterOne, FilterTwo, FilterThree
These are the individual filters that are mapped to a target. The FilterChain
coordinates their processing.
Target
The Target is the resource requested by the client.
Custom Filter Strategy
Filter is implemented via a custom strategy defined by the developer. This is less
flexible and less powerful than the preferred Standard Filter Strategy, which is
presented in the next section and is only available in containers supporting the 2.3
servlet specification. The Custom Filter Strategy is less powerful because it cannot
provide for the wrapping of request and response objects in a standard and portable
way. Additionally, the request object cannot be modified, and some sort of buffering
mechanism must be introduced if filters are to control the output stream. To
implement the Custom Filter Strategy, the developer could use the Decorator
pattern [GoF] to wrap filters around the core request processing logic. For example,
there may be a debugging filter that wraps an authentication filter. Example 7.1 and
Example 7.2 show how this mechanism might be created programmatically:
In the servlet controller, we delegate to a method called process-Request to
handle incoming requests, as shown in Example 7.3.
For example purposes only, imagine that each processing component writes to
standard output when it is executed. Example 7.4 shows the possible execution
output.
A chain of processors is executed in order. Each processor, except for the last one in
the chain, is considered a filter. The final processor component is where we
encapsulate the core processing we want to complete for each request. Given this
design, we will need to change the code in the CoreProcessor class, as well as in any
filter classes, when we want to modify how we handle requests.
Figure 7.3 is a sequence diagram describing the flow of control when using the filter
code of Example 7.1, Example 7.2, and Example 7.3.
Notice that when we use a decorator implementation, each filter invokes on the next
filter directly, though using a generic interface. Alternatively, this strategy can be
implemented using a FilterManager and FilterChain. In this case, these two
components coordinate and manage filter processing and the individual filters do
not communicate with one another directly. This design approximates that of a
servlet 2.3-compliant implementation, though it is still a custom strategy. Example
7.5 is the listing of just such a FilterManager class that creates a FilterChain, which
is shown in Example 7.6. The FilterChain adds filters to the chain in the appropriate
order (for the sake of brevity, this is done in the FilterChain constructor, but would
normally be done in place of the comment), processes the filters, and finally
processes the target resource. Figure 7.4 is a sequence diagram for this code.
This strategy does not allow us to create filters that are as flexible or as powerful as
we would like. For one, filters are added and removed programmatically. While we
could write a proprietary mechanism for handling adding and removing filters via a
configuration file, we still would have no way of wrapping the request and response
objects. Additionally, without a sophisticated buffering mechanism, this strategy
does not provide flexible postprocessing.
The Standard Filter Strategy provides solutions to these issues, leveraging features
of the 2.3 Servlet specification, which has provided a standard solution to the filter
dilemma.
Note
As of this writing, the Servlet 2.3 specification is in final draft form.
Standard Filter Strategy
Filters are controlled declaratively using a deployment descriptor, as described in
the servlet specification version 2.3, which, as of this writing, is in final draft form.
The servlet 2.3 specification includes a standard mechanism for building filter chains
and unobtrusively adding and removing filters from those chains. Filters are built
around interfaces, and added or removed in a declarative manner by modifying the
deployment descriptor for a Web application.
Our example for this strategy will be to create a filter that preprocesses requests of
any encoding type such that each request may be handled similarly in our core
request handling code. Why might this be necessary? HTML forms that include a file
upload use a different encoding type than that of most forms. Thus, form data that
accompanies the upload is not available via simple getParameter() invocations. So,
we create two filters that preprocess requests, translating all encoding types into a
single consistent format. The format we choose is to have all form data available as
request attributes.
One filter handles the standard form encoding of type
application/x-www-form-urlencoded and the other handles the less common
encoding type multipart/form-data, which is used for forms that include file
uploads. The filters translate all form data into request attributes, so the core
request handling mechanism can work with every request in the same manner,
instead of with special casing for different encodings.
Example 7.8 shows a filter that translates requests using the common application
form encoding scheme. Example 7.9 shows the filter that handles the translation of
requests that use the multipart form encoding scheme. The code for these filters is
based on the final draft of the servlet specification, version 2.3. A base filter is used
as well, from which both of these filters inherit (see the section ¡°Base Filter
Strategy¡±). The base filter, shown in Example 7.7, provides default behavior for the
standard filter callback methods.
The following excerpt in Example 7.10 is from the deployment descriptor for the
Web application containing this example. It shows how these two filters are
registered and then mapped to a resource, in this case a simple test servlet.
Additionally, the sequence diagram for this example is shown in Figure 7.5.
The StandardEncodeFilter and the MultiPartEncodeFilter intercept control when a
client makes a request to the controller servlet. The container fulfills the role of filter
manager and vectors control to these filters by invoking their doFilter methods.
After completing its processing, each filter passes control to its containing
FilterChain, which it instructs to execute the next filter. Once both of the filters have
received and subsequently relinquished control, the next component to receive
control is the actual target resource, in this case the controller servlet.
Filters, as supported in version 2.3 of the servlet specification, also support
wrapping the request and response objects. This feature provides for a much more
powerful mechanism than can be built using the custom implementation suggested
by the Custom Filter Strategy. Of course, a hybrid approach combining the two
strategies could be custom built as well, but would still lack the power of the
Standard Filter Strategy as supported by the servlet specification.
Base Filter Strategy
A base filter serves as a common superclass for all filters. Common features can be
encapsulated in the base filter and shared among all filters. For example, a base
filter is a good place to include default behavior for the container callback methods
in the Declared Filter Strategy. Example 7.11 shows how this can be done.
Template Filter Strategy
Using a base filter from which all others inherit (see ¡°Base Filter Strategy¡± in this
chapter) allows the base class to provide template method [Gof] functionality. In
this case, the base filter is used to dictate the general steps that every filter must
complete, while leaving the specifics of how to complete that step to each filter
subclass. Typically, these would be coarsely defined, basic methods that simply
impose a limited structure on each template. This strategy can be combined with
any other filter strategy, as well. The listings in Example 7.12 and Example 7.13
show how to use this strategy with the Declared Filter Strategy.
Example 7.12 shows a base filter called TemplateFilter, as follows.
Given this class definition for TemplateFilter, each filter is implemented as a
subclass that must only implement the doMainProcessing method. These
subclasses have the option, though, of implementing all three methods if they
desire. Example 7.13 is an example of a filter subclass that implements the one
mandatory method (dictated by our template filter) and the optional preprocessing
method. Additionally, a sequence diagram for using this strategy is shown in Figure
7.6.
Strategy sequence diagram
In the sequence diagram in Figure 7.6, filter subclasses, such as DebuggingFilter,
define specific processing by overriding the abstract doMainProcessing method
and, optionally, doPreProcessing and doPostProcessing. Thus, the template filter
imposes a structure to each filter's processing, as well as providing a place for
encapsulating code that is common to every filter.
? Centralizes Control with Loosely Coupled Handlers
Filters provide a central place for handling processing across multiple
requests, as does a controller. Filters are better suited to massaging
requests and responses for ultimate handling by a target resource, such as a
controller. Additionally, a controller often ties together the management of
numerous unrelated common services, such as authentication, logging,
encryption, and so forth, while filtering allows for much more loosely coupled
handlers, which can be combined in various combinations.
? Improves Reusability
Filters promote cleaner application partitioning and encourages reuse. These
pluggable interceptors are transparently added or removed from existing
code, and due to their standard interface, they work in any combination and
are reusable for varying presentations.
? Declarative and Flexible Configuration
Numerous services are combined in varying permutations without a single
recompile of the core code base.
? Information Sharing is Inefficient
Sharing information between filters can be inefficient, since by definition
each filter is loosely coupled. If large amounts of information must be shared
between filters, then this approach may prove to be costly.
? Front Controller
The controller solves some similar problems, but is better suited to handling
core processing.
? Decorator [GoF]
The Intercepting Filter pattern is related to the Decorator pattern, which
provides for dynamically pluggable wrappers.
? Template Method [GoF]
The Template Method pattern is used to implement the Template Filter
Strategy.
? Interceptor [POSA2]
The Intercepting Filter pattern is related to the Interceptor pattern, which
allows services to be added transparently and triggered automatically.
? Pipes and Filters [POSA1]
The Intercepting Filter pattern is related to the Pipes and Filters pattern.
##%%&&
The presentation-tier request handling mechanism must control and coordinate
processing of each user across multiple requests. Such control mechanisms may be
managed in either a centralized or decentralized manner.
The system requires a centralized access point for presentation-tier request
handling to support the integration of system services, content retrieval, view
management, and navigation. When the user accesses the view directly without
going through a centralized mechanism, two problems may occur:
? Each view is required to provide its own system services, often resulting in
duplicate code.
? View navigation is left to the views. This may result in commingled view
content and view navigation.
Additionally, distributed control is more difficult to maintain, since changes will
often need to be made in numerous places.
? Common system services processing completes per request. For example,
the security service completes authentication and authorization checks.
? Logic that is best handled in one central location is instead replicated within
numerous views.
? Decision points exist with respect to the retrieval and manipulation of data.
? Multiple views are used to respond to similar business requests.
? A centralized point of contact for handling a request may be useful, for
example, to control and log a user's progress through the site.
? System services and view management logic are relatively sophisticated.
Use a controller as the initial point of contact for handling a request. The
controller manages the handling of the request, including invoking
security services such as authentication and authorization, delegating
business processing, managing the choice of an appropriate view, handling
errors, and managing the selection of content creation strategies.
The controller provides a centralized entry point that controls and manages Web
request handling. By centralizing decision points and controls, the controller also
helps reduce the amount of Java code, called scriptlets, embedded in the JSP.
Centralizing control in the controller and reducing business logic in the view
promotes code reuse across requests. It is a preferable approach to the
alternative¡ªembedding code in multiple views¡ªbecause that approach may lead to
a more error-prone, reuse-by-copy- and-paste environment.
Typically, a controller coordinates with a dispatcher component. Dispatchers are
responsible for view management and navigation. Thus, a dispatcher chooses the
next view for the user and vectors control to the resource. Dispatchers may be
encapsulated within the controller directly or can be extracted into a separate
component.
While the Front Controller pattern suggests centralizing the handling of all requests,
it does not limit the number of handlers in the system, as does a Singleton. An
application may use multiple controllers in a system, each mapping to a set of
distinct services.
Figure 7.7 represents the Front Controller class diagram pattern.
Figure 7.8 shows the sequence diagram representing the Front Controller pattern. It
depicts how the controller handles a request.
The controller is the initial contact point for handling all requests in the system. The
controller may delegate to a helper to complete authentication and authorization of
a user or to initiate contact retrieval.
A dispatcher is responsible for view management and navigation, managing the
choice of the next view to present to the user, and providing the mechanism for
vectoring control to this resource.
A dispatcher can be encapsulated within a controller or can be a separate
component working in coordination. The dispatcher provides either a static
dispatching to the view or a more sophisticated dynamic dispatching mechanism.
The dispatcher uses the RequestDispatcher object (supported in the servlet
specification) and encapsulates some additional processing.
A helper is responsible for helping a view or controller complete its processing. Thus,
helpers have numerous responsibilities, including gathering data required by the
view and storing this intermediate model, in which case the helper is sometimes
referred to as a value bean. Additionally, helpers may adapt this data model for use
by the view. Helpers can service requests for data from the view by simply providing
access to the raw data or by formatting the data as Web content.
A view may work with any number of helpers, which are typically implemented as
JavaBeans (JSP 1.0+) and custom tags (JSP 1.1+). Additionally, a helper may
represent a Command object, a delegate (see ¡°Business Delegate¡±), or an XSL
Transformer, which is used in combination with a stylesheet to adapt and convert
the model into the appropriate form.
A view represents and displays information to the client. The view retrieves
information from a model. Helpers support views by encapsulating and adapting the
underlying data model for use in the display.
There are several strategies for implementing a controller.
This strategy suggests implementing the controller as a servlet. Though
semantically equivalent, it is preferred to the JSP Front Strategy. The controller
manages the aspects of request handling that are related to business processing
and control flow. These responsibilities are related to, but logically independent of,
display formatting, and are more appropriately encapsulated in a servlet rather than
in a JSP.
The Servlet Front Strategy does have some potential drawbacks. In particular, it
does not leverage some of the JSP runtime environment utilities, such as automatic
population of request parameters into helper properties. Fortunately, this drawback
is minimal because it is relatively easy to create or obtain similar utilities for general
use. There is also the possibility that the functionality of some of the JSP utilities
may be included as standard servlet features in a future version of the servlet
specification. Example 7.14 is an example of the Servlet Front Strategy.
This strategy suggests implementing the controller as a JSP. Though semantically
equivalent, the Servlet Front Strategy is preferred to the JSP Front Strategy. Since
the controller handles processing that is not specifically related to display formatting,
it is a mismatch to implement this component as a JSP.
Implementing the controller as a JSP is clearly not preferred for another reason: It
requires a software developer to work with a page of markup in order to modify
request handling logic. Thus, a software developer will typically find the JSP Front
Strategy more cumbersome when completing the cycle of coding, compilation,
testing, and debugging. Example 7.15 is an example of the JSP Front Strategy.
Based on the Command pattern [GoF], the Command and Controller Strategy
suggests providing a generic interface to the helper components to which the
controller may delegate responsibility, minimizing the coupling among these
components (see ¡°View Helper¡± for more information on helper components).
Adding to or changing the work that needs to be completed by these helpers does
not require any changes to the interface between the controller and the helpers, but
rather to the type and/or content of the commands. This provides a flexible and
easily extensible mechanism for developers to add request handling behaviors.
Finally, because the command processing is not coupled to the command invocation,
the command processing mechanism may be reused with various types of clients,
not just with Web browsers. This strategy also facilitates the creation of composite
commands (see Composite pattern [GoF]). See Example 7.16 for sample code and
Figure 7.9 for a sequence diagram.
All requests are made to specific physical resource names rather than logical names.
An example is the following URL: http://some.server.com/resource1.jsp. In
the case of a controller, an example URL might be
http://some.server.com/servlet/Controller. The Logical Resource Mapping
Strategy is typically preferred over this strategy because it provides much greater
flexibility. The Logical Resource Mapping Strategy lets you modify resource
mappings in a declarative manner, via a configuration file. This is much more
flexible than the Physical Resource Mapping Strategy, which requires that you make
changes to each resource, as is necessary when implementing this strategy.
Requests are made to logical resource names rather than to specific physical names.
The physical resources to which these logical names refer may then be modified in
a declarative manner.
For example, the URL http://some.server.com/process may be mapped as
follows:
This is actually a substrategy of Logical Resource Naming Strategy. This strategy
maps not just a single logical name, but an entire set of logical names, to a single
physical resource. For example, a wildcard mapping might map all requests that end
with .ctrl to a specific handler.
A request and mapping might look as shown in Table 7-1
In fact, this is the strategy JSP engines use in order to ensure that requests for JSP
resources (that is, resources whose names end in .jsp) are processed by a specific
handler.
Additional information can also be added to a request, providing further details to
leverage for this logical mapping. See Table 7-2.
A key benefit of using this strategy is that it provides great flexibility when designing
your request handling components. When combined with other strategies, such as
the Command and Controller Strategy, you can create a powerful request handling
framework.
Consider a controller that handles all requests ending in .ctrl, as described above.
Also, consider the left side of this dot-delimited resource name (profile in the
above example) to be one part of the name of a use case. Now combine this name
with the query parameter value (create in the above example). We are signaling
our request handler that we want to process a use case called create profile. Our
multiplexed resource mapping sends the request to our servletController, which is
part of the mapping shown in Table 7-2 . Our controller creates the appropriate
command object, as described in the Command and Controller Strategy. How does
the controller know the command object to which it should delegate? Leveraging the
additional information in the request URI, the controller delegates to the command
object that handles profile creation. This might be a ProfileCommand object that
services requests for Profile creation and modification, or it might be a more specific
ProfileCreationCommand object.
When the dispatcher functionality is minimal, it can be folded into the controller, as
shown in Figure 7.10.
Used in combination with the Servlet Front Strategy, this strategy suggests
implementing a controller base class, whose implementation other controllers may
extend. The base front may contain common and default implementations, while
each subclass can override these implementations. The drawback of this strategy is
the fact that any shared superclass, while promoting reuse and sharing, raises the
issue of creating a fragile hierarchy, where changes necessary for one subclass
affect all subclasses.
Filters provide similar support for centralizing request processing control (see
Intercepting Filter pattern). Thus, some aspects of a controller can reasonably be
implemented as a filter. At the same time, filters primarily focus on request
interception and decoration, not request processing and response generation. While
there are overlapping responsibilities, such as managing logging or debugging, each
component complements the other when used appropriately.
? Centralizes Control
A controller provides a central place to handle system services and business
logic across multiple requests. A controller manages business logic
processing and request handling. Centralized access to an application means
that requests are easily tracked and logged. Keep in mind, though, that as
control centralizes, it is possible to introduce a single point of failure. In
practice, this rarely is a problem, though, since multiple controllers typically
exist, either within a single server or in a cluster.
? Improves Manageability of Security
A controller centralizes control, providing a choke point for illicit access
attempts into the Web application. In addition, auditing a single entrance
into the application requires fewer resources than distributing security
checks across all pages.
? Improves Reusability
A controller promotes cleaner application partitioning and encourages reuse,
as code that is common among components moves into a controller or is
managed by a controller.
? View Helper
The Front Controller pattern, in conjunction with the View Helper pattern,
describes factoring business logic out of the view and providing a central
point of control and dispatch. Flow logic is factored forward into the
controller and data handling code moves back into the helpers.
? Intercepting Filter
Both Intercepting Filter and Front Controller describe ways to centralize
control of certain types of request processing, suggesting different
approaches to this issue.
? Dispatcher View and Service to Worker
The Dispatcher View and Service to Worker patterns are another way to
name the combination of the View Helper pattern with a dispatcher, and
Front Controller pattern. Dispatcher View and Service to Worker, while
structurally the same, describe different divisions of labor among
components.
##%%&&
The system creates presentation content, which requires processing of dynamic
business data.
Presentation tier changes occur often and are difficult to develop and maintain when
business data access logic and presentation formatting logic are interwoven. This
makes the system less flexible, less reusable, and generally less resilient to change.
Intermingling the business and systems logic with the view processing reduces
modularity and also provides a poor separation of roles among Web production and
software development teams.
? Business data assimilation requirements are nontrivial.
? Embedding business logic in the view promotes a copy-and-paste type of
reuse. This causes maintenance problems and bugs because a piece of logic
is reused in the same or different view by simply duplicating it in the new
location.
? It is desirable to promote a clean separation of labor by having different
individuals fulfill the roles of software developer and Web production team
member.
? One view is commonly used to respond to a particular business request.
A view contains formatting code, delegating its processing responsibilities
to its helper classes, implemented as JavaBeans or custom tags. Helpers
also store the view's intermediate data model and serve as business data
adapters.
There are multiple strategies for implementing the view component. The JSP View
Strategy suggests using a JSP as the view component. This is the preferred strategy,
and it is the one most commonly used. The other principal strategy is the Servlet
View Strategy, which utilizes a servlet as the view (see the section ¡°Strategies¡± for
more information).
Encapsulating business logic in a helper instead of a view makes our application
more modular and facilitates component reuse. Multiple clients, such as controllers
and views, may leverage the same helper to retrieve and adapt similar model state
for presentation in multiple ways. The only way to reuse logic embedded in a view is
by copying and pasting it elsewhere. Furthermore, copy-and-paste duplication
makes a system harder to maintain, since the same bug potentially needs to be
corrected in multiple places.
A signal that one may need to apply this pattern to existing code is when scriptlet
code dominates the JSP view. The overriding goal when applying this pattern, then,
is the partitioning of business logic outside of the view. While some logic is best
encapsulated within helper objects, other logic is better placed in a centralized
component that sits in front of the views and the helpers¡ªthis might include logic
that is common across multiple requests, such as authentication checks or logging
services, for example. Refer to the ¡°Intercepting Filter¡± and ¡°Front Controller¡± for
more information on these issues.
If a separate controller is not employed in the architecture, or is not used to handle
all requests, then the view component becomes the initial contact point for handling
some requests. For certain requests, particularly those involving minimal
processing, this scenario works fine. Typically, this situation occurs for pages that
are based on static information, such as the first of a set of pages that will be served
to a user to gather some information (see ¡°Dispatcher View¡±). Additionally, this
scenario occurs in some cases when a mechanism is employed to create composite
pages (see ¡°Composite View¡±).
The View Helper pattern focuses on recommending ways to partition your
application responsibilities. For related discussions about issues dealing with
directing client requests directly to a view, please refer to the section ¡°Dispatcher
View¡±.
Figure 7.11 is the class diagram representing the View Helper pattern.
Figure 7.12 shows the sequence diagram representing the View Helper pattern. A
controller typically mediates between the client and the view. In some cases,
though, a controller is not used and the view becomes the initial contact point for
handling the request. (Also, see Dispatcher View pattern.)
As noted in the class diagram, there may be no helpers associated with a view. In
this simple case, the page may be entirely static or include very small amounts of
inline scriptlet code. This scenario is described in the sequence diagram in Figure
7.13.
A view represents and displays information to the client. The information that is
used in a dynamic display is retrieved from a model. Helpers support views by
encapsulating and adapting a model for use in a display.
A helper is responsible for helping a view or controller complete its processing. Thus,
helpers have numerous responsibilities, including gathering data required by the
view and storing this intermediate model, in which case the helper is sometimes
referred to as a value bean. Additionally, helpers may adapt this data model for use
by the view. Helpers can service requests for data from the view by simply providing
access to the raw data or by formatting the data as Web content.
A view may work with any number of helpers, which are typically implemented as
JavaBeans (JSP 1.0+) and custom tags (JSP 1.1+). Additionally, a helper may
represent a Command object, a delegate (see ¡°Business Delegate¡±), or an XSL
Transformer, which is used in combination with a stylesheet to adapt and convert
the model into the appropriate form.
A value bean is another name for a helper that is responsible for holding
intermediate model state for use by a view. A typical case, as shown in the sequence
diagram in Figure 7.12, has the business service returning a value bean in response
to a request. In this case, ValueBean fulfills the role of a Value Object (see ¡°Value
Object¡±).
The business service is a role that is fulfilled by the service the client is seeking to
access. Typically, the business service is accessed via a Business delegate. The
business delegate's role is to provide control and protection for the business service
(see the ¡°Business Delegate¡±).
The JSP View Strategy suggests using a JSP as the view component. This is the
preferred strategy to the Servlet View Strategy. While it is semantically equivalent
to the Servlet View Strategy, it is a more elegant solution and is more commonly
used. Views are the domain of Web designers, who prefer markup to Java code.
Example 7.17 shows a code sample for this strategy. The excerpt is from a source
file called welcome.jsp, to which a servlet controller dispatches after placing the
WelcomeHelper JavaBean in request scope.
The alternative Servlet View Strategy is typically implemented by embedding HTML
markup directly within Java Servlet code. Intermingling Java code and markup tags
creates a poor separation of user roles within a project and increases the
dependencies on the same resources among multiple members of different teams.
When an individual works on a template containing unfamiliar code or tags, it
increases the likelihood of an accidental change introducing problems into the
system. There is also a reduction in work environment efficiency (too many people
sharing the same physical resource) and an increase in source control management
complexity. These problems are more likely to occur in larger enterprise
environments that have more complicated system requirements and that use teams
of developers. They are less likely to occur with small systems that have simple
business requirements and use few developers, because the same individual may
likely fill the roles mentioned above. However, keep in mind that projects often start
small¡ªwith simple requirements and few developers¡ªbut may ultimately evolve to
become sophisticated enough to benefit from these suggestions.
The Servlet View Strategy utilizes a servlet as the view. It is semantically equivalent
to the preferred JSP View Strategy. However, the Servlet View Strategy, as seen in
Example 7.18, is often more cumbersome for the software development and Web
production teams because it embeds markup tags directly within the Java code.
When tags are embedded within the code, the view template is more difficult to
update and modify.
The helper is implemented as a JavaBean. Using helpers results in a cleaner
separation of the view from the business processing in an application, since
business logic is factored out of the view and into the helper component. In this case
the business logic is encapsulated in a JavaBean, which aids in content retrieval and
adapts and stores the model for use by the view.
Using the JavaBean Helper Strategy requires less upfront work than does the
Custom Tag Helper Strategy, since JavaBeans are more easily constructed and
integrated into a JSP environment. Additionally, even novice developers understand
JavaBeans. This strategy is also easier from a manageability standpoint, since the
only resulting artifacts are the completed JavaBeans. An example of this strategy is
shown in Example 7.19.
The helper is implemented as a custom tag (JSP 1.1+ only). Using helpers results in
a cleaner separation of the view from the business processing in an application,
since business logic is factored out of the view and into the helper component. In
this case the business logic is encapsulated in a custom tag component, which may
aid in content retrieval and adapts the model for use by the view.
Using the Custom Tag Helper Strategy requires more upfront work than does the
JavaBean Helper Strategy, since custom tag development is moderately
complicated relative to JavaBean development. Not only is there more complexity in
the development process, but there is much more complexity with respect to
integrating and managing the completed tags. To use this strategy, the
environment must be configured with numerous generated artifacts, including the
tag itself, a tag library descriptor, and configuration files. An excerpt of a JSP View
using this strategy is shown in Example 7.20.
Helper components often make distributed invocations to the business tier. We
suggest using a business delegate in order to hide the underlying implementation
details of this request, such that the helper simply invokes a business service
without knowing details about its physical implementation and distribution (see
¡°Business Delegate¡±).
Both a helper and a business delegate may be implemented as a JavaBean. Thus,
one could combine the notion of the helper component and the business delegate
and implement the business delegate as a specialized type of helper. One major
distinction between a helper and a business delegate, though, is as follows: A helper
component is written by a developer working in the presentation tier, while the
delegate is typically written by a developer working on the services in the business
tier. (Note: The delegate may also be provided as part of a framework.) Thus, this
strategy is as much about who actually writes the delegate as it is about the
implementation. If there is some overlap in developer roles, then the business
delegate as helper is a strategy to consider.
JavaBean helpers are best used for aiding in content retrieval
and storing and adapting the model for the view. JavaBean
helpers are often used as command objects as well.
Like JavaBean helpers, custom tag helpers may fulfill each of
these roles, except for acting as a command object. Unlike
JavaBean helpers, custom tag helpers are well suited to control
flow and iteration within a view. Custom tag helpers used in
this way encapsulate logic that would otherwise be embedded
directly within the JSP as scriptlet code. Another area where
custom tag helpers are preferred is formatting raw data for
display. A custom tag is able to iterate over a collection of
results, format those results into an HTML table, and embed
the table within a JSP View without requiring any Java Scriptlet
code.
Consider an example in which a Web client is requesting
account information from a system, as shown in Figure 7.14.
There are five helpers shown in this diagram. The four
JavaBean helpers are the AccountCommand object, Account
object, AccountDAO, and AccountDetails. The sole custom tag
helper is the TableFormatter object.
The controller handles the request. It creates or looks up the
appropriate command object, which is implemented as a
JavaBean helper. In this case, it is a command object that
processes requests for account information. The controller
invokes the Command object, which asks a JavaBean Account
object for information about the account. The Account object
invokes the business service, asking for these details, which
are returned in the form of a Value object (see ¡°Value Object¡±),
implemented as a JavaBean.
So how does the Account object access the business services?
Let us examine two cases, one simple and the other more
sophisticated. In the simple case, imagine that a project is
taking a phased approach, phasing Enterprise JavaBeans (EJB)
into the business tier over time. Assume at the moment that
the database is being accessed via JDBC calls from the
presentation tier. In this case, the Account object uses a Data
Access object (see ¡°Data Access Object¡±), hiding the
underlying implementation details of accessing the database.
The Data Access object knows what SQL queries are necessary
to retrieve the information. These details are hidden from the
rest of the application, reducing coupling and making each
component more modular and reusable. This case is described
in the previous sequence diagram.
When the architecture becomes more sophisticated, and EJB is
introduced in the business tier, then the Data Access object is
replaced with a business delegate (see ¡°Business Delegate¡±),
typically written by the developers of the business service. The
delegate hides the implementation details of EJB lookup,
invocation, and exception handling from its client. It might also
improve performance by providing caching services. Again, the
object reduces coupling between tiers, improving the
reusability and modularity of the various components.
Regardless of the specific implementation of this object, its
interface may remain unchanged during this transition. Figure
7.15 describes this scenario after the transition to the business
delegate.
The command object now has a handle to the AccountDetails
object, which it stores before returning control to the
controller. The Controller dispatches to the appropriate view,
called AccountView.jsp. The view then grabs a combination of
raw data and formatted data from the AccountDetails helper
and the TableFormatter helper, respectively. The
TableFormatter helper is implemented as a custom tag that
cycles through the raw data and formats it into an HTML table
for display. As stated, this conversion requires no scriptlet code
in the view, which would be necessary to perform the same
functionality with a JavaBean helper.
Additionally, the Account object or the AccountDetails helper
could provide convenient methods to adapt the raw data in
other ways. While such methods would not introduce HTML
markup into the data, they might provide different
combinations of data. An example is to return the full name of
the user in various formats, such as ¡°Lastname, Firstname¡± or
¡°Firstname Lastname¡±, and so forth.
The completed view is then displayed to the user.
The helper is implemented as an eXtensible Stylesheet Language Transformer. This
is particularly useful with models that exist as structured markup, such as
eXtensible Markup Language (XML), either natively within legacy systems or via
some form of conversion. Using this strategy can help to enforce the separation of
the model from the view, since much of the view markup must be factored into a
separate stylesheet.
Figure 7.16 describes a potential implementation of this strategy.
The controller handles the request and invokes a Command object, implemented as
a JavaBean helper. The Command object initiates the retrieval of Account data. The
Account object invokes the business service, which returns the data in the form of a
Value Object (see ¡°Value Object¡±), implemented as a JavaBean.
Content retrieval is complete and control is dispatched to the AccountView, which
uses its custom tag transformer to manipulate the model state. The transformer
relies on a stylesheet, which describes how to transform the model, typically
describing how to format it with markup for display to the client. The stylesheet is
usually retrieved as a static file, though it may also be dynamically generated.
An example of how the custom tag helper might look in AccountView follows:
The integration of eXtensible Stylesheets and XML with JSP is evolving, as tag
libraries in this area continue to mature. For now, it is a less preferred strategy,
given the immature state of the supporting libraries and the additional sophisticated
skills necessary to generate and maintain the stylesheets.
? Improves Application Partitioning, Reuse, and Maintainability
Using helpers results in a cleaner separation of the view from the business
processing in an application. The helpers, in the form of JavaBeans (JSP
1.0+) and custom tags (JSP 1.1+), provide a place external to the view to
encapsulate business logic. Otherwise, scriptlet code clutters the JSP, a
cumbersome and unwieldy situation, especially in larger projects.
Additionally, business logic that is factored out of JSPs and into JavaBeans
and custom tags is reused, reducing duplication and easing maintenance.
? Improves Role Separation
Separating formatting logic from application business logic reduces
dependencies that individuals fulfilling different roles might have on the
same resources. For example, a software developer might own code that is
embedded within HTML markup, while a Web production team member
might need to modify page layout and design components that are
intermingled with business logic. Neither individual fulfilling these roles may
be familiar with the implementation specifics of the other individual's work,
thus raising the likelihood of accidental modifications introducing bugs into
the system.
? Business Delegate
The helper components need to access methods in the business service API.
It is also important to reduce the coupling among helpers in the presentation
tier and among business services in the business tier. It is recommended
that a delegate be used because these tiers may be physically distributed
across a network. The delegate hides from the client the underlying details of
looking up and accessing the business services, and it may also provide
intermediate caching to reduce network traffic.
? Dispatcher View and Service to Worker
When centralized control becomes desirable to handle such issues as
security, workflow management, content retrieval, and navigation, consider
the Dispatcher View or Service to Worker patterns.
? Front Controller
This pattern is paired with the View Helper pattern to create the Dispatcher
View pattern or Service to Worker pattern.
##%%&&
Sophisticated Web pages present content from numerous data sources, using
multiple subviews that comprise a single display page. Additionally, a variety of
individuals with different skill sets contribute to the development and maintenance
of these Web pages.
Instead of providing a mechanism to combine modular, atomic portions of a view
into a composite whole, pages are built by embedding formatting code directly
within each view.
Modification to the layout of multiple views is difficult and error prone, due to the
duplication of code.
? Atomic portions of view content change frequently.
? Multiple composite views use similar subviews, such as a customer inventory
table. These atomic portions are decorated with different surrounding
template text, or they appear in a different location within the page.
? Layout changes are more difficult to manage and code harder to maintain
when subviews are directly embedded and duplicated in multiple views.
? Embedding frequently changing portions of template text directly into views
also potentially affects the availability and administration of the system. The
server may need to be restarted before clients see the modifications or
updates to these template components.
Use composite views that are composed of multiple atomic subviews. Each
component of the template may be included dynamically into the whole
and the layout of the page may be managed independently of the content.
This solution provides for the creation of a composite view based on the inclusion
and substitution of modular dynamic and static template fragments. It promotes the
reuse of atomic portions of the view by encouraging modular design. It is
appropriate to use a composite view to generate pages containing display
components that may be combined in a variety of ways. This scenario occurs, for
example, with portal sites that include numerous independent subviews, such as
news feeds, weather information, and stock quotes on a single page. The layout of
the page is managed and modified independent of the subview content.
Another benefit of this pattern is that Web designers can prototype the layout of a
site, plugging static content into each of the template regions. As site development
progresses, the actual content is substituted for these placeholders.
Figure 7.17 shows a screen capture of Sun's Java homepage, java.sun.com. Four
regions are identified: Navigation, Search, Feature Story, and Headlines. While the
content for each of these component subviews may originate from different data
sources, they are laid out seamlessly to create a single composite page.
This pattern is not without its drawbacks. There is a runtime overhead associated
with it, a tradeoff for the increased flexibility that it provides. Also, the use of a more
sophisticated layout mechanism brings with it some manageability and
development issues, since there are more artifacts to maintain and a level of
implementation indirection to understand.
Figure 7.18 shows the class diagram that represents the Composite View pattern.
Figure 7.19 shows the sequence diagram for the Composite View pattern.
A composite view is a view that is an aggregate of multiple subviews.
The View Manager manages the inclusion of portions of template fragments into the
composite view. The View Manager may be part of a standard JSP runtime engine,
in the form of the standard JSP include tag (<jsp:include>), or it may be
encapsulated in a JavaBean helper (JSP 1.0+) or custom tag helper (JSP 1.1+) to
provide more robust functionality.
A benefit of using a mechanism other than the standard include tag is that
conditional inclusion is easily done. For example, certain template fragments may
be included only if the user fulfills a particular role or certain system conditions are
satisfied. Furthermore, using a helper component as a View Manager allows for
more sophisticated control of the page structure as a whole, which is useful for
creating reusable page layouts.
An included view is a subview that is one atomic piece of a larger whole view. This
included view could also potentially be a composite, itself including multiple
subviews.
View management is implemented using JavaBeans, as shown in Example 7.22. The
view delegates to the JavaBean, which implements the custom logic to control view
layout and composition. The decisions on page layout may be based on user roles or
security policies, making it much more powerful than the standard JSP include
functionality. While it is semantically equivalent to the Custom Tag View
Management Strategy, it is not nearly as elegant, since it introduces scriptlet code
into the view.
Using the JavaBean View Management Strategy requires less up-front work than
using the preferred Custom Tag View Management Strategy, since it is easier to
construct JavaBeans and integrate them into a JSP environment. Additionally, even
novice developers understand JavaBeans. This strategy is also easier from a
manageability standpoint, because the completed JavaBeans are the only resulting
artifacts to manage and configure.
View management is implemented using standard JSP tags, such as the
<jsp:include> tag. Using standard tags for managing the layout and composition
of views is an easy strategy to implement, but does not provide the power and
flexibility of the preferred Custom Tag View Management Strategy, since the layout
for individual pages remains embedded within that page. Thus, while this strategy
allows for the underlying content to vary dynamically, any site-wide layout changes
would require individual modifications to numerous JSPs. This is shown in Example
7.23.
When creating a composite display using standard tags, both static content, such as
an HTML file, and dynamic content, such as a JSP, can be included. Additionally, the
content can be included at translation time or at runtime. If the content is included
at translation time, then the page display will remain unchanged until the JSP is
recompiled, at which point any modifications to included content will be visible. In
other words, the page is laid out and generated once, each time the JSP is
recompiled. Example 7.24 shows an excerpt of a JSP that generates a composite
page in this way, using the standard JSP include directive <%@ include %>, which
includes content at translation time.
Runtime inclusion of content means that changes to underlying subviews are visible
in the composite page the next time a client accesses the page. This is much more
dynamic and can be accomplished using the standard JSP include tag
<jsp:include>, as shown in Example 7.25. There is of course some runtime
overhead associated with this type of view generation, but it is the tradeoff for the
increased flexibility of on-the-fly content modifications.
View management is implemented using custom tags (JSP 1.1+), which is the
preferred strategy. Logic implemented within the tag controls view layout and
composition. These tags are much more powerful and flexible than the standard JSP
include tag, but also require a higher level of effort. Custom actions can base page
layout and composition on such things as user roles or security policies.
Using this strategy requires more upfront work than do the other view management
strategies, since custom tag development is more complicated than simply using
JavaBeans or standard tags. Not only is there more complexity in the development
process, but there is much more complexity with respect to integrating and
managing the completed tags. Using this strategy requires the generation of
numerous artifacts, including the tag itself, a tag library descriptor, configuration
files, and configuring the environment with these artifacts.
The following JSP excerpt shows a possible implementation of this strategy and is
excerpted from Example 7.26. Please refer to that code sample for more detail.
View management is implemented using an XSL Transformer. This strategy would
typically be combined with the Custom Tag View Management Strategy, using
custom tags to implement and delegate to the appropriate components. Using this
strategy can help to enforce the separation of the model from the view, since much
of the view markup must be factored into a separate stylesheet. At the same time,
it involves technologies that require new and sophisticated skill sets to implement
correctly, an issue that makes this strategy impractical in many environments
where these technologies are not already established.
The following excerpt shows the use of a custom tag from within a JSP to convert a
model using a stylesheet and transformer:
This is another name for translation-time content inclusion, as described in the
Standard Tag View Management Strategy and shown in Example 7.24. It is
appropriate for maintaining and updating a relatively static template and is
recommended if a view includes headers and footers that change infrequently.
This is another name for runtime-content inclusion, as described in the Standard
Tag View Management Strategy and shown in Example 7.25. It is appropriate for
composite pages that may change frequently. One note: If the subview included at
runtime is a dynamic resource, such as a JSP, then this subview may also be a
composite view, including more runtime content. The flexibility offered by such
nested composite structures should be weighed against their runtime overhead and
considered in light of specific project requirements.
? Improves Modularity and Reuse
The pattern promotes modular design. It is possible to reuse atomic portions
of a template, such as a table of stock quotes, in numerous views and to
decorate these reused portions with different information. This pattern
permits the table to be moved into its own module and simply included
where necessary. This type of dynamic layout and composition reduces
duplication, fosters reuse, and improves maintainability.
? Enhances Flexibility
A sophisticated implementation may conditionally include view template
fragments based on runtime decisions, such as user role or security policy.
? Enhances Maintainability and Manageability
It is much more efficient to manage changes to portions of a template when
the template is not hardcoded directly into the view markup. When kept
separate from the view, it is possible to modify modular portions of template
content independent of the template layout. Additionally, these changes are
available to the client immediately, depending on the implementation
strategy. Modifications to the layout of a page are more easily managed as
well, since changes are centralized.
? Reduces Manageability
Aggregating atomic pieces of the display together to create a single view
introduces the potential for display errors, since subviews are page
fragments. This is a limitation that can become a manageability issue. For
example, if a JSP page is generating an HTML page using a main page that
includes three subviews, and the subviews each include the HTML open and
close tag (that is, <HTML> and </HTML>), then the composed page will be
invalid. Thus, it is important when using this pattern to be aware that
subviews must not be complete views. Tag usage must be accounted for
quite strictly in order to create valid composite views, and this can become a
manageability issue.
? Performance Impact
Generating a display that includes numerous subviews may slow
performance. Runtime inclusion of subviews will result in a delay each time
the page is served to the client. In an environment with strict Service Level
Agreements that mandate specific response times, such performance
slowdowns, though typically extremely minimal, may not be acceptable. An
alternative is to move the subview inclusion to translation time, though this
limits the subview to changing when the page is retranslated.
The Composite View pattern can be implemented using any number of strategies,
but one of the more popular is the Custom Tag View Management Strategy. In fact,
there are a number of custom tag libraries currently available for implementing
composite views that separate view layout from view content and provide for
modular and pluggable template subviews.
This sample will use a template library written by David Geary and featured in detail
in ¡°Advanced JavaServer Pages¡± [Geary].
The template library describes three basic components: sections, regions, and
templates.
? A section is a reusable component that renders HTML or JSP.
? A region describes content by defining sections.
? A template controls the layout of regions and sections in a rendered page.
A region can be defined and rendered as shown in Example 7.26.
A region defines its content by matching logical section names with a portion of
content, such as banner.jsp.
The layout for the region and its sections is defined by a template, to which each
region is associated. In this case, the template is named portal.jsp, as defined in
Example 7.27.
A site with numerous views and a single consistent layout has one JSP containing
code that looks similar to the template definition in Example 7.27, and many JSPs
that look similar to Example 7.26, defining alternate regions and sections.
Sections are JSP fragments that are used as subviews to build a composite whole as
defined by a template. The banner.jsp section is shown in Example 7.28.
Composite views are a modular, flexible and extensible way to build JSP views for
your J2EE application.
? View Helper
The Composite View pattern may be used as the view in the View Helper
pattern.
? Composite [GoF]
The Composite View pattern is based on the Composite pattern, which
describes part-whole hierarchies where a composite object is comprised of
numerous pieces, all of which are treated as logically equivalent.
##%%&&
The system controls flow of execution and access to business data, from which it
creates presentation content.
Note
The Service to Worker pattern, like the Dispatcher View pattern, describes a
common combination of other patterns from the catalog. Both of these macro
patterns describe the combination of a controller and dispatcher with views and
helpers. While describing this common structure, they emphasize related but
different usage patterns.
The problem is a combination of the problems solved by the Front Controller and
View Helper patterns in the presentation tier. There is no centralized component for
managing access control, content retrieval, or view management, and there is
duplicate control code scattered throughout various views. Additionally, business
logic and presentation formatting logic are intermingled within these views, making
the system less flexible, less reusable, and generally less resilient to change.
Intermingling business logic with view processing also reduces modularity and
provides a poor separation of roles among Web production and software
development teams.
? Authentication and authorization checks are completed per request.
? Scriptlet code within views should be minimized.
? Business logic should be encapsulated in components other than the view.
? Control flow is relatively complex and based on values from dynamic
content.
? View management logic is relatively sophisticated, with multiple views
potentially mapping to the same request.
Combine a controller and dispatcher with views and helpers (see ¡°Front
Controller¡± and ¡°View Helper¡±) to handle client requests and prepare a
dynamic presentation as the response. Controllers delegate content
retrieval to helpers, which manage the population of the intermediate
model for the view. A dispatcher is responsible for view management and
navigation and can be encapsulated either within a controller or a separate
component.
Service to Worker describes the combination of the Front Controller and View Helper
patterns with a dispatcher component.
While this pattern and the Dispatcher View pattern describe a similar structure, the
two patterns suggest a different division of labor among the components. In Service
to Worker, the controller and the dispatcher have more responsibilities.
Since the Service to Worker and Dispatcher View patterns represent a common
combination of other patterns from the catalog, each warrants its own name to
promote efficient communication among developers. Unlike the Service to Worker
pattern, the Dispatcher View pattern suggests deferring content retrieval to the
time of view processing.
In the Dispatcher View pattern, the dispatcher typically plays a limited to moderate
role in view management. In the Service to Worker pattern, the dispatcher typically
plays a moderate to large role in view management.
A limited role for the dispatcher occurs when no outside resources are utilized in
order to choose the view. The information encapsulated in the request is sufficient
to determine the view to dispatch the request. For example,
http://some.server.com/servlet/Controller?next=login.jsp
The sole responsibility of the dispatcher component in this case is to dispatch to the
view login.jsp.
An example of the dispatcher playing a moderate role is the case where the client
submits a request directly to a controller with a query parameter that describes an
action to be completed:
http://some.server.com/servlet/Controller?action=login
The responsibility of the dispatcher component here is to translate the logical name
login into the resource name of an appropriate view, such as login.jsp, and
dispatch to that view. To accomplish this translation, the dispatcher may access
resources such as an XML configuration file that specifies the appropriate view to
display.
On the other hand, in the Service to Worker pattern, the dispatcher might be more
sophisticated. The dispatcher may invoke a business service to determine the
appropriate view to display.
The shared structure of Service to Worker and Dispatcher View consists of a
controller working with a dispatcher, views, and helpers.
The class diagram in Figure 7.20 represents the Service to Worker pattern.
Figure 7.21 shows the sequence diagram that represents the Service to Worker
pattern.
As stated, Service to Worker and Dispatcher View represent a similar structure. The
main difference is that Service to Worker describes architectures with more
behavior ¡°up front¡± in the controller and dispatcher, while Dispatcher View describes
architectures with more behavior moved back to the time of view processing. Thus,
the two patterns suggest a continuum, where behavior is either encapsulated closer
to the front or moved farther back in the process flow.
The controller is typically the initial contact point for handling a request. It works
with a dispatcher to complete view management and navigation. The controller
manages authentication, authorization, content retrieval, validation, and other
aspects of request handling. It delegates to helpers to complete portions of this
work.
A dispatcher is responsible for view management and navigation, managing the
choice of the next view to present to the user and providing the mechanism for
vectoring control to this resource.
A dispatcher can be encapsulated within a controller (see ¡°Front Controller¡±) or it
can be a separate component working in coordination with the controller. The
dispatcher can provide static dispatching to the view or it may provide a more
sophisticated dynamic dispatching mechanism.
The dispatcher uses the RequestDispatcher object (supported in the servlet
specification), but it also typically encapsulates some additional processing. The
more responsibilities that this component encapsulates, the more it fits into the
Service to Worker pattern. Conversely, when the dispatcher plays a more limited
role, it fits more closely into the Dispatcher View pattern.
A View represents and displays information to the client. The information that is
used in a display is retrieved from a model. Helpers support views by encapsulating
and adapting a model for use in a display.
A helper is responsible for helping a view or controller complete its processing. Thus,
helpers have numerous responsibilities, including gathering data required by the
view and storing this intermediate model, in which case the helper is sometimes
referred to as a value bean. Additionally, helpers may adapt this data model for use
by the view. Helpers can service requests for data from the view by simply providing
access to the raw data or by formatting the data as Web content.
A view may work with any number of helpers, which are typically implemented as
JavaBeans (JSP 1.0+) and custom tags (JSP 1.1+). Additionally, a helper may
represent a Command object or a delegate (see ¡°Business Delegate¡±).
A value bean is another name for a helper that is responsible for holding
intermediate model state for use by a view. A typical case, as shown in the sequence
diagram in Figure 7.12, has the business service returning a value bean in response
to a request. In this case, ValueBean fulfills the role of a Value Object (see ¡°Value
Object¡±).
The business service is a role that is fulfilled by the service the client is seeking to
access. Typically, the business service is accessed via a Business delegate. The
business delegate's role is to provide control and protection for the business service
(see the ¡°Business Delegate¡±).
As stated, the Service to Worker and Dispatcher View patterns suggest a continuum,
where behavior is encapsulated closer to the front or moved farther back in the
process flow. Figure 7.22 describes a scenario in which the controller is heavily
loaded with upfront work, but the dispatcher functionality is minimal.
? Centralizes Control and Improves Modularity and Reuse
This pattern suggests providing a central place to handle system services
and business logic across multiple requests. The controller manages
business logic processing and request handling. Keep in mind, though, that
as control centralizes, it is possible to introduce a single point of failure.
The pattern also promotes cleaner application partitioning and encourages
reuse. Common code is moved into a controller and reused per request and
moved into helper components, to which controllers and views delegate. The
improved modularity and reuse means less duplication, which typically
means a more bug-free environment.
? Improves Application Partitioning
Using helpers results in a cleaner separation of the view from the business
processing in an application. Helpers, in the form of JavaBeans (JSP 1.0+)
and Custom tags (JSP 1.1+), provide a place for business logic to be factored
out of the JSP. If the business logic is left in a JSP, large projects result in
cumbersome and unwieldy scriptlet code.
? Improves Role Separation
Separating the formatting logic from the application business logic also
reduces dependencies on the same resources among individuals fulfilling
different roles. Without this separation, for example, a software developer
would own code that is embedded within HTML markup, while a Web
production team member would need to modify page layout and design
components that are intermingled with business logic. Because neither
individual fulfilling these roles is familiar with the implementation specifics of
the other individual's work, it raises the likelihood of modifications
accidentally introducing bugs into the system.
The following sample code shows an implementation of the Service to Worker
pattern, using a controller servlet, a command helper, a dispatcher component, and
a view. The implementation includes the Servlet Front Strategy, Command and
Controller Strategy, JSP View Strategy, and JavaBean Helper Strategy. A very basic
composite view is used as well. A screen shot of the resulting display is shown in
Figure 7.23.
Example 7.29 shows the controller servlet, which delegates to a Command object
(Command and Controller Strategy) to complete the control processing. The
Command object is retrieved via a factory invocation, which returns the generic
Command type, an interface shown in Example 7.30. The sample code uses a
LogManager to log messages. The screen shots in Figure 7.23 and Figure 7.28 show
these messages displayed at the bottom of the page, for the purposes of this
example.
Each Command Object helper implements this generic interface, which is an
example of the GoF Command pattern. The Command object is an instance of the
ViewAccountDetails class, which is shown in Example 7.31. The command instance
delegates to an AccountingAdapter to make an invocation to the business tier via
business delegate. The adapter class is shown in Example 7.32. It uses a separate
dispatcher component to determine the next view to which control should be
dispatched and to actually dispatch to this view.
The invocation on the business service via the delegate yields an Account Value
object, which the adapter stores in a request attribute for use by the view. Example
7.33 shows accountdetails.jsp, the JSP to which the request is dispatched. The
Value object is imported via the standard <jsp:useBean> tag and its properties
accessed with the standard <jsp:getProperty> tag. Also, the view uses a very
simple composite strategy, doing a translation-time inclusion of the trace.jsp
subview, which is responsible for displaying log information on the display solely for
example purposes.
? Front Controller and View Helper
The Service to Worker pattern is the result of combining the View Helper
pattern with a dispatcher, in coordination with the Front Controller pattern.
? Dispatcher View
The Dispatcher View pattern is another name for the combination of the
Front Controller pattern with a dispatcher, and the View Helper pattern. The
Service to Worker and the Dispatcher View patterns are identical with
respect to the components involved, but differ in the division of labor among
those components. The Dispatcher View pattern suggests deferring content
retrieval to the time of view processing. Also, the dispatcher plays a more
limited role in view management, as the choice of view is typically already
included in the request.
##%%&&
System controls flow of execution and access to presentation processing, which is
responsible for generating dynamic content.
The Dispatcher View pattern, like the Service to Worker pattern, describes a
common combination of other patterns from the catalog. Both of these macro
patterns describe the combination of a controller and dispatcher with views and
helpers. While describing this common structure, they emphasize related but
different usage patterns.
The problem is a combination of the problems solved by the Front Controller and
View Helper patterns in the presentation tier. There is no centralized component for
managing access control, content retrieval or view management, and there is
duplicate control code scattered throughout various views. Additionally, business
logic and presentation formatting logic are intermingled within these views, making
the system less flexible, less reusable, and generally less resilient to change.
Intermingling business logic with view processing also reduces modularity and
provides a poor separation of roles among Web production and software
development teams.
? Authentication and authorization checks are completed per request.
? Scriptlet code within views should be minimized.
? Business logic should be encapsulated in components other than the view.
? Control flow is relatively simple and is typically based on values
encapsulated with the request.
? View management logic is limited in complexity.
Combine a controller and dispatcher with views and helpers (see ¡°Front
Controller¡± and ¡°View Helper¡±) to handle client requests and prepare a
dynamic presentation as the response. Controllers do not delegate content
retrieval to helpers, because these activities are deferred to the time of
view processing. A dispatcher is responsible for view management and
navigation and can be encapsulated either within a controller, a view, or a
separate component.
Dispatcher View describes the combination of the Front Controller and View Helper
patterns with a dispatcher component. While this pattern and the Service to Worker
pattern describe a similar structure, the two patterns suggest a different division of
labor among the components. The controller and the dispatcher typically have
limited responsibilities, as compared to the Service to Worker pattern, since the
upfront processing and view management logic are basic. Furthermore, if
centralized control of the underlying resources is considered unnecessary, then the
controller is removed and the dispatcher may be moved into a view.
Since the Service to Worker and Dispatcher View patterns represent a common
combination of other patterns from the catalog, each warrants its own name to
promote efficient communication among developers. Unlike the Service to Worker
pattern, the Dispatcher View pattern suggests deferring content retrieval to the
time of view processing.
In the Dispatcher View pattern, the dispatcher typically plays a limited to moderate
role in view management. In the Service to Worker pattern, the dispatcher typically
plays a moderate to large role in view management.
A limited role for the dispatcher occurs when no outside resources are utilized in
order to choose the view. The information encapsulated in the request is sufficient
to determine the view to dispatch the request. For example:
http://some.server.com/servlet/Controller?next=login.jsp
The sole responsibility of the dispatcher component in this case is to dispatch to the
view login.jsp.
An example of the dispatcher playing a moderate role is the case where the client
submits a request directly to a controller with a query parameter that describes an
action to be completed:
http://some.server.com/servlet/Controller?action=login
The responsibility of the dispatcher component here is to translate the logical name
login into the resource name of an appropriate view, such as login.jsp, and
dispatch to that view. To accomplish this translation, the dispatcher may access
resources such as an XML configuration file that specifies the appropriate view to
display.
On the other hand, in the Service to Worker pattern, the dispatcher might be more
sophisticated. The dispatcher may invoke a business service to determine the
appropriate view to display.
The shared structure of these two patterns, as mentioned above, consists of a
controller working with a dispatcher, views, and helpers.
Figure 7.24 shows the class diagram that represents the Dispatcher View pattern.
Figure 7.25 shows the Dispatcher View sequence pattern.
While the controller responsibilities are limited to system services, such as
authentication and authorization, it is often still beneficial to centralize these
aspects of the system. Notice also that, unlike in Service to Worker, the dispatcher
does not make invocations on a business service in order to complete its view
management processing.
The dispatcher functionality may be encapsulated in its own component. At the
same time, when the responsibilities of the dispatcher are limited, as described by
this pattern, the dispatcher functionality is often folded into another component,
such as the controller or the view (see ¡°Dispatcher in Controller Strategy¡± and
¡°Dispatcher in View Strategy¡±).
In fact, the dispatcher functionality may even be completed by the container, in the
case where there is no extra application-level logic necessary. An example is a view
called main.jsp that is given the alias name first. The container will process the
following request, translate the alias name to the physical resource name, and
dispatch directly to that resource:
http://some.server.com/first --> /mywebapp/main.jsp
In this case, we are left with the View Helper pattern, with the request being
handled directly by the view. Since the view is the initial contact point for handling
a request, custom tag helpers are typically used in these cases to perform business
processing or to delegate this processing to other components. See the listing in
Example 7.35 in the ¡°Sample Code¡± section for an implementation sample.
Thus, the Dispatcher View pattern describes a continuum of related scenarios,
moving from a scenario that is very structurally similar to Service to Worker to one
that is similar to View Helper.
The controller is typically the initial contact point for handling a request. The
controller manages authentication and authorization, and delegates to a dispatcher
to do view management.
A dispatcher is responsible for view management and navigation, managing the
choice of the next view to present to the user and providing the mechanism for
vectoring control to this resource.
A dispatcher can be encapsulated within a controller (see ¡°Front Controller¡±) or can
be a separate component working in coordination. The dispatcher can provide static
dispatching to the view or may provide a more sophisticated dynamic dispatching
mechanism.
A view represents and displays information to the client. The information that is
used in a display is retrieved from a model. Helpers support views by encapsulating
and adapting a model for use in a display.
A helper is responsible for helping a view or controller complete its processing. Thus,
helpers have numerous responsibilities, including gathering data required by the
view and storing this intermediate model, in which case the helper is sometimes
referred to as a value bean. Additionally, helpers may adapt this data model for use
by the view. Helpers can service requests for data from the view by simply providing
access to the raw data or by formatting the data as Web content.
A view may work with any number of helpers, which are typically implemented as
JavaBeans (JSP 1.0+) and custom tags (JSP 1.1+). Additionally, a helper may
represent a Command object or a Delegate (see ¡°Business Delegate¡±).
A value bean is another name for a helper that is responsible for holding
intermediate model state for use by a view. A typical case, as shown in the sequence
diagram in Figure 7.12, has the business service returning a value bean in response
to a request. In this case, ValueBean fulfills the role of a Value Object (see ¡°Value
Object¡±).
The business service is a role that is fulfilled by the service the client is seeking to
access. Typically, the business service is accessed via a business delegate. The
business delegate's role is to provide control and protection for the business service
(see ¡°Business Delegate¡±).
As stated, the Service to Worker and Dispatcher View patterns suggest a continuum,
where behavior is encapsulated closer to the front in Service to Worker or moved
farther back in Dispatcher View.
Figure 7.26 shows the interactions for this strategy.
The controller does not create an explicit dispatcher object. Instead, the controller
takes care of dispatching to the view. Alternatively, one could implement a
dispatcher to which the controller can delegate the dispatching function.
If the controller is removed due to its limited role, the dispatcher may be moved into
a view. This design can be useful in cases where there is typically one view that
maps to a specific request, but where a secondary view may be used on an
infrequent basis. For example, based on some information in the request or results
of some processing in a view, a custom tag helper in the view might vector control
to a secondary view. A typical case is when a client request is submitted to a specific
view, and will be serviced by that view in almost every case. Consider the case
where the user has not been authenticated, but requests access to one of the few
protected JSPs on a site. Since the site has only a few protected pages, and limited
dynamic content, authentication can be performed within those JSPs, instead of
using a site-wide centralized controller. Those pages that need authentication
include a custom tag helper at the top of the page. This helper performs the
authentication check and either displays the page for the user or forwards the user
to an authentication page.
Figure 7.27 represents this scenario.
? Centralizes Control and Improves Reuse and Maintainability
Control processing is handled in a central place for multiple requests. It is
easier to manage these activities and perform dispatching from a centralized
point, since a central access point means code is reused across multiple
requests, reducing duplication and easing maintenance.
? Improves Application Partitioning
Use of helpers results in a cleaner separation of the view from an
application's business processing. The helpers, in the form of JavaBeans
(JSP 1.0+) and Custom tags (JSP 1.1+), provide a place for business logic to
be factored out of the JSP, where scriptlet code quickly becomes
cumbersome and unwieldy in large projects.
? Improves Role Separation
Separating the formatting logic from the application business logic also
reduces dependencies that individuals fulfilling different roles might have on
the same resources. For example, a software developer would own code that
is embedded within HTML markup, while a Web production team member
would need to modify page layout and design components that are
intermingled with business logic. Because neither individual fulfilling these
roles may be familiar with the implementation specifics of the other
individual's work, there is the likelihood of inadvertent modifications
introducing bugs into the system.
The following sample code shows an implementation of the Dispatcher View pattern,
using a controller servlet and a view with JavaBean and custom tag helpers. The
implementation includes the Servlet Front Strategy, Dispatcher in Controller
Strategy, JSP View Strategy, and custom tag and JavaBean helper strategies. A
very basic composite view is used as well. A screen shot of the resulting display is
shown in Figure 7.28.
Example 7.34 shows the controller servlet, which simply completes an
authentication check and passes control to the appropriate view. Notice that the
controller does not directly delegate to any helper components in order to make
invocations to the business tier via a Delegate. These responsibilities are deferred to
the view, which is called accountdetails.jsp and can be seen in Example 7.35.
The sample code uses a LogManager to log messages. These messages are
displayed at the bottom of the output page, for the purposes of this example, and
can be seen in the screen shots in Figure 7.23 and Figure 7.28.
Notice that the view uses custom tag helpers to manage content retrieval, since this
activity was not completed in the controller. When custom tags are used in this
manner, they typically become thin facades for standalone components to which
they delegate to complete this processing. This way, the general processing logic is
loosely coupled to the tag implementation. If custom tags are not used with
Dispatcher View, then too much scriptlet code typically ends up in the JSP, a
situation to be avoided.
? Front Controller
The Service to Worker pattern is the result of combining the View Helper
pattern with a dispatcher in coordination with the Front Controller pattern.
? View Helper
The Service to Worker pattern is the result of combining the View Helper
pattern with a dispatcher in coordination with the Front Controller pattern.
? Service to Worker
The Service to Worker pattern is another name for the combination of the
Front Controller pattern with a dispatcher and the View Helper pattern. The
Service to Worker and Dispatcher View patterns are identical with respect to
the components involved, but differ in the division of labor among those
components. The Dispatcher View pattern suggests deferring content
retrieval to the time of view processing. Also, the dispatcher plays a more
limited role in view management, as the choice of view is typically already
included in the request.
##%%&&
A multitiered, distributed system requires remote method invocations to send and
receive data across tiers. Clients are exposed to the complexity of dealing with
distributed components.
Presentation-tier components interact directly with business services. This direct
interaction exposes the underlying implementation details of the business service
application program interface (API) to the presentation tier. As a result, the
presentation-tier components are vulnerable to changes in the implementation of
the business services: When the implementation of the business services change,
the exposed implementation code in the presentation tier must change too.
Additionally, there may be a detrimental impact on network performance because
presentation-tier components that use the business service API make too many
invocations over the network. This happens when presentation-tier components use
the underlying API directly, with no client-side caching mechanism or aggregating
service.
Lastly, exposing the service APIs directly to the client forces the client to deal with
the networking issues associated with the distributed nature of EJB technology.
? Presentation-tier clients need access to business services.
? Different clients, such as devices, Web clients, and thick clients, need access
to business service.
? Business services APIs may change as business requirements evolve.
? It is desirable to minimize coupling between presentation-tier clients and the
business service, thus hiding the underlying implementation details of the
service, such as lookup and access.
? Clients may need to implement caching mechanisms for business service
information.
? It is desirable to reduce network traffic between client and business services.
Use a Business Delegate to reduce coupling between presentation-tier
clients and business services. The Business Delegate hides the underlying
implementation details of the business service, such as lookup and access
details of the EJB architecture.
The Business Delegate acts as a client-side business abstraction; it provides an
abstraction for, and thus hides, the implementation of the business services. Using
a Business Delegate reduces the coupling between presentation-tier clients and the
system's business services. Depending on the implementation strategy, the
Business Delegate may shield clients from possible volatility in the implementation
of the business service API. Potentially, this reduces the number of changes that
must be made to the presentation-tier client code when the business service API or
its underlying implementation changes.
However, interface methods in the Business Delegate may still require modification
if the underlying business service API changes. Admittedly, though, it is more likely
that changes will be made to the business service rather than to the Business
Delegate.
Often, developers are skeptical when a design goal such as abstracting the business
layer causes additional upfront work in return for future gains. However, using this
pattern or its strategies results in only a small amount of additional upfront work
and provides considerable benefits. The main benefit is hiding the details of the
underlying service. For example, the client can become transparent to naming and
lookup services. The Business Delegate also handles the exceptions from the
business services, such as java.rmi.Remote exceptions, JMS exceptions and so on.
The Business Delegate may intercept such service level exceptions and generate
application level exceptions instead. Application level exceptions are easier to
handle by the clients, and may be user friendly. The Business Delegate may also
transparently perform any retry or recovery operations necessary in the event of a
service failure without exposing the client to the problem until it is determined that
the problem is not resolvable. These gains present a compelling reason to use the
pattern.
Another benefit is that the delegate may cache results and references to remote
business services. Caching can significantly improve performance, because it limits
unnecessary and potentially costly round trips over the network.
A Business Delegate uses a component called the Lookup Service. The Lookup
Service is responsible for hiding the underlying implementation details of the
business service lookup code. The Lookup Service may be written as part of the
Delegate, but we recommend that it be implemented as a separate component, as
outlined in the Service Locator pattern (See ¡°Service Locator¡±.).
When the Business Delegate is used with a Session Facade, typically there is a
one-to-one relationship between the two. This one-to-one relationship exists
because logic that might have been encapsulated in a Business Delegate relating to
its interaction with multiple business services (creating a one-to-many relationship)
will often be factored back into a Session Facade.
Finally, it should be noted that this pattern could be used to reduce coupling
between other tiers, not simply the presentation and the business tiers.
Figure 8.1 shows the class diagram representing the Business Delegate pattern. The
client requests the BusinessDelegate to provide access to the underlying business
service. The BusinessDelegate uses a LookupService to locate the required
BusinessService component.
Figure 8.2 and Figure 8.3 show sequence diagrams that illustrate typical
interactions for the Business Delegate pattern.
The BusinessDelegate uses a LookupService for locating the business service. The
business service is used to invoke the business methods on behalf of the client. The
Get ID method shows that the BusinessDelegate can obtain a String version of the
handle (such as EJBHandle object) for the business service and return it to the client
as a String. The client can use the String version of the handle at a later time to
reconnect to the business service it was using when it obtained the handle. This
technique will avoid new lookups, since the handle is capable of reconnecting to its
business service instance. It should be noted that handle objects are implemented
by the container provider and may not be portable across containers from different
vendors.
The sequence diagram in Figure 8.3 shows obtaining a BusinessService (such as a
session or an entity bean) using its handle.
The BusinessDelegate's role is to provide control and protection for the business
service. The BusinessDelegate can expose two types of constructors to clients. One
type of request instantiates the BusinessDelegate without an ID, while the other
instantiates it with an ID, where ID is a String version of the reference to a remote
object, such as EJBHome or EJBObject.
When initialized without an ID, the BusinessDelegate requests the service from the
Lookup Service, typically implemented as a Service Locator (see ¡°Service Locator¡±),
which returns the Service Factory, such as EJBHome. The BusinessDelegate
requests that the Service Factory locate, create, or remove a BusinessService, such
as an enterprise bean.
When initialized with an ID string, the BusinessDelegate uses the ID string to
reconnect to the BusinessService. Thus, the BusinessDelegate shields the client
from the underlying implementation details of BusinessService naming and lookup.
Furthermore, the presentation-tier client never directly makes a remote invocation
on a BusinessSession; instead, the client uses the BusinessDelegate.
The BusinessDelegate uses the LookupService to locate the BusinessService. The
LookupService encapsulates the implementation details of BusinessService lookup.
The BusinessService is a business-tier component, such as an enterprise bean or a
JMS component, that provides the required service to the client.
The Business Delegate exposes an interface that provides clients access to the
underlying methods of the business service API. In this strategy, a Business
Delegate provides proxy function to pass the client methods to the session bean it is
encapsulating. The Business Delegate may additionally cache any necessary data,
including the remote references to the session bean's home or remote objects to
improve performance by reducing the number of lookups. The Business Delegate
may also convert such references to String versions (IDs) and vice versa, using the
services of a Service Locator.
The example implementation for this strategy is discussed in the ¡°Sample Code¡±
section of this pattern.
The Business Delegate proves to be a nice fit in a B2B environment when
communicating with J2EE services. Disparate systems may use an XML as the
integration language. Integrating one system to another typically requires an
Adapter [GoF] to meld the two disparate systems. Figure 8.4 gives an example.
? Reduces Coupling, Improves Manageability
The Business Delegate reduces coupling between the presentation tier and
the business tier by hiding all business-tier implementation details. It is
easier to manage changes because they are centralized in one place, the
Business Delegate.
? Translates Business Service Exceptions
The Business Delegate is responsible for translating any network or
infrastructure-related exceptions into business exceptions, shielding clients
from knowledge of the underlying implementation specifics.
? Implements Failure Recovery and Thread Synchronization
The Business Delegate on encountering a business service failure, may
implement automatic recovery features without exposing the problem to the
client. If the recovery succeeds, the client need not know about the failure.
If the recovery attempt does not succeed, then the Business Delegate needs
to inform the client of the failure. Additionally, the business delegate
methods may be synchronized, if necessary.
? Exposes Simpler, Uniform Interface to Business Tier
The Business Delegate, to better serve its clients, may provide a variant of
the interface provided by the underlying enterprise beans.
? Impacts Performance
The Business Delegate may provide caching services (and better
performance) to the presentation tier for common service requests.
? Introduces Additional Layer
The Business Delegate may be seen as adding an unnecessary layer between
the client and the service, thus introducing added complexity and decreasing
flexibility. Some developers may feel that it is an extra effort to develop
Business Delegates with implementations that use the Delegate Proxy
strategy. At the same time, the benefits of the pattern typically outweigh
such drawbacks.
? Hides Remoteness
While location transparency is one of the benefits of this pattern, a different
problem may arise due to the developer treating a remote service as if it was
a local one. This may happen if the client developer does not understand that
the Business Delegate is a client side proxy to a remote service. Typically, a
method invocations on the Business Delegate results in a remote method
invocation under the wraps. Ignoring this, the developer may tend to make
numerous method invocations to perform a single task, thus increasing the
network traffic.
Consider a Professional Services Application (PSA), where a Web-tier client needs to
access a session bean that implements the Session Facade pattern. The Business
Delegate pattern can be applied to design a Delegate class ResourceDelegate, which
encapsulates the complexity of dealing with the session bean ResourceSession. The
ResourceDelegate implementation for this example is shown in Example 8.1, and
the corresponding remote interface for the Session Facade bean ResourceSession is
shown in Example 8.2.
? Service Locator
The Service Locator pattern may be used to create the Business Delegate's
Service Locator, hiding the implementation details of any business service
lookup and access code.
? Proxy [GoF]
A Business Delegate may act as a proxy, providing a stand-in for objects in
the business tier.
? Adapter [GoF]
A Business Delegate may use the Adapter pattern to provide coupling for
disparate systems.
? Broker [POSA1]
A Business Delegate performs the role of a broker to decouple the business
tier objects from the clients in other tiers.
##%%&&
Application clients need to exchange data with enterprise beans.
J2EE applications implement server-side business components as session beans
and entity beans. Some methods exposed by the business components return data
to the client. Often, the client invokes a business object's get methods multiple
times until it obtains all the attribute values.
Session beans represent the business services and are not shared between users. A
session bean provides coarse-grained service methods when implemented per the
Session Facade pattern.
Entity beans, on the other hand, are multiuser, transactional objects representing
persistent data. An entity bean exposes the values of attributes by providing an
accessor method (also referred to as a getter or get method) for each attribute it
wishes to expose.
Every method call made to the business service object, be it an entity bean or a
session bean, is potentially remote. Thus, in an EJB application such remote
invocations use the network layer regardless of the proximity of the client to the
bean, creating a network overhead. Enterprise bean method calls may permeate the
network layers of the system even if the client and the EJB container holding the
entity bean are both running in the same JVM, OS, or physical machine. Some
vendors may implement mechanisms to reduce this overhead by using a more
direct access approach and bypassing the network.
As the usage of these remote methods increases, application performance can
significantly degrade. Therefore, using multiple calls to get methods that return
single attribute values is inefficient for obtaining data values from an enterprise
bean.
? All access to an enterprise bean is performed via remote interfaces to the
bean. Every call to an enterprise bean is potentially a remote method call
with network overhead.
? Typically, applications have a greater frequency of read transactions than
update transactions. The client requires the data from the business tier for
presentation, display, and other read-only types of processing. The client
updates the data in the business tier much less frequently than it reads the
data.
? The client usually requires values for more than one attribute or dependent
object from an enterprise bean. Thus, the client may invoke multiple remote
calls to obtain the required data.
? The number of calls made by the client to the enterprise bean impacts
network performance. Chattier applications¡ªthose with increased traffic
between client and server tiers¡ªoften degrade network performance.
Use a Value Object to encapsulate the business data. A single method call
is used to send and retrieve the value object. When the client requests the
enterprise bean for the business data, the enterprise bean can construct
the value object, populate it with its attribute values, and pass it by value
to the client.
Clients usually require more than one value from an enterprise bean. To reduce the
number of remote calls and to avoid the associated overhead, it is best to use value
objects to transport the data from the enterprise bean to its client.
When an enterprise bean uses a value object, the client makes a single remote
method invocation to the enterprise bean to request the value object instead of
numerous remote method calls to get individual attribute values. The enterprise
bean then constructs a new value object instance, copies values into the object and
returns it to the client. The client receives the value object and can then invoke
accessor (or getter) methods on the value object to get the individual attribute
values from the value object. Or, the implementation of the value object may be
such that it makes all attributes public. Because the value object is passed by value
to the client, all calls to the value object instance are local calls instead of remote
method invocations.
Figure 8.5 shows the class diagram that represents the Value Object pattern in its
simplest form.
As shown in this class diagram, the value object is constructed on demand by the
enterprise bean and returned to the remote client. However, the Value Object
pattern can adopt various strategies, depending on requirements. The ¡°Strategies¡±
section explains these approaches.
Figure 8.6 contains the sequence diagram that shows the interactions for the Value
Object pattern.
This represents the client of the enterprise bean. The client can be an end-user
application, as in the case of a rich client application that has been designed to
directly access the enterprise beans. The client can be Business Delegates (see
¡°Business Delegate¡±) or a different BusinessObject
The BusinessObject represents a role in this pattern that can be fulfilled by a session
bean, an entity bean, or a Data Access Object (DAO). The BusinessObject is
responsible for creating the value object and returning it to the client upon request.
The BusinessObject may also receive data from the client in the form of a value
object and use that data to perform an update.
The ValueObject is an arbitrary serializable Java object referred to as a value object.
A value object class may provide a constructor that accepts all the required
attributes to create the value object. The constructor may accept all entity bean
attribute values that the value object is designed to hold. Typically, the members in
the value object are defined as public, thus eliminating the need for get and set
methods. If some protection is necessary, then the members could be defined as
protected or private, and methods are provided to get the values. By offering no
methods to set the values, a value object is protected from modification after its
creation. If only a few members are allowed to be modified to facilitate updates,
then methods to set the values can be provided. Thus, the value object creation
varies depending on an application's requirements. It is a design choice as to
whether the value object's attributes are private and accessed via getters and
setters, or all the attributes are made public.
The first two strategies discussed are applicable when the enterprise bean is
implemented as a session bean or as an entity bean. These strategies are called
Updatable Value Objects Strategy and Multiple Value Objects Strategy
The following strategies are applicable only when the BusinessObject is
implemented as an entity bean: Entity Inherits Value Object Strategy and Value
Object Factory Strategy
In this strategy, the value object not only carries the values from the
BusinessObject to the client, but also can carry the changes required by the client
back to the business object.
Figure 8.7 is a class diagram showing the relationship between the BusinessObject
and the value object.
The BusinessObject creates the value object. Recall that a client may need to access
the BusinessObject values not only to read them but to modify these values. For the
client to be able to modify the BusinessObject attribute values, the BusinessObject
must provide mutator methods. Mutator methods are also referred to as setters or
set methods.
Instead of providing fine-grained set methods for each attribute, which results in
network overhead, the BusinessObject can expose a coarse-grained setData()
method that accepts a value object as an argument. The value object passed to this
method holds the updated values from the client. Since the value object has to be
mutable, the value object class has to provide set methods for each attribute that
can be modified by the client. The set methods for the value object can include field
level validations and integrity checks as needed. Once the client obtains a value
object from the BusinessObject, the client invokes the necessary set methods
locally to change the attribute values. Such local changes do not impact the
BusinessObject until the setData() method is invoked.
The setData() method serializes the client's copy of the value object and sends it to
the BusinessObject. The BusinessObject receives the modified value object from the
client and merges the changes into its own attributes. The merging operation may
complicate the design of the BusinessObject and the value object; the
¡°Consequences¡± section discusses these potential complications. One strategy to
use here is to update only attributes that have changed, rather than updating all
attributes. A change flag placed in the value object can be used to determine the
attributes to update, rather than doing a direct comparison.
There is an impact on the design using the updatable value objects in terms of
update propagation, synchronization, and version control.
Figure 8.8 shows the sequence diagram for the entire update interaction.
Some application business objects can be very complex. In such cases, it is possible
that a single business object produces different value objects, depending on the
client request. There exists a one-to-many relationship between the business object
and the many value objects it can produce. In these circumstances, this strategy
may be considered.
For instance, when the business object is implemented as a session bean, typically
applying the Session Facade pattern, the bean may interact with numerous other
business components to provide the service. The session bean produces its value
object from different sources. Similarly, when the BusinessObject is implemented
as a coarse-grained entity bean, typically applying the Composite Entity pattern,
the entity bean will have complex relationships with a number of dependent objects.
In both these cases, it is good practice to provide mechanisms to produce value
objects that actually represent parts of the underlying coarse-grained components.
For example, in a trading application, a Composite Entity that represents a
customer portfolio can be a very coarse-grained complex component that can
produce value objects that provide data for parts of the portfolio, like customer
information, lists of stocks held, and so on. A similar example is a customer
manager session bean that provides services by interacting with a number of other
BusinessObjects and components to provide its service. The customer manager
bean can produce discrete small value objects, like customer address, contact list,
and so on, to represent parts of its model.
For both these scenarios, it is possible to adopt and apply the Multiple Value Objects
Strategy so that the business component, whether a session bean or an entity bean,
can create multiple types of value objects. In this strategy, the business entity
provides various methods to get different value objects. Each such method creates
and returns a different type of value object. The class diagram for this strategy is
shown Figure 8.9.
When a client needs a value object of type ValueObjectA, it invokes the entity's
getDataA() method requesting ValueObjectA. When it needs a value object of type
ValueObjectB, it invokes the entity's getDataB() method requesting ValueObjectB,
and so on. This is shown in the sequence diagram in Figure 8.10.
When the BusinessObject is implemented as an entity bean and the clients typically
need to access all the data from the entity bean, then the entity bean and the value
object both have the same attributes. In this case, since there exists a one-to-one
relationship between the entity bean and its value object, the entity bean may be
able to use inheritance to avoid code duplication.
In this strategy, the entity bean extends (or inherits from) the value object class.
The only assumption is that the entity bean and the value object share the same
attribute definitions. The class diagram for this strategy is shown in Figure 8.11.
The ValueObject implements one or more getData() methods as discussed in the
Multiple Value Objects Strategy. When the entity inherits this value object class, the
client invokes an inherited getData() method on the entity bean to obtain a value
object.
Thus, this strategy eliminates code duplication between the entity and the value
object. It also helps manage changes to the value object requirements by isolating
the change to the value object class and preventing the changes from affecting the
entity bean.
This strategy has a trade-off related to inheritance. If the value object is shared
through inheritance, then changes to this value object class will affect all its
subclasses, potentially mandating other changes to the hierarchy.
The sequence diagram in Figure 8.12 demonstrates this strategy.
The sample implementation for the Entity Inherits Value Object Strategy is shown in
Example 8.10 (ContactVO ¨C Value Object Class) and Example 8.11 (ContactEntity ¨C
Entity Bean Class).
The Entity Inherits Value Object Strategy can be further extended to support
multiple value objects for an entity bean by employing a value object factory to
create value objects on demand using reflection. This results in an even more
dynamic strategy for value object creation.
To achieve this, define a different interface for each type of value object that must
be returned. The entity bean implementation of value object superclass must
implement all these interfaces. Furthermore, you must create a separate
implementation class for each defined interface, as shown in the class diagram for
this strategy in Figure 8.13.
Once all interfaces have been defined and implemented, create a method in the
ValueObjectFactory that is passed two arguments:
? The entity bean instance for which a value object must be created.
? The interface that identifies the kind of value object to create.
The ValueObjectFactory can then instantiate an object of the correct class, set its
values, and return the newly created value object instance.
The sequence diagram for this strategy is shown in Figure 8.14.
The client requests the value object from the BusinessEntity. The BusinessEntity
passes the required value object's class to the ValueObjectFactory, which creates a
new value object of that given class. The ValueObjectFactory uses reflection to
dynamically obtain the class information for the value object class and construct a
new value object instance. Getting values from and setting values into the
BusinessEntity by the ValueObjectFactory is accomplished by using dynamic
invocation.
An example implementation for this strategy is shown in the ¡°Sample Code¡± section
for ¡°Implementing Value Object Factory Strategy¡±.
The benefits of applying the Value Object Factory Strategy are as follows:
There is less code to write in order to create value objects. The same value object
factory class can be reused by different enterprise beans. When a value object class
definition changes, the value object factory automatically handles this change
without any additional coding effort. This increases maintainability and is less error
prone to changes in value object definitions.
The Value Object Factory Strategy has the following consequences:
It is based on the fact that the enterprise bean implementation extends (inherits)
from the complete value object. The complete value object needs to implement all
the interfaces defined for different value objects that the entity bean needs to
supply. Naming conventions must be adhered to in order to make this strategy work.
Since reflection is used to dynamically inspect and construct value objects, there is
a slight performance loss in construction. However, when the overall
communication time is considered, such loss may be negligible in comparison.
There is a trade-off associated with this strategy. Its power and flexibility must be
weighed against the performance overhead associated with runtime reflection.
? Simplifies Entity Bean and Remote Interface
The entity bean provides a getData() method to get the value object
containing the attribute values. This may eliminate having multiple get
methods implemented in the bean and defined in the bean's remote
interface. Similarly, if the entity bean provides a setData() method to
update the entity bean attribute values in a single method call, it may
eliminate having multiple set methods implemented in the bean and defined
in the bean's remote interface.
? Transfers More Data in Fewer Remote Calls
Instead of multiple client calls over the network to the BusinessObject to get
attribute values, this solution provides a single method call. At the same
time, this one method call returns a greater amount of data to the client than
the individual accessor methods each returned. When considering this
pattern, you must consider the trade-off between fewer network calls versus
transmitting more data per call. Alternatively, you can provide both
individual attribute accessor methods (fine-grained get and set methods)
and value object methods (coarse-grained get and set methods). The
developer can choose the appropriate technique depending on the
requirement.
? Reduces Network Traffic
A value object transfers the values from the entity bean to the client in one
remote method call. The value object acts as a data carrier and reduces the
number of remote network method calls required to obtain the attribute
values from the entity beans. The reduced chattiness of the application
results in better network performance.
? Reduces Code Duplication
By using the Entity Inherits Value Object Strategy and the Value Object
Factory Strategy, it is possible to reduce or eliminate the duplication of code
between the entity and its value object. However, with the use of Value
Object Factory Strategy, there could be increased complexity in
implementation. There is also a runtime cost associated with this strategy
due to the use of dynamic reflection. In most cases, the Entity Inherits Value
Object Strategy may be sufficient to meet the needs.
? May Introduce Stale Value Objects
Adopting the Updatable Value Objects Strategy allows the client to perform
modifications on the local copy of the value object. Once the modifications
are completed, the client can invoke the entity's setData() method and
pass the modified value object to the entity. The entity receives the
modifications and merges the new (modified) values with its attributes.
However, there may be a problem with stale value objects. The entity
updates its values, but it is unaware of other clients that may have
previously requested the same value object. These clients may be holding in
their local cache value object instances that no longer reflect the current
copy of the entity's data. Because the entity is not aware of these clients, it
is not possible to propagate the update to the stale value objects held by
other clients.
? May Increase Complexity due to Synchronization and Version
Control
The entity merges modified values into its own stored values when it
receives a mutable value object from a client. However, the entity must
handle the situation where two or more clients simultaneously request
conflicting updates to the entity's values. Allowing such updates may result
in data conflicts.
Version control is one way of avoiding such conflict. As one of its attributes,
the entity can include a version number or a last-modified time stamp. The
version number or time stamp is copied over from the entity bean into the
value object. An update transaction can resolve conflicts using the time
stamp or version number attribute. If a client holding a stale value object
tries to update the entity, the entity can detect the stale version number or
time stamp in the value object and inform the client of this error condition.
The client then has to obtain the latest value object and retry the update. In
extreme cases this can result in client starvation¡ªthe client might never
accomplish its updates.
? Concurrent Access and Transactions
When two or more clients concurrently access the BusinessObject, the
container applies the transaction semantics of the EJB architecture. If, for an
Enterprise bean, the transaction isolation level is set to
TRANSACTION_SERIALIZED in the deployment descriptor, the container
provides the maximum protection to the transaction and ensures its integrity.
For example, suppose the workflow for the first transaction involves
obtaining a value object, then subsequently modifying the BusinessObject
attributes in the process. The second transaction, since it is isolated to
serialized transactions, will obtain the value object with the correct (most
recently updated) values. However, for transactions with lesser restrictions
than serialized, protection is less rigid, leading to inconsistencies in the value
objects obtained by competing accesses. In addition, problems related to
synchronization, stale value objects, and version control will have to be dealt
with.
Consider an example where a business object called Project is modeled and
implemented as an entity bean. The Project entity bean needs to send data to its
clients in a value object when the client invokes its getProjectData() method. The
value object class for this example, ProjectVO, is shown in Example 8.3
The sample code for the entity bean that uses this value object is shown in Example
8.4.
Example 8.4 can be extended to implement Updatable Value Objects Strategy. In
this case, the entity bean would provide a setProjectData() method to update the
entity bean by passing a value object that contains the data to be used to perform
the update. The sample code for this strategy is shown in Example 8.5.
Consider an example where a Resource entity bean is accessed by clients to request
different value objects. The first type of value object, ResourceVO, is used to
transfer data for a small set of attributes. The second type of value object,
ResourceDetailsVO, is used to transfer data for a larger set of attributes. The client
can use the former value object if it needs only the most basic data represented by
that value object, and can use the latter if it needs more detailed information. Note
that this strategy can be applied in producing two or more value objects that contain
different data, and not just subset-superset as shown here.
The sample code for the two value objects for this example are shown in Example
8.6 and Example 8.7. The sample code for the entity bean that produces these value
objects is shown in Example 8.8, and finally the entity bean client is shown in
Example 8.9.
Consider an example where an entity bean ContactEntity inherits all its properties
from a value object ContactVO. Example 8.10 shows the code sample for an
example value object ContactVO that illustrates this strategy.
The entity bean sample code relevant to this pattern strategy is shown in Example
8.11.
Example 8.12 demonstrates the Value Object Factory strategy. The entity bean
extends a complete value object called CustomerContactVO. The
CustomerContactVO value object implements two interfaces, Customer and Contact.
The CustomerVO value object implements Customer, and the ContactVO value
object implements Contact.
The entity bean code sample to obtain these three different value objects is shown
Example 8.13.
? Session Facade
The Session Facade, which is the business interface for clients of J2EE
applications, frequently uses value objects as an exchange mechanism with
participating entity beans. When the facade acts as a proxy to the underlying
business service, the value object obtained from the entity beans can be
passed to the client.
? Value Object Assembler
The Value Object Assembler is a pattern that builds composite value objects
from different data sources. The data sources are usually session beans or
entity beans that may be requested to provide their data to the Value Object
Assembler as value objects. These value objects are considered to be parts
of the composite object that the Value Object Assembler assembles.
? Value List Handler
The Value List Handler is another pattern that provides lists of value objects
constructed dynamically by accessing the persistent store at request time.
? Composite Entity
The Value Object pattern addresses the need of getting data from
BusinessObjects across tiers. This certainly is one aspect of design
considerations for entity beans. The Composite Entity pattern discusses
issues involved in designing coarse-grained entity beans. The Composite
Entity pattern addresses complex requirements and discusses other factors
and considerations involved in entity bean design.
##%%&&
Enterprise beans encapsulate business logic and business data and expose their
interfaces, and thus the complexity of the distributed services, to the client tier.
In a multitiered J2EE application environment, the following problems arise:
? Tight coupling, which leads to direct dependence between clients and
business objects;
? Too many method invocations between client and server, leading to network
performance problems;
? Lack of a uniform client access strategy, exposing business objects to
misuse.
A multitiered J2EE application has numerous server-side objects that are
implemented as enterprise beans. In addition, some other arbitrary objects may
provide services, data, or both. These objects are collectively referred to as
business objects, since they encapsulate business data and business logic.
J2EE applications implement business objects that provide processing services as
session beans. Coarse-grained business objects that represent an object view of
persistent storage and are shared by multiple users are usually implemented as
entity beans.
Application clients need access to business objects to fulfill their responsibilities and
to meet user requirements. Clients can directly interact with these business objects
because they expose their interfaces. When you expose business objects to the
client, the client must understand and be responsible for the business data object
relationships, and must be able to handle business process flow.
However, direct interaction between the client and the business objects leads to
tight coupling between the two, and such tight coupling makes the client directly
dependent on the implementation of the business objects. Direct dependence
means that the client must represent and implement the complex interactions
regarding business object lookups and creations, and must manage the
relationships between the participating business objects as well as understand the
responsibility of transaction demarcation.
As client requirements increase, the complexity of interaction between various
business objects increases. The client grows larger and more complex to fulfill these
requirements. The client becomes very susceptible to changes in the business
object layer; in addition, the client is unnecessarily exposed to the underlying
complexity of the system.
Tight coupling between objects also results when objects manage their relationship
within themselves. Often, it is not clear where the relationship is managed. This
leads to complex relationships between business objects and rigidity in the
application. Such lack of flexibility makes the application less manageable when
changes are required.
When accessing the enterprise beans, clients interact with remote objects. Network
performance problems may result if the client directly interacts with all the
participating business objects. When invoking enterprise beans, every client
invocation is potentially a remote method call. Each access to the business object is
relatively fine-grained. As the number of participants increases in a scenario, the
number of such remote method calls increases. As the number of remote method
calls increases, the chattiness between the client and the server-side business
objects increases. This may result in network performance degradation for the
application, because the high volume of remote method calls increases the amount
of interaction across the network layer.
A problem also arises when a client interacts directly with the business objects.
Since the business objects are directly exposed to the clients, there is no unified
strategy for accessing the business objects. Without such a uniform client access
strategy, the business objects are exposed to clients and may reduce consistent
usage.
? Provide a simpler interface to the clients by hiding all the complex
interactions between business components.
? Reduce the number of business objects that are exposed to the client across
the service layer over the network.
? Hide from the client the underlying interactions and interdependencies
between business components. This provides better manageability,
centralization of interactions (responsibility), greater flexibility, and greater
ability to cope with changes.
? Provide a uniform coarse-grained service layer to separate business object
implementation from business service abstraction.
? Avoid exposing the underlying business objects directly to the client to keep
tight coupling between the two tiers to a minimum.
Use a session bean as a facade to encapsulate the complexity of
interactions between the business objects participating in a workflow. The
Session Facade manages the business objects, and provides a uniform
coarse-grained service access layer to clients.
The Session Facade abstracts the underlying business object interactions and
provides a service layer that exposes only the required interfaces. Thus, it hides
from the client's view the complex interactions between the participants. The
Session Facade manages the interactions between the business data and business
service objects that participate in the workflow, and it encapsulates the business
logic associated with the requirements. Thus, the session bean (representing the
Session Facade) manages the relationships between business objects. The session
bean also manages the life cycle of these participants by creating, locating (looking
up), modifying, and deleting them as required by the workflow. In a complex
application, the Session Facade may delegate this lifestyle management to a
separate object. For example, to manage the lifestyle of participant session and
entity beans, the Session Facade may delegate that work to a Service Locator object
(see ¡°Service Locator¡± ).
It is important to examine the relationship between business objects. Some
relationships between business objects are transient, which means that the
relationship is applicable to only that interaction or scenario. Other relationships
may be more permanent. Transient relationships are best modeled as workflow in a
facade, where the facade manages the relationships between the business objects.
Permanent relationships between two business objects should be studied to
determine which business object (if not both objects) maintains the relationship.
So, how do you identify the Session Facades through studying
use cases? Mapping every use case to a Session Facade will
result in too many Session Facades. This defeats the intention
of having fewer coarse-grained session beans. Instead, as you
derive the Session Facades during your modeling, look to
consolidate them into fewer numbers of session beans based
on some logical partitioning.
For example, for a banking application, you may group the
interactions related to managing an account into a single
facade. The use cases Create New Account, Change Account
Information, View Account information, and so on all deal with
the coarse-grained entity object Account. Creating a session
bean facade for each use case is not recommended. Thus, the
functions required to support these related use cases could be
grouped into a single Session Facade called
AccountSessionFacade.
In this case, the Session Facade will become a highly
coarse-grained controller with high-level methods that can
facilitate each interaction (that is, createNewAccount,
changeAccount, getAccount). Therefore, we recommend that
you design Session Facades to aggregate a group of the
related interactions into a single Session Facade. This results in
fewer Session Facades for the application, and leverages the
benefits of the Session Facade pattern.
Figure 8.15 shows the class diagram representing the Session Facade pattern.
Figure 8.16 contains the sequence diagram that shows the interactions of a Session
Facade with two entity beans, one session bean, and a DAO, all acting as
participants in fulfilling the request from the client.
This represents the client of the Session Facade, which needs access to the business
service. This client can be another session bean (Session Facade) in the same
business tier or a business delegate (see ¡°Business Delegate¡±) in another tier
The SessionFacade is implemented as a session bean. The SessionFacade manages
the relationships between numerous BusinessObjects and provides a higher level
abstraction to the client. The SessionFacade offers coarse-grained access to the
participating BusinessObject represented by the Invoke invocation to the session
bean.
The BusinessObject is a role object that facilitates applying different strategies,
such as session beans entity beans and a DAO (see the next section, ¡°Strategies¡±).
A BusinessObject provides data and/or some service in the class diagram. The
SessionFacade interacts with multiple BusinessObject instances to provide the
service.
The Session Facade is a business-tier controller object that controls the interactions
between the client and the participant business data and business service objects.
In a complex application, there may be numerous Session Facades that can
intermediate between the client and these objects. You can identify where a Session
Facade might be useful by studying the client requirements and interactions
typically documented in use cases and scenarios. This analysis enables you to
identify a controller layer¡ªcomposed of Session Facades¡ªthat can act as facades
for these scenarios.
This section explains different strategies for implementing a Session Facade.
When implementing the Session Facade, you must first decide whether the facade
session bean is a stateful or a stateless session bean. Base this decision on the
business process that the Session Facade is modeling.
A business process that needs only one method call to complete the service is a
nonconversational business process. Such processes are suitably implemented
using a stateless session bean.
A careful study of the use cases and scenarios enables you to determine the Session
Facade definitions. If the use case is nonconversational, then the client initiates the
use case, using a single method in the Session Facade. When the method completes,
the use case completes too. There is no need to save the conversational state
between one method invocation and the next. In this scenario, the Session Facade
can be implemented as a stateless session bean.
A business process that needs multiple method calls to complete the service is a
conversational business process. The conversational state must be saved between
each client method invocation. In this scenario, a stateful session bean may be a
more suitable approach for implementing the Session Facade.
In both the Stateless Session Facade and the Stateful Session Facade strategies,
the business object's role can be fulfilled in different ways, as explained next.
You can implement a business object as a session bean, entity bean, DAO, or
regular Java object. The following strategies discuss each of these choices.
The business object can be implemented as a session bean. The session bean
typically provides a business service and, in some cases, it may also provide
business data. When such a session bean needs access to data, it may use a DAO to
manipulate the data. The Session Facade can wrap one or more such
service-oriented or data-oriented session beans acting as business objects.
Representing the business object by an entity bean is the most common use of the
Session Facade. When multiple entity beans participate in the use case, it is not
necessary to expose all the entity beans to the clients. Instead, the Session Facade
can wrap these entity beans and provide a coarse-grained method to perform the
required business function, thus hiding the complexity of entity bean interactions.
The Session Facade can directly use one or more DAOs to represent the business
data. This is done when the application is so simple that it requires no entity beans,
or when the application's architecture is based only on session beans and does not
use entity beans. Using DAOs inside session beans partially simulates the persistent
nature of entity beans.
The application might need the services provided by an arbitrary Java object (that is,
an object that is not an enterprise bean or a DAO, though a DAO can be viewed as
a type of arbitrary Java object). In such cases, the Session Facade accesses this
arbitrary Java object to provide the necessary functionality.
? Introduces Business-Tier Controller Layer
Session Facades can represent a control layer between clients and the
business tier, as identified through analysis modeling. A Session Facade
encompasses the interactions between the client and the business
components. In a sophisticated application, you can identify numerous
Session Facades that can intermediate between the client and the
participating business-tier objects. For simpler applications, one might feel
that a Session Facade is not adding much value, as it may act to mostly
proxy the client requests to a single business component. However, as
applications grow more complex over time, using a Session Facade up front
will yield benefit at a later stage.
? Exposes Uniform Interface
The underlying interactions between the business components can be very
complex. A Session Facade pattern abstracts this complexity and presents
the client a simpler interface that is easy to understand and to use. By
applying a Session Facade, you can design a service layer that exposes
simpler interfaces to the system as a whole. Thus a facade provides a
uniform coarse-grained access layer to all types of clients and can protect
and hide the underlying participant business components.
? Reduces Coupling, Increases Manageability
Using a Session Facade decouples the business objects from the clients, thus
reducing tight coupling and the client's dependency on the business objects.
It is best to use a Session Facade to manage workflow among business
objects, rather than making the business objects aware of each other. A
business object should only be responsible for its own (data and logic)
management. Inter-business object interactions can be abstracted into a
workflow in a facade. This provides better manageability, centralization of
interactions (responsibility and workflow), greater flexibility, and greater
ability to cope with changes.
Separating workflow into a Session Facade eliminates the direct dependency
of the client on the participant objects and promotes design flexibility.
Although changes to participants may require changes in the Session Facade,
centralizing the workflow in the facade makes such changes more
manageable. You change only the Session Facade rather than having to
change all the clients. Client code is also simpler because it now delegates
the workflow responsibility to the Session Facade. The client no longer
manages the complex workflow interactions between business objects, nor
is the client aware of interdependencies between business objects.
? Improves Performance, Reduces Fine-Grained Methods
The Session Facade also impacts performance. The Session Facade reduces
network overhead between the client and the server because its use
eliminates the direct interaction between the client and the business data
and business service objects. Instead, all interactions are routed via the
Session Facade in a coarse-grained manner. The Session Facade and its
participants are closer to each other, making it more efficient for the facade
to manage interactions between the participant objects. All data transfer and
method invocations from the facade to the participants are presumably on a
relatively high-speed network. The network performance can be further
tuned to provide maximum throughput by applying the Value Object pattern
for the participant objects where applicable.
? Provides Coarse-Grained Access
A Session Facade is meant to be a highly coarse-grained abstraction of the
workflow. Thus, it is not desirable to have one Session Facade per entity
bean interaction, which would represent a fine-grained abstraction rather
than a coarse-grained one. Analyze the interaction between the client and
the application services, using use cases and scenarios to determine the
coarseness of the facade. Determine the optimal granularity of the Session
Facade for the application by partitioning the application into logical
subsystems and providing a Session Facade for each subsystem. However,
providing a single facade for the entire system can result in a very large
Session Facade whose numerous methods make it inefficient. A single
facade may be sufficient for very simple applications that do not warrant
subsystems.
? Centralizes Security Management
Security policies for the application can be managed at the Session Facade
level, since this is the tier presented to the clients. Because of the Session
Facade's coarse-grained access, it is easier and more manageable to define
security policies at this level rather than at the participating business
component level. Business components offer fine-grained control points. It is
easier to manage security for Session Facades that provide coarse-grained
access, because there are relatively fewer coarse-grained methods to be
securely managed.
? Centralizes Transaction Control
Because the Session Facade represents the workflow for the use cases, it is
more logical to apply transaction management at the Session Facade level.
Centralized transaction control has advantages similar to centralized
security. The facade offers a central place for managing and defining
transaction control in a coarse-grained fashion. It is much more work to do
transaction management individually on participant business components,
especially since they are more fine-grained than the facade. Also, not using
a Session Facade, but rather having the client access the enterprise beans
directly, puts the transaction demarcation burden on the client and can
produce unwanted results.
? Exposes Fewer Remote Interfaces to Clients
Clients that interact directly with the business data and business service
objects cause an increase in chattiness between the client and the server.
Increased chattiness may degrade network performance. All access to the
business object must be via the higher level of abstraction represented by a
facade. Since the facade presents a coarse-grained access mechanism to the
business components, this reduces the number of business components that
are exposed to the client. Thereby, the scope for application performance
degradation is reduced due to the limited number of interactions between
the clients and the Session Facade when compared to direct interaction by
the client to the individual business components.
Consider a Professional Services Application (PSA), where the workflow related to
entity beans (such as Project, Resource) is encapsulated in
ProjectResourceManagerSession, implemented using the Session Facade pattern.
Example 8.15 shows the interaction with Resource and Project entity beans, as well
as other business components, like Value List Handlers (see ¡°Value List Handler¡±)
and Value Object Assemblers (see ¡°Value Object Assembler¡± ).
The remote interface for the Session Facade is listed in Example 8.16.
The Home interface for the Session Facade is shown in Example 8.17.
? Facade [GoF]
The Session Facade is based on the Facade Design pattern.
? Data Access Object
One of the strategies for the business component in the Session Facade
pattern is to use the DAO. This can be the case in simpler applications
designed using session beans and DAOs instead of entity beans.
? Service Locator
The Session Facade is a coarse-grained object that allows encapsulation of
the workflow by managing business data and business service objects
interactions. Business data objects can be entity beans or DAOs, and the
business service objects can be session beans and other objects that provide
service. The Session Facade can use the Service Locator pattern to reduce
the code complexity and to exploit the benefits offered by the Service
Locator.
? Business Delegate
The Session Facade is used by the Business Delegate when the client
requests access to business services. The Business Delegate proxies or
adapts the client request to a Session Facade that provides the requested
service.
? Broker [POSA1]
The Session Facade performs the role of a broker to decouple the entity
beans from their clients.
##%%&&
Entity beans are not intended to represent every persistent object in the object
model. Entity beans are better suited for coarse-grained persistent business
objects.
In a J2EE application, clients (applications, JSPs, servlets, JavaBeans) access entity
beans via their remote interfaces. Thus, every client invocation potentially routes
through network stubs and skeletons, even if the client and the enterprise bean are
in the same JVM, OS, or machine. When entity beans are fine-grained objects,
clients tend to invoke more individual entity bean methods, resulting in high
network overhead.
Entity beans represent distributed persistent business objects. Whether developing
or migrating an application to the J2EE platform, object granularity is very
important when deciding what to implement as an entity bean. Entity beans should
represent coarse-grained business objects, such as those that provide complex
behavior beyond simply getting and setting field values. These coarse-grained
objects typically have dependent objects. A dependent object is an object that has
no real domain meaning when not associated with its coarse-grained parent.
A recurring problem is the direct mapping of the object model to an EJB model
(specifically entity beans). This creates a relationship between the entity bean
objects without consideration of coarse-grained versus fine-grained (or dependent)
objects. Determining what to make coarse-grained versus fine-grained is typically
difficult and can best be done via modeling relationships in Unified Modeling
Language (UML) models.
There are a number of areas impacted by the fine-grained entity bean design
approach:
? Entity Relationships¡ª Directly mapping an object model to an EJB model
does not take into account the impact of relationships between the objects.
The inter-object relationships are directly transformed into inter-entity bean
relationships. As a result, an entity bean might contain or hold a remote
reference to another entity bean. However, maintaining remote references
to distributed objects involves different techniques and semantics than
maintaining references to local objects. Besides increasing the complexity of
the code, it reduces flexibility, because the entity bean must change if there
are any changes in its relationships.
Also, there is no guarantee as to the validity of the entity bean references to
other entity beans over time. Such references are established dynamically
using the entity's home object and the primary key for that entity bean
instance. This implies a high maintenance overhead of reference validity
checking for each such entity-bean-to-entity-bean reference.
? Manageability¡ª Implementing fine-grained objects as entity beans
results in a large number of entity beans in the system. An entity bean is
defined using several classes. For each entity bean component, the
developer must provide classes for the home interface, the remote interface,
the bean implementation, and the primary key.
In addition, the container may generate classes to support the entity bean
implementation. When the bean is created, these classes are realized as real
objects in the container. In short, the container creates a number of objects
to support each entity bean instance. Large numbers of entity beans result in
more classes and code to maintain for the development team. It also results
in a large number of objects in the container. This can negatively impact the
application performance.
? Network Performance¡ª Fine-grained entity beans potentially have more
inter-entity bean relationships. Entity beans are distributed objects. When
one entity bean invokes a method on another entity bean, the call is
potentially treated as a remote call by the container, even if both entity
beans are in the same container or JVM. If the number of
entity-bean-to-entity-bean relationships increases, then this decreases
system scalability due to heavy network overhead.
? Database Schema Dependency¡ª When the entity beans are
fine-grained, each entity bean instance usually represents a single row in a
database. This is not a proper application of the entity bean design, since
entity beans are more suitable for coarse-grained components. Fine-grained
entity bean implementation typically is a direct representation of the
underlying database schema in the entity bean design. When clients use
these fine-grained entity beans, they are essentially operating at the row
level in the database, since each entity bean is effectively a single row.
Because the entity bean directly models a single database row, the clients
become dependent on the database schema. When the schema changes, the
entity bean definitions must change as well. Further, since the clients are
operating at the same granularity, they must observe and react to this
change. This schema dependency causes a loss of flexibility and increases
the maintenance overhead whenever schema changes are required.
? Object Granularity (Coarse-Grained versus Fine-Grained)¡ª Object
granularity impacts data transfer between the enterprise bean and the client.
In most applications, clients typically need a larger chunk of data than one or
two rows from a table. In such a case, implementing each of these
fine-grained objects as an entity bean means that the client would have to
manage the relationships between all these fine-grained objects. Depending
on the data requirements, the client might have to perform many lookups of
a number of entity beans to obtain the required information.
? Entity beans are best implemented as coarse-grained objects due to the high
overhead associated with each entity bean. Each entity bean is implemented
using several objects, such as EJB home object, remote object, bean
implementation, and primary key, and each is managed by the container
services.
? Applications that directly map relational database schema to entity beans
(where each row in a table is represented by an entity bean instance) tend to
have a large number of fine-grained entity beans. It is desirable to keep the
entity beans coarse-grained and reduce the number of entity beans in the
application.
? Direct mapping of object model to EJB model yields fine-grained entity beans.
Fine-grained entity beans usually map to the database schema. This
entity-to-database row mapping causes problems related to performance,
manageability, security, and transaction handling. Relationships between
tables are implemented as relationships between entity beans, which means
that entity beans hold references to other entity beans to implement the
fine-grained relationships. It is very expensive to manage inter-entity bean
relationships, because these relationships must be established dynamically,
using the entity home objects and the enterprise beans' primary keys.
? Clients do not need to know the implementation of the database schema to
use and support the entity beans. With fine-grained entity beans, the
mapping is usually done so that each entity bean instance maps to a single
row in the database. This fine-grained mapping creates a dependency
between the client and the underlying database schema, since the clients
deal with the fine-grained beans and they are essentially a direct
representation of the underlying schema. This results in tight coupling
between the database schema and entity beans. A change to the schema
causes a corresponding change to the entity bean, and in addition requires a
corresponding change to the clients.
? There is an increase in chattiness of applications due to intercommunication
among fine-grained entity beans. Excessive inter-entity bean
communication often leads to a performance bottleneck. Every method call
to the entity bean is made via the network layer, even if the caller is in the
same address space as the called bean (that is, both the client, or caller
entity bean, and the called entity bean are in the same container). While
some container vendors optimize for this scenario, the developer cannot rely
on this optimization in all containers.
? Additional chattiness can be observed between the client and the entity
beans because the client may have to communicate with many fine-grained
entity beans to fulfill a requirement. It is desirable to reduce the
communication between or among entity beans and to reduce the chattiness
between the client and the entity bean layer.
Use Composite Entity to model, represent, and manage a set of interrelated
persistent objects rather than representing them as individual
fine-grained entity beans. A Composite Entity bean represents a graph of
objects.
In order to understand this solution, let us first define what is meant by persistent
objects and discuss their relationships.
A persistent object is an object that is stored in some type of data store. Multiple
clients usually share persistent objects. Persistent objects can be classified into two
types: coarse-grained objects and dependent objects.
A coarse-grained object is self-sufficient. It has its own life cycle and manages its
relationships to other objects. Each coarse-grained object may reference or contain
one or more other objects. The coarse-grained object usually manages the lifestyles
of these objects. Hence, these objects are called dependent objects. A dependent
object can be a simple self-contained object or may in turn contain other dependent
objects.
The life cycle of a dependent object is tightly coupled to the life cycle of the
coarse-grained object. A client may only indirectly access a dependent object
through the coarse-grained object. That is, dependent objects are not directly
exposed to clients because their parent (coarse-grained) object manages them.
Dependent objects cannot exist by themselves. Instead, they always need to have
their coarse-grained (or parent) object to justify their existence.
Typically, you can view the relationship between a coarse-grained object and its
dependent objects as a tree. The coarse-grained object is the root of the tree (the
root node). Each dependent object can be a standalone dependent object (a leaf
node) that is a child of the coarse-grained object. Or, the dependent object can have
parent-child relationships with other dependent objects, in which case it is
considered a branch node.
A Composite Entity bean can represent a coarse-grained object and all its related
dependent objects. Aggregation combines interrelated persistent objects into a
single entity bean, thus drastically reducing the number of entity beans required by
the application. This leads to a highly coarse-grained entity bean that can better
leverage the benefits of entity beans than can fine-grained entity beans.
Without the Composite Entity approach, there is a tendency to view each
coarse-grained and dependent object as a separate entity bean, leading to a large
number of entity beans.
While there are many strategies in implementing the Composite Entity pattern, the
first one we discuss is represented by the class diagram in Figure 8.17. Here the
Composite Entity contains the coarse-grained object, and the coarse-grained object
contains dependent objects.
The sequence diagram in Figure 8.18 shows the interactions for this pattern.
CompositeEntity is the coarse-grained entity bean. The CompositeEntity may be the
coarse-grained object, or it may hold a reference to the coarse-grained object. The
¡°Strategies¡± section explains the different implementation strategies for a
Composite Entity.
A coarse-grained object is an object that has its own life cycle and manages its own
relationships to other objects. A coarse-grained object can be a Java object
contained in the Composite Entity. Or, the Composite Entity itself can be the
coarse-grained object that holds dependent objects. These strategies are explained
in the ¡°Strategies¡± section.
A dependent object is an object that depends on the coarse-grained object and has
its life cycle managed by the coarse-grained object. A dependent object can contain
other dependent objects; thus there may be a tree of objects within the Composite
Entity.
This section explains different strategies for implementing a Composite Entity. The
strategies consider possible alternatives and options for persistent objects
(coarse-grained and dependent) and the use of value objects.
In this strategy, the Composite Entity holds or contains the coarse-grained object.
The coarse-grained object continues to have relationships with its dependent
objects. The structure section of this pattern describes this as the main strategy.
In this strategy, the Composite Entity itself is the coarse-grained object and it has
the coarse-grained object's attributes and methods. The dependent objects are
attributes of the Composite Entity. Since the Composite Entity is the coarse-grained
object, the entity bean expresses and manages all relationships between the
coarse-grained object and the dependent objects.
Figure 8.19 is the class diagram for this strategy.
The sequence diagram for this strategy is shown in Figure 8.20.
A Composite Entity can be composed of many levels of dependent objects in its tree
of objects. Loading all the dependent objects when the Composite Entity's ejbLoad()
method is called by the EJB Container may take considerable time and resources.
One way to optimize this is by using a lazy loading strategy for loading the
dependent objects. When the ejbLoad() method is called, at first only load those
dependent objects that are most crucial to the Composite Entity clients.
Subsequently, when the clients access a dependent object that has not yet been
loaded from the database, the Composite Entity can perform a load on demand.
Thus, if some dependent objects are not used, they are not loaded on initialization.
However, when the clients subsequently need those dependent objects, they get
loaded at that time. Once a dependent object is loaded, subsequent container calls
to the ejbLoad() method must include those dependent objects for reload to
synchronize the changes with the persistent store.
A common problem with bean-managed persistence occurs when persisting the
complete object graph during an ejbStore() operation. Since the EJB Container
has no way of knowing what data has changed in the entity bean and its dependent
objects, it puts the burden on the developer to determine what and how to persist
the data. Some EJB containers provide a feature to identify what objects in
Composite Entity's graph need to be stored due to a prior update. This may be done
by having the developers implement a special method in the dependent objects,
such as isDirty(), that is called by the container to check if the object has been
updated since the previous ejbStore() operation.
A generic solution may be to use an interface, DirtyMarker, as shown in the class
diagram in Figure 8.21. The idea is to have dependent objects implement the
DirtyMarker interface to let the caller (typically the ejbStore() method) know if the
state of the dependent object has changed. This way, the caller can choose to obtain
the data for subsequent storage.
Figure 8.22 contains a sequence diagram showing an example interaction for this
strategy.
The client performs an update to the Composite Entity, which results in a change to
DependentObject3. DependentObject3 is accessed via its parent DependentObject2.
The Composite Entity is the parent of DependentObject2. When this update is
performed, the setDirty() method is invoked in the DependentObject3.
Subsequently, when the container invokes the ejbStore() method on this
Composite Entity instance, the ejbStore() method can check which dependent
objects have gone dirty and selectively save those changes to the database. The
dirty marks are reset once the store is successful.
The DirtyMarker interface can also include methods that can recognize other the
persistence status of the dependent object. For example, if a new dependent object
is included into the Composite Entity, the ejbStore() method should be able to
recognize what operation to use¡ªin this case, the dependent object is not dirty, but
is a new object. By extending the DirtyMarker interface to include a method called
isNew(), the ejbStore() method can invoke an insert operation instead of an
update operation. Similarly, by including a method called isDeleted(), the
ejbStore() method can invoke delete operation as required.
In cases where ejbStore() is invoked with no intermediate updates to the
Composite Entity, none of the dependent objects have been updated.
This strategy avoids the huge overhead of having to persist the entire dependent
objects graph to the database whenever the ejbStore() method is invoked by the
container.
The EJB 2.0 specification addresses the Lazy Loading strategy and the Store
Optimization strategy. The 2.0 specification is in final draft at the time of this writing.
However, it is possible to use these strategies in pre-EJB 2.0 implementations.
Please follow the EJB 2.0 developments to understand how these strategies will be
finalized in the specification.
With a Composite Entity, a client can obtain all required information with just one
remote method call. Because the Composite Entity either implements or holds the
coarse-grained object and the hierarchy (or tree) of dependent objects, it can create
the required value object and return it to the client by applying the Value Object
pattern (see ¡°Value Object¡±). The sequence diagram for this strategy is shown in
Figure 8.23.
The value object can be a simple object or a composite object that has subobjects (a
graph), depending on the data requested by the client. The value object is
serializable and it is passed by value to the client. The value object functions only as
a data transfer object; it has no responsibility with respect to security, transaction,
and business logic. The value object packages all information into one object,
obtaining the information with one remote call rather than multiple remote calls.
Once the client receives the value object, all further calls from the client to the value
object are local to the client.
This discussion points to how the entity can package all its data into a composite
value object and return it to the client. However, this strategy also allows the entity
bean to return only the required data to the client. If the client needs data only from
a subset of dependent objects, then the composite value object returned can
contain data derived from only those required parts and not from all the dependent
objects. This would be an application of the Multiple Value Objects Strategy from the
Value Object pattern (see ¡°Value Object¡±).
? Eliminates Inter-Entity Relationships
Using the Composite Entity pattern, the dependent objects are composed
into a single entity bean, eliminating all inter-entity-bean relationships. This
pattern provides a central place to manage both relationships and object
hierarchy.
? Improves Manageability by Reducing Entity Beans
As discussed, implementing persistent objects as fine-grained entity beans
results in a large number of classes that need to be developed and
maintained. Using a Composite Entity reduces the number of EJB classes and
code, and makes maintenance easier. It improves the manageability of the
application by having fewer coarse-grained components instead of many
more fine-grained components.
? Improves Network Performance
Aggregation of the dependent objects improves overall performance.
Aggregation eliminates all fine-grained communications between dependent
objects across the network. If each dependent object were designed as a
fine-grained entity bean, a huge network overhead would result due to
inter-entity bean communications.
? Reduces Database Schema Dependency
When the Composite Entity pattern is used, it results in coarse-grained
entity bean implementations. The database schema is hidden from the
clients, since the mapping of the entity bean to the schema is internal to the
coarse-grained entity bean. Changes to the database schema may require
changes to the Composite Entity beans. However, the clients are not affected
since the Composite Entity beans do not expose the schema to the external
world.
? Increases Object Granularity
With a Composite Entity, the client typically looks up a single entity bean
instead of a large number of fine-grained entity beans. The client requests
the Composite Entity for data. The Composite Entity can create a composite
value object that contains all the data from the entity bean and return the
value object to the client in a single remote method call. This reduces the
chattiness between the client and the business tier.
? Facilitates Composite Value Object Creation
By using this strategy, chattiness of the communication between the client
and the entity bean is reduced, since the Composite Entity bean can return a
composite value object by providing a mechanism to send serialized value
objects from the Composite Entity bean. Although a value object returns all
data in one remote call, the amount of data returned with this one call is
much larger than the amount of data returned by separate remote calls to
obtain individual entity bean properties. This trade-off works well when the
goal is to avoid repeated remote calls and multiple lookups.
? Overhead of Multi-level Dependent Object Graphs
If the dependent objects graph managed by the Composite Entity has many
levels, then the overhead of loading and storing the dependent objects
increases. This can be reduced by using the optimization strategies for load
and store, but then there may be an overhead associated with checking the
dirty objects to store and loading the required objects.
Consider a Professional Service Automation application (PSA) where a Resource
business object is implemented using the Aggregate Entity pattern. The Resource
represents the employee resource that is assigned to projects. Each Resource
object can have different dependent objects as follows:
? BlockOutTime¡ª This dependent object represents the time period the
Resource is unavailable for reasons such as training, vacation, timeoffs, etc.
Since each resource can have multiple blocked out times, the
Resource-to-BlockOutTime relationship is a one-to-many relationship.
? SkillSet¡ª This dependent object represents the Skill that a Resource
possesses. Since each resource can have multiple skills, the
Resource-to-SkillSet relationship is a one-to-many relationship.
The pattern for the Resource business object is implemented as a Composite Entity
(ResourceEntity), as shown in Example 8.18. The one-to-many relationship with its
dependent objects (BlockOutTime and SkillSet objects) are implemented using
collections.
When the Composite Entity is first loaded in the ejbLoad() method by the container,
let us assume that only the resource data is to be loaded. This includes the
attributes listed in the ResourceEntity bean, excluding the dependent object
collections. The dependent objects can then be loaded only if the client invokes a
business method that needs these dependent objects to be loaded. Subsequently,
the ejbLoad() needs to keep track of the dependent objects loaded in this manner
and include them for reloading.
The relevant methods from the ResourceEntity class are shown in Example 8.19.
To use the Store Optimization strategy, the dependent objects need to have
implemented the DirtyMarker interface, as shown in Example 8.20. The ejbStore()
method to optimize using this strategy is listed in Example 8.21.
Now consider the requirement where the client needs to obtain all the data from the
ResourceEntity, and not just one part. This can be done using the Composite Value
Object Strategy, as shown in Example 8.22.
The ResourceEntity provides a getResourceDetailsData() method to return the
ResourceCompositeVO composite value object, as shown in Example 8.23.
? Value Object
The Composite Entity pattern uses the Value Object pattern for creating the
value object and returning it to the client. The Value Object pattern is used to
serialize the coarse-grained and dependent objects tree, or part of the tree,
as required.
? Session Facade
If dependent objects tend to be entity beans rather than the arbitrary Java
objects, try to use the Session Facade pattern to manage the
inter-entity-bean relationships.
? Value Object Assembler
When it comes to obtaining a composite value object from the Composite
Entity (see the ¡°Facilitates Composite Value Object Creation¡± under the
¡°Consequences¡± section), this pattern is similar to the Value Object
Assembler pattern. However, in this case, the data sources for all the value
objects in the composite are parts of the Composite Entity itself, whereas for
the Value Object Assembler, the data sources can be different entity beans,
session beans, DAOs, Java objects, and so on.
##%%&&
In a J2EE application, the server-side business components are implemented using
session beans, entity beans, DAOs, and so forth. Application clients frequently need
to access data that is composed from multiple objects.
Application clients typically require the data for the model or parts of the model to
present to the user or to use for an intermediate processing step before providing
some service. The application model is an abstraction of the business data and
business logic implemented on the server side as business components. A model
may be expressed as a collection of objects put together in a structured manner
(tree or graph). In a J2EE application, the model is a distributed collection of objects
such as session beans, entity beans, or DAOs and other objects. For a client to
obtain the data for the model, such as to display to the user or to perform some
processing, it must access individually each distributed object that defines the
model. This approach has several drawbacks:
? Because the client must access each distributed component individually,
there is a tight coupling between the client and the distributed components
of the model over the network
? The client accesses the distributed components via the network layer, and
this can lead to performance degradation if the model is complex with
numerous distributed components. Network and client performance
degradation occur when a number of distributed business components
implement the application model and the client directly interacts with these
components to obtain model data from that component. Each such access
results in a remote method call that introduces network overhead and
increases the chattiness between the client and the business tier.
? The client must reconstruct the model after obtaining the model's parts from
the distributed components. The client therefore needs to have the
necessary business logic to construct the model. If the model construction is
complex and numerous objects are involved in its definition, then there may
be an additional performance overhead on the client due to the construction
process. In addition, the client must contain the business logic to manage
the relationships between the components, which results in a more complex,
larger client. When the client constructs the application model, the
construction happens on the client side. Complex model construction can
result in a significant performance overhead on the client side for clients with
limited resources.
? Because the client is tightly coupled to the model, changes to the model
require changes to the client. Furthermore, if there are different types of
clients, it is more difficult to manage the changes across all client types.
When there is tight coupling between the client and model implementation,
which occurs when the client has direct knowledge of the model and
manages the business component relationships, then changes to the model
necessitate changes to the client. There is the further problem of code
duplication for model access, which occurs when an application has many
types of clients. This duplication makes client (code) management difficult
when the model changes.
? Separation of business logic is required between the client and the
server-side components.
? Because the model consists of distributed components, access to each
component is associated with a network overhead. It is desirable to minimize
the number of remote method calls over the network.
? The client typically needs only to obtain the model to present it to the user.
If the client must interact with multiple components to construct the model
on the fly, the chattiness between the client and the application increases.
Such chattiness may reduce the network performance.
? Even if the client wants to perform an update, it usually updates only certain
parts of the model and not the entire model.
? Clients do not need to be aware of the intricacies and dependencies in the
model implementation. It is desirable to have loose coupling between the
clients and the business components that implement the application model.
? Clients do not otherwise need to have the additional business logic required
to construct the model from various business components.
Use a Value Object Assembler to build the required model or submodel. The
Value Object Assembler uses value objects to retrieve data from various
business objects and other objects that define the model or part of the
model.
The Value Object Assembler constructs a composite value object that represents
data from different business components. The value object caries the data for the
model to the client in a single method call. Since the model data can be complex, it
is recommended that this value object be immutable. That is, the client obtains such
value objects with the sole purpose of using them for presentation and processing in
a read-only manner. Clients are not allowed to make changes to the value objects.
When the client needs the model data, and if the model is represented by a single
coarse-grained component (such as a Composite Entity), then the process of
obtaining the model data is simple. The client simply requests the coarse-grained
component for its composite value object. However, most real-world applications
have a model composed of a combination of many coarse-grained and fine-grained
components. In this case, the client must interact with numerous such business
components to obtain all the data necessary to represent the model. The immediate
drawbacks of this approach can be seen in that the clients become tightly coupled to
the model implementation (model elements) and that the clients tend to make
numerous remote method invocations to obtain the data from each individual
component.
In some cases, a single coarse-grained component provides the model or parts of
the model as a single value object (simple or composite). However, when multiple
components represent the model, a single value object (simple or composite) may
not represent the entire model. To represent the model, it is necessary to obtain
value objects from various components and assemble them into a new composite
value object. The server, not the client, should perform such ¡°on-the-fly¡±
construction of the model.
Figure 8.27 shows the class diagram representing the relationships for the Value
Object Assembler pattern.
The sequence diagram in Figure 8.28 shows the interaction between the various
participants in the Value Object Assembler pattern.
The ValueObjectAssembler is the main class of this pattern. The
ValueObjectAssembler constructs a new value object based on the requirements of
the application when the client requests a composite value object. The
ValueObjectAssembler then locates the required BusinessObject instances to
retrieve data to build the composite value object. BusinessObjects are business-tier
components such as entity beans and session beans, DAOs, and so forth.
If the ValueObjectAssembler is implemented as an arbitrary Java object, then the
client is typically a Session Facade that provides the controller layer to the business
tier. If the ValueObjectAssembler is implemented as a session bean, then the client
can be a Session Facade or a Business Delegate.
The BusinessObject participates in the construction of the new value object by
providing the required data to the ValueObjectAssembler. Therefore, the
BusinessObject is a role that can be fulfilled by a session bean, an entity bean, a
DAO, or a regular Java object.
The ValueObject is a composite value object that is constructed by the
ValueObjectAssembler and returned to the client. This represents the complex data
from various components that define the application model.
BusinessObject is a role that can be fulfilled by a session bean, entity bean, or DAO.
When the assembler needs to obtain data directly from the persistent storage to
build the value object, it can use a DAO. This is shown as the DataAccessObject
object in the diagrams.
This section explains different strategies for implementing a Value Object Assembler
pattern.
The ValueObjectAssembler can be an arbitrary Java object and need not be an
enterprise bean. In such implementations, a session bean usually fronts the
ValueObjectAssembler. This session bean is typically a Session Facade that
performs its other duties related to providing business services. The
ValueObjectAssembler runs in the business tier, regardless of the implementation
strategies. The motivation for this is to prevent the remote invocations from the
ValueObjectAssembler to the source objects from crossing the tier.
This strategy implements the ValueObjectAssembler as a session bean (as shown in
the class diagram). If a session bean implementation is preferred to provide the
ValueObjectAssembler as a business service, it is typically implemented as a
stateless session bean. The business components that make up the application
model are constantly involved in transactions with various clients. As a result, when
a ValueObjectAssembler constructs a new composite value object from various
business components, it produces a snapshot of the model at the time of
construction. The model could change immediately thereafter if another client
changes one or more business components, effectively changing the business
application model.
Therefore, implementing ValueObjectAssembler as a stateful session bean provides
no benefits over implementing it as a stateless session bean, as preserving the state
of the composite model data value when the underlying model is changing is futile.
If the underlying model changes, it causes the value object held by the assembler to
become stale. The ValueObjectAssembler, when next asked for the value object,
either returns a stale state or reconstructs the value object to obtain the most
recent snapshot. Therefore, it is recommended that the assembler be a stateless
session bean to leverage the benefits of stateless over stateful session beans.
However, if the underlying model rarely changes, then the assembler may be a
stateful session bean and retain the newly constructed value object. In this case,
the ValueObjectAssembler must include mechanisms to recognize changes to the
underlying model and to reconstruct the model for the next client request.
The BusinessObject role in this pattern can be supported by different types of
objects, as explained below.
? The BusinessObject can be a session bean. The Value Object Assembler may
use a Service Locator (see ¡°Service Locator¡±) to locate the required session
bean. The Value Object Assembler requests this session bean to provide the
data to construct the composite value object.
? The BusinessObject can be an entity bean. The Value Object Assembler may
use a Service Locator to locate the required entity bean. The Value Object
Assembler requests this entity bean to provide the data to construct the
composite value object.
? The BusinessObject can be a DAO. The Value Object Assembler requests this
DAO to provide the data to construct the composite value object.
? The BusinessObject can be an arbitrary Java object. The Value Object
Assembler requests this Java object to provide the data to construct the
composite value object.
? The BusinessObject can be another Value Object Assembler. The first Value
Object Assembler requests the second Value Object Assembler to provide
the data to construct the composite value object.
? Separates Business Logic
When the client includes logic to manage the interactions with distributed
components, it becomes difficult to clearly separate business logic from the
client tier. The Value Object Assembler contains the business logic to
maintain the object relationships and to construct the composite value
object representing the model. The client needs no knowledge of how to
construct the model or the different components that provide data to
assemble the model.
? Reduces Coupling Between Clients and the Application Model
The Value Object Assembler hides the complexity of the construction of
model data from the clients and establishes a loose coupling between clients
and the model. With loose coupling, if the model changes, then the Value
Object Assembler requires a corresponding change. However, the client is
not dependent on the model construction and interrelationships between
model business components, so model changes do not directly affect the
client. In general, loose coupling is preferred to tight coupling.
? Improves Network Performance
The Value Object Assembler drastically reduces the network overhead of
remote method calls and chattiness. The client can request the data for the
application model from the Value Object Assembler in a single remote
method call. The assembler constructs and returns the composite value
object for the model. However, the composite value object may contain a
large amount of data. Thus, while use of the Value Object Assembler reduces
the number of network calls, there is an increase in the amount of data
transported in a single call. This trade-off should be considered in applying
this pattern.
? Improves Client Performance
The server-side Value Object Assembler constructs the model as a
composite value object without using any client resources. The client spends
no time assembling the model.
? Improves Transaction Performance
Typically, updates are isolated to a very small part of the model and can be
performed by fine-grained transactions. These transactions focus on isolated
parts of the model instead of locking up the coarse-grained object (model).
After the client obtains the model and displays or processes it locally, the
user (or the client) may need to update or otherwise modify the model. The
client can interact directly with a Session Facade to accomplish this at a
suitable granularity level. The Value Object Assembler is not involved in the
transaction to update or modify the model. There is better performance
control because transactional work with the model happens at the
appropriate level of granularity.
? May Introduce Stale Value Objects
The Value Object Assembler constructs value objects on demand. These
value objects are snapshots of the current state of the model, represented
by various business components. Once the client obtains a value object from
the assembler, that value object is entirely local to the client. Since the value
objects are not network-aware, other changes made to the business
components used to construct the value object are not reflected in the value
objects. Therefore, after the value object is obtained, it can quickly become
stale if there are transactions on the business components.
Consider a Project Management application where a number of business-tier
components define the complex model. Suppose a client wants to obtain the model
data composed of data from various business objects, such as:
? Project Information from the Project component
? Project Manager information from the ProjectManager component
? List of Project Tasks from the Project component
? Resource Information from the Resource component
A composite value object to contain this data can be defined as shown in Example
8.24. A Value Object Assembler pattern can be implemented to assemble this
composite value object. The Value Object Assembler sample code is listed in
Example 8.28.
The list of tasks in the ProjectDetailsData is a collection of TaskResourceVO objects.
The TaskResourceVO is a combination of TaskVO and ResourceVO. These classes
are shown in Example 8.25, Example 8.26, and Example 8.27.
The ProjectDetailsAssembler class that assembles the ProjectDetailsData object is
listed in Example 8.28.
? Value Object
The Value Object Assembler uses the Value Object pattern in order to create
and transport value objects to the client. The value objects created carry the
data representing the application model from the business tier to the clients
requesting the data.
? Composite Entity
The Composite Entity pattern promotes a coarse-grained entity bean design,
where entities can produce composite value objects similar to the one
produced by the Value Object Assembler. However, the Value Object
Assembler is more applicable when the composite value object constructed
is derived from a number of components (session beans, entity beans, DAOs,
and so forth), whereas the Composite Entity pattern constructs the value
object from its own data (that is, a single entity bean).
? Session Facade
The Value Object Assembler is typically implemented as a stateless session
bean. As such, it could be viewed as a limited special application of the
Session Facade pattern. More importantly, Value Object Assembler
constructs composite value objects that are immutable. Therefore, the client
receiving this composite value object can only use the data for its
presentation and processing purposes. The client cannot update the value
object. If the client needs to update the business objects that derive the
composite value object, it may have to access the Session Facade (session
bean) that provides that business service.
? Data Access Object
A possible strategy for the Value Object Assembler involves obtaining data
for the composite value object from the persistent store without enterprise
bean involvement. The Data Access Object pattern can be applied, thus
leveraging its benefits to provide persistent storage access to the Value
Object Assembler.
? Service Locator
The Value Object Assembler needs to locate and use various business
objects. The Service Locator pattern can be used in conjunction with the
Value Object Assembler pattern whenever a business object or a service
needs to be located.
##%%&&
The client requires a list of items from the service for presentation. The number of
items in the list is unknown and can be quite large in many instances.
Most J2EE applications have a search and query requirement to search and list
certain data. In some cases, such a search and query operation could yield results
that can be quite large. It is impractical to return the full result set when the client's
requirements are to traverse the results, rather than process the complete set.
Typically, a client uses the results of a query for read-only purposes, such as
displaying the result list. Often, the client views only the first few matching records,
and then may discard the remaining records and attempt a new query. The search
activity often does not involve an immediate transaction on the matching objects.
The practice of getting a list of values represented in entity beans by calling an
ejbFind() method, which returns a collection of remote objects, and then calling
each entity bean to get the value, is very network expensive and is considered a bad
practice.
There are consequences associated with using EJB finder methods that result in
large results sets. Every container implementation has a certain amount of finder
method overhead for creating a collection of EJBObject references. Finder method
behavior performance varies, depending on a vendor's container implementation.
According to the EJB specification, a container may invoke ejbActivate() methods
on entities found by a finder method. At a minimum, a finder method returns the
primary keys of the matching entities, which the container returns to the client as a
collection of EJBObject references. This behavior applies for all container
implementations. Some container implementations may introduce additional finder
method overhead by associating the entity bean instances to these EJBObject
instances to give the client access to those entity beans. However, this is a poor use
of resources if the client is not interested in accessing the bean or invoking its
methods. This overhead can significantly impede application performance if the
application includes queries that produce many matching results.
? The application client needs an efficient query facility to avoid having to call
the entity bean's ejbFind() method and invoking each remote object
returned.
? A server-tier caching mechanism is needed to serve clients that cannot
receive and process the entire results set.
? A query that is repeatedly executed on reasonably static data can be
optimized to provide faster results. This depends on the application and on
the implementation of this pattern.
? EJB finder methods are not suitable for browsing entire tables in the
database or for searching large result sets from a table.
? Finder methods may have considerable overhead when used to find large
numbers of result objects. The container may create a large number of
infrastructure objects to facilitate the finders.
? EJB finder methods are not suitable for caching results. The client may not be
able to handle the entire result set in a single call. If so, the client may need
server-side caching and navigation functions to traverse the result set.
? EJB finder methods have predetermined query constructs and offer
minimum flexibility. The EJB specification 2.0 allows a query language, EJB
QL, for container-managed entity beans. EJB QL makes it easier to write
portable finders and offers greater flexibility for querying.
? Client wants to scroll forward and backward within a result set.
Use a Value List Handler to control the search, cache the results, and
provide the results to the client in a result set whose size and traversal
meets the client's requirements.
This pattern creates a ValueListHandler to control query execution functionality and
results caching. The ValueListHandler directly accesses a DAO that can execute the
required query. The ValueListHandler stores the results obtained from the DAO as a
collection of value objects. The client requests the ValueListHandler to provide the
query results as needed. The ValueListHandler implements an Iterator pattern [GoF]
to provide the solution.
The class diagram in Figure 8.29 illustrates the Value List Handler pattern.
The sequence diagram in Figure 8.30 shows the interactions for the Value List
Handler.
This interface may provide iteration facility with the following example methods:
? getSize() obtains the size of the result set.
? getCurrentElement()obtains the current value object from the list.
? getPreviousElements(int howMany) obtains a collection of value objects
that are in the list prior to the current element.
? getNextElements(int howMany) obtains a collection of value objects that
are in the list after the current element.
? resetIndex() resets the index to the start of the list.
Depending on the need, other convenience methods can be included to be part of
the ValueListIterator interface.
This is a list handler object that implements the ValueListIterator interface. The
ValueListHandler executes the required query when requested by the client. The
ValueListHandler obtains the query results, which it manages in a privately held
collection represented by the ValueList object. The ValueListHandler creates and
manipulates the ValueList collection. When the client requests the results, the
ValueListHandler obtains the value objects from the cached ValueList, creates a new
collection of value objects, serializes the collection, and sends it back to the client.
The ValueListHandler also tracks the current index and size of the list.
The ValueListHandler can make use of a DataAccessObject to keep separate the
implementation of the database access. The DataAccessObject provides a simple
API to access the database (or any other persistent store), execute the query, and
retrieve the results.
The ValueList is a collection (a list) that holds the results of the query. The results
are stored as value objects. If the query fails to return any matching results, then
this list is empty. The ValueListHandler session bean caches ValueList to avoid
repeated, unnecessary execution of the query.
The ValueObject represents an object view of the individual record from the query's
results. It is an immutable serializable object that provides a placeholder for the
data attributes of each record.
The ValueListHandler can be implemented as an arbitrary Java object. In this case,
the ValueListHandler can be used by any client that needs the listing functionality.
For applications that do not use enterprise beans, this strategy is useful. For
example, simpler applications may be built using servlets, JSPs, Business Delegates,
and DAOs. In this scenario, the Business Delegates can use a ValueListHandler
implemented as a Java object to obtain list of values.
When an application uses enterprise beans in the business tier, it may be preferable
to implement a session bean that uses the ValueListHandler. In this case, the
session bean simply fronts an instance of a ValueListHandler. Thus, the session
bean may be implemented as a stateful session bean to hold on to the list handler as
its state, and thus may simply act as a facade (see ¡°Session Facade¡±) or as a pro
? Provides Alternative to EJB Finders for Large Queries
Typically, an EJB finder method is a resource-intensive and an expensive
way of obtaining a list of items, since it involves a number of EJBObject
references. The Value List Handler implements a session bean that uses a
DAO to perform the query and to create a collection of value objects that
match the query criteria. Because value objects have relatively low overhead
compared to EJBObject references and their associated infrastructure, this
pattern provides benefits when application clients require queries resulting
in large result sets.
? Caches Query Results on Server Side
The result set obtained from a query execution needs to be cached when a
client must display the results in small subsets rather than in one large list.
However, not all browser-based clients can perform such caching. When
they cannot, the server must provide this functionality. The Value List
Handler pattern provides a caching facility in the Value List Handler session
bean to hold the result set obtained from a query execution. The result set is
a collection of value objects that can be serialized if required.
When the client requests a collection, or a subset of a collection, the handler
bean returns the requested results as a serialized collection of value objects.
The client receives the collection and now has a local copy of the requested
information, which the client can display or process. When the client needs
an additional subset of the results, it requests the handler to return another
serialized collection containing the required results. The client can process
the query results in smaller, manageable chunks. The handler bean also
provides the client with navigation facilities (previous and next) so that the
results may be traversed forward and backward as necessary.
? Provides Better Querying Flexibility
Adding a new query may require creating a new finder method or modifying
an existing method, especially when using bean-managed entity beans.
(With bean-managed entity beans, the developer implements the finder
methods in the bean implementation.) With a container-managed entity
bean, the deployer specifies the entity bean finder methods in the bean's
deployment descriptor. Changes to a query for a container-managed bean
require changes to the finder method specification in the deployment
descriptor. Therefore, finder methods are ill-suited to handle query
requirements that change dynamically. You can implement a Value List
Handler to be more flexible than EJB finder methods by providing ad hoc
query facilities, constructing runtime query arguments using template
methods, and so forth. In other words, a Value List Handler developer can
implement intelligent searching and caching algorithms without being
limited by the finder methods.
? Improves Network Performance
Network performance may improve because only requested data, rather
than all data, is shipped (serialized) to the client on an as-needed basis. If
the client displays the first few results and then abandons the query, the
network bandwidth is not wasted, since the data is cached on the server side
and never sent to the client. However, if the client processes the entire result
set, it makes multiple remote calls to the server for the result set. When the
client knows in advance that it needs the entire result set, the handler bean
can provide a method that sends the client the entire result set in one
method call, and the pattern's caching feature is not used.
? Allows Deferring Entity Bean Transactions
Caching results on the server side and minimizing finder overhead may
improve transaction management. When the client is ready to further
process an entity bean, it accesses the bean within a transaction context
defined by the use case. For example, a query to display a list of books uses
a Value List Handler to obtain the list. When the user wants to view a book in
detail, it involves the book's entity bean in a transaction.
Consider an example where a list of Project business objects are to be retrieved and
displayed. The Value List Handler pattern can be applied in this case. The sample
code for this implementation is listed in Example 8.29 as ProjectListHandler, which
is responsible to provide the list of Projects. This class extends the
ValueListHandler base class, which provides the generic iteration functionality for
all Value List Handler implementations in this application. The ValueListHandler
sample code is listed in Example 8.30. The ValueListHandler implements the
generic iterator interface ValueListIterator, which is shown in Example 8.32. The
relevant code sample from the data access object ProjectDAO, used by
ValueListHandler to execute the query and obtain matching results, is shown in
Example 8.31.
The Value List Handler is a generic iterator class that provides the iteration
functionality.
? Iterator [GoF]
This Value List Handler pattern is based on Iterator pattern, described in the
GoF book, Design Patterns: Elements of Reusable Object-Oriented Software.
? Session Facade
Since the Value List Handler is a session bean, it may appear as a specialized
Session Facade. However, in isolation, it is a specialized session bean rather
than a specialized Session Facade. A Session Facade has other motivations
and characteristics (explained in the Session Facade pattern), and it is much
coarser grained.
##%%&&
Service lookup and creation involves complex interfaces and network operations.
J2EE clients interact with service components, such as EJB and JMS components,
which provide business services and persistence capabilities. To interact with these
components, clients must either locate the service component (referred to as a
lookup operation) or create a new component. For instance, an EJB client must
locate the enterprise bean's home object, which the client then uses either to find an
object or to create or remove one or more enterprise beans. Similarly, a JMS client
must first locate the JMS Connection Factory to obtain a JMS Connection or a JMS
Session.
All J2EE application clients use the JNDI common facility to look up and create EJB
and JMS components. The JNDI API enables clients to obtain an initial context object
that holds the component name to object bindings. The client begins by obtaining
the initial context for a bean's home object. The initial context remains valid while
the client session is valid. The client provides the JNDI registered name for the
required object to obtain a reference to an administered object. In the context of an
EJB application, a typical administered object is an enterprise bean's home object.
For JMS applications, the administered object can be a JMS Connection Factory (for
a Topic or a Queue) or a JMS Destination (a Topic or a Queue).
So, locating a JNDI-administered service object is common to all clients that need to
access that service object. That being the case, it is easy to see that many types of
clients repeatedly use the JNDI service, and the JNDI code appears multiple times
across these clients. This results in an unnecessary duplication of code in the clients
that need to look up services.
Also, creating a JNDI initial context object and performing a lookup on an EJB home
object utilizes significant resources. If multiple clients repeatedly require the same
bean home object, such duplicate effort can negatively impact application
performance.
Let us examine the lookup and creation process for various J2EE components.
1. The lookup and creation of enterprise beans relies upon the following:
o A correct setup of the JNDI environment so that it connects to the
naming and directory service used by the application. Setup entails
providing the location of the naming service and the necessary
authentication credentials to access that service.
o The JNDI service can then provide the client with an initial context
that acts as a placeholder for the component name-to-object
bindings. The client requests this initial context to look up the
EJBHome object for the required enterprise bean by providing the
JNDI name for that EJBHome object.
o Find the EJBHome object using the initial context's lookup
mechanism.
o After obtaining the EJBHome object, create, remove, or find the
enterprise bean, using the EJBHome object's create, move, and find
(for entity beans only).
2. The lookup and creation of JMS components (Topic, Queue,
QueueConnection, QueueSession, TopicConnection, TopicSession, and so
forth) involves the following steps. Note that in these steps, Topic refers to
the publish/subscribe messaging model and Queue refers to the
point-to-point messaging model.
o Set up the JNDI environment to the naming service used by the
application. Setup entails providing the location of the naming service
and the necessary authentication credentials to access that service.
o Obtain the initial context for the JMS service provider from the JNDI
naming service.
o Use the initial context to obtain a Topic or a Queue by supplying the
JNDI name for the topic or the queue. Topic and Queue are
JMSDestination objects.
o Use the initial context to obtain a TopicConnectionFactory or a
QueueConnectionFactory by supplying the JNDI name for the topic or
queue connection factory.
o Use the TopicConnectionFactory to obtain a TopicConnection or
QueueConnectionFactory to obtain a QueueConnection.
o Use the TopicConnection to obtain a TopicSession or a
QueueConnection to obtain a QueueSession.
o Use the TopicSession to obtain a TopicSubscriber or a TopicPublisher
for the required Topic. Use the QueueSession to obtain a
QueueReceiver or a QueueSender for the required Queue.
The process to look up and create components involves a vendor-supplied context
factory implementation. This introduces vendor dependency in the application
clients that need to use the JNDI lookup facility to locate the enterprise beans and
JMS components, such as topics, queues, and connection factory objects.
? EJB clients need to use the JNDI API to look up EJBHome objects by using the
enterprise bean's registered JNDI name.
? JMS clients need to use JNDI API to look up JMS components by using the
JNDI names registered for JMS components, such as connection factories,
queues, and topics.
? The context factory to use for the initial JNDI context creation is provided by
the service provider vendor and is therefore vendor- dependent. The context
factory is also dependent on the type of object being looked up. The context
for JMS is different from the context for EJB, with different providers.
? Lookup and creation of service components could be complex and may be
used repeatedly in multiple clients in the application.
? Initial context creation and service object lookups, if frequently required, can
be resource-intensive and may impact application performance. This is
especially true if the clients and the services are located in different tiers.
? EJB clients may need to reestablish connection to a previously accessed
enterprise bean instance, having only its Handle object.
Use a Service Locator object to abstract all JNDI usage and to hide the
complexities of initial context creation, EJB home object lookup, and EJB
object re-creation. Multiple clients can reuse the Service Locator object to
reduce code complexity, provide a single point of control, and improve
performance by providing a caching facility.
This pattern reduces the client complexity that results from the client's dependency
on and need to perform lookup and creation processes, which are
resource-intensive. To eliminate these problems, this pattern provides a
mechanism to abstract all dependencies and network details into the Service
Locator.
Figure 8.31 shows the class diagram representing the relationships for the Service
Locator pattern.
Figure 8.32 contains the sequence diagram that shows the interaction between the
various participants of the Service Locator pattern.
This is the client of the Service Locator. The client is an object that typically requires
access to business objects such as a Business Delegate (see ¡°Business Delegate¡± )
The Service Locator abstracts the API lookup (naming) services, vendor
dependencies, lookup complexities, and business object creation, and provides a
simple interface to clients. This reduces the client's complexity. In addition, the
same client or other clients can reuse the Service Locator.
The InitialContext object is the start point in the lookup and creation process.
Service providers provide the context object, which varies depending on the type of
business object provided by the Service Locator's lookup and creation service. A
Service Locator that provides services for multiple types of business objects (such
as enterprise beans, JMS components, and so forth) utilizes multiple types of
context objects, each obtained from a different provider (e.g., context provider for
an EJB application server may be different from the context provider for JMS
service).
The ServiceFactory object represents an object that provides life cycle management
for the BusinessService objects. The ServiceFactory object for enterprise beans is
an EJBHome object. The ServiceFactory for JMS components can be a JMS
ConnectionFactory object, such as a TopicConnectionFactory (for publish/subscribe
messaging model) or a QueueConnectionFactory (for point-to-point messaging
model).
The BusinessService is a role that is fulfilled by the service the client is seeking to
access. The BusinessService object is created or looked up or removed by the
ServiceFactory. The BusinessService object in the context of an EJB application is an
enterprise bean. The BusinessService object in the context of a JMS application can
be a TopicConnection or a QueueConnection. The TopicConnection and
QueueConnection can then be used to produce a JMSSession object, such as
TopicSession or a QueueSession respectively.
The Service Locator for enterprise bean components uses EJBHome object, shown
as BusinessHome in the role of the ServiceFactory. Once the EJBHome object is
obtained, it can be cached in the ServiceLocator for future use to avoid another JNDI
lookup when the client needs the home object again. Depending on the
implementation, the home object can be returned to the client, which can then use
it to look up, create, and remove enterprise beans. Otherwise, the ServiceLocator
can retain (cache) the home object and gain the additional responsibility of proxying
all client calls to the home object. The class diagram for the EJB Service Locator
strategy is shown in Figure 8.33
The interaction between the participants in a Service Locator for an enterprise bean
is shown in Figure 8.34.
This strategy is applicable to point-to-point messaging requirements. The Service
Locator for JMS components uses QueueConnectionFactory objects in the role of the
ServiceFactory. The QueueConnectionFactory is looked up using its JNDI name. The
QueueConnectionFactory can be cached by the ServiceLocator for future use. This
avoids repeated JNDI calls to look it up when the client needs it again. The
ServiceLocator may otherwise hand over the QueueConnectionFactory to the client.
The Client can then use it to create a QueueConnection. A QueueConnection is
necessary in order to obtain a QueueSession or to create a Message, a QueueSender
(to send messages to the queue), or a QueueReceiver (to receive messages from a
queue). The class diagram for the JMS Queue Service Locator strategy is shown in
Figure 8.35. In this diagram, the Queue is a JMS Destination object registered as a
JNDI-administered object representing the queue. The Queue object can be directly
obtained from the context by looking it up using its JNDI name.
The interaction between the participants in a Service Locator for point-to-point
messaging using JMS Queues is shown in Figure 8.36.
This strategy is applicable to publish/subscribe messaging requirements. The
Service Locator for JMS components uses TopicConnectionFactory objects in the
role of the ServiceFactory. The TopicConnectionFactory is looked up using its JNDI
name. The TopicConnectionFactory can be cached by the ServiceLocator for future
use. This avoids repeated JNDI calls to look it up when the client needs it again. The
ServiceLocator may otherwise hand over the TopicConnectionFactory to the client.
The Client can then use it to create a TopicConnection. A TopicConnection is
necessary in order to obtain a TopicSession or to create a Message, a TopicPublisher
(to publish messages to a topic), or a TopicSubscriber (to subscribe to a topic). The
class diagram for the JMS Topic Service Locator strategy is shown in Figure 8.37. In
this diagram, the Topic is a JMS Destination object registered as a
JNDI-administered object representing the topic. The Topic object can be directly
obtained from the context by looking it up using its JNDI name
The interaction between the participants in a Service Locator for publish/subscribe
messaging using JMS Topics is shown in Figure 8.38.
These strategies for EJB and JMS can be used to provide separate Service Locator
implementations, since the clients for EJB and JMS may more likely be mutually
exclusive. However, if there is a need to combine these strategies, it is possible to
do so to provide the Service Locator for all objects¡ªenterprise beans and JMS
components.
The diagrams in Figures 8.37 and 8.38 provide lookup facilities by passing in the
service lookup name. For an enterprise bean lookup, the Service Locator needs a
class as a parameter to the PortableRemoteObject.narrow() method. The Service
Locator can provide a getHome() method, which accepts as arguments the JNDI
service name and the EJBHome class object for the enterprise bean. Using this
method of passing in JNDI service names and EJBHome class objects can lead to
client errors. Another approach is to statically define the services in the
ServiceLocator, and instead of passing in string names, the client passes in a
constant. Example 8.34 illustrates such a strategy
This strategy has trade-offs. It reduces the flexibility of lookup, which is in the
Services Property Locator strategy, but add the type checking of passing in a
constant to the ServiceLocator.getHome() method.
This strategy helps to address the trade-offs of the type checking strategy. This
strategy suggests the use of property files and/or deployment descriptors to specify
the JNDI names and the EJBHome class name. For presentation-tier clients, such
properties can be specified in the presentation-tier deployment descriptors or
property files. When the presentation tier accesses the business tier, it typically
uses the Business Delegate pattern.
The Business Delegate interacts with the Service Locator to locate business
components. If the presentation tier loads the properties on initialization and can
provide a service to hand out the JNDI names and the EJB class names for the
required enterprise bean, then the Business Delegate could request this service to
obtain them. Once the Business Delegate has the JNDI name and the EJBHome
Class name, it can request the Service Locator for the EJBHome by passing these
properties as arguments.
The Service Locator can in turn use Class.forName(EJBHome ClassName) to obtain
the EJBHome Class object and go about its business of looking up the EJBHome and
using the Portable RemoteObject.narrow() method to cast the object, as shown
by the getHome() method in the ServiceLocator sample code in Example 8.33. The
only thing that changes is where the JNDI name and the Class objects are coming
from. Thus, this strategy avoids hardcoded JNDI names in the code and provides for
flexibility of deployment. However, due to the lack of type checking, there is scope
for avoiding errors and mismatches in specifying the JNDI names in different
deployment descriptors.
? Abstracts Complexity
The Service Locator pattern encapsulates the complexity of this lookup and
creation process (described in the problem) and keeps it hidden from the
client. The client does not need to deal with the lookup of component factory
objects (EJBHome, QueueConnectionFactory, and TopicConnectionFactory,
among others) because the ServiceLocator is delegated that responsibility.
? Provides Uniform Service Access to Clients
The Service Locator pattern abstracts all the complexities, as explained
previously. In doing so, it provides a very useful and precise interface that all
clients can use. The pattern interface ensures that all types of clients in the
application uniformly access business objects, in terms of lookup and
creation. This uniformity reduces development and maintenance overhead.
? Facilitates Adding New Business Components
Because clients of enterprise beans are not aware of the EJBHome objects,
it's possible to add new EJBHome objects for enterprise beans developed and
deployed at a later time without impacting the clients. JMS clients are not
directly aware of the JMS connection factories, so new connection factories
can be added without impacting the clients.
? Improves Network Performance
The clients are not involved in JNDI lookup and factory/home object creation.
Because the Service Locator performs this work, it can aggregate the
network calls required to look up and create business objects.
? Improves Client Performance by Caching
The Service Locator can cache the initial context objects and references to
the factory objects (EJBHome, JMS connection factories) to eliminate
unnecessary JNDI activity that occurs when obtaining the initial context and
the other objects. This improves the application performance.
A sample implementation of the Service Locator pattern is shown in Example 8.33.
An example for implementing the Type Checked Service Locator strategy is listed in
Example 8.34
The client code to use the Service Locator for this strategy may look like the code in
Example 8.35.
This strategy is about applying type checking to client lookup. It encapsulates the
static service values inside the ServiceLocator and creates an inner class Services,
which declares the service constants (PROJECT and RESOURCE). The Tester client
gets an instance to the ServiceLocator singleton and calls getHome(), passing in the
PROJECT. ServiceLocator in turn gets the JNDI entry name and the Home class and
returns the EJBHome.
? Business Delegate
The Business Delegate pattern uses Service Locator to gain access to the
business service objects such as EJB objects, JMS topics, and JMS queues.
This separates the complexity of service location from the Business Delegate,
leading to loose coupling and increased manageability.
? Session Facade
The Session Facade pattern uses Service Locator to gain access to the
enterprise beans that are involved in a workflow. The Session Facade could
directly use the Service Locator or delegate the work to a Business Delegate
(See ¡°Business Delegate¡± .).
? Value Object Assembler
The Value Object Assembler pattern uses Service Locator to gain access to
the various enterprise beans it needs to access to build its composite value
object. The Value Object Assembler could directly use the Service Locator or
delegate the work to a Business Delegate (See ¡°Business Delegate¡±.)
##%%&&
Access to data varies depending on the source of the data. Access to persistent
storage, such as to a database, varies greatly depending on the type of storage
(relational databases, object-oriented databases, flat files, and so forth) and the
vendor implementation.
Many real-world J2EE applications need to use persistent data at some point. For
many applications, persistent storage is implemented with different mechanisms,
and there are marked differences in the APIs used to access these different
persistent storage mechanisms. Other applications may need to access data that
resides on separate systems. For example, the data may reside in mainframe
systems, Lightweight Directory Access Protocol (LDAP) repositories, and so forth.
Another example is where data is provided by services through external systems
such as business-to-business (B2B) integration systems, credit card bureau service,
and so forth.
Typically, applications use shared distributed components such as entity beans to
represent persistent data. An application is considered to employ bean-managed
persistence (BMP) for its entity beans when these entity beans explicitly access the
persistent storage¡ªthe entity bean includes code to directly access the persistent
storage. An application with simpler requirements may forego using entity beans
and instead use session beans or servlets to directly access the persistent storage to
retrieve and modify the data. Or, the application could use entity beans with
container-managed persistence, and thus let the container handle the transaction
and persistent details.
Applications can use the JDBC API to access data residing in a relational database
management system (RDBMS). The JDBC API enables standard access and
manipulation of data in persistent storage, such as a relational database. JDBC
enables J2EE applications to use SQL statements, which are the standard means for
accessing RDBMS tables. However, even within an RDBMS environment, the actual
syntax and format of the SQL statements may vary depending on the particular
database product.
There is even greater variation with different types of persistent storage. Access
mechanisms, supported APIs, and features vary between different types of
persistent stores such as RDBMS, object-oriented databases, flat files, and so forth.
Applications that need to access data from a legacy or disparate system (such as a
mainframe, or B2B service) are often required to use APIs that may be proprietary.
Such disparate data sources offer challenges to the application and can potentially
create a direct dependency between application code and data access code. When
business components¡ªentity beans, session beans, and even presentation
components like servlets and helper objects for Java Server Pages (JSPs)¡ªneed to
access a data source, they can use the appropriate API to achieve connectivity and
manipulate the data source. But including the connectivity and data access code
within these components introduces a tight coupling between the components and
the data source implementation. Such code dependencies in components make it
difficult and tedious to migrate the application from one type of data source to
another. When the data source changes, the components need to be changed to
handle the new type of data source.
? Components such as bean-managed entity beans, session beans, servlets,
and other objects like helpers for JSPs need to retrieve and store information
from persistent stores and other data sources like legacy systems, B2B,
LDAP, and so forth.
? Persistent storage APIs vary depending on the product vendor. Other data
sources may have APIs that are nonstandard and/or proprietary. These APIs
and their capabilities also vary depending on the type of storage¡ªRDBMS,
object-oriented database management system (OODBMS), XML documents,
flat files, and so forth. There is a lack of uniform APIs to address the
requirements to access such disparate systems.
? Components typically use proprietary APIs to access external and/or legacy
systems to retrieve and store data.
? Portability of the components is directly affected when specific access
mechanisms and APIs are included in the components.
? Components need to be transparent to the actual persistent store or data
source implementation to provide easy migration to different vendor
products, different storage types, and different data source types.
Use a Data Access Object (DAO) to abstract and encapsulate all access to
the data source. The DAO manages the connection with the data source to
obtain and store data.
The DAO implements the access mechanism required to work with the data source.
The data source could be a persistent store like an RDBMS, an external service like
a B2B exchange, a repository like an LDAP database, or a business service accessed
via CORBA Internet Inter-ORB Protocol (IIOP) or low-level sockets. The business
component that relies on the DAO uses the simpler interface exposed by the DAO for
its clients. The DAO completely hides the data source implementation details from
its clients. Because the interface exposed by the DAO to clients does not change
when the underlying data source implementation changes, this pattern allows the
DAO to adapt to different storage schemes without affecting its clients or business
components. Essentially, the DAO acts as an adapter between the component and
the data source.
Figure 9.1 shows the class diagram representing the relationships for the DAO
pattern.
Figure 9.2 contains the sequence diagram that shows the interaction between the
various participants in this pattern.
The BusinessObject represents the data client. It is the object that requires access
to the data source to obtain and store data. A BusinessObject may be implemented
as a session bean, entity bean, or some other Java object, in addition to a servlet or
helper bean that accesses the data source.
The DataAccessObject is the primary object of this pattern. The DataAccessObject
abstracts the underlying data access implementation for the BusinessObject to
enable transparent access to the data source. The BusinessObject also delegates
data load and store operations to the DataAccessObject.
This represents a data source implementation. A data source could be a database
such as an RDBMS, OODBMS, XML repository, flat file system, and so forth. A data
source can also be another system (legacy/mainframe), service (B2B service or
credit card bureau), or some kind of repository (LDAP).
This represents a value object used as a data carrier. The DataAccessObject may
use a value object to return data to the client. The DataAccessObject may also
receive the data from the client in a value object to update the data in the data
source.
Since each BusinessObject corresponds to a specific DAO, it is possible to establish
relationships between the BusinessObject, DAO, and underlying implementations
(such as the tables in an RDBMS). Once the relationships are established, it is
possible to write a simple application-specific code-generation utility that generates
the code for all DAOs required by the application. The metadata to generate the
DAO can come from a developer-defined descriptor file. Alternatively, the code
generator can automatically introspect the database and provide the necessary
DAOs to access the database. If the requirements for DAOs are sufficiently complex,
consider using third-party tools that provide object-to-relational mapping for
RDBMS databases. These tools typically include GUI tools to map the business
objects to the persistent storage objects and thereby define the intermediary DAOs.
The tools automatically generate the code once the mapping is complete, and may
provide other value-added features such as results caching, query caching,
integration with application servers, integration with other third-party products
(e.g., distributed caching), and so forth.
The DAO pattern can be made highly flexible by adopting the Abstract Factory [GoF]
and the Factory Method [GoF] patterns (see ¡°Related Patterns¡± in this chapter).
When the underlying storage is not subject to change from one implementation to
another, this strategy can be implemented using the Factory Method pattern to
produce a number of DAOs needed by the application. The class diagram for this
case is shown in Figure 9.3.
When the underlying storage is subject to change from one implementation to
another, this strategy may be implemented using the Abstract Factory pattern. The
Abstract Factory can in turn build on and use the Factory Method implementation, as
suggested in Design Patterns: Elements of Reusable Object-Oriented Software
[GoF]. In this case, this strategy provides an abstract DAO factory object (Abstract
Factory) that can construct various types of concrete DAO factories, each factory
supporting a different type of persistent storage implementation. Once you obtain
the concrete DAO factory for a specific implementation, you use it to produce DAOs
supported and implemented in that implementation.
The class diagram for this strategy is shown in Figure 9.4. This class diagram shows
a base DAO factory, which is an abstract class that is inherited and implemented by
different concrete DAO factories to support storage implementation-specific access.
The client can obtain a concrete DAO factory implementation such as
RdbDAOFactory and use it to obtain concrete DAOs that work with that specific
storage implementation. For example, the data client can obtain an RdbDAOFactory
and use it to get specific DAOs such as RdbCustomerDAO, RdbAccountDAO, and so
forth. The DAOs can extend and implement a generic base class (shown as DAO1
and DAO2) that specifically describe the DAO requirements for the business object
it supports. Each concrete DAO is responsible for connecting to the data source and
obtaining and manipulating data for the business object it supports.
The sample implementation for the DAO pattern and its strategies is shown in the
¡°Sample Code¡± section of this chapter.
The sequence diagram describing the interactions for this strategy is shown in
Figure 9.5.
? Enables Transparency
Business objects can use the data source without knowing the specific details
of the data source's implementation. Access is transparent because the
implementation details are hidden inside the DAO.
? Enables Easier Migration
A layer of DAOs makes it easier for an application to migrate to a different
database implementation. The business objects have no knowledge of the
underlying data implementation. Thus, the migration involves changes only
to the DAO layer. Further, if employing a factory strategy, it is possible to
provide a concrete factory implementation for each underlying storage
implementation. In this case, migrating to a different storage
implementation means providing a new factory implementation to the
application.
? Reduces Code Complexity in Business Objects
Because the DAOs manage all the data access complexities, it simplifies the
code in the business objects and other data clients that use the DAOs. All
implementation-related code (such as SQL statements) is contained in the
DAO and not in the business object. This improves code readability and
development productivity.
? Centralizes All Data Access into a Separate Layer
Because all data access operations are now delegated to the DAOs, the
separate data access layer can be viewed as the layer that can isolate the
rest of the application from the data access implementation. This
centralization makes the application easier to maintain and manage.
? Not Useful for Container-Managed Persistence
Because the EJB container manages entity beans with container-managed
persistence (CMP), the container automatically services all persistent
storage access. Applications using container-managed entity beans do not
need a DAO layer, since the application server transparently provides this
functionality. However, DAOs are still useful when a combination of CMP (for
entity beans) and BMP (for session beans, servlets) is required.
? Adds Extra Layer
The DAOs create an additional layer of objects between the data client and
the data source that need to be designed and implemented to leverage the
benefits of this pattern. But the benefit realized by choosing this approach
pays off for the additional effort.
? Needs Class Hierarchy Design
When using a factory strategy, the hierarchy of concrete factories and the
hierarchy of concrete products produced by the factories need to be
designed and implemented. This additional effort needs to be considered if
there is sufficient justification warranting such flexibility. This increases the
complexity of the design. However, you can choose to implement the factory
strategy starting with the Factory Method pattern first, and then move
towards the Abstract Factory if necessary.
An example DAO code for a persistent object that represents Customer information
is shown in Example 9.4. The CloudscapeCustomerDAO creates a Customer value
object when the findCustomer() method is invoked.
The sample code to use the DAO is shown in Example 9.6. The class diagram for this
example is shown in Figure 9.6.
Consider an example where we are implementing this strategy in which a DAO
factory produces many DAOs for a single database implementation (e.g., Oracle).
The factory produces DAOs such as CustomerDAO, AccountDAO, OrderDAO, and so
forth. The class diagram for this example is shown in Figure 9.7.
The example code for the DAO factory (CloudscapeDAOFactory) is listed in Example
9.2.
Consider an example where we are considering implementing this strategy for three
different databases. In this case, the Abstract Factory pattern can be employed. The
class diagram for this example is shown in Figure 9.8. The sample code in Example
9.1 shows code excerpt for the abstract DAOFactory class. This factory produces
DAOs such as CustomerDAO, AccountDAO, OrderDAO, and so forth. This strategy
uses the Factory Method implementation in the factories produced by the Abstract
Factory.
The sample code for CloudscapeDAOFactory is shown in Example 9.2. The
implementation for OracleDAOFactory and SybaseDAOFactory are similar except for
specifics of each implementation, such as JDBC driver, database URL, and
differences in SQL syntax, if any.
The CustomerDAO interface shown in Example 9.3 defines the DAO methods for
Customer persistent object that are implemented by all concrete DAO
implementations, such as CloudscapeCustomerDAO, OracleCustomerDAO, and
SybaseCustomerDAO. Similar, but not listed here, are AccountDAO and OrderDAO
interfaces that define the DAO methods for Account and Order business objects
respectively.
The CloudscapeCustomerDAO implements the CustomerDAO as shown in Example
9.4. The implementation of other DAOs, such as CloudscapeAccountDAO,
CloudscapeOrderDAO, OracleCustomerDAO, OracleAccountDAO, and so forth, are
similar.
The Customer value object class is shown in Example 9.5. This is used by the DAOs
to send and receive data from the clients. The usage of value objects is discussed in
detail in the Value Object pattern.
Example 9.6 shows the usage of the DAO factory and the DAO. If the
implementation changes from Cloudscape to another product, the only required
change is the getDAOFactory() method call to the DAO factory to obtain a different
factory.
? Value Object
A DAO uses value objects to transport data to and from its clients.
? Factory Method [GoF] and Abstract Factory [GoF]
The Factory for Data Access Objects Strategy uses the Factory Method
pattern to implement the concrete factories and its products (DAOs). For
added flexibility, the Abstract Factory pattern may be employed as discussed
in the strategies.
? Broker [POSA1]
The DAO pattern is related to the Broker pattern, which describes
approaches for decoupling clients and servers in distributed systems. The
DAO pattern more specifically applies this pattern to decouple the resource
tier from clients in another tier, such as the business or presentation tier.
##%%&&
Enterprise beans and other business services need a way to be activated
asynchronously.
When a client needs to access an enterprise bean, it first looks up the bean's home
object. The client requests the EJB home to provide a remote reference to the
required enterprise bean. The client then invokes business method calls on the
remote reference to access the enterprise bean services. All these method calls,
such as lookup and remote method calls, are synchronous. The client has to wait
until these methods return.
Another factor to consider is the life cycle of an enterprise bean. The EJB
specification permits the container to passivate an enterprise bean to secondary
storage. As a result, the EJB container has no mechanism by which it can provide a
process-like service to keep an enterprise bean constantly in an activated and ready
state. Because the client must interact with the enterprise bean using the bean's
remote interface, even if the bean is in an activated state in the container, the client
still needs to obtain its remote interface via the lookup process and still interacts
with the bean in a synchronous manner.
If an application needs synchronous processing for its server-side business
components, then enterprise beans are an appropriate choice. Some application
clients may require asynchronous processing for the server-side business objects
because the clients do not need to wait or do not have the time to wait for the
processing to complete. In cases where the application needs a form of
asynchronous processing, enterprise beans do not offer this capability in
implementations prior to EJB 2.0.
EJB 2.0 provides integration by introducing message-driven bean, which is a special
type of stateless session bean that offers asynchronous invocation capabilities.
However, the new specification does not offer asynchronous invocation for other
types of enterprise beans, such as stateful or entity beans.
In general, a business service such as a session or entity bean provides only
synchronous processing and thus presents a challenge to implementing
asynchronous processing.
? Enterprise beans are exposed to their clients via their remote interfaces,
which allow only synchronous access.
? The container manages enterprise beans, allowing interactions only via the
remote references. The EJB container does not allow direct access to the
bean implementation and its methods. Thus, implementing the JMS
message listener in an enterprise bean is not feasible, since this violates the
EJB specification by permitting direct access to the bean implementation.
? An application needs to provide a publish/subscribe or point-to-point
messaging framework where clients can publish requests to enterprise
beans for asynchronous processing.
? Clients need asynchronous processing capabilities from the enterprise beans
and other business components that can only provide synchronous access,
so that the client can send a request for processing without waiting for the
results.
? Clients want to use the message-oriented middleware (MOM) interfaces
offered by the Java Messaging Service (JMS). These interfaces are not
integrated into EJB server products that are based on the pre-EJB 2.0
specification.
? An application needs to provide daemon-like service so that an enterprise
bean can be in a quiet mode until an event (or a message) triggers its
activity.
? Enterprise beans are subject to the container life cycle management, which
includes passivation due to time-outs, inactivity and resource management.
The client will have to invoke on an enterprise bean to activate it again.
? EJB 2.0 introduces a message-driven bean as a stateless session bean, but it
is not possible to invoke other types of enterprise beans asynchronously.
Use a Service Activator to receive asynchronous client requests and
messages. On receiving a message, the Service Activator locates and
invokes the necessary business methods on the business service
components to fulfill the request asynchronously.
The ServiceActivator is a JMS Listener and delegation service that requires
implementing the JMS message listener¡ªmaking it a JMS listener object that can
listen to JMS messages. The ServiceActivator can be implemented as a standalone
service. Clients act as the message generator, generating events based on their
activity.
Any client that needs to asynchronously invoke a business service, such as an
enterprise bean, may create and send a message to the Service Activator. The
Service Activator receives the message and parses it to interpret the client request.
Once the client's request is parsed or unmarshalled, the Service Activator identifies
and locates the necessary business service component and invokes business
methods to complete processing of the client's request asynchronously.
The Service Activator may optionally send an acknowledgement to the client after
successfully completing the request processing. The Service Activator may also
notify the client or other services on failure events if it fails to complete the
asynchronous request processing.
The Service Activator may use the services of a Service Locator to locate a business
component. See ¡°Service Locator¡±.
Figure 9.9 represents the class relationships for the Service Activator pattern.
Figure 9.10 shows the interactions between the various participants in the Service
Activator pattern.
The client requires an asynchronous processing facility from the business objects
participating in a workflow. The client can be any type of application that has the
capability to create and send JMS messages. The client can also be an EJB
component that needs to invoke another EJB component's business methods in an
asynchronous manner. The client can use the services offered by the Service
Locator pattern to look up or create EJB components, JMS services, and JMS objects,
as necessary.
The Request is the message object created by the client and sent to the
ServiceActivator via the MOM. According to the JMS specification, the Request is an
object that implements the javax.jms.Message interface. The JMS API provides
several message types, such as TextMessage, ObjectMessage, and so forth, that
can be used as request objects.
The ServiceActivator is the main class of the pattern. It implements the
javax.jms.MessageListener interface, which is defined by the JMS specification. The
ServiceActivator implements an onMessage() method that is invoked when a new
message arrives. The ServiceActivator parses (unmarshals) the message (request)
to determine what needs to be done. The ServiceActivator may use the services
offered by a Service Locator (see Service Locator) pattern to look up or create
Business Service components such as enterprise beans.
BusinessObject is the target object to which the client needs access in an
asynchronous mode. The business object is a role fulfilled by either a session or
entity bean. It is also possible that the BusinessObject is an external service instead
of an entity bean.
Both session and entity beans can fulfill the role of a BusinessObject. When J2EE
applications implement a Session Fa ade pattern to provide coarse-grained access
to entity beans and to encapsulate the workflow, then the session bean from the
Session Fa ade fulfills the BusinessObject role.
In simple applications with minimal workflow, an entity bean may fulfill the
BusinessObject role. However, for complex workflow involving multiple entity beans
and other business objects, the ServiceActivator typically interacts with a Session
Facade which encapsulates such workflow.
When a session bean fulfills the role of the BusinessObject, the business
requirements determine whether the bean should be stateful or stateless. Since the
client for the BusinessObject is a ServiceActivator that activates the BusinessObject
on receiving a new message, the workflow to process the message can define
whether the bean should be stateful or not. In most cases, a message delivery
simply activates a single method in the BusinessObject that delegates the
processing of the message within. A stateless session bean can be used in these
cases. If the ServiceActivator needs to invoke multiple methods in the
BusinessObject or to work with more than one BusinessObject to fulfill the
processing requirements for a message, it may be useful to consider a stateful
session bean to retain state between multiple invocations. See ¡°Stateless Session
Facade Strategy¡± and ¡°Stateful Session Facade Strategy¡±.
The most straightforward strategy for implementing the listener or ServiceActivator
is as a standalone JMS application that listens and processes JMS messages.
An alternative is to implement the ServiceActivator as a service of the application
server. This may make it easier to manage the ServiceActivator, because it uses the
application server features to monitor the ServiceActivator state and to start,
restart, and stop the ServiceActivator as needed, either manually or automatically.
The Client can be any client, including another enterprise bean that requires
asynchronous processing from the enterprise bean. When integrating legacy
applications to the J2EE platform, it is logical to choose Java application clients to
act as the message generators based on the activity in the legacy system. The
ServiceActivator can receive messages and perform the necessary enterprise bean
invocations to process the request from the legacy system.
? Integrates JMS into Pre-EJB 2.0 Implementations
Prior to the EJB 2.0 specification, there was no integration between
enterprise bean and JMS components. This pattern provides a means to
integrate JMS into an EJB application and enable asynchronous processing.
The EJB 2.0 specification defines a new type of session bean, called a
message-driven bean, to integrate JMS and EJB components. This special
bean implements the JMS Message Listener interface and it receives
asynchronous messages. In this case, the application server plays the role of
the Service Activator. This pattern makes it possible to run applications in
EJB 2.0 implementations as well as pre-EJB 2.0 implementations.
? Provides Asynchronous Processing for any Enterprise Beans
In EJB 2.0, the message-driven bean is a stateless session bean. Using the
Service Activator pattern, it is possible to provide asynchronous invocation
on all types of enterprise beans, including stateless session beans, stateful
session beans, and entity beans. As previously explained, since the Service
Activator is implemented in its own right, without any limitations of the
message-driven bean, the Service Activator can perform asynchronous
invocations on any type of business service. Thus, this pattern provides a
way to enable asynchronous processing for clients that either have no need
to wait for the results or do not want to wait for processing to complete. The
processing can be deferred and performed at a later time, enabling the client
to complete the service in less time.
? Standalone Process
The Service Activator can be run as a standalone process. However, in a
critical application, Service Activator needs to be monitored to ensure
availability. The additional management and maintenance of this process
can add to application support overhead.
Consider an order processing application where the customers shop online and the
order fulfillment process happens in the background. In some cases, order
fulfillment may be outsourced to a third-party warehouse. In such cases, the online
store needs to invoke these fulfillment services asynchronously. This is an example
that demonstrates usage of point-to-point (PTP) messaging to accomplish
asynchronous processing. However, using publish/subscribe messaging would be
similar, except that Topic is used instead of a Queue. Choosing which method to use,
PTP or publish/subscribe, depends on the business and application requirements,
and hence is outside the scope of this pattern.
The class diagram with only the relevant methods for this example is shown in
Figure 9.11.
The code excerpt shown in Example 9.7 demonstrates a sample Service Activator
implementation. This is the class that can be instantiated in an application server or
run in a stand-alone server, as explained in the Service Activator Server strategy.
This example demonstrates using the Business Delegate pattern between business
and integration tiers. OrderProcessorDelegate logically resides in the integration
tier and accesses the Order Processor session bean, which resides in the business
tier.
The sample session facade code responsible to dispatch orders to this asynchronous
service is shown in the code excerpt in Example 9.8. The Service Activator client can
be a session bean that implements the Session Facade pattern to provide order
processing services to the online store application. When the session bean's
createOrder() method is called, after successfully validating and creating a new
order, it invokes sendOrder() to dispatch the new order to the backend order
fulfillment service.
The JMS code can be separated into a different class so that it can be reused by
different clients. This JMS delegate class is shown as OrderSender in the Example
9.9 code listing.
? Session Facade
The Session Facade pattern encapsulates the complexity of the system and
provides coarse-grained access to business objects. This Service Activator
pattern may access a Session Facade as the primary business object to
invoke business service methods in the Session Facade asynchronously on
behalf of the client.
? Business Delegate
The Service Activator pattern may use a Business Delegate to access the
Session Facade or other enterprise bean implementations. This results in
simpler code for the Service Activator and results in Business Delegate reuse
across different tiers, as intended by the Business Delegate pattern.
? Service Locator
The client can use the Service Locator pattern to look up and create
JMS-related service objects. The Service Activator can use the Service
Locator pattern to look up and create enterprise bean components.
? Half-Sync/Half-Async [POSA2]
The Service Activator pattern is related to the Half-Sync/Half-Async pattern,
which describes architectural decoupling of synchronous and asynchronous
processing by suggesting different layers for synchronous, asynchronous
and an intermediate queuing layer in between.
##%%&&
Model¨Cview¨Cviewmodel (MVVM) is a software architectural pattern.
MVVM facilitates a separation of development of the graphical user interface ¨C be it via a markup language or GUI code ¨C from development of the business logic or back-end logic (the data model). The view model of MVVM is a value converter,[1] meaning the view model is responsible for exposing (converting) the data objects from the model in such a way that objects are easily managed and presented. In this respect, the view model is more model than view, and handles most if not all of the view's display logic.[1] The view model may implement a mediator pattern, organizing access to the back-end logic around the set of use cases supported by the view.
MVVM is a variation of Martin Fowler's Presentation Model design pattern.[2][3] MVVM abstracts a view's state and behavior in the same way,[3] but a Presentation Model abstracts a view (creates a view model) in a manner not dependent on a specific user-interface platform.
MVVM was invented by Microsoft architects Ken Cooper and Ted Peters specifically to simplify event-driven programming of user interfaces. The pattern was incorporated into Windows Presentation Foundation (WPF) (Microsoft's .NET graphics system) and Silverlight (WPF's Internet application derivative).[3] John Gossman, one of Microsoft's WPF and Silverlight architects, announced MVVM on his blog in 2005.[3]
Model¨Cview¨Cviewmodel is also referred to as model¨Cview¨Cbinder, especially in implementations not involving the .NET platform. ZK (a web application framework written in Java) and KnockoutJS (a JavaScript library) use model¨Cview¨Cbinder.[3][4][5]
Model
Model refers either to a domain model, which represents real state content (an object-oriented approach), or to the data access layer, which represents content (a data-centric approach).[citation needed]
View
As in the model-view-controller (MVC) and model-view-presenter (MVP) patterns, the view is the structure, layout, and appearance of what a user sees on the screen.[6] It displays a representation of the model and receives the user's interaction with the view (clicks, keyboard, gestures, etc.), and it forwards the handling of these to the view model via the data binding (properties, event callbacks, etc.) that is defined to link the view and view model.
View model
The view model is an abstraction of the view exposing public properties and commands. Instead of the controller of the MVC pattern, or the presenter of the MVP pattern, MVVM has a binder, which automates communication between the view and its bound properties in the view model. The view model has been described as a state of the data in the model.[7]
The main difference between the view model and the Presenter in the MVP pattern, is that the presenter has a reference to a view whereas the view model does not. Instead, a view directly binds to properties on the view model to send and receive updates. To function efficiently, this requires a binding technology or generating boilerplate code to do the binding.[6]
Binder
Declarative data and command-binding are implicit in the MVVM pattern. In the Microsoft solution stack, the binder is a markup language called XAML.[8] The binder frees the developer from being obliged to write boiler-plate logic to synchronize the view model and view. When implemented outside of the Microsoft stack, the presence of a declarative data binding technology is what makes this pattern possible,[4][9] and without a binder, one would typically use MVP or MVC instead and have to write more boilerplate (or generate it with some other tool).
MVVM was designed to make use of data binding functions in WPF (Windows Presentation Foundation) to better facilitate the separation of view layer development from the rest of the pattern, by removing virtually all GUI code ("code-behind") from the view layer.[3] Instead of requiring user experience (UX) developers to write GUI code, they can use the framework markup language (e.g., XAML) and create data bindings to the view model, which is written and maintained by application developers. The separation of roles allows interactive designers to focus on UX needs rather than programming of business logic. The layers of an application can thus be developed in multiple work streams for higher productivity. Even when a single developer works on the entire code base, a proper separation of the view from the model is more productive, as user interface typically changes frequently and late in the development cycle based on end-user feedback.[citation needed]
The MVVM pattern attempts to gain both advantages of separation of functional development provided by MVC, while leveraging the advantages of data bindings and the framework by binding data as close to the pure application model as possible.[3][10][11][clarification needed] It uses the binder, view model, and any business layers' data-checking features to validate incoming data. The result is that the model and framework drive as much of the operations as possible, eliminating or minimizing application logic which directly manipulates the view (e.g., code-behind).
A criticism of the pattern comes from MVVM creator John Gossman himself,[12] who points out that the overhead in implementing MVVM is "overkill" for simple UI operations. He says that for larger applications, generalizing the ViewModel becomes more difficult. Moreover, he illustrates that data binding in very large applications can result in considerable memory consumption.
##%%&&
In software engineering, dependency injection is a technique whereby one object (or static method) supplies the dependencies of another object. A dependency is an object that can be used (a service). An injection is the passing of a dependency to a dependent object (a client) that would use it. The service is made part of the client's state.[1] Passing the service to the client, rather than allowing a client to build or find the service, is the fundamental requirement of the pattern.
The intent behind dependency injection is to decouple objects to the extent that no client code has to be changed simply because an object it depends on needs to be changed to a different one. This permits following the Open / Closed principle.
Dependency injection is one form of the broader technique of inversion of control. As with other forms of inversion of control, dependency injection supports the dependency inversion principle. The client delegates the responsibility of providing its dependencies to external code (the injector). The client is not allowed to call the injector code;[2] it is the injecting code that constructs the services and calls the client to inject them. This means the client code does not need to know about the injecting code, how to construct the services or even which actual services it is using; the client only needs to know about the intrinsic interfaces of the services because these define how the client may use the services. This separates the responsibilities of use and construction.
There are three common means for a client to accept a dependency injection: setter-, interface- and constructor-based injection. Setter and constructor injection differ mainly by when they can be used. Interface injection differs in that the dependency is given a chance to control its own injection. Each requires that separate construction code (the injector) takes responsibility for introducing a client and its dependencies to each other.[3]
The Dependency Injection design pattern solves problems like: [4]
?	How can an application or class be independent of how its objects are created?
?	How can the way objects are created be specified in separate configuration files?
?	How can an application support different configurations?
Creating objects directly within the class that requires the objects is inflexible because it commits the class to particular objects and makes it impossible to change the instantiation later independently from (without having to change) the class. It stops the class from being reusable if other objects are required, and it makes the class hard to test because real objects can't be replaced with mock objects.
A class is no longer responsible for creating the objects it requires, and it doesn't have to delegate instantiation to a factory object as in the Abstract Factory [5] design pattern.
Dependency injection separates the creation of a client's dependencies from the client's behavior, which allows program designs to be loosely coupled[9] and to follow the dependency inversion and single responsibility principles.[6][10] It directly contrasts with the service locator pattern, which allows clients to know about the system they use to find dependencies.
An injection, the basic unit of dependency injection, is not a new or a custom mechanism. It works in the same way that "parameter passing" works.[11] Referring to "parameter passing" as an injection carries the added implication that it's being done to isolate the client from details.
An injection is also about what is in control of the passing (never the client) and is independent of how the passing is accomplished, whether by passing a reference or a value.
Dependency injection involves four roles:
?	the service object(s) to be used
?	the client object that is depending on the service(s) it uses
?	the interfaces that define how the client may use the services
?	the injector, which is responsible for constructing the services and injecting them into the client
Any object that may be used can be considered a service. Any object that uses other objects can be considered a client. The names have nothing to do with what the objects are for and everything to do with the role the objects play in any one injection.
The interfaces are the types the client expects its dependencies to be. An issue is what they make accessible. They may truly be interface types implemented by the services but also may be abstract classes or even the concrete services themselves, though this last would violate DIP[12] and sacrifice the dynamic decoupling that enables testing. It's only required that the client does not know which they are and therefore never treats them as concrete, say by constructing or extending them.
The client should have no concrete knowledge of the specific implementation of its dependencies. It should only know the interface's name and API. As a result, the client won't need to change even if what is behind the interface changes. However, if the interface is refactored from being a class to an interface type (or vice versa) the client will need to be recompiled.[13] This is significant if the client and services are published separately. This unfortunate coupling is one that dependency injection cannot resolve.
The injector introduces the services into the client. Often, it also constructs the client. An injector may connect together a very complex object graph by treating an object like a client and later as a service for another client. The injector may actually be many objects working together but may not be the client. The injector may be referred to by other names such as: assembler, provider, container, factory, builder, spring, construction code, or main.
Dependency injection can be applied as a discipline, one that asks that all objects separate construction and behavior. Relying on a DI framework to perform construction can lead to forbidding the use of the new keyword, or, less strictly, only allowing direct construction of value objects.[14][15][16][17]
Inversion of control (IoC) is more general than DI. Put simply, IoC means letting other code call you rather than insisting on doing the calling. An example of IoC without DI is the template method pattern. Here, polymorphism is achieved through subclassing, that is, inheritance.[18]
Dependency injection implements IoC through composition so is often identical to that of the strategy pattern, but while the strategy pattern is intended for dependencies to be interchangeable throughout an object's lifetime, in dependency injection it may be that only a single instance of a dependency is used.[19] This still achieves polymorphism, but through delegation and composition.
Application frameworks such as CDI and its implementation Weld, Spring, Guice, Play framework, Salta, Glassfish HK2, Dagger, and Managed Extensibility Framework (MEF) support dependency injection but are not required to do dependency injection.[20][21]
?	Dependency injection allows a client the flexibility of being configurable. Only the client's behavior is fixed. The client may act on anything that supports the intrinsic interface the client expects.
?	Dependency injection can be used to externalize a system's configuration details into configuration files, allowing the system to be reconfigured without recompilation. Separate configurations can be written for different situations that require different implementations of components. This includes, but is not limited to, testing.
?	Because dependency injection doesn't require any change in code behavior it can be applied to legacy code as a refactoring. The result is clients that are more independent and that are easier to unit test in isolation using stubs or mock objectsthat simulate other objects not under test. This ease of testing is often the first benefit noticed when using dependency injection.
?	Dependency injection allows a client to remove all knowledge of a concrete implementation that it needs to use. This helps isolate the client from the impact of design changes and defects. It promotes reusability, testability and maintainability.[22]
?	Reduction of boilerplate code in the application objects, since all work to initialize or set up dependencies is handled by a provider component.[22]
?	Dependency injection allows concurrent or independent development. Two developers can independently develop classes that use each other, while only needing to know the interface the classes will communicate through. Plugins are often developed by third party shops that never even talk to the developers who created the product that uses the plugins.
?	Dependency Injection decreases coupling between a class and its dependency.[23][24]
?	Dependency injection creates clients that demand configuration details be supplied by construction code. This can be onerous when obvious defaults are available.
?	Dependency injection can make code difficult to trace (read) because it separates behavior from construction. This means developers must refer to more files to follow how a system performs.
?	Dependency injection frameworks are implemented with reflection or dynamic programming. This can hinder use of IDE automation, such as "find references", "show call hierarchy" and safe refactorings.
?	Dependency injection typically requires more upfront development effort since one can not summon into being something right when and where it is needed but must ask that it be injected and then ensure that it has been injected.
?	Dependency injection forces complexity to move out of classes and into the linkages between classes which might not always be desirable or easily managed.[25]
?	Dependency injection can encourage dependence on a dependency injection framework.[25][26][27]
In the above UML class diagram, the Client class that requires ServiceA and ServiceB objects doesn't instantiate the ServiceA1 and ServiceB1 classes directly. Instead, an Injector class creates the objects and injects them into the Client, which makes the Client independent of how the objects are created (which concrete classes are instantiated). 
The UML sequence diagram shows the run-time interactions: The Injector object creates the ServiceA1 and ServiceB1 objects. Thereafter, the Injector creates the Client object and injects the ServiceA1 and ServiceB1 objects.
In the following Java example, the Client class contains a Service member variable that is initialized by the Client constructor. The client controls which implementation of service is used and controls its construction. In this situation, the client is said to have a hard-coded dependency on ExampleService.
Dependency injection is an alternative technique to initialize the member variable rather than explicitly creating a service object as shown above.
There are at least three ways an object can receive a reference to an external module:[29]
?	constructor injection: the dependencies are provided through a class constructor.
?	setter injection: the client exposes a setter method that the injector uses to inject the dependency.
?	interface injection: the dependency provides an injector method that will inject the dependency into any client passed to it. Clients must implement an interface that exposes a setter method that accepts the dependency.
It is possible for DI frameworks to have other types of injection beyond those presented above.[30]
Testing frameworks may also use other types. Some modern testing frameworks do not even require that clients actively accept dependency injection thus making legacy code testable. In particular, in the Java language it is possible to use reflection to make private attributes public when testing and thus accept injections by assignment.[31]
Some attempts at Inversion of Control do not provide full removal of dependency but instead simply substitute one form of dependency for another. As a rule of thumb, if a programmer can look at nothing but the client code and tell what framework is being used, then the client has a hard-coded dependency on the framework.
This method requires the client to provide a parameter in a constructor for the dependency.
This method requires the client to provide a setter method for the dependency.
This is simply the client publishing a role interface to the setter methods of the client's dependencies. It can be used to establish how the injector should talk to the client when injecting dependencies.
Preferred when all dependencies can be constructed first because it can be used to ensure the client object is always in a valid state, as opposed to having some of its dependency references be null (not be set). However, on its own, it lacks the flexibility to have its dependencies changed later. This can be a first step towards making the client immutable and therefore thread safe.
Requires the client to provide a setter method for each dependency. This gives the freedom to manipulate the state of the dependency references at any time. This offers flexibility, but if there is more than one dependency to be injected, it is difficult for the client to ensure that all dependencies are injected before the client could be provided for use.
Because these injections happen independently there is no way to tell when the injector is finished wiring the client. A dependency can be left null simply by the injector failing to call its setter. This forces the check that injection was completed from when the client is assembled to whenever it is used.
The advantage of interface injection is that dependencies can be completely ignorant of their clients yet can still receive a reference to a new client and, using it, send a reference-to-self back to the client. In this way, the dependencies become injectors. The key is that the injecting method (which could just be a classic setter method) is provided through an interface.
An assembler is still needed to introduce the client and its dependencies. The assembler would take a reference to the client, cast it to the setter interface that sets that dependency, and pass it to that dependency object which would turn around and pass a reference-to-self back to the client.
For interface injection to have value, the dependency must do something in addition to simply passing back a reference to itself. This could be acting as a factory or sub-assembler to resolve other dependencies, thus abstracting some details from the main assembler. It could be reference-counting so that the dependency knows how many clients are using it. If the dependency maintains a collection of clients, it could later inject them all with a different instance of itself.[example needed]
Manually assembling in main by hand is one way of implementing dependency injection.
The example above constructs the object graph manually and then invokes it at one point to start it working. Important to note is that this injector is not pure. It uses one of the objects it constructs. It has a purely construction-only relationship with ExampleService but mixes construction and using of Client. This should not be common. It is, however, unavoidable. Just like object oriented software needs a non-object oriented static method like main() to get started, a dependency injected object graph needs at least one (preferably only one) entry point to get the whole thing started.
Manual construction in the main method may not be this straight forward and may involve calling builders, factories, or other construction patterns as well. This can be fairly advanced and abstract. The line is crossed from manual dependency injection to framework dependency injection once the constructing code is no longer custom to the application and is instead universal.[32]
Frameworks like Spring can construct these same objects and wire them together before returning a reference to client. All mention of the concrete ExampleService can be moved from the code to the configuration data.
Frameworks like Spring allow assembly details to be externalized in configuration files. This code (above) constructs objects and wires them together according to Beans.xml (below). ExampleService is still constructed even though it's only mentioned below. A long and complex object graph can be defined this way and the only class mentioned in code would be the one with the entry point method, which in this case is greet().
In the example above Client and Service have not had to undergo any changes to be provided by spring. They are allowed to remain simple POJOs.[33][34][35] This shows how spring can connect services and clients that are completely ignorant of its existence. This could not be said if spring annotations are added to the classes. By keeping spring specific annotations and calls from spreading out among many classes, the system stays only loosely dependent on spring.[26] This can be important if the system intends to outlive spring.
The choice to keep POJOs pure doesn't come without cost. Rather than spending the effort to develop and maintain complex configuration files it is possible to simply use annotations to mark classes and let spring do the rest of the work. Resolving dependencies can be simple if they follow a convention such as matching by type or by name. This is choosing convention over configuration.[36] It is also arguable that, when refactoring to another framework, removing framework specific annotations would be a trivial part of the task[37] and many injection annotations are now standardized.[38][39]
The different injector implementations (factories, service locators, and dependency injection containers) are not that different as far as dependency injection is concerned. What makes all the difference is where they are allowed to be used. Move calls to a factory or a service locator out of the client and into main and suddenly main makes a fairly good dependency injection container.
By moving all knowledge of the injector out, a clean client, free of knowledge of the outside world, is left behind. However, any object that uses other objects can be considered a client. The object that contains main is no exception. This main object is not using dependency injection. It's actually using the service locator pattern. This can't be avoided because the choice of service implementations must be made somewhere.
Externalizing the dependencies into configuration files doesn't change this fact. What makes this reality part of a good design is that the service locator is not spread throughout the code base. It's confined to one place per application. This leaves the rest of the code base free to use dependency injection to make clean clients.
The examples until now have been overly simple examples about constructing a string. However, the dependency injection pattern is most useful when constructing an object graph where objects communicate via messages. Objects constructed in main will last for the life of the program. The typical pattern is to construct the graph and then call one method on one object to send the flow of control into the object graph. Just as main is the entry point to the static code, this one method is the entry point to the applications non-static code.
In the AngularJS framework, there are only three ways a component (object or function) can directly access its dependencies:
1.	The component can create the dependency, typically using the new operator.
2.	The component can look up the dependency, by referring to a global variable.
3.	The component can have the dependency passed to it where it is needed.
The first two options of creating or looking up dependencies are not optimal because they hard code the dependency to the component. This makes it difficult, if not impossible, to modify the dependencies. This is especially problematic in tests, where it is often desirable to provide mock dependencies for test isolation.
The third option is the most viable, since it removes the responsibility of locating the dependency from the component. The dependency is simply handed to the component.
In the above example SomeClass is not concerned with creating or locating the greeter dependency, it is simply handed the greeter when it is instantiated.
This is desirable, but it puts the responsibility of getting hold of the dependency on the code that constructs SomeClass.
To manage the responsibility of dependency creation, each AngularJS application has an injector. The injector is a service locator that is responsible for construction and look-up of dependencies.
Here is an example of using the injector service:
Create a new injector that can provide components defined in the myModule module and request our greeter service from the injector. (This is usually done automatically by the AngularJS bootstrap).
Asking for dependencies solves the issue of hard coding, but it also means that the injector needs to be passed throughout the application. Passing the injector breaks the Law of Demeter. To remedy this, we use a declarative notation in our HTML templates, to hand the responsibility of creating components over to the injector, as in this example:
When AngularJS compiles the HTML, it processes the ng-controller directive, which in turn asks the injector to create an instance of the controller and its dependencies.
This is all done behind the scenes. Because the ng-controller defers to the injector to instantiate the class, it can satisfy all of the dependencies of MyController without the controller ever knowing about the injector. The application code simply declares the dependencies it needs, without having to deal with the injector. This setup does not break the Law of Demeter.
##%%&&
In many applications, the business logic accesses data from data stores such as databases, SharePoint lists, or Web services. Directly accessing the data can result in the following:
?	Duplicated code
?	A higher potential for programming errors
?	Weak typing of the business data
?	Difficulty in centralizing data-related policies such as caching
?	An inability to easily test the business logic in isolation from external dependencies
Use the Repository pattern to achieve one or more of the following objectives:
?	You want to maximize the amount of code that can be tested with automation and to isolate the data layer to support unit testing.
?	You access the data source from many locations and want to apply centrally managed, consistent access rules and logic.
?	You want to implement and centralize a caching strategy for the data source.
?	You want to improve the code's maintainability and readability by separating business logic from data or service access logic.
?	You want to use business entities that are strongly typed so that you can identify problems at compile time instead of at run time.
?	You want to associate a behavior with the related data. For example, you want to calculate fields or enforce complex relationships or business rules between the data elements within an entity.
?	You want to apply a domain model to simplify complex business logic.
Use a repository to separate the logic that retrieves the data and maps it to the entity model from the business logic that acts on the model. The business logic should be agnostic to the type of data that comprises the data source layer. For example, the data source layer can be a database, a SharePoint list, or a Web service.
The repository mediates between the data source layer and the business layers of the application. It queries the data source for the data, maps the data from the data source to a business entity, and persists changes in the business entity to the data source. A repository separates the business logic from the interactions with the underlying data source or Web service. The separation between the data and business tiers has three benefits:
?	It centralizes the data logic or Web service access logic.
?	It provides a substitution point for the unit tests.
?	It provides a flexible architecture that can be adapted as the overall design of the application evolves.
There are two ways that the repository can query business entities. It can submit a query object to the client's business logic or it can use methods that specify the business criteria. In the latter case, the repository forms the query on the client's behalf. The repository returns a matching set of entities that satisfy the query. The following diagram shows the interactions of the repository with the client and the data source.
The client submits new or changed entities to the repository for persistence. In more complex situations, the client business logic can use the Unit of Work pattern. This pattern demonstrates how to encapsulate several related operations that should be consistent with each other or that have related dependencies. The encapsulated items are sent to the repository for update or delete actions. This guidance does not include an example of the Unit of Work pattern. For more information, see Unit of Work on Martin Fowler's Web site.
Repositories are bridges between data and operations that are in different domains. A common case is mapping from a domain where data is weakly typed, such as a database or SharePoint list, into a domain where objects are strongly typed, such as a domain entity model. One example is a database that uses IDbCommand objects to execute queries and returns IDataReader objects. Another example is SharePoint, which uses SPQuery objects to return SPListItem collections. A repository issues the appropriate queries to the data source, and then it maps the result sets to the externally exposed business entities. Repositories often use the Data Mapper pattern to translate between representations. Repositories remove dependencies that the calling clients have on specific technologies. For example, if a client calls a catalog repository to retrieve some product data, it only needs to use the catalog repository interface. For example, the client does not need to know if the product information is retrieved with SQL queries to a database or Collaborative Application Markup Language (CAML) queries to a SharePoint list. Isolating these types of dependences provides flexibility to evolve implementations.
This section discusses the implementation strategies for SharePoint list repositories and Web service repositories.
The following diagram illustrates the interactions of a SharePoint list repository with SharePoint lists and the business logic.
Interactions of a SharePoint list repository
Using the Repository pattern in a SharePoint application addresses several concerns.
?	SharePoint applications often store business information in SharePoint lists. To retrieve data from SharePoint lists requires careful use of the SharePoint API, knowledge of the GUIDs that are related to the lists and their fields, and a working knowledge of CAML. Repositories centralize this logic.
?	The amount of code that is required to query or update a SharePoint list item is enough to warrant its encapsulation into helper methods. When Web Forms, event receivers, and workflow business logic all require access to the same lists, the code that accesses the SharePoint lists can be duplicated throughout the application. This can make the application prone to bugs and difficult to maintain. Repositories eliminate this duplication.
?	Without a repository, the application is difficult to unit test because the business logic has direct dependencies on the SharePoint lists. Repositories centralize the access logic and provide a substitution point for the unit tests.
Externally, the repository exposes strongly-typed business entities. Internally, it works with SharePoint-specific objects, such as the SPQuery and the SPListItem objects. The SharePoint Guidance Library, which is a part of this guidance, provides classes for mapping and querying that make it easier to build repositories for SharePoint lists. The ListItemFieldMapper class converts strongly-typed business entities to and from SPListItem objects based on a set of mapping definitions. The CAMLQueryBuilder class builds SPQueryobjects based on common query operations. The SPQuery object is used to query a SharePoint list.
The following sections show how the repository pattern is implemented in the SharePoint Guidance Library. For more information, see List-Based Repositories.
The following diagram shows the major components of a SharePoint list repository.
Components of a SharePoint list repository
The list repository contains a query object and a data mapper object that are specific to SharePoint. These are the ListItemFieldMapper and CAMLQueryBuilder classes. The data mapper translates between an SPListItem and the business entity that is defined by the application. The query object internally constructs an SPQuery object and uses CAML to query the list.
When you design a SharePoint list repository, keep in mind that a list can contain fields from multiple content types. The logic that is implemented in the ListItemFieldMapper and CAMLQueryBuilder objects does not prevent fields from multiple content types from being retrieved. In some cases, if the content types have the same parent content type, you can use a single repository to project a common view across these content types.
However, it is generally inadvisable to create repositories that deal with dissimilar content types and return different business entities from the same repository. In this situation, create a repository for each content type because the content types logically represent different entities.
When you create a SharePoint list repository, you should consider how the repository locates the list that it is going to access. A list typically resides in a site, and it can be accessed either through its Uniform Resource Identifier (URI) or its GUID. The repository needs one of these, but passing this information to a repository can be challenging if you use the repository in conjunction with a service location. For more information, see The Service Locator Pattern.
There are three ways in which a repository can access a list:
?	A list can be centrally located. In this case, a repository is associated with a list that is at a fixed location. All sites retrieve the data from this central location. The PartnerPromotionsRepository class in the Partner Portal application is an example of a repository that uses such as list.
?	A list can be accessed relative to the current site context. In this case, a repository is associated with a list whose location is relative to the current site. The IncidentManagementRepository class in the Partner Portal application is an example of such a repository.
?	A list can be accessed according to a context that is supplied by a consumer. In this case, only the consumer of the repository knows which list the repository should access. There is no example of this in the Partner Portal application. However, you can extend the Training Management application to support this use case. You can implement a repository that accesses the list of training courses for your department on your local installation and also accesses related training courses that are located on another departmental site. In this case, the consumer instructs the repository to target a particular list.
The following sections describe more details about how to associate repositories with lists.
In this case, a list is at a fixed location, and all sites access it from this central point. Its location cannot be determined based on the current context. Although it is possible to hard code the location of the list, this is not recommended, because the topology of the site can change. It is often better to make the location of the list a configuration setting. In that case, defining the list's location is an administrative task. The location is established when the site topology is set up.
For example, the Partner Portal application centrally manages the published promotions for all partners by locating them on one site collection. Partners see their particular promotions on their collaboration home pages. Each partner collaboration site is hosted in its own site collection. This establishes security boundaries and isolates the data intended for one partner from the data intended for another partner. Because the relationship between the list and consumers of the list is based on the operational topology, the list location is defined with configuration data.
The following are characteristics of a list with a fixed location:
?	There is typically only one instance of that list within the Web application scope.
?	The location of the list is determined when the site topology is designed or when the site is installed.
?	The list location should be retrieved from configuration data that is shared by all consumers. This data is typically at the Web application level or Web farm level.
The following diagram shows the flow of information among components that access a list at a central location.
Associating a repository with a list at a central location
The service locator constructs a repository object, which then reads the configuration information. The repository accesses the list based on this data. Because the repository relies on the configuration data, it can be constructed independently of the application context. It does not need any additional information from the list consumers.
This approach is susceptible to run-time exceptions because it depends on configuration data, which can be erroneous, lost, or corrupted. Make sure that you provide adequate diagnostics that inform IT administrators of any configuration errors. Problems that are caused by configuration errors are difficult to resolve without adequate logging information.
In this case, the repository is associated with a list whose location is fixed, relative to the current context. For example, in the Training Management application, registration and course lists are located at the same relative location within a site. However, there can be several Training Management sites within a SharePoint farm. In this situation, the list repository is loaded from the current context.
The following are characteristics of a list with a location that is relative to the context:
?	The list has a fixed location that is relative to a site (an SPWeb object).
?	The location is independent of the site topology.
?	The list location is based on the current SharePoint context.
The following diagram shows the components and flow of information that access a list with a location that is relative to the context.
Associating a repository with a list whose location is relative to the context
SharePoint often has a number of instances of the same Web application. In this situation, the repository gets the current site from the SharePoint context (the SPContext.Current.Web object) and loads the list information from this context. Because this relationship is fixed relative to the current site, the repository needs no additional information. The repository instance can be directly constructed by the service locator.
In this case, the repository is associated with a particular content type and can access any list that has SPListItems of this content type. The consumer must provide context to the repository. For example, the consumer might provide the SPWeb object that holds the list or the GUID of the list. Although it is generally a good practice to keep technology-specific dependencies (such as a reliance on SQL Server) out of the repository interface, providing context when the repository is constructed is an accepted, widely used practice.
This scenario occurs with sites that have a dynamic topology, or where relationships are established by a user who supplies configuration information. If you add or remove sites at run time that contain lists that the repository accesses, you often have to provide the context. An example is if you use the Finance training site to view courses but you also want to see the courses on the Human Resources training site.
To provide this capability, you can build a general purpose Web Part to view the courses from other departmental training sites. You can add this Web Part to the Finance training site and configure it to view the related Human Resources department courses. The repository that the Web Part on the Finance site uses receives the location of the Human Resources course list as context information when the Web Part constructs it.
One challenge with this type of repository is using it in combination with the SharePoint Guidance Library service locator. The ActivatingServiceLocator class can only use parameterless constructors for the repositories. It is not possible to pass the contextual information (in this case, the location of the list) into the repository through the constructor. One way to solve this is to pass the location of the list with each method call, but this inserts a dependency on the list's URL into the interface definition. The Training Management application uses this approach.
A better, but more complicated way to pass the location of the list to the repository is to use a factory. The factory includes a method that creates the repository. The consumer passes the location of the list to the method. The consumer then uses the ActivatingServiceLocator to access the factory and uses it to create the repository. With this approach, the consumer provides the location of the list to the factory, which in turn creates the repository. The factory passes the context through the repository constructor. This technique is known as constructor injection.
The following are characteristics of a list whose context is supplied by a consumer:
?	The consumer can determine which list the repository should access.
?	The location of the list is often determined at run time.
?	This list location is derived from the current business context.
The following diagram shows the components and flow of information that are involved in accessing a list whose context is supplied by the consumer.
Associating a repository with a list whose context is supplied by the consumer
The consumer constructs the context for the repository. The consumer retrieves an instance of a repository factory from the service locator. The consumer then uses the repository factory to construct the repository. The consumer provides the context for the list. The repository uses this information to locate the list. Because the repository is decoupled from both the configuration data and the context, it is suitable for many scenarios. However, because the consumer provides the context, it increases the coupling between the consuming code and the repository.
A common backing store for data is a business service that is exposed by a line-of-business (LOB) application. Generally, these business services are at a higher level of abstraction than the standard Create/Read/Update/Delete (CRUD) semantics of a database or SharePoint list. However, from the perspective of the client, they often are equivalent to a data source. Like with SharePoint lists, accessing Web services can be complex and prone to error. A repository centralizes the access logic for a service and provides a substitution point for unit tests. Note that services are often expensive to invoke and benefit from caching strategies that are implemented within the repository.
The following diagram shows a service back-end repository that uses caching.
Using a repository with a Web service
In this case, the query logic in the repository first checks to see whether the queried items are in the cache. If they are not, the repository accesses the Web service to retrieve the information. Although it is possible to access services directly, it is also possible to access them through the SharePoint Business Data Catalog (BDC). The BDC can aggregate several data sources, including Web services, and expose them through a uniform, generic interface. The BDC allows you to use standard Web Parts to display and modify data. For more information, see Consuming Web Services with the Business Data Catalog (BDC).
You may need more complex security options than the BDC supports. In this situation, you can use the Windows Communication Foundation (WCF). This requires that your own code and configuration data manage the service information and security context. For more information, see Integrating Line-of-Business Systems.
For an example of the list repository pattern, see Development How-to Topics. Also, the Partner Portal application includes the following list repositories that can be used as starting points:
?	The Partner Promotion Repository is in the PartnerPromotionRepository.cs file of the PartnerPortal\Contoso.PartnerPortal.Promotions directory. There is also a mock implementation for unit testing in the PartnerPromotionsPresenterFixture.cs file of the PartnerPortal\Contoso.PartnerPortal.Promotions.Tests directory.
?	The Business Event Type Configuration Repository is in the BusinessEventTypeConfigurationRepository.cs file of the Microsoft.Practices.SPG2\Microsoft.Practices.SPG.SubSiteCreation\BusinessEventTypeConfiguration directory. There is also a mock implementation for unit testing in the ResolveSiteTemplateFixture.cs file of the Microsoft.Practices.SPG2\Microsoft.Practices.SPG.SubSiteCreation.Tests directory.
?	The Subsite Creation Requests Repository is in the SubSiteCreationRequestsRepository.cs file of the directory Microsoft.Practices.SPG2\Microsoft.Practices.SPG.SubSiteCreation\SubSiteCreationRequests.
For an example of the data repository pattern using Web services, see the following areas of the reference implementation:
?	The Incident Management Repository is in the IncidentManagementRepository.cs file of the directory PartnerPortal\Contoso.LOB.Services.Client\Repositories.
?	The Pricing Repository is in the PricingRepository.cs file of the directory PartnerPortal\Contoso.LOB.Services.Client\Repositories.
?	The Cached BDC Product Catalog Repository is in the CachedBdcProductCatalogRepository.cs file of the directory PartnerPortal\Contoso.LOB.Services.Client\Repositories. There is also a mock implementation for unit testing in the ProductDetailsPresenterFixture.cs file of the directory PartnerPortal\Contoso.PartnerPortal.ProductCatalog.Tests.
The Partner Portal application also contains two other repositories:
?	The Full Text Search IncidentTask Repository uses SharePoint Search as its data source. This repository is found in the FullTextSearchIncidentTaskRepository.cs file of the directory PartnerPortal\Contoso.PartnerPortal.Collaboration.Incident\Repositories.
?	The Partner Site Directory uses the site directory list to provide the Partner site collection URL and the user profile to provide the PartnerID. The repository is implemented in the PartnerSiteDirectory.cs file of the directory PartnerPortal\Contoso.PartnerPortal.PartnerDirectory.
For more information about the Repository pattern, Unit of Work pattern, and Data Mapper pattern, see Repository on Martin Fowler's Web site.
The Repository pattern increases the level of abstraction in your code. This may make the code more difficult to understand for developers who are unfamiliar with the pattern. Although implementing the pattern reduces the amount of redundant code, it generally increases the number of classes that must be maintained.
The Repository pattern helps to isolate both the service and the list access code. Isolation makes it easier to treat them as independent services and to replace them with mock objects in unit tests. Typically, it is difficult to unit test the repositories themselves, so it is often better to write integration tests for them.
When caching data in a multithreaded environment, consider synchronizing access to the cache in addition to the cached objects. Often, common caches, such as the ASP.NET cache, are already thread safe, but you must also ensure that the objects themselves can operate in a multithreaded environment.
If you are caching data in heavily loaded systems, performance can be an issue. Consider synchronizing access to the data source. This ensures that only a single request for the data is issued to the list or back-end service. All other clients rely on the retrieved data. For more information, see Techniques for Aggregating List and Site Information.
The following two patterns are often used in conjunction with the Repository pattern:
?	Data Mapper. This pattern describes how to map data to different schemas. It is often used to map between a data store and a domain model.
?	Unit of Work. This pattern keeps track of everything that happens during a business transaction that affects the database. At the conclusion of the transaction, it determines how to update the database to conform to the changes.
##%%&&
Model¨Cview¨Cpresenter (MVP) is a derivation of the model¨Cview¨Ccontroller (MVC) architectural pattern, and is used mostly for building user interfaces.
In MVP, the presenter assumes the functionality of the "middle-man". In MVP, all presentation logic is pushed to the presenter.[1]
The model-view-presenter software pattern originated in the early 1990s at Taligent, a joint venture of Apple, IBM, and Hewlett-Packard. MVP is the underlying programming model for application development in Taligent's C++-based CommonPoint environment. The pattern was later migrated by Taligent to Java and popularized in a paper by Taligent CTO Mike Potel.[2]
After Taligent's discontinuation in 1998, Andy Bower and Blair McGlashan of Dolphin Smalltalk adapted the MVP pattern to form the basis for their Smalltalk user interface framework.[3] In 2006, Microsoft began incorporating MVP into its documentation and examples for user interface programming in the .NET framework.[4][5]
The evolution and multiple variants of the MVP pattern, including the relationship of MVP to other design patterns such as MVC, is discussed in detail in an article by Martin Fowler[6] and another by Derek Greer.[7]
MVP is a user interface architectural pattern engineered to facilitate automated unit testing and improve the separation of concerns in presentation logic:
?	The model is an interface defining the data to be displayed or otherwise acted upon in the user interface.
?	The view is a passive interface that displays data (the model) and routes user commands (events) to the presenter to act upon that data.
?	The presenter acts upon the model and the view. It retrieves data from repositories (the model), and formats it for display in the view.
Normally, the view implementation instantiates the concrete presenter object, providing a reference to itself. The following C# code demonstrates a simple view constructor, where ConcreteDomainPresenter implements the IDomainPresenter interface:
The degree of logic permitted in the view varies among different implementations. At one extreme, the view is entirely passive, forwarding all interaction operations to the presenter. In this formulation, when a user triggers an event method of the view, it does nothing but invoke a method of the presenter that has no parameters and no return value. The presenter then retrieves data from the view through methods defined by the view interface. Finally, the presenter operates on the model and updates the view with the results of the operation. Other versions of model-view-presenter allow some latitude with respect to which class handles a particular interaction, event, or command. This is often more suitable for web-based architectures, where the view, which executes on a client's browser, may be the best place to handle a particular interaction or command.
From a layering point of view, the presenter class might be considered as belonging to the application layer in a multilayered architecture system, but it can also be seen as a presenter layer of its own between the application layer and the user interface layer.
The .NET environment supports the MVP pattern much like any other development environment. The same model and presenter class can be used to support multiple interfaces, such as an ASP.NET Web application, a Windows Forms application, or a Silverlight application. The presenter gets and sets information from/to the view through an interface that can be accessed by the interface (view) component.
In addition to manually implementing the pattern, a model-view-presenter framework may be used to support the MVP pattern in a more automated fashion. Below is a list of such frameworks under the .NET platform.
In a Java (AWT/Swing/SWT) application, the MVP pattern can be used by letting the user interface class implement a view interface.
The same approach can be used for Java web-based applications, since modern Java component-based Web frameworks allow development of client-side logic using the same component approach as thick clients.
Implementing MVP in Google Web Toolkit requires only that some component implement the view interface. The same approach is possible using Vaadin or the Echo2 Web framework.
MVP can be implemented in Java SE (AWT and Swing) applications using the Biscotti and MVP4J frameworks.
Java frameworks include the following:
As of PHP's flexible runtime environment, there are wide possibilities of approaches of an application logic. A great example of MVP pattern implementation is Nette Framework implementing rich presenter layer and view layer through templating system Latte (web template engine). Implementation of model layer is left on the end application programmer.
##%%&&
Page Object is a Design Pattern which has become popular in test automation for enhancing test maintenance and reducing code duplication. A page object is an object-oriented class that serves as an interface to a page of your AUT. The tests then use the methods of this page object class whenever they need to interact with the UI of that page. The benefit is that if the UI changes for the page, the tests themselves don¡¯t need to change, only the code within the page object needs to change. Subsequently all changes to support that new UI are located in one place.
The Page Object Design Pattern provides the following advantages
1. There is a clean separation between test code and page specific code such as locators (or their use if you¡¯re using a UI Map) and layout.
2. There is a single repository for the services or operations offered by the page rather than having these services scattered throughout the tests.
In both cases this allows any modifications required due to UI changes to all be made in one place. Useful information on this technique can be found on numerous blogs as this ¡®test design pattern¡¯ is becoming widely used. We encourage the reader who wishes to know more to search the internet for blogs on this subject. Many have written on this design pattern and can provide useful tips beyond the scope of this user guide. To get you started, though, we¡¯ll illustrate page objects with a simple example.
First, consider an example, typical of test automation, that does not use a page object.
There are two problems with this approach.
There is no separation between the test method and the AUT¡¯s locators (IDs in this example); both are intertwined in a single method. If the AUT¡¯s UI changes its identifiers, layout, or how a login is input and processed, the test itself must change.
The ID-locators would be spread in multiple tests, in all tests that had to use this login page.
Applying the page object techniques, this example could be rewritten like this in the following example of a page object for a Sign-in page.
and page object for a Home page could look like this.
So now, the login test would use these two page objects as follows.
There is a lot of flexibility in how the page objects may be designed, but there are a few basic rules for getting the desired maintainability of your test code.
Page objects themselves should never make verifications or assertions. This is part of your test and should always be within the test¡¯s code, never in an page object. The page object will contain the representation of the page, and the services the page provides via methods but no code related to what is being tested should be within the page object.
There is one, single, verification which can, and should, be within the page object and that is to verify that the page, and possibly critical elements on the page, were loaded correctly. This verification should be done while instantiating the page object. In the examples above, both the SignInPage and HomePage constructors check that the expected page is available and ready for requests from the test.
A page object does not necessarily need to represent an entire page. The Page Object design pattern could be used to represent components on a page. If a page in the AUT has multiple components, it may improve maintainability if there is a separate page object for each component.
There are other design patterns that also may be used in testing. Some use a Page Factory for instantiating their page objects. Discussing all of these is beyond the scope of this user guide. Here, we merely want to introduce the concepts to make the reader aware of some of the things that can be done. As was mentioned earlier, many have blogged on this topic and we encourage the reader to search for blogs on these topics.
##%%&&
The curiously recurring template pattern (CRTP) is an idiom in C++ in which a class X derives from a class template instantiation using X itself as template argument.[1] More generally it is known as F-bound polymorphism, and it is a form of F-bounded quantification.
The technique was formalized in 1989 as "F-bounded quantification."[2] The name "CRTP" was independently coined by Jim Coplien in 1995,[3] who had observed it in some of the earliest C++ template code as well as in code examples that Timothy Budd created in his multiparadigm language Leda.[4] It is sometimes called "Upside-Down Inheritance"[5][6] due to the way it allows class hierarchies to be extended by substituting different base classes.
The Microsoft Implementation of CRTP in Active Template Library (ATL) was independently discovered, also in 1995 by Jan Falkin who accidentally derived a base class from a derived class. Christian Beaumont, first saw Jan's code and initially thought it couldn't possibly compile in the Microsoft compiler available at the time. Following this revelation that it did indeed work, Christian based the entire ATL and Windows Template Library (WTL) design on this mistake.[citation needed]
Some use cases for this pattern are static polymorphism and other metaprogramming techniques such as those described by Andrei Alexandrescu in Modern C++ Design.[7] It also figures prominently in the C++ implementation of the Data, Context, and Interaction paradigm.[8]
Typically, the base class template will take advantage of the fact that member function bodies (definitions) are not instantiated until long after their declarations, and will use members of the derived class within its own member functions, via the use of a cast; e.g.:
In the above example, note in particular that the function Base<Derived>::interface(), though declared before the existence of the struct Derived is known by the compiler (i.e., before Derived is declared), is not actually instantiated by the compiler until it is actually called by some later code which occurs after the declaration of Derived (not shown in the above example), so that at the time the function "implementation" is instantiated, the declaration of Derived::implementation() is known.
This technique achieves a similar effect to the use of virtual functions, without the costs (and some flexibility) of dynamic polymorphism. This particular use of the CRTP has been called "simulated dynamic binding" by some.[9] This pattern is used extensively in the Windows ATL and WTL libraries.
To elaborate on the above example, consider a base class with no virtual functions. Whenever the base class calls another member function, it will always call its own base class functions. When we derive a class from this base class, we inherit all the member variables and member functions that weren't overridden (no constructors or destructors). If the derived class calls an inherited function which then calls another member function, that function will never call any derived or overridden member functions in the derived class.
However, if base class member functions use CRTP for all member function calls, the overridden functions in the derived class will be selected at compile time. This effectively emulates the virtual function call system at compile time without the costs in size or function call overhead (VTBL structures, and method lookups, multiple-inheritance VTBL machinery) at the disadvantage of not being able to make this choice at runtime.
The main purpose of an object counter is retrieving statistics of object creation and destruction for a given class.[10] This can be easily solved using CRTP:
Each time an object of class X is created, the constructor of counter<X> is called, incrementing both the created and alive count. Each time an object of class X is destroyed, the alive count is decremented. It is important to note that counter<X> and counter<Y> are two separate classes and this is why they will keep separate counts of X's and Y's. In this example of CRTP, this distinction of classes is the only use of the template parameter (T in counter<T>) and the reason why we cannot use a simple un-templated base class.
Method chaining, also known as named parameter idiom, is a common syntax for invoking multiple method calls in object-oriented programming languages. Each method returns an object, allowing the calls to be chained together in a single statement without requiring variables to store the intermediate results.
When the named parameter object pattern is applied to an object hierarchy, things can get wrong. Suppose we have such a base class:
Prints can be easily chained:
However, if we define the following derived class:
we "lose" the concrete class as soon as we invoke a function of the base:
This happens because 'print' is a function of the base - 'Printer' - and then it returns a 'Printer' instance.
The CRTP can be used to avoid such problem and to implement "Polymorphic chaining":[11]
When using polymorphism, one sometimes needs to create copies of objects by the base class pointer. A commonly used idiom for this is adding a virtual clone function that is defined in every derived class. The CRTP can be used to avoid having to duplicate that function or other similar functions in every derived class.
This allows obtaining copies of squares, circles or any other shapes by shapePtr->clone().
One issue with static polymorphism is that without using a general base class like "Shape" from the above example, derived classes cannot be stored homogeneously as each CRTP base class is a unique type. For this reason, it is more common to inherit from a shared base class with a virtual destructor, like the example above.
##%%&&
Hierarchical model¨Cview¨Ccontroller (HMVC) is a software architectural pattern, a variation of model¨Cview¨Ccontroller (MVC) similar to presentation¨Cabstraction¨Ccontrol (PAC), that was published in 2000 in an article[1] in JavaWorld Magazine, the authors apparently unaware[2] of PAC, which was published 13 years earlier.
The controller has some oversight in that it selects first the model and then the view, realizing an approval mechanism by the controller. The model prevents the view from accessing the data source directly.
The largest practical benefit of using an HMVC architecture is the "widgetization" of content structures.[3] An example might be comments, ratings, Twitter or blog RSS feed displays, or the display of shopping cart contents for an e-commerce website. It is essentially a piece of content that needs to be displayed across multiple pages, and possibly even in different places, depending on the context of the main HTTP request.
Traditional MVC frameworks generally do not provide a direct answer for these types of content structures, so programmers often end up duplicating and switching layouts, using custom helpers, creating their own widget structures or library files, or pulling in unrelated data from the main requested Controller to push through to the View and render in a partial. The downside is that the responsibility of rendering a particular piece of content or loading required data leaks into multiple areas and gets duplicated in the respective places.
HMVC, or specifically the ability to dispatch sub-requests to a Controller to handle these responsibilities aims to solve this problem. The structure mimics that of traditional MVC. For example, if one needs to load some data about comments, and display them in HTML format, one would send a request to the comments Controller with some parameters. The request then interacts with the Model, picks a View, which displays the content. The difference from a traditional MVC is that instead of displaying the comments in a fully separate page, they are displayed inline below the article the user is viewing. In this regard, HMVC strives to increase code modularity, aid reusability, and maintain a better separation of concerns.
##%%&&
View controllers are the foundation of your app¡¯s internal structure. Every app has at least one view controller, and most apps have several. Each view controller manages a portion of your app¡¯s user interface as well as the interactions between that interface and the underlying data. View controllers also facilitate transitions between different parts of your user interface.
Because they play such an important role in your app, view controllers are at the center of almost everything you do. The UIViewController class defines the methods and properties for managing your views, handling events, transitioning from one view controller to another, and coordinating with other parts of your app. You subclass UIViewController (or one of its subclasses) and add the custom code you need to implement your app¡¯s behavior.
There are two types of view controllers:
?	Content view controllers manage a discrete piece of your app¡¯s content and are the main type of view controller that you create.
?	Container view controllers collect information from other view controllers (known as child view controllers) and present it in a way that facilitates navigation or presents the content of those view controllers differently.
Most apps are a mixture of both types of view controllers.
The most important role of a view controller is to manage a hierarchy of views. Every view controller has a single root view that encloses all of the view controller¡¯s content. To that root view, you add the views you need to display your content. Figure 1-1 illustrates the built-in relationship between the view controller and its views. The view controller always has a reference to its root view and each view has strong references to its subviews.
It is common practice to use outlets to access other views in your view controller¡¯s view hierarchy. Because a view controller manages the content of all its views, outlets let you store references to the views that you need. The outlets themselves are connected to the actual view objects automatically when the views are loaded from the storyboard.
A content view controller manages all of its views by itself. A container view controller manages its own views plus the root views from one or more of its child view controllers. The container does not manage the content of its children. It manages only the root view, sizing and placing it according to the container¡¯s design. Figure 1-2 illustrates the relationship between a split view controller and its children. The split view controller manages the overall size and position of its child views, but the child view controllers manage the actual contents of those views.
For information about managing your view controller¡¯s views, see Managing View Layout.
A view controller acts as an intermediary between the views it manages and the data of your app. The methods and properties of the UIViewController class let you manage the visual presentation of your app. When you subclass UIViewController, you add any variables you need to manage your data in your subclass. Adding custom variables creates a relationship like the one in Figure 1-3, where the view controller has references to your data and to the views used to present that data. Moving data back and forth between the two is your responsibility.
You should always maintain a clean separation of responsibilities within your view controllers and data objects. Most of the logic for ensuring the integrity of your data structures belongs in the data objects themselves. The view controller might validate input coming from views and then package that input in the format that your data objects require, but you should minimize the view controller¡¯s role in managing the actual data.
A UIDocument object is one way to manage your data separately from your view controllers. A document object is a controller object that knows how to read and write data to persistent storage. When you subclass, you add whatever logic and methods you need to extract that data and pass it to a view controller or other parts of your app. The view controller might store a copy of any data it receives to make it easier to update views, but the document still owns the true data.
View controllers are responder objects and are capable of handling events that come down the responder chain. Although they are able to do so, view controllers rarely handle touch events directly. Instead, views usually handle their own touch events and report the results to a method of an associated delegate or target object, which is usually the view controller. So most events in a view controller are handled using delegate methods or action methods.
For more information about implementing action methods in your view controller, see Handling User Interactions. For information about handling other types of events, see Event Handling Guide for iOS.
A view controller assumes all responsibility for its views and any objects that it creates. The UIViewController class handles most aspects of view management automatically. For example, UIKit automatically releases any view-related resources that are no longer needed. In your UIViewController subclasses, you are responsible for managing any objects you create explicitly.
When the available free memory is running low, UIKit asks apps to free up any resources that they no longer need. One way it does this is by calling the didReceiveMemoryWarning method of your view controllers. Use that method to remove references to objects that you no longer need or can recreate easily later. For example, you might use that method to remove cached data. It is important to release as much memory as you can when a low-memory condition occurs. Apps that consume too much memory may be terminated outright by the system to recover memory.
View controllers are responsible for the presentation of their views and for adapting that presentation to match the underlying environment. Every iOS app should be able to run on iPad and on several different sizes of iPhone. Rather than provide different view controllers and view hierarchies for each device, it is simpler to use a single view controller that adapts its views to the changing space requirements.
In iOS, view controllers need to handle coarse-grained changes and fine-grained changes. Coarse-grained changes happen when a view controller¡¯s traits change. Traits are attributes that describe the overall environment, such as the display scale. Two of the most important traits are the view controller¡¯s horizontal and vertical size classes, which indicate how much space the view controller has in the given dimension. You can use size class changes to change the way you lay out your views, as shown in Figure 1-4. When the horizontal size class is regular, the view controller takes advantage of the extra horizontal space to arrange its content. When the horizontal size class is compact, the view controller arranges its content vertically.
Within a given size class, it is possible for more fine-grained size changes to occur at any time. When the user rotates an iPhone from portrait to landscape, the size class might not change but the screen dimensions usually change. When you use Auto Layout, UIKit automatically adjusts the size and position of views to match the new dimensions. View controllers can make additional adjustments as needed.
The relationships among your app¡¯s view controllers define the behaviors required of each view controller. UIKit expects you to use view controllers in prescribed ways. Maintaining the proper view controller relationships ensures that automatic behaviors are delivered to the correct view controllers when they are needed. If you break the prescribed containment and presentation relationships, portions of your app will stop behaving as expected.
The root view controller is the anchor of the view controller hierarchy. Every window has exactly one root view controller whose content fills that window. The root view controller defines the initial content seen by the user. Figure 2-1 shows the relationship between the root view controller and the window. Because the window has no visible content of its own, the view controller¡¯s view provides all of the content.
The root view controller is accessible from the rootViewController property of the UIWindow object. When you use storyboards to configure your view controllers, UIKit sets the value of that property automatically at launch time. For windows you create programmatically, you must set the root view controller yourself.
Container view controllers let you assemble sophisticated interfaces from more manageable and reusable pieces. A container view controller mixes the content of one or more child view controllers together with optional custom views to create its final interface. For example, a UINavigationController object displays the content from a child view controller together with a navigation bar and optional toolbar, which are managed by the navigation controller. UIKit includes several container view controllers, including UINavigationController, UISplitViewController, and UIPageViewController.
A container view controller¡¯s view always fills the space given to it. Container view controllers are often installed as root view controllers in a window (as shown in Figure 2-2), but they can also be presented modally or installed as children of other containers. The container is responsible for positioning its child views appropriately. In the figure, the container places the two child views side by side. Although it depends on the container interface, child view controllers may have minimal knowledge of the container and any sibling view controllers.
Because a container view controller manages its children, UIKit defines rules for how you set up those children in custom containers. For detailed information about how to create a custom container view controller, see Implementing a Container View Controller.
Presenting a view controller replaces the current view controller¡¯s contents with those of a new one, usually hiding the previous view controller¡¯s contents. Presentations are most often used for displaying new content modally. For example, you might present a view controller to gather input from the user. You can also use them as a general building block for your app¡¯s interface.
When you present a view controller, UIKit creates a relationship between the presenting view controller and the presented view controller, as shown in Figure 2-3. (There is also a reverse relationship from the presented view controller back to its presenting view controller.) These relationships form part of the view controller hierarchy and are a way to locate other view controllers at runtime.
When container view controllers are involved, UIKit may modify the presentation chain to simplify the code you have to write. Different presentation styles have different rules for how they appear onscreen¡ªfor example, a full-screen presentation always covers the entire screen. When you present a view controller, UIKit looks for a view controller that provides a suitable context for the presentation. In many cases, UIKit chooses the nearest container view controller but it might also choose the window¡¯s root view controller. In some cases, you can also tell UIKit which view controller defines the presentation context and should handle the presentation.
Figure 2-4 shows why containers usually provide the context for a presentation. When performing a full-screen presentation, the new view controller needs to cover the entire screen. Rather than requiring the child to know the bounds of its container, the container decides whether to handle the presentation. Because the navigation controller in the example covers the entire screen, it acts as the presenting view controller and initiates the presentation.
View controllers are an essential tool for apps running on iOS, and the view controller infrastructure of UIKit makes it easy to create sophisticated interfaces without writing a lot of code. When implementing your own view controllers, use the following tips and guidelines to ensure that you are not doing things that might interfere with the natural behavior expected by the system.
Many iOS frameworks define view controllers that you can use as-is in your apps. Using these system-supplied view controllers saves time for you and ensures a consistent experience for the user.
Most system view controllers are designed for specific tasks. Some view controllers provide access to user data such as contacts. Others might provide access to hardware or provide specially tuned interfaces for managing media. For example, the UIImagePickerController class in UIKit displays a standard interface for capturing pictures and video and for accessing the user¡¯s camera roll.
Before you create your own custom view controller, look at the existing frameworks to see if a view controller already exists for the task you want to perform.
?	The UIKit framework provides view controllers for displaying alerts, taking pictures and video, and managing files on iCloud. UIKit also defines many standard container view controllers that you can use to organize your content.
?	The GameKit framework provides view controllers for matching players and for managing leaderboards, achievements, and other game features.
?	The Address Book UI framework provides view controllers for displaying and picking contact information.
?	The MediaPlayer framework provides view controllers for playing and managing video, and for choosing media assets from the user¡¯s library.
?	The EventKit UI framework provides view controllers for displaying and editing the user¡¯s calendar data.
?	The GLKit framework provides a view controller for managing an OpenGL rendering surface.
?	The Multipeer Connectivity framework provides view controllers for detecting other users and inviting them to connect.
?	The Message UI framework provides view controllers for composing emails and SMS messages.
?	The PassKit framework provides view controllers for displaying passes and adding them to Passbook.
?	The Social framework provides view controllers for composing messages for Twitter, Facebook, and other social media sites.
?	The AVFoundation framework provides a view controller for displaying media assets.
Never modify the view hierarchy of system-provided view controllers. Each view controller owns its view hierarchy and is responsible for maintaining the integrity of that hierarchy. Making changes might introduce bugs into your code or prevent the owning view controller from operating correctly. In the case of system view controllers, always rely on the publicly available methods and properties of the view controller to make all modifications.
For information about using a specific view controller, see the reference documentation for the corresponding framework.
View controllers should always be self-contained objects. No view controller should have knowledge about the internal workings or view hierarchy of another view controller. In cases where two view controllers need to communicate or pass data back and forth, they should always do so using explicitly defined public interfaces.
The delegation design pattern is frequently used to manage communication between view controllers. With delegation, one object defines a protocol for communicating with an associated delegate object, which is any object that conforms to the protocol. The exact type of the delegate object is unimportant. All that matters is that it implements the methods of the protocol.
Use the root view of your view controller solely as a container for the rest of your content. Using the root view as a container gives all of your views a common parent view, which makes many layout operations simpler. Many Auto Layout constraints require a common parent view to lay out the views properly.
In the model-view-controller design pattern, a view controller¡¯s role is to facilitate the movement of data between your model objects and your view objects. A view controller might store some data in temporary variables and perform some validation, but its main responsibility is to ensure that its views contain accurate information. Your data objects are responsible for managing the actual data and for ensuring the overall integrity of that data.
An example of the separation of data and interface exists in the relationship between the UIDocument and UIViewController classes. Specifically, no default relationship exists between the two. A UIDocument object coordinates the loading and saving of data, while a UIViewController object coordinates the display of views onscreen. If you create a relationship between the two objects, remember that the view controller should only cache information from the document for efficiency. The actual data still belongs to the document object.
Apps can run on a variety of iOS devices, and view controllers are designed to adapt to different-sized screens on those devices. Rather than use separate view controllers to manage content on different screens, use the built-in adaptivity support to respond to size and size class changes in your view controllers. The notifications sent by UIKit give you the opportunity to make both large-scale and small-scale changes to your user interface without having to change the rest of your view controller code.
##%%&&
In object-oriented programming (OOP), a factory is an object for creating other objects ¨C formally a factory is a function or method that returns objects of a varying prototype or class[1] from some method call, which is assumed to be "new".[a] More broadly, a subroutine that returns a "new" object may be referred to as a "factory", as in factory method or factory function. This is a basic concept in OOP, and forms the basis for a number of related software design patterns.
In class-based programming, a factory is an abstraction of a constructor of a class, while in prototype-based programming a factory is an abstraction of a prototype object. A constructor is concrete in that it creates objects as instances of a single class, and by a specified process (class instantiation), while a factory can create objects by instantiating various classes, or by using other allocation schemes such as an object pool. A prototype object is concrete in that it is used to create objects by being cloned, while a factory can create objects by cloning various prototypes, or by other allocation schemes.
Factories may be invoked in various ways, most often a method call (a factory method), sometimes by being called as a function if the factory is a function object (a factory function). In some languages factories are generalizations of constructors, meaning constructors are themselves factories and these are invoked in the same way. In other languages factories and constructors are invoked differently, for example using the keyword new to invoke constructors but an ordinary method call to invoke factories; in these languages factories are an abstraction of constructors but not strictly a generalization, as constructors are not themselves factories.
Terminology differs as to whether the concept of a factory is itself a design pattern ¨C in the seminal book Design Patterns there is no "factory pattern", but instead two patterns (factory method pattern and abstract factory pattern) that use factories. Some sources refer to the concept as the factory pattern,[2][3] while others consider the concept itself a programming idiom,[4] reserving the term "factory pattern" or "factory patterns" to more complicated patterns that use factories, most often the factory method pattern; in this context, the concept of a factory itself may be referred to as a simple factory.[4] In other contexts, particularly the Python language, "factory" itself is used, as in this article.[5] More broadly, "factory" may be applied not just to an object that returns objects from some method call, but to a subroutine that returns objects, as in a factory function (even if functions are not objects) or factory method.[6] Because in many languages factories are invoked by calling a method, the general concept of a factory is often confused with the specific factory method pattern design pattern.
OOP provides polymorphism on object use by method dispatch, formally subtype polymorphism via single dispatch determined by the type of the object on which the method is called. However, this does not work for constructors, as constructors create an object of some type, rather than use an existing object. More concretely, when a constructor is called, there is no object yet on which to dispatch.[b]
Using factories instead of constructors or prototypes allows one to use polymorphism for object creation, not only object use. Specifically, using factories provides encapsulation, and means the code is not tied to specific classes or objects, and thus the class hierarchy or prototypes can be changed or refactored without needing to change code that uses them ¨C they abstract from the class hierarchy or prototypes.
More technically, in languages where factories generalize constructors, factories can usually be used anywhere constructors can be,[c] meaning that interfaces that accept a constructor can also in general accept a factory ¨C usually one only need something that creates an object, rather than needing to specify a class and instantiation.
For example, in Python, the collections.defaultdict class[7] has a constructor which creates an object of type defaultdict[d] whose default values are produced by invoking a factory. The factory is passed as an argument to the constructor, and can itself be a constructor, or anything that behaves like a constructor ¨C a callable object that returns an object, i.e., a factory. For example, using the list constructor for lists:
Factory objects are used in situations where getting hold of an object of a particular kind is a more complex process than simply creating a new object, notably if complex allocation or initialization is desired. Some of the processes required in the creation of an object include determining which object to create, managing the lifetime of the object, and managing specialized build-up and tear-down concerns of the object. The factory object might decide to create the object's class (if applicable) dynamically, return it from an object pool, do complex configuration on the object, or other things. Similarly, using this definition, a singleton implemented by the singleton pattern is a formal factory ¨C it returns an object, but does not create new objects beyond the single instance.
The simplest example of a factory is a simple factory function, which just invokes a constructor and returns the result. In Python, a factory function f that instantiates a class A can be implemented as:
A simple factory function implementing the singleton pattern is:
This will create an object when first called, and always return the same object thereafter.
Factories may be invoked in various ways, most often a method call (a factory method), sometimes by being called as a function if the factory is a callable object (a factory function). In some languages constructors and factories have identical syntax, while in others constructors have special syntax. In languages where constructors and factories have identical syntax, like Python, Perl, Ruby, Object Pascal, and F#,[e] constructors can be transparently replaced by factories. In languages where they differ, one must distinguish them in interfaces, and switching between constructors and factories requires changing the calls.
In languages where objects are dynamically allocated, as in Java or Python, factories are semantically equivalent to constructors. However, in languages such as C++ that allow some objects to be statically allocated, factories are different from constructors for statically allocated classes, as the latter can have memory allocation determined at compile time, while allocation of the return values of factories must be determined at run time. If a constructor can be passed as an argument to a function, then invocation of the constructor and allocation of the return value must be done dynamically at run time, and thus have similar or identical semantics to invoking a factory.
Main article: Creational pattern
Factories are used in various design patterns, specifically in creational patterns such as the Design pattern object library. Specific recipes have been developed to implement them in many languages. For example, several "GoF patterns", like the "Factory method pattern", the "Builder" or even the "Singleton" are implementations of this concept. The "Abstract factory pattern" instead is a method to build collections of factories.
In some design patterns, a factory object has a method for every kind of object it is capable of creating. These methods optionally accept parameters defining how the object is created, and then return the created object.
Factory objects are common in toolkits and frameworks where library code needs to create objects of types which may be subclassed by applications using the framework. They are also used in test-driven development to allow classes to be put under test.[8]
Factories determine the actual concrete type of object to be created, and it is here that the object is actually created. As the factory only returns an abstract interface to the object, the client code does not know ¨C and is not burdened by ¨C the actual concrete type of the object which was just created. However, the type of a concrete object is known by the abstract factory. In particular, this means:
?	The client code has no knowledge whatsoever of the concrete type, not needing to include any header files or class declarations relating to the concrete type. The client code deals only with the abstract type. Objects of a concrete type are indeed created by the factory, but the client code accesses such objects only through their abstract interface.
?	Adding new concrete types is done by modifying the client code to use a different factory, a modification which is typically one line in one file. This is significantly easier than modifying the client code to instantiate a new type, which would require changing every location in the code where a new object is created.
Factories can be used when:
1.	The creation of an object makes reuse impossible without significant duplication of code.
2.	The creation of an object requires access to information or resources that should not be contained within the composing class.
3.	The lifetime management of the generated objects must be centralized to ensure a consistent behavior within the application.
Factories, specifically factory methods, are common in toolkits and frameworks, where library code needs to create objects of types that may be subclassed by applications using the framework.
Parallel class hierarchies often require objects from one hierarchy to be able to create appropriate objects from another.
Factory methods are used in test-driven development to allow classes to be put under test.[9] If such a class Foo creates another object Dangerous that can't be put under automated unit tests (perhaps it communicates with a production database that isn't always available), then the creation of Dangerous objects is placed in the virtual factory method createDangerous in class Foo. For testing, TestFoo (a subclass of Foo) is then created, with the virtual factory method createDangerousoverridden to create and return FakeDangerous, a fake object. Unit tests then use TestFoo to test the functionality of Foo without incurring the side effect of using a real Dangerous object.
Besides use in design patterns, factories, especially factory methods, have various benefits and variations.
A factory method has a distinct name. In many object-oriented languages, constructors must have the same name as the class they are in, which can lead to ambiguity if there is more than one way to create an object (see overloading). Factory methods have no such constraint and can have descriptive names; these are sometimes known as alternative constructors. As an example, when complex numbers are created from two real numbers the real numbers can be interpreted as Cartesian or polar coordinates, but using factory methods, the meaning is clear, as illustrated by the following example in C#.
When factory methods are used for disambiguation like this, the raw constructors are often made private to force clients to use the factory methods.
Factory methods encapsulate the creation of objects. This can be useful if the creation process is very complex; for example, if it depends on settings in configuration files or on user input.
Consider as an example a program that reads image files. The program supports different image formats, represented by a reader class for each format.
Each time the program reads an image, it needs to create a reader of the appropriate type based on some information in the file. This logic can be encapsulated in a factory method. This approach has also been referred to as the Simple Factory.
There are three limitations associated with the use of the factory method. The first relates to refactoring existing code; the other two relate to extending a class.
?	The first limitation is that refactoring an existing class to use factories breaks existing clients. For example, if class Complex were a standard class, it might have numerous clients with code like:
Once we realize that two different factories are needed, we change the class (to the code shown earlier). But since the constructor is now private, the existing client code no longer compiles.
?	The second limitation is that, since the pattern relies on using a private constructor, the class cannot be extended. Any subclass must invoke the inherited constructor, but this cannot be done if that constructor is private.
?	The third limitation is that, if the class were to be extended (e.g., by making the constructor protected¡ªthis is risky but feasible), the subclass must provide its own re-implementation of all factory methods with exactly the same signatures. For example, if class StrangeComplex extends Complex, then unless StrangeComplex provides its own version of all factory methods, the call
will yield an instance of Complex (the superclass) rather than the expected instance of the subclass. The reflection features of some languages can avoid this issue.
All three problems could be alleviated by altering the underlying programming language to make factories first-class class members (see also Virtual class).[10]
##%%&&
Service Layer is a design pattern, applied within the service-orientation design paradigm, which aims to organize the services, within a service inventory, into a set of logical layers. Services that are categorized into a particular layer share functionality. This helps to reduce the conceptual overhead related to managing the service inventory, as the services belonging to the same layer address a smaller set of activities.
Grouping services into functional layers reduces the impact of change. Most changes affect only the layer in which they're made, with few side-effects that impact other layers. This fundamentally simplifies service maintenance.
The service reusability principle dictates that services should be designed to maximize reuse. Similarly, the service composability principle advocates designing services so that they can be composed in various ways. Both principles require that a service contain only a specific type of logic e.g., either reusable or process-specific logic.[1] Restricting each layer to a particular functionality, simplifies the design of the service.
In the absence of any layers, services contain a mixture of different types of logic. This makes it difficult to manage these services.
A service inventory divided into layers where each layer contains the same type of logic.
Applying this pattern requires creating a service inventory blueprint, a list of services with associated functionality. Next, group the services into layers according to function. Adopting a common layering strategy across the enterprise facilitates reuse in other applications, because developers don't have as much to learn (or invent) when they join a project. One common layering uses task, entity and utility.[2]
An alternative layering from Bieberstein et al.,[3] involves five layers, namely enterprise, process, service, component and object.
The service layer pattern invokes a specific service architecture.
The top-down service delivery approach facilitates the use of this pattern.
##%%&&
Post/Redirect/Get (PRG) is a web development design pattern that prevents some duplicate form submissions, creating a more intuitive interface for user agents (users). PRG supports bookmarks and the refresh button in a predictable way that does not create duplicate form submissions.
When a web form is submitted to a server through an HTTP POST request, a web user that attempts to refresh the server response in certain user agents can cause the contents of the original POST request to be resubmitted, possibly causing undesired results, such as a duplicate web purchase.[1]
To avoid this problem, many web developers use the PRG pattern[2]¡ªinstead of returning a web page directly, the POST operation returns a redirection command. The HTTP 1.1 specification introduced the HTTP 303 ("See other") response code to ensure that in this situation, the web user's browser can safely refresh the server response without causing the initial POST request to be resubmitted. However most common commercial applications in use today (new and old alike) still continue to issue HTTP 302 ("Found") responses in these situations.
The PRG pattern cannot address every scenario of duplicate form submission. Some known duplicate form submissions that PRG cannot solve are:
?	If a web user refreshes before the initial submission has completed because of server lag, resulting in a duplicate POST request in certain user agents.
User agents (such as browsers) store only the URI of an HTTP request as a bookmark. Because of this, an HTTP POST request that results in a response based on the body of the HTTP POST request cannot be bookmarked. By using the PRG pattern, the URI of the request can safely be bookmarked by a web user.
Since redirects are using absolute URIs, one has to take care about proxy servers (HTTP¡úHTTPS) and reverse proxy servers. If your application is such that a user uses an SSL tunnel to reach your site, this can cause problems also. (You may be able to use the Referer header to discover the domain and port the user is actually entering.)
##%%&&
The object pool pattern is a software creational design pattern that uses a set of initialized objects kept ready to use ¨C a "pool" ¨C rather than allocating and destroying them on demand. A client of the pool will request an object from the pool and perform operations on the returned object. When the client has finished, it returns the object to the pool rather than destroying it; this can be done manually or automatically.
Object pools are primarily used for performance: in some circumstances, object pools significantly improve performance. Object pools complicate object lifetime, as objects obtained from and returned to a pool are not actually created or destroyed at this time, and thus require care in implementation.
When it is necessary to work with a large number of objects that are particularly expensive to instantiate and each object is only needed for a short period of time, the performance of an entire application may be adversely affected. An object pool design pattern may be deemed desirable in cases such as these.
The object pool design pattern creates a set of objects that may be reused. When a new object is needed, it is requested from the pool. If a previously prepared object is available it is returned immediately, avoiding the instantiation cost. If no objects are present in the pool, a new item is created and returned. When the object has been used and is no longer needed, it is returned to the pool, allowing it to be used again in the future without repeating the computationally expensive instantiation process. It is important to note that once an object has been used and returned, existing references will become invalid.
In some object pools the resources are limited so a maximum number of objects is specified. If this number is reached and a new item is requested, an exception may be thrown, or the thread will be blocked until an object is released back into the pool.
The object pool design pattern is used in several places in the standard classes of the .NET Framework. One example is the .NET Framework Data Provider for SQL Server. As SQL Server database connections can be slow to create, a pool of connections is maintained. When you close a connection it does not actually relinquish the link to SQL Server. Instead, the connection is held in a pool from which it can be retrieved when requesting a new connection. This substantially increases the speed of making connections.
Object pooling can offer a significant performance boost in situations where the cost of initializing a class instance is high and the rate of instantiation and destruction of a class is high ¨C in this case objects can frequently be reused, and each reuse saves a significant amount of time. Object pooling requires resources ¨C memory and possibly other resources, such as network sockets, and thus it is preferable that the number of instances in use at any one time is low, but this is not required.
The pooled object is obtained in predictable time when creation of the new objects (especially over network) may take variable time. These benefits are mostly true for objects that are expensive with respect to time, such as database connections, socket connections, threads and large graphic objects like fonts or bitmaps.
In other situations, simple object pooling (that hold no external resources, but only occupy memory) may not be efficient and could decrease performance.[1] In case of simple memory pooling, the slab allocation memory management technique is more suited, as the only goal is to minimize the cost of memory allocation and deallocation by reducing fragmentation.
Object pools can be implemented in an automated fashion in languages like C++ via smart pointers. In the constructor of the smart pointer, an object can be requested from the pool, and in the destructor of the smart pointer, the object can be released back to the pool. In garbage-collected languages, where there are no destructors (which are guaranteed to be called as part of a stack unwind), object pools must be implemented manually, by explicitly requesting an object from the factory and returning the object by calling a dispose method (as in the dispose pattern). Using a finalizer to do this is not a good idea, as there are usually no guarantees on when (or if) the finalizer will be run. Instead, "try ... finally" should be used to ensure that getting and releasing the object is exception-neutral.
Manual object pools are simple to implement, but harder to use, as they require manual memory management of pool objects.
Object pools employ one of three strategies to handle a request when there are no spare objects in the pool.
1.	Fail to provide an object (and return an error to the client).
2.	Allocate a new object, thus increasing the size of the pool. Pools that do this usually allow you to set the high water mark (the maximum number of objects ever used).
3.	In a multithreaded environment, a pool may block the client until another thread returns an object to the pool.
When writing an object pool, the programmer has to be careful to make sure the state of the objects returned to the pool is reset back to a sensible state for the next use of the object. If this is not observed, the object will often be in some state that was unexpected by the client program and may cause the client program to fail. The pool is responsible for resetting the objects, not the clients. Object pools full of objects with dangerously stale state are sometimes called object cesspools and regarded as an anti-pattern.
The presence of stale state is not always an issue; it becomes dangerous when the presence of stale state causes the object to behave differently. For example, an object that represents authentication details may break if the "successfully authenticated" flag is not reset before it is passed out, since it will indicate that a user is correctly authenticated (possibly as someone else) when they haven't yet attempted to authenticate. However, it will work just fine if you fail to reset some value only used for debugging, such as the identity of the last authentication server used.
Inadequate resetting of objects may also cause an information leak. If an object contains confidential data (e.g. a user's credit card numbers) that isn't cleared before the object is passed to a new client, a malicious or buggy client may disclose the data to an unauthorized party.
If the pool is used by multiple threads, it may need the means to prevent parallel threads from grabbing and trying to reuse the same object in parallel. This is not necessary if the pooled objects are immutable or otherwise thread-safe.
Some publications do not recommend using object pooling with certain languages, such as Java, especially for objects that only use memory and hold no external resources.[2] Opponents usually say that object allocation is relatively fast in modern languages with garbage collectors; while the operator new needs only ten instructions, the classic new - delete pair found in pooling designs requires hundreds of them as it does more complex work. Also, most garbage collectors scan "live" object references, and not the memory that these objects use for their content. This means that any number of "dead" objects without references can be discarded with little cost. In contrast, keeping a large number of "live" but unused objects increases the duration of garbage collection.[1]
In the .NET Base Class Library there are a few objects that implement this pattern. System.Threading.ThreadPool is configured to have a predefined number of threads to allocate. When the threads are returned, they are available for another computation. Thus, one can use threads without paying the cost of creation and disposal of threads.
The following shows the basic code of the object pool design pattern implemented using C#. For brevity the properties of the classes are declared using C# 3.0 automatically implemented property syntax. These could be replaced with full property definitions for earlier versions of the language. Pool is shown as a static class, as it's unusual for multiple pools to be required. However, it's equally acceptable to use instance classes for object pools.
In the code above, the PooledObject includes two properties that are not shown in the UML diagram. One holds the time at which the object was first created. The other holds a string that can be modified by the client but that is reset when the PooledObject is released back to the pool. This shows the clean-up process on release of an object that ensures it is in a valid state before it can be requested from the pool again.
Java supports thread pooling via java.util.concurrent.ExecutorService and other related classes. The executor service has a certain number of "basic" threads that are never discarded. If all threads are busy, the service allocates the allowed number of extra threads that are later discarded if not used for the certain expiration time. If no more threads are allowed, the tasks can be placed in the queue. Finally, if this queue may get too long, it can be configured to suspend the requesting thread.
##%%&&
In software engineering, the multiton pattern is a design pattern which generalizes the singleton pattern. Whereas the singleton allows only one instance of a class to be created, the multiton pattern allows for the controlled creation of multiple instances, which it manages through the use a map.
Rather than having a single instance per application (e.g. the java.lang.Runtime object in the Java programming language) the multiton pattern instead ensures a single instance per key.
Most people and textbooks consider this a singleton pattern[citation needed]. For example, multiton does not explicitly appear in the highly regarded object-oriented programming textbook Design Patterns (it appears as a more flexible approach named registry of singletons).
While it may appear that the multiton is no more than a simple hash table with synchronized access there are two important distinctions. First, the multiton does not allow clients to add mappings. Secondly, the multiton never returns a null or empty reference; instead, it creates and stores a multiton instance on the first request with the associated key. Subsequent requests with the same key return the original instance. A hash table is merely an implementation detail and not the only possible approach. The pattern simplifies retrieval of shared objects in an application.
Since the object pool is created only once, being a member associated with the class (instead of the instance), the multiton retains its flat behavior rather than evolving into a tree structure.
The multiton is unique in that it provides centralized access to a single directory (i.e. all keys are in the same namespace, per se) of multitons, where each multiton instance in the pool may exist having its own state. In this manner, the pattern advocates indexed storage of essential objects for the system (such as would be provided by an LDAP system, for example). However, a multiton is limited to wide use by a single system rather than a myriad of distributed systems.
This pattern, like the Singleton pattern, makes unit testing far more difficult,[1] as it introduces global state into an application.
With garbage collected languages it may become a source of memory leaks as it introduces global strong references to the objects.
In Java, the multiton pattern can be implemented using an enumerated type, with the values of the type corresponding to the instances. In the case of an enumerated type with a single value, this gives the singleton pattern.
##%%&&
The non-virtual interface pattern (NVI) controls how methods in a base class are overridden. Such methods may be called by clients and overridable methods with core functionality.[1] It is a pattern that is strongly related to the template method pattern. The NVI pattern recognizes the benefits of a non-abstract method invoking the subordinate abstract methods. This level of indirection allows for pre and post operations relative to the abstract operations both immediately and with future unforeseen changes. The NVI pattern can be deployed with very little software production and runtime cost. Many commercial software frameworks employ the NVI pattern.
A design that adheres to this pattern results in a separation of a class interface into two distinct interfaces:
1.	Client interface: This is the public non-virtual interface
2.	Subclass interface: This is the private interface, which can have any combination virtual and non-virtual methods.
With such a structure, the fragile base class interface problem is mitigated. The only detriment is that the code is enlarged a little.[2]
##%%&&
In software engineering, Canonical Schema is a design pattern, applied within the service-orientation design paradigm, which aims to reduce the need for performing data model [1] transformation when services[2] exchange messages that reference the same data model.[3]
The interaction between services often requires exchanging business documents. In order for a service consumer to send data (related to a particular business entity e.g. a purchase order), it needs to know the structure of the data i.e. the data model. For this, the service provider publishes the structure of the data that it expects within the incoming message from the service consumer. In case of services being implemented as web services,[4] this would be the XML schema document. Once the service consumer knows the required data model, it can structure the data accordingly. However, under some conditions it may be possible that the service consumer already possesses the required data, which relates to a particular business document, but the data does not conform to the data model as specified by the service provider. This disparity among the data models results in the requirement of data model transformation so that the message is transformed into the required structure as dictated by the service provider. Building upon the aforementioned example, it is entirely possible that, after processing the received business document, the service provider sends back the processed document to the service consumer that once again performs the data model transformation to convert the processed business document back to the data model that it uses within its logic to represent the business document. 
This runtime data model transformation adds processing overhead and complicates the design of service compositions.[5] In order to avoid the need for data model transformation, the Canonical Schema pattern dictates the use of standardized data models for those business documents that are commonly processed by the services in a service inventory.[6][7]
Service A is using a different data model as compared to Service B for the same business document. When messages are exchanged, runtime data model transformation needs to be performed.
Both services are using the same data model for representing a particular business document. As a result, no data model transformation is required when messages are exchanged.
This design pattern is fully supported by the application of the Standardized Service Contract design principle. The Standardized Service Contract design principle advocates that the service contracts be based on standardized data models. This is achieved by performing an analysis of the service inventory blueprint[8] in order to find out the commonly occurring business documents that are exchanged between services. These business documents are then modeled in a standardized manner. For example, in case of web services, the business documents are modeled as XML schemas. Once a standardized data representation layer exists in a service inventory, different service contracts can make use of the same data models if they need to exchange the same business documents. This eliminates the need for any data model transformation and reduces the processing overhead associated with the data model transformation. It also increases the reusability potential of a service as now the service can be consumed without requiring any custom data model transformation logic. In a way, the application of the Canonical Schema pattern reduces the need for the application of the Data Model Transformation[9] design pattern.
The application of this design pattern requires design standards[10] in place that make the use of standardized data models mandatory, as the mere creation of data models does not guarantee their use.[11]Although simple in principle but difficult to enforce as it needs commitment from different project teams which may entail extra efforts, on part of each team, in terms of designing solutions that accommodate standardized data models. 
On some occasions, either because of the sheer size of the organization or because of the resistance from different segments of the enterprise, the Canonical Schema design pattern may need to be applied within a particular domain inventory, created by the application of the Domain Inventory design pattern.[7] 
The schemas need to be designed separately than the service contract design so that there is no dependency between them.[11]
##%%&&
Content negotiation refers to mechanisms defined as a part of HTTP that make it possible to serve different versions of a document (or more generally, representations of a resource) at the same URI, so that user agents can specify which version fits their capabilities the best. One classical use of this mechanism is to serve an image in GIF or PNG format, so that a browser that cannot display PNG images (e.g. MS Internet Explorer 4) will be served the GIF version.
A resource may be available in several different representations; for example, it might be available in different languages or different media types. One way of selecting the most appropriate choice is to give the user an index page and let them select the most appropriate choice; however it is often possible to automate the choice based on some selection criteria.
HTTP provides for several different content negotiation mechanisms including: server-driven (or proactive), agent-driven (or reactive), transparent, and/or hybrid combinations thereof.
Server-driven or proactive content negotiation is performed by algorithms on the server which choose among the possible variant representations. This is commonly performed based on a user-agent provided acceptance criteria.
To summarize how this works, when a user agent submits a request to a server, the user agent informs the server what media types it understands with ratings of how well it understands them. More precisely, the user agent provides an Accept HTTP header that lists acceptable media types and associated quality factors. The server is then able to supply the version of the resource that best fits the user agent's needs.
This works because browsers can send information as part of each request about the representations they prefer. For example, a browser could indicate that it would like to see information in German, if possible, else English will do. Browsers indicate their preferences by headers in the request. To request only German representations, the browser would send:
Note that this preference will only be applied when there is a choice of representations and they vary by language.
As an example of a more complex request, this browser has been configured to accept German and English, but prefer German, and to accept various media types, preferring HTML over plain text or other text types, and preferring GIF or JPEG over other media types, but also allowing any other media type as a last resort:
In addition to server-driven content negotiation by content type and by language, there is an extension to use content negotiation to retrieve prior version in time with the Accept-Datetime header.[1]
RFC 7231 does not specify how to resolve trade-offs (such as, in the above example, choosing between an HTML page in English and a GIF image in German).
Agent-driven or reactive content negotiation is performed by algorithms in the user-agent which choose among the possible variant representations. This is commonly performed based on a server provided list of representations and metadata about them.
To summarize how this works, when a user agent submits a request to a server, the server informs the user-agent which representations it has available as well as any metadata it has about each representation (e.g., content-type, quality, language, etc.). The user-agent then resubmits the request to a specific URL for the chosen representation. This can be automatically chosen by the user-agent or the user-agent can present the user with the choices and the user can directly choose such. More precisely, the server responds with either 300 Multiple Choices or 406 Not Acceptable (when server-driven, user-agent provided acceptance criteria is provided but the server cannot automatically make a selection). Unfortunately HTTP leaves the format of the list of representations and metadata along with selection mechanisms unspecified.
User-agents can request data in specified formats from web services or web APIs, such as application/json or application/xml.
##%%&&
Private class data is a design pattern in computer programming used to encapsulate class attributes and their manipulation.
The following documentation categories for the private class data design pattern follows the design pattern documentation style precedent set by the Gang of Four.
This pattern is known as the private class data design pattern.
This pattern is a structural pattern.
The private class data design pattern seeks to reduce exposure of attributes by limiting their visibility. It reduces the number of class attributes by encapsulating them in single Data object. It allows the class designer to remove write privilege of attributes that are intended to be set only during construction, even from methods of the target class.
PIMPL (Private IMPLementation) or Opaque pointer
A class may expose its attributes (class variables) to manipulation when manipulation is no longer desirable, e.g. after construction. Using the private class data design pattern prevents that undesirable manipulation.
A class may have one-time mutable attributes that cannot be declared final. Using this design pattern allows one-time setting of those class attributes.
The motivation for this design pattern comes from the design goal of protecting class state by minimizing the visibility of its attributes (data).
This design pattern applies to any class in any object oriented language.
The consequences of using this design pattern include the following:
?	Controlling write access to class attributes;
?	Separating of data from methods that use it;
?	Encapsulating class attribute (data) initialization; and
?	Providing new type of final: final after constructor.
The private class data design pattern solves the problems above by extracting a data class for the target class and giving the target class instance an instance of the extracted data class.
?	The data class exposes each attribute (variable or property) through a getter.
?	The data class exposes each attribute that must change after construction through a setter.
The following C# code illustrates an opportunity to use the private class data design pattern:
The attributes radius, color, and origin above should not change after the Circle() constructor. Note that the visibility is already limited by scoping them as private, but doing methods of class Circle can still modify them.
The excess exposure of the attributes creates a type of (undesirable) coupling between methods that access those attributes. To reduce the visibility of the attributes and thus reduce the coupling, implement the private class data design pattern, as follows:
The Circle class in the resulting code has an attribute of type CircleData to encapsulate the attributes previously exposed to all of the methods of the class Circle. That encapsulation prevents methods from changing the attributes after the Circle()constructor. Note, however, that any method of Circle can still retrieve the values of the encapsulated attributes.
The Qt framework uses the private class data pattern in its shared libraries.[1] The classes that implement the pattern include a "d-pointer" to the data class. Methods are provided for manipulating member variables in the data class, allowing changes without breaking binary compatibility.
##%%&&
Within the service-orientation design paradigm, service loose coupling is a design principle[1] that is applied to the services[2] in order to ensure that the service contract is not tightly coupled to the service consumers and to the underlying service logic and implementation. This results in service contracts that could be freely evolved without affecting either the service consumers or the service implementation.[3]
The concept of loose coupling within SOA is directly influenced by the object-oriented design paradigm,[4] whereby the objective is to reduce coupling between classes in order to foster an environment where both the classes, although somehow related to each other, can be changed in a manner that such a change does not break the existing relationship, which is necessary for the working of a software program. The same concept applies within SOA world as well, however, within SOA particular emphasis is on the service contract as the service contract acts as an interface through which service consumers communicate with the service logic and vice versa. Apart from this, SOA strongly advocates development of physically independent service contracts from the service logic (decoupled contract[5] design pattern) in favor of interoperability and technology independence. As the contracts are physically independent, there is a need to not only look into the coupling between service consumers and service contracts but also between service contracts and their underlying logic and implementation. This is where the application of this design principle helps in identifying the various types of couplings that exist (inter service as well as intra service) and how to design the contracts in order to minimize negative coupling types and maximize positive coupling types. A service-oriented solution that consists of services having loosely coupled contracts directly supports the increased vendor diversity options and increased interoperability goals of service-orientation.
The application of the service loose coupling design principle requires delving into the different types of couplings that exist between the service consumer and the service contract as well as the service contract and the service¡¯s implementation. Only by understanding these different types, their impact on the service-orientation can be correctly analyzed.
Service loose coupling dictates that this kind of coupling should to be favored so that the service logic is developed exclusively in support of the service contract. However, this requires following the 'contract first' approach as advocated by the standardized service contract principle so that the service logic is coupled to a standardized contract. This way the service contract is not coupled to the logic so the logic could be replaced in future if need be without affecting the service consumers.
This type of coupling exists when the contract is built based on existing logic e.g. through automated tools.[6] This is a negative form of coupling and needs to be avoided as it inhibits the evolution of service contract. This is because the service contract is not designed independently according to the design standards and is dictated by the underlying logic.
When contracts are designed in a manner that they are based on the underlying implementation details e.g. data models used within the underlying database, this results in a negative form of coupling that needs to be avoided. This way, a change in the underlying implementation will require a corresponding change in the service contract. This type of coupling can be reduced with the introduction of a facade component in between the service logic and its implementation as advocated by the Service Facade[7] design pattern.
A contract that exposes proprietary technology elements used by the service logic e.g. a contract based on .NET Remoting technology, forms a negative form of coupling as the service consumers are limited to that particular technology. This greatly hampers the service¡¯s ability to be counted as an interoperable enterprise-resource.
This type of coupling normally exists when the service contract is developed by keeping a particular kind of consumer in mind e.g. services built to enable communication with a business partner or a service that executes a part of the business process logic or is itself the parent controller service in a service composition that executes the business process logic. This is also a negative form of coupling and needs to be avoided. Although in case of agnostic services there is a clear need to reduce this coupling type, however, in case of non-agnostic services e.g. the task services, existence of such coupling is intentional because the service is not required to be particularly reusable and hence could be tightly coupled to a particular consumer for better efficiency.
This is a negative form of coupling that exists because the service consumers access the service directly either via its logic or implementation. This can happen because of number of reasons. For example, the service consumers used to access the current service through streamlined proprietary interfaces long before it actually existed as a service i.e. before the move towards service-orientation. The application of the contract centralization[8] design pattern helps to avoid this kind of coupling.
This is a favorable type of coupling as it helps to evolve the service without impacting its consumers. However, it is quite important to bear in mind that this coupling should only be restricted to the service contract and should not leak into the service architecture. This could happen if all of the negative contract related coupling types are not addressed, consequently the service consumer can easily become coupled to the service implementation, logic or technology.
Designing service contracts that are totally decoupled from their internal and external surroundings would no doubt result in services that are interoperable and scalable but on the other side, this may create contracts that have capabilities which are too generic or the capabilities¡¯ message exchange is too generic which would result in more roundtrips, requiring increased processing resources and time.
Analyzing all of the above different types of coupling requires extra time and efforts and may increase the delivery time of services. Consequently, there is a need to apply this design principle to a meaningful extent as set by the design standards within the individual organization.
##%%&&
Provide a document type which is defined to be a holder for other, arbitrary XML data.
Different sets of data need to be delivered to a system in a consistent way.
This pattern applies when different sets of data need to be used in some system. The structure of the data itself varies, or is not known at the time the system is being built.
This pattern allows for Flexibility by allowing elements from other documents to be embedded into a consistent holder.
This pattern allows for a separation of concerns between different document types. Mixing issues such as transport with document data is usually not a good idea.
Create a document type which will act as a holder for the varying set of data.
This example shows a very simple envelope which consists of sender and a receiver children elements. Note the use of two different namespaces to allow an XML parser to read the document, which contains elements from both the Envelope (envelope.dtd) and the embedded data (my-stuff.dtd)
The Envelope is used as a delivery mechanism for XML data. Document types created for holding domain data should not be responsible, for defining transport ways to do deal with things such as transport, security, delivery and packaging. The envelope provide for a clear separation between these things and the data itself.
This is similar to the Catch-All Element, except that the document type of the Envelope pattern exists solely as an holder for other data and, the Catch-All Element is for embedding new elements within existing data.
##%%&&
Provide a set of attributes that can be placed on all, or most, elements in the document type.
There are properties that belong to every element in the document. These properties need to be presented to the document author in an understandable way.
This is a widely applicable pattern and can be used anywhere that a large number of elements need the same attributes applied. It is a common technique for attributes such as IDs, security, and roles.
The usability of the document type depends a lot on consistency, and this pattern can be used to add consistency.
Create a common set of attributes that will be used by all or most of the elements in the document type.
This examples shows a DTD that has two common attributes, id and role, on its all of its elements. The common attribute definition is placed in a parameter entity which is referenced by all of the ATTLIST definitions.
This example shows how to implement the same document structure as the above examples using the XSL Schema language Attributes Groups .
Global attributes often add many choices at any point in the document. Users have more selections available to them, and this could complicate authoring, however, some of the complexity that adding attributes to every element brings is lessened if these attributes are applied consistently. If users can expect the same common attribute on every element, it does not take a lot of extra effort to process the information. It will be easier to author or process a document that has a consistently applied set of common attributes on all elements compared to one that has inconsistently applied attributes on most of the elements.
An ID attribute is a typical use for a common attribute. It is useful to be able to 
The common attributes are usually declared in a Flyweight to help maintainability.
The XMLspec DTD Common Attributes Chapter explains the use of common attributes in the DTD.
XHTML defines a set of core attributes. These attributes are applied to almost every element in the XHTML document type. The core attributes are defined like this:
Even the simple line break element has these attributes added to it:
##%%&&
Creating structures that for different elements that are very similar to one another makes DTD easier to use and understand.
Authoring a document with complex structure can be difficult if there are many elements, and each of these elements has their own particular content models.
Any document that is large enough to be potentially difficult to learn.
Having consistency in the structure of document types makes document instances easier to learn and process.
For elements that are similar in function use structures that are as close to identical as possible.
These two elements "Figure" and "Table" have very similar content and identical attribute lists. This makes it very easy for authors of the document. Once they have learned the structure of one of the elements, the other one will be easy to use. The more variation there is between the two, the easier it is to confuse the two structures.
The more consistent the structure of various elements is the easier it is to learn to use a document type. Subtle differences in structure can be particularly difficult to learn. If, for example, in the above example, one of the "id" attributes was IMPLIED and the other one was REQUIRED, this would be a difficult distinction to remember. Users would constantly need to refer to the documentation to remind them which element's id was required and which one wasn't.
Often the parts of the structure that are the same between two elements can be represented as Flyweights. This will help ensure that the two structures remain similar even through revisions of the document type.
The DocBook DTD defines these four entities:
All four of the elements have identical content model. This helps enormously in learning the specifics of this DTD.
##%%&&
A container has multiple elements as child elements. A new element type is created to group related elements. This is a very general pattern and many other patterns specialize this one.
There are multiple elements that occur at the same level within a document. These elements can be separated into distinct groups.
There are multiple elements that can occur as child elements of a single, higher level element, and some of these elements are related.
Documents need structure, and this is the easiest way to add structure to a document.
Create a new element that contains some of the multiple child elements.
The above document describes a computer's configuration. If however processing software was only interested in the hardware aspects of the configuration it would need to check all of the elements and extract the RAM and HardDriveSize elements. Compare this to the following:
In this example the software and hardware aspects of the configuration are separated. Processing software that need to extract individual aspects can do so easily.
This pattern adds a level of abstraction to the structure of a document. This abstraction can be used to simply provide grouping of elements as in the Examples section. This grouping provides additional semantic information about the data.
This extra element can also be a useful place to provide metadata about the 
The Marketplace pattern can be used if there is more than one category that can apply to each type.
The Head-Body pattern uses two specific types of Container Elements, one for metadata and one for content.
The Isogen whitepaper Object-Oriented SGML Deconstructing SGML for Storage Section contains an excellent example of where a container was used to help in the storage and retrieval of data.
##%%&&
XML technology can be used to represent structured information. This pattern helps determine when XML is an appropriate solution.
When data in a system needs to be imported or exported, it needs some type of representation. There are many possible representations for example: Comma Separated Values, Java Object Serialization, Proprietary Binary Formats, HTML, CORBA IIOP Streams, RDBMS tables, and of course XML. Choosing the proper representation for the data can be difficult.
XML can be an appropriate choice when one or more of the following is needed:
?	Content needs to be separate from its formatting.
?	Data is shared between computers, applications or organizations.
?	Human readable representation is needed.
?	Readily available tools and resources.
There are some situations where XML may not be a good choice, among them is systems where:
?	Terseness is important.
?	In a homogeneous environment.
There are many forces to take into account when considering a data representation. Among the ones that XML can provide are simplicity, openness, extensibility, interoperability, and a technology with proven use.
It is impossible to have a complete discussion of the factors involved in make the decision of a data representation in the space of a single pattern, but it can try to provide some pointers.
All other patterns in this language depend on first using this pattern, because they assume that XML is being used.
There are many places where XML is being used successfully. Here is a small sampling of them.
?	Data syndication. XML has been popular in this area, and several XML languages have been developed for this purpose.
?	Data exchange protocols for transport of messages from one system to another across a network.
?	Configuration files
##%%&&
When a large amount of metadata needs to be included in an element the designer may create two children for the element, one for the metadata and one for the body of the document.
Sometimes, an element has a large amount of metadata, and this metadata may need to be structured. In common XML usage, attributes are used for metadata. But with a large amount of metadata it is difficult for authors to include this information within attributes, which have limited structure. Including metadata as sibling elements of content elements leads to the potential to confuse the metadata and the data meant to be in the body of the element.
There is a large amount of metadata about some piece of content. For example information about the title, creation date, and authors of a document should be included with the document, but this is not really information that belong in the body of the document.
A clear separation is needed between what is metadata and what is data that forms the body of the document. This affects ease of authoring and processing of the document because the context of the data is implied. The metadata needed for an element needs to structured in such a way that attributes are difficult to use.
Create head and body elements. The element that contains the head and body elements can be referred to as the skeleton element. The metadata for the document goes into the head element, and the data for the document itself goes into the body element. The XML takes the following form:
In this case, the skeleton element is the document root, and the metadata is structured information about the authors of the document and the creation date of the document. The Head-Body pattern is often used at the document level, but it can also be used at levels deeper within a document. For example the pattern could be used inside of a Table element like this:
In this case the skeleton element is a table, which needs structure for its Title data.
Two new elements are introduced which distinguish the metadata from the body of the document. This gives processing software context to distinguish these two types of data.
Because this introduced two additional elements, the resulting documents are larger than they would be without the application of this pattern
The resulting documents are often easier to understand because of the clear separation of metadata and the document body itself. The metadata is in the Head element and the document itself goes in the Body element. Authors and processing software can clearly distinguish the context of the data they are using.
Documents are also often easier to process because they allow metadata to be structured, unlike if attributes were use for the metadata of the document.
The Head-Body Pattern consists of two new Container Elements added to the structure of the document, one for the Head element, and one for the Body. This is a specialization of the Separate Metadata and Data.
##%%&&
Instead of organizing objects in a hierarchical fashion, objects are organized in a linear way, with signs on each object to indicate its classifications.
A natural way to use mark up to classify objects is to use a container for each new type in the classification.
This can lead to problems, if processing software needs to find all of the employees, or just the salaried employees, it needs to iterate through two different lists, the managers and the workers.
Any place where data can be organized by more than one type.
Ease-of-processing can be greatly affected by the way that elements are organized. Ease-of-Authoring can also be a problem, pick one structural organization can lead to awkward groupings of elements, and the choice of structure may seem arbitrary.
The objects to be modeled can all be grouped at the same level, and the categorization can be done with attributes on the element. This allows multiple characteristics of the objects to be modeled without enforcing a possibly artificial hierarchy on the objects.
The following model can be contrasted to the sample in the problem section, which uses a tree structure to model employees. Each employee can have two different types, whether they are Salaried or Contractors and whether they are Workers or Managers. In the tree structure, one of the types, the Worker/Manager distinction is given more importance than the Salaried/Contractor. Extra effort would be required to extract just the Salaried employees.
The sample below uses the Marketplace pattern to organize the same data into a flatter structure. In this organization both types are given equal precedence, and both types are equally easy to process.
This pattern results in a organization that can make mapping to database tables easier. The elements in the marketplace are equivalent to rows in a table, while the attributes are the columns. A more hierarchical structure can be harder to map to databases.
The Marketplace is often contained in a Collection Element.
##%%&&
The Message Queuing Pattern provides a simple means for threads to communicate information among one another. Although the communication is a fairly heavyweight approach to sharing information, it is the most common one because it is readily supported by operating systems and because it is the easiest to prove correct. This is because it does not share data resources that must be protected from mutual exclusion problems. The Message Queuing Pattern uses asynchronous communications, implemented via queued messages, to synchronize and share information among tasks. This approach has the advantage of simplicity and does not display mutual exclusion problems because no resource is shared by reference. Any information shared among threads is passed by value to the separate thread. While this limits the complexity of the collaboration among the threads, this approach in its pure form is immune to the standard resource corruption problems that plague concurrent systems that share information passed by reference. In passed by value sharing, a copy of the information is made and sent to the receiving thread for processing. The receiving thread fully owns the data it receives and so may modify it freely without concern for corrupting data due to multiple writers or due to sharing it among a writer and multiple readers. In most multithreaded systems, threads must synchronize and share information with others. Two primary things must be accomplished. First, the tasks must synchronize to permit sharing of the information. Second, the information must be shared in such a way that there is no chance of corruption or race conditions.
##%%&&
Interrupts have much to recommend them. They occur when the event of interest occurs, they execute very quickly and with little overhead, and they can provide a means for timely response to urgent needs. This accounts for their widespread use in real-time and embedded systems. Nevertheless, they are not a panacea for timely response to aperiodic phenomena. There are circumstances in which they are highly effective, but there are other circumstances when their use can lead to system failure. This pattern explores those issues. In many real-time and embedded applications, certain events must be responded to quickly and efficiently, almost regardless of when they occur or what the system is currently doing. When those responses are relatively short and can be made atomic (non interruptible), then the Interrupt Pattern can be an excellent design selection for handling those events. In a system in which certain events are highly urgent, such as those that can occur at high frequencies or those in which the response must be as fast as possible, a means is needed to quickly identify and respond to those events. This can occur in many different kinds of systems from simple to complex.
##%%&&
Sometimes asynchronous communication schemes, such as the Message Queuing Pattern, do not provide timely responses across a thread boundary. An alternative is to synchronously invoke a method of an object, nominally running in another thread. This is the Guarded Call Pattern. It is a simple pattern, although care must be taken to ensure data integrity and to avoid synchronization and deadlock problems. The Message Queuing Pattern enforces an asynchronous rendezvous between two threads, modeled as active objects. In general, this approach works very well, but it means a rather slow exchange of information because the receiving thread does not process the information immediately. The receiving thread will process it the next time the thread executes. This can be problematic when the synchronization between the threads is urgent (when there are tight time constraints). An obvious solution is to simply call the method of the appropriate object in the other thread, but this can lead to mutual exclusion problems if the called object is currently active doing something else. The Guarded Call Pattern handles this case through the use of a mutual exclusion semaphore. The problem this pattern addresses is the need for a timely synchronization or data exchange between threads. In such cases, it may not be possible to wait for an asynchronous rendezvous. A synchronous rendezvous can be made more timely, but this must be done carefully to avoid data corruption and erroneous computation.
##%%&&
The Rendezvous Pattern is a simplified form of the Guarded Call pattern used to either synchronize a set of threads or permit data sharing among a set of threads. It reifies the synchronization of multiple threads as an object itself. There are many subtle variants of this pattern. The Rendezvous object may contain data to be shared as the threads synchronize, or it may simply provide a means for synchronizing an arbitrary number of threads at a synchronization point with some synchronization policy or precondition before allowing them all to continue independently. The simplest of these preconditions is that a certain number of threads have registered at their synchronization points. This special case is called the Thread Barrier Pattern. A precondition is something that is specified to be true prior to an action or activity. Preconditions are a type of constraint that is usually generative that is, it can be used to generate code either to force the precondition to be true or to check that a precondition is true. In fact, the most common way to ensure preconditions in UML or virtually any design language is through the use of state machines. A state is a precondition for the transitions exiting it. The Rendezvous Pattern is concerned with modeling the preconditions for synchronization or rendezvous of threads. It is a general pattern and easy to apply to ensure that arbitrarily complex sets of preconditions can be met at run-time. The basic behavioral model is that as each thread becomes ready to rendezvous, it 
registers with the Rendezvous class and then blocks until the Rendezvous class releases it to run. Once the set of preconditions is met, then the registered tasks are released to run using whatever scheduling policy is currently in force. The problem addressed by this pattern is to codify a collaboration structure that allows any arbitrary set of preconditional invariants to be met for thread synchronization, independent of task phasings, scheduling policies, and priorities.
##%%&&
For very small systems, or for systems in which execution predictability is crucial, the Cyclic Executive Pattern is a commonly used approach. It is used a lot in both small systems and in avionic flight systems for both aircraft and spacecraft applications environments. Although not without difficulties, its ease-of-implementation makes this an attractive choice when the dynamic properties of the system are stable and well understood. A Cyclic Executive Pattern has the advantage of almost mindless simplicity coupled with extremely predictable behavior. This results in a number of desirable properties for such systems. First, implementation of this pattern is so easy it is hard to get it wrong, at least in any gross way. Also, it can be written to run in highly memory-constrained systems where a full RTOS may not be an option. In many very small applications, not only is a full RTOS not required, it may not even be feasible due to memory constraints. Often, the only RTOS support that is really required is the ability to run a set of more or less independent tasks. The simplest way to accomplish this is to run what is called an "event loop" or Cyclic Executive that simply executes the tasks in turn, from the first to the last, and then starts over again. Each task runs if it has something to do, and it relinquishes control immediately if not. Another issue occurs when the execution time for a set of tasks is constant and it is desirable to have a highly predictable system. "Highly predictable" means you can predict the assembly language instruction that will be executing at any point in the future, based solely on the time.
##%%&&
The Round Robin Pattern employs a "fairness" scheduling doctrine that may be appropriate for systems in which it is more important for all tasks to progress than it is for specific deadlines to be met. The Round Robin Pattern is similar to the Cyclic Executive Pattern except that the former does employ preemption based on time. Most of the literature of real-time systems has focused on hard real-time systems (primarily because of its computation tractability). Hard real-time systems have tasks that are either time-driven (periodic) or event-driven (aperiodic with well-defined deadlines, after which the tasks are late). Such systems are inherently "unfair" because in an overload situation, low-priority tasks are selectively starved. There are other systems where individual deadlines are not as crucial as overall progress of the system. For such systems, a priority-based preemption approach may not be preferred. The Round Robin Pattern addresses the issue of moving an entire set of tasks forward at more or less equal rates, particularly tasks that do not complete within a single scheduling cycle.
##%%&&
The static priority pattern is the most common approach to scheduling in a real-time system. It has the advantages of being simple and scaling fairly well to large numbers of tasks. It is also simple to analyze for schedulability, using standard rate monotonic analysis methods. Two important concepts in the design of real-time systems are urgency and criticality. Urgency refers to the nearness of a deadline, while criticality refers to how important the meeting of that deadline is in terms of system functionality or correctness. It is clear that these are distinctly different concepts, yet what operating systems typically provide to deal with both of these issues is a single concept: priority. The operating system uses the priority of a task to determine which tasks should run preferentially when more than one task is ready to run. That is, the operating system always runs the highest-priority task of all the tasks currently ready to run. The most common approach to assigning priorities in a real-time system is to define the priority of each task during design. This priority is called static because it is assigned during design and never changed during the execution of the system. This approach has the advantages of (1) simplicity, (2) stability, and, (3) if good policies are used in the selection of the tasks and their priorities, optimality. By stability, we mean that in an overloaded situation, we can predict which tasks will fail to meet their deadlines (that is, the lower-priority ones). By optimality, we mean that if the task set can be scheduled using other approaches, then it can also be scheduled using static priority assignment. We will discuss a common optimal approach to priority assignment later in this section. When we say "real-time" we mean that predictably timely execution of behavior is essential to correctness. This constraint can arise from any number of real-world problems, such as the length of time a safety-critical system can tolerate a fault before an "incident" occurs or the stability characteristics of a PID control loop. [4] By far, the most common way to model time for such systems is to assign a time interval, beginning with an incoming event or the start of the thread execution and ending with the completion of the task invocation. This is called a deadline. It is a common simplification that is used because systems can be easily analyzed for schedulability, given the periodicity, [5] the worst-case execution time, and deadline for all tasks in the system, as long as the priority of the tasks is also known.
##%%&&
The Dynamic Priority Pattern is similar to the Static Priority Pattern except that the former automatically updates the priority of tasks as they run to reflect changing conditions. There are a large number of possibly strategies to change the task priority dynamically. The most common is called Earliest Deadline First, in which the highest-priority task is the one with the nearest deadline. The Dynamic Priority Pattern explicitly emphasizes urgency over criticality. As mentioned in the previous section, the two most important concepts around schedulability are urgency and criticality, but what operating systems typically provide to manage both is a single value: priority. In the Static Priority Pattern, priorities are set at design time, usually reflecting a combination of the urgency and criticality of the two. In the Dynamic Priority Pattern, the priority of a task is set at run-time based solely on the urgency of the task. The Dynamic Priority Pattern sets the priority of each task as a function of the time remaining until its deadline the closer the deadline, the higher the priority. Another common name for such a scheduling policy is Earliest Deadline First, or EDF. Such a strategy is demonstrably optimal. This means that if the task set can be scheduled by any approach, then it can also be scheduled by this one. However, the Dynamic Priority Pattern isn't stable; this means that it is impossible to predict at design time which tasks will fail in an overload situation. The Dynamic Priority Pattern is best suited for task sets that are at least of approximately equal criticality so that urgency is the overriding concern. It is also well suited for highly complex situations in which it may be impossible to predict the set of tasks that will be running simultaneously. In such complex situations, it is difficult or impossible to construct optimal static priorities for the tasks. For small real-time systems, the permutations of tasks that will be running are known, and the tasks themselves are stable: Their deadlines are consistent from task invocation to task invocation, and their execution time is roughly the same as well. This simplifies the analysis well enough to permit each computation of the schedulability of the system in absolute terms. In a complex system, such as fully symmetric multitasking systems, in which the assignment of a task to a processor isn't known until execution time, such analysis is difficult or impossible. Furthermore, even if the analysis can be done, it is complicated work that must be completely redone to add even a single task to the analysis.
##%%&&
The Static Allocation Pattern applies only to simple systems with highly predictable and consistent loads. However, where it does apply, the application of this pattern results in systems that are easy to design and maintain. Dynamic memory allocation has two primary problems that are particularly poignant for real-time and embedded systems: nondeterministic timing of memory allocation and deallocation and memory fragmentation. This pattern takes a very simple approach to solving both these problems: disallow dynamic memory allocation. The application of this pattern means that all objects are allocated during system initialization. Provided that the memory loading can be known at design time and the worst-case loading can be allocated entirely in memory, the system will take a bit longer to initialize, but it will operate well during execution. Dynamic memory allocation is very common in both structured and object design implementations. C++, for example, uses new and delete, whereas C uses malloc and free to allocate and deallocate memory, respectively. In both these languages, the programmer must explicitly perform these operations, but it is difficult to imagine any sizable program in either of these languages that doesn't use pointers to allocated memory. The Java language is even worse: All objects are allocated in dynamic memory, so all object creation implicitly uses dynamic memory allocation. Further, Java invisibly deallocates memory once it is no longer used, but when and where that occurs is not under programmer control. As common as it is, dynamic memory allocation is somewhat of an anathema to real-time systems because it has two primary difficulties. First, allocation and deallocation are nondeterministic with respect to time because generally they require searching data structures to find free memory to allocate. Second, deallocation is not without problems either. There are two strategies for deallocation: explicit and implicit. Explicit deallocation can be deterministic in terms of time (since the system has a pointer to its exact location), and the programmer must keep track of all conditions under which the memory must be released and explicitly release it. Failure to do so correctly is called a memory leak, since not all memory allocated is ultimately reclaimed, so the amount of allocable storage decreases over time until system failure. Implicit deallocation is done by means of a Garbage Collector an object that either continuously or periodically scans memory looking for lost memory and reclaiming it. Garbage collectors can be used, but they are more nondeterministic than allocation strategies, require a fair amount of processing in and of themselves, and may require more memory in some cases (for example, if memory is to be compacted). Garbage collectors do, on the other hand, solve the most common severe defects in software systems. The third issue around dynamic memory is fragmentation. As memory is allocated in blocks of various sizes, the deallocation order is usually unrelated to the allocation order. This means that what was once a contiguous block of free memory ends up as a hodgepodge of free and used blocks of memory. The fragmentation increases the longer the system runs until eventually a request for a block of memory cannot be fulfilled because there is no single block large enough to fulfill the request, even though there may be more than enough total memory free. This is a serious problem for all real-time and embedded systems that use dynamic memory allocation not just for those that must run for longer periods of time between reboots.
##%%&&
The Static Allocation Pattern only works well for systems that are, well, static in nature. Sometimes you need sets of objects for different purposes at different times during execution. When this is the case, the Pool Allocation Pattern works well by creating pools of objects, created at startup, available to clients upon request. This pattern doesn't address needs for dynamic memory but still provides for the creation of more complex programs than the Static Allocation Pattern. In many applications, objects may be required by a large number of clients. For example, many clients may need to create data objects or message objects as the system operates in a complex, changing environment. The need for these objects may come and go, and it may not be possible to predict an optimal dispersement of the objects even if it is possible to bound the total number of objects needed. In this case, it makes sense to have pools of these objects¡ªcreated but not necessarily initialized and available upon request. Clients can request them as necessary and release them back to the pool when they're done with them. The prototypical candidate system for the Pooled Allocation Pattern is a system that cannot deal with the issues of dynamic memory allocation, but it is too complex to permit a static allocation of all objects. Typically, a number of similar, typically small, objects, such as events, messages, or data objects, may need to be created and destroyed but are not needed a priori by any particular clients for the entire run-time of the system.
##%%&&
Many real-time and embedded systems are complex enough to be unpredictable in the order in which memory must be allocated and too complex to allocate enough memory for all possible worst cases. Such systems would be relatively simple to design using dynamic memory allocation. However, many such systems in the real-time and embedded world must function reliably for long periods of time¡ªoften years or even decades-between reboots. That means that while they are complex enough to require dynamic random allocation of memory, they cannot tolerate one of the major problems associated with dynamic allocation: fragmentation. For such systems, the Fixed Sized Buffer Pattern offers a viable solution: fragmentation-free dynamic memory allocation at the cost of some loss of memory usage optimality. The Fixed Sized Buffer Pattern provides an approach for true dynamic memory allocation that does not suffer from one of the major problems that affect most such systems: memory fragmentation. It is a pattern supported by most real-time operating systems directly. Although it requires static memory analysis to minimize nonoptimal memory usage, it is a simple and easy to implement approach. One of the key problems with dynamic memory allocation is memory fragmentation. Memory fragmentation is the random inter-mixing of free and allocated memory in the heap. For memory fragmentation to occur, the following conditions must be met. The order of memory allocation is unrelated to the order in which it is released. Memory is allocated in various sizes from the heap. When these preconditions are met, then memory fragmentation will inevitably occur if the system runs long enough. Note that this is not a problem solely related to object-oriented systems, functionally decomposed systems written in C are just as affected as those written in C++. [2] The problem is severe enough that it will usually lead to system failure if the system runs long enough. The failure occurs even when analysis has demonstrated that there is adequate memory because if the memory is highly fragmented, there may be more than enough memory to satisfy a request, but it may not be in a contiguous block of adequate size. When this occurs, the memory allocation request fails even though there is enough total memory to satisfy the request.
##%%&&
In my experience over the last couple of decades leading and managing development projects implemented 
in C and C++, pointer problems are by far the most common defects and the hardest to identify. They are 
common because the pointer metaphor is very low level and requires precise management, but it is easy to 
forget about when dealing with all possible execution paths. Inevitably, somewhere a pointer is destroyed 
(or goes out of scope), but the memory is not properly freed (a memory leak), memory is released but 
nevertheless accessed (dangling pointer), or memory is accessed but not properly allocated (uninitialized 
pointer). These problems are notoriously difficult to identify using standard means of testing and peer 
reviews. Tools such as Purify and LINT can identify "questionable practices," but sometimes they flag so 
many things it is virtually impossible to use the results. The Smart Pointer Pattern is an approach that is 
mechanistic (medium scope) rather than architectural (large scope) but has produced excellent results. Pointers are by far the most common way to realize an association between objects. The most common 
implementation of a navigable association is to use a pointer. This pointer attribute is dereferenced to send 
messages to the target object. The problem with pointers per se is that they are not objects; they are just 
data. Because they are not objects, the primitive operations you can perform on them are not checked for 
validity. Thus, we are free to access a pointer that has never been initialized or after the memory to which it points has been freed. We are also free to destroy the pointer without releasing the memory, resulting in 
the loss of the now no-longer-referenceable memory to the system. 
The Smart Pointer Pattern solves these problems by making the pointer itself an object. Because a Smart 
Pointer is an object, it can have constructors and destructors and operations that can ensure that its 
preconditional invariants ("rules of proper usage") are maintained. In many ways, pointers are the bane of the programmer's existence. If they weren't so incredibly useful, we 
would have discarded them a long time ago. Because they allow us to dynamically allocate, deallocate, and 
reference memory dynamically, they form an important part of the programmer's toolkit. However, their 
use commonly results in a number of different kinds of defects. Memory leaks¡ª destroying a pointer before the memory they reference is released. This means 
that the memory block is never put back in the heap free store, so its loss is permanent, at least 
until the system is rebooted. Over time, the available memory in the heap free store (that is, 
memory that can now be allocated by request) shrinks, and eventually the system fails because it 
cannot satisfy memory requests. Uninitialized pointer¡ª using a pointer as if it was pointing to a valid object (or memory block) 
but neglecting to properly allocate the memory. This can also occur if the memory request is made 
but refused. Dangling pointer¡ª using a pointer as if it was pointing to a valid object (or memory block) but 
after the memory to which it points has been freed. Pointer arithmetic defects¡ª using a pointer as an iterator over an array of values but inappropriately. This can be because the pointer goes beyond the bounds of the array (in either 
direction), possibly stepping on memory allocated to other objects, or becoming misaligned, 
pointing into the middle of a value rather than at its beginning. 
These problems arise because pointers are inherently stupid. They are only data values (addresses), and the 
operations defined on them are primitive and without checks on their correct use. If only they were objects, 
their operations could be extended to include validity checks and they could identify or prevent 
inappropriate use.
##%%&&
Memory defects are among the most common and yet most difficult to identify errors. They are common 
because the programming languages provide very low access to memory but do not provide the means to 
identify when the memory is being accessed properly. This can lead to memory leaks and dangling 
pointers. The insidious aspect of these defects is that they tend to have global, rather than local, impact, so 
while they can crash the entire system, they leave no trace as to where the defect may occur. The Garbage 
Collection Pattern addresses memory access defects in a clean and simple way as far as the application 
programmer is concerned. The standard implementation of this pattern does not address memory 
fragmentation, but it does allow the system to 
operate properly in the face of poorly managed memory. The Garbage Collection Pattern can eliminate memory leaks in programs that must use dynamic memory 
allocation. Memory leaks occur because programmers make mistakes about when and how memory should 
be deallocated. The solution offered by the Garbage Collection Pattern removes the defects by taking the 
programmer out of the loop¡ªthe programmer no longer explicitly deallocates memory. By removing the 
programmer, that source of defects is effectively removed. The costs of this pattern are run-time overhead 
to identify and remove inaccessible memory and a loss of execution predictability because it cannot be 
determined at design time when it may be necessary to reclaim freed memory. The Garbage Collection Pattern addresses the problem of how we can make sure we won't have any 
memory leaks. Many high-availability or high-reliability systems must function for long periods of time 
without being periodically shut down. Since memory leaks lead to unstable behavior, it may be necessary 
to completely avoid them in such systems. Furthermore, reference counting Smart Pointers 
have the disadvantages that they require programmer discipline to 
use correctly and cannot be used when there are cyclic object references.
##%%&&
The Garbage Compactor Pattern is a variant of the Garbage Collection Pattern that also removes memory 
fragmentation. It accomplishes this goal by maintaining two memory segments in the heap. During 
garbage collection, live objects are moved from one segment to the next, so in the target segment, the 
objects are juxtapositioned adjacent to each other. The free memory in the segment then starts out as a 
contiguous block. The Garbage Collection Pattern solves the problem of programmers forgetting to release memory by every 
so often finding inaccessible objects and removing them. The pattern has a couple of problems, including 
maintaining the timeliness of the application and fragmentation. Fragmentation means that the free 
memory is broken up into noncontiguous blocks. If the application is allowed to allocate blocks in 
whatever size they may be needed, most applications that dynamically allocate and release blocks will 
eventually get into the situation where although there is enough total memory to meet the allocation 
request, there isn't a single contiguous block large enough. At this point, the application fails. Garbage 
collection per se does not solve this problem just because it finds and removes dead objects. To compact 
memory, the allocated blocks must be moved around periodically to leave the free memory as a single, 
large contiguous block.
##%%&&
The Critical Section Pattern is the simplest pattern to share resources that cannot be shared simultaneously. 
It is lightweight and easy to implement, but it may prevent high priority tasks, even ones that don't use any 
resources, from meeting their deadlines if the critical section lasts too long. This pattern has been long used in the design of real-time and embedded systems whenever a resource 
must have at most a single owner at any given time. The basic idea is to lock the Scheduler whenever a 
resource is accessed to prevent another task from simultaneously accessing it. The primary advantage of 
this pattern is its simplicity, both in terms of understandability and in terms of implementation. It becomes 
less applicable when the resource access may take a long time because it means that higher-priority tasks 
may be blocked from execution for a long period of time. The main problem addressed by the Critical Section Pattern is how to robustly share resources that may 
have, at most, a single owner at any given time. 
##%%&&
The Priority Inheritance Pattern reduces priority inversion by manipulating the executing priorities of tasks 
that lock resources. While not an ideal solution, it significantly reduces priority inversion at a relatively 
low run-time overhead cost. The problem of unbounded priority inversion is a very real one and has accounted for many difficult-to-
identify system failures. In systems running many tasks, such problems may not be at all obvious, and 
typically the only symptom is that occasionally the system fails to meet one or more deadlines. The 
Priority Inheritance Pattern is a simple, low-overhead solution for limiting the priority inversion to at most 
a single level¡ªthat is, at most, a task will only be blocked by a single, lower-priority task owning a needed 
resource. The unbounded priority inversion problem is discussed in the chapter introduction in some detail. The 
problem addressed by this pattern is to bound the maximum amount of priority inversion.
##%%&&
The Highest Locker Pattern defines a 
priority ceiling with each resource. The basic idea is that the task 
owning the resource runs at the highest-priority ceiling of all the resources that it currently owns, provided 
that it is blocking one or more higher-priority tasks. This limits priority inversion to at most one level. The Highest Locker Pattern is another solution to the unbounded blocking/unbounded priority inversion 
problem. It is perhaps a minor elaboration from the Priority Inheritance Pattern, but it is different enough 
to have some different properties with respects to schedulability. The Highest Locker Pattern limits priority 
inversion to a single level as long as a task does not suspend itself while owning a resource. In this case, 
you may get chained blocking similar to the Priority Inheritance Pattern. Unlike the Priority Inheritance 
Pattern, however, you cannot get chained blocking if a task is preempted while owning a resource. The unbounded priority inversion problem is discussed in the chapter introduction in some detail. The 
problem addressed by this pattern is to limit the maximum amount of priority inversion to a single level¡ª
that is, there is at most a single lower-priority task blocking a higher-priority task from executing.
##%%&&
The Priority Ceiling Pattern, or Priority Ceiling Protocol (PCP) as it is sometimes called, addresses both 
issues of bounding priority inversion (and hence bounding blocking time) and removal of deadlock. It is a 
relatively sophisticated approach, more complex than the previous methods. It is not as widely supported 
by commercial RTOSs, however, and so its implementation often requires writing extensions to the RTOS. The Priority Ceiling Pattern is used to ensure bounded priority inversion and task blocking times and also 
to ensure that deadlocks due to resource contention cannot occur. It has somewhat more overhead than the 
Highest Locker Pattern. It is used in highly reliable multitasking systems. The unbounded priority inversion problem is discussed in the chapter introduction in some detail. The 
Priority Ceiling Pattern exists to limit the maximum amount of priority inversion to a single level and to 
completely prevent resource-based deadlock.
##%%&&
The Simultaneous Locking Pattern is a pattern solely concerned with deadlock avoidance. It achieves this 
by breaking condition 2 (holding resources while waiting for others). The pattern works in an all-or-none 
fashion. Either all resources needed are locked at once or none are. Deadlock can be solved by breaking any of the four conditions required for its existence. This pattern 
prevents the condition of holding some resources by requesting others by allocating them all at once. This 
is similar to the Critical Section Pattern. However, it has the additional benefit of allowing higher-priority 
tasks to run if they don't need any of the locked resources. The problem of deadlock is such a serious one in highly reliable computing that many systems design in 
specific mechanisms to detect it or avoid it. As previously discussed, deadlock occurs when a task is 
waiting on a condition that can never, in principle, be satisfied. There are four conditions that must be true 
for deadlock to occur, and it is sufficient to deny the existence of any one of these. The Simultaneous 
Locking Pattern breaks condition 2, not allowing any task to lock resources while waiting for other 
resources to be free.
##%%&&
The Ordered Locking Pattern is another way to ensure that deadlock cannot occur¡ªthis time by preventing condition 4 (circular waiting) from occurring. It does this by ordering the resources and requiring that they 
always be accessed by any client in that specified order. If this is religiously enforced, then no circular 
waiting condition can ever occur. The Ordered Locking Pattern eliminates deadlock by ordering resources and enforcing a policy in which 
resources must be allocated only in a specific order. Unlike "normal" resource access, but similar to the 
Simultaneous Locking Pattern, the client must explicitly lock and release the resources, rather than doing it 
implicitly by merely invoking a service on a resource. This means that the potential for neglecting to 
unlock the resource exists. The Ordered Locking Pattern solely addresses the problem of deadlock elimination, as does the previous 
Simultaneous Locking Pattern.
##%%&&
The Shared Memory Pattern uses a common memory area addressable by multiple processors as a means 
to send messages and share data. This is normally accomplished with the addition of special hardware¡ª
specifically, multi-ported RAM chips. The Shared Memory Pattern is a simple solution when data must be shared among more than one processor, 
but timely responses to messages and events between the processors are not required. The pat-tern almost 
always involves a combined hardware/software solution. Hardware support for single CPU-cycle 
semaphore and memory access can avoid memory conflicts and data corruption, but usually some software 
support to assist the low-level hardware features is required for robust access. If the data to be shared is 
read-only, as for code that is to be executed on multiple processors, then such concurrency protection 
mechanisms may not be required. Many systems have to share data between multiple processors¡ªthis is the essence of distribution, after all. 
In some cases, the access to the data may persist for a long period of time, and the amount of data shared 
may be large. In such cases, sending messages may be an inefficient method for sharing such information. 
Multiple computers may need to update this "global" data, such as in a shared database, or they may need 
to only read it, as is the case with executable code that may run on many processors or configuration tables. 
A means by which such data may be effectively shared is needed.
##%%&&
Remote Procedure Calls (RPCs) are a common method for invoking services synchronously between 
processors. The object-oriented equivalent, Remove Method Calls, work in the same way. This approach 
requires underlying OS support, but it works much the same way that local method calls work: The client 
invokes a service on the server and waits in a blocked condition until the called operation completes. RPCs are provided by Unix-based [2]
 and other operating systems such as VxWorks [3] as a more abstract 
and usable form of Inter-process Communications (IPC). RPCs allow the invocation of services across a 
network in a way very similar to how a local service would be invoked. In the object-oriented world, we 
refer to RMCs (Remote Method Calls), but the concept is the same: provide a means to invoke services 
across a network in a manner as similar as possible to how they are invoked locally. The programming model used to invoke services locally is very well understood: dereference an 
association (most commonly implemented as a pointer or reference) and invoke the service. This is done in 
a synchronous fashion¡ªthe caller blocking until the server completes the request and returns whatever 
values were requested. What is needed is a means to do the same thing even when the client and server are 
not colocated.
##%%&&
The Observer Pattern is perhaps arguably more of a mechanistic design than architectural design pattern. 
However, it will serve as the basis for other distribution collaboration architecture patterns, and so it is 
included here. The Observer Pattern (aka "Publish-Subscribe") addresses the specific issue of how to notify a set of 
clients in a timely way that a value that they care about has changed, especially when the notification 
process is to be repeated for a relatively long period of time. The basic solution offered by the Observer 
Pattern is to have the clients "subscribe" to the server to be notified about the value in question according 
to some policy. This policy can be "when the value changes," "at least every so often," "at most every so 
often," etc. This minimizes computational effort for notification of clients and across a communications 
bus and minimizes the bus bandwidth required for notification of the appropriate clients. The problem addressed by the Observer Pattern is how to notify some number of clients in a timely fashion 
of a data value according to some abstract policy, such as "when it changes," "every so often," "at most 
every so often," and "at least every so often." One approach is for every client to query the data value but 
this can be computationally wasteful, especially when a client wants to be notified only when the data 
value changes. Another solution is for the server of this information to be designed knowing its clients. 
However, we don't want to "break" the classic client-server model by giving the server knowledge about its 
clients. That makes the addition of new clients a design change, making it more difficult to do dynamically 
at run-time.
##%%&&
The Data Bus Pattern further abstracts the Observer Pattern by providing a common (logical) bus to which 
multiple servers post their information and where multiple clients come to get various events and data 
posted to the bus. This pattern is useful when a large number of servers and clients must share data and 
events and is easily supported by some hardware bus structures that broadcast messages, such as the CAN 
(Control Area Network) bus architectures. The Data Bus Pattern provides a single locale (the "Data Bus") for the location of information to be shared 
across multiple processors. Clients desiring information have a common location for pulling information as 
desired or subscribing for pushed data. The Data Bus Pattern is basically a Proxy Pattern with a centralized 
store into which various data objects may be plugged. Many systems need to share many different data among a mixture of servers and clients, some of whom 
might not be known when the client or data is designed. This pattern solves the problem by providing a 
central storage facility into which data that is to be shared may be plugged along with metadata that 
describes its contents.
##%%&&
The Proxy Pattern abstracts the true server from the client by means of a "stand-in" or surrogate class 
providing a separation of a client and a server, allowing the hiding of specified properties of the server 
from the clients. The Proxy Pattern abstracts the true server from the client by means of a "stand-in" or surrogate class. 
There are a number of reasons why this may be useful, such as to hide some particular implementation 
properties from the clients and thus allow them to vary transparently to the client. For our purposes here, 
the primary reason to use the Proxy Pattern is to hide the fact that a server may be actually located in 
another address space from its client. This allows the server to be located in any accessible location, and 
the clients need not concern themselves with how to contact the true server to access required information 
or services. The design of modern embedded systems must often be deployed across multiple address spaces, such as 
different processors. Often such details are subject to change during the design process or, even worse, 
during the implementation of the system. It is problematic to "hard-code" the knowledge that a server may 
be remote because this may change many times as the design progresses. Further, the clients and servers 
may be redeployed in other physical architectures and using different communications media. If the clients 
are intimately aware of these design details, then porting the clients to the new platforms is more difficult. 
The two primary problems addressed by the Proxy Pattern are the transparency of the potential remoteness 
of the servers and the hiding and encapsulation of the means by which to contact such remote servers.
##%%&&
The Broker Pattern may be thought of as a symmetric version of the Proxy Pattern¡ªthat is, it provides a Proxy Pattern in situations where the location of the clients and servers are not known at design time. The Broker Pattern extends the Proxy Pattern through the inclusion of the 
Broker¡ªan "object reference 
repository" globally visible to both the clients and the servers. This broker facilitates the location of the 
servers for the clients so that their respective locations need not be known at design time. This means that 
more complex systems that can use a symmetric deployment architecture, such as is required for dynamic 
load balancing, can be employed. In addition to the problems addressed by the Proxy Pattern (such as communication infrastructure 
transparency), a limitation of most of the distribution patterns is that they require a priori knowledge of the 
location of the servers. This limits their use to asymmetric distribution architectures. Ideally, the solution 
should provide a means that can locate and then invoke services at the request of the client, including 
subscription to published data.
##%%&&
Complete redundancy is costly. Sometimes it is costly in terms of recurring cost (cost per shipped system) 
because hardware is replicated. Sometimes it is costly also in development cost (due to diverse, or n-way, 
redundancy). Not all safety-critical and high-reliability systems need the heavy weight and expensive 
redundancy required by some safety and reliability patterns. The Protected Single Channel Pattern is a 
lightweight means to get some safety and reliability by adding additional checks and actions (and possibly 
some level of redundant hardware as well). The Protected Single Channel Pattern uses a single channel to handle sensing and actuation. Safety and 
reliability are enhanced through the addition of checks at key points in the channel, which may require some additional hardware. The Protected Single Channel Pattern will not be able to continue to function in 
the presence of persistent faults, but it detects and may be able to handle transient faults. Since redundancy is expensive in recurring cost, and the safety and reliability requirements of some 
systems may not be as high as with others, a means is needed to improve safety and reliability in an 
inexpensive manner even if the improvements in safety and reliability are not as great as with some other 
approaches.
##%%&&
The Homogeneous Redundancy Pattern is primarily a pattern to improve reliability by offering multiple 
channels. These channels can operate in sequence, as in the Switch To Backup Pattern (another name for 
this pattern), or in parallel, as in the Triple Modular Redundancy Pattern, described later. The pattern 
improves reliability by addressing random faults (failures). Since the redundancy is homogeneous, by 
definition any systematic fault in one copy of the system is replicated in its clones, so it provides no 
protection against systematic faults (errors). An obvious approach to solving the problem of things breaking is to provide multiple copies of that thing. 
In safety and reliability architectures, the fundamental unit is called a channel. A channel is a kind of 
subsystem, or run-time organizational unit, which is end-to-end in its scope, from the monitoring of real-
world signals to the control of actuators that do the work of the system. The Homogeneous Redundancy 
Pattern replicates channels with a switch-to-backup policy in the case of an error. The problem addressed by the Homogeneous Redundancy Pattern is to provide protection against random 
faults¡ªthat is, failures¡ªin the system execution and to be able to continue to provide functionality in the 
presence of a failure. The primary channel should continue to run as long as there are no problems. In the 
case of failure within the channel, the system must be able to detect the fault and switch to the backup 
channel.
##%%&&
The Triple Modular Redundancy Pattern (TMR, for short) is a pattern used to enhance reliability and 
safety in situations where there is no fail-safe state. The TMR pattern offers an odd number of channels 
(three) operating in parallel, each in effect checking the results of all the others. The computational results 
or resulting actuation signals are compared, and if there is a disagreement, then a two-out-of-three majority 
wins policy is invoked. The TMR pattern is a variation of the Homogeneous Redundancy Pattern that operates three channels in 
parallel rather than operating a single channel and switching over to an alternative when a fault is detected.  
By operating the channels in parallel, the TMR pattern detects random faults as outliers (assuming a single 
point failure and common mode fault independence of the channels) that are discarded as erroneous 
automatically. The TMR pattern runs the channels in parallel and at the end compares the results of the 
computational channels together. As long as two channels agree on the output, then any deviating 
computation of the third channel is discarded. This allows the system to operate in the presence of a fault 
and continue to provide functionality. The problem addressed by the Triple Modular Redundancy Pattern is the same as the Homogeneous 
Redundancy Pattern¡ªthat is, to provide protection against random faults (failures) with the additional 
constraint that when a fault is detected, the input data should not be lost, nor should additional time be 
required to provide a correct output response.
##%%&&
The Heterogeneous Redundancy Pattern [3] improves detection of faults over homogeneous redundancy by 
also detecting systematic faults. This is achieved by using multiple channels that have independent designs 
and/or implementations. This is the most expensive kind of redundancy because not only is the recurring 
cost increased (similar to the Homogeneous Redundancy Pattern) but development cost is increased as 
well due to the doubled or tripled design effort required. [3] Also known as 
Diverse Redundancy and N-way Programming. For high-safety and reliability systems, it is common to provide redundant channels to enable the system to 
identify faults and to continue safe and reliable operation in the presence of faults. Similar to its 
homogeneous cousin, the Heterogeneous Redundancy Pattern provides redundant channels as an 
architectural means to improve safety and reliability. What sets the Heterogeneous Redundancy Pattern 
apart is that the channels are not mere replicas but are constructed from independent designs. This means 
that identical design errors are unlikely to appear in multiple channels. The primary downside of this 
pattern is its high design development cost that comes on top of the high recurring cost typical of 
heavyweight redundant channels. 
There are a number of useful variants of the Heterogeneous Redundancy Pattern that provide the detection 
of both kinds of faults but are lower cost and may not provide continued operation in the presence of faults. 
See, for example, the Monitor-Actuator and Sanity Check Patterns. The Heterogeneous Redundancy Pattern provides protection against both kinds of faults¡ªsystematic 
errors as well as random failures. Assuming that the design includes independence of faults, the pattern 
provides single fault safety in the same way as the Homogeneous Redundancy Pattern¡ªthat is, when the 
primary channel detects a fault, the secondary channel takes over.
##%%&&
All safety-critical and reliable architectures have redundancy in some form or another. In some of these 
patterns, the entire channel, from original data sensing to final output actuation, is replicated in some form 
or another. In the Monitor-Actuator Pattern, an independent sensor maintains a watch on the actuation 
channel looking for an indication that the system should be commanded into its fail-safe state. Many safety-critical systems have what is called a fail-safe state. This is a condition of the system known 
to be always safe. When this is true, and when the system doesn't have extraordinarily high availability 
requirements (that is, in the case of a fault detection it is appropriate to enter the fail-safe state), then the 
safety of the system can be maintained at a lower cost than some of the other patterns discussed in this 
chapter. The Monitor-Actuator Pattern is a specialized form of the Heterogeneous Redundancy Pattern 
because the redundancy provided is different from the primary actuation channel: It provides monitoring, 
typically of the commanded actuation itself (although it may also monitor the internal operation of the 
actuation channel as well). 
Assuming fault independence and a single point fault protection requirement, the basic principle of the 
Monitor-Actuator Pattern may be summed up this way: If the actuation channel has a fault, the monitoring 
channel detects it. If the monitoring channel breaks, then the actuation channel continues to operate 
properly. The Monitor-Actuator Pattern addresses the problem of improving safety in a system with moderate to low 
availability requirements at a low cost.
##%%&&
The Sanity Check Pattern is a very lightweight pattern that provides minimal fault coverage. The purpose of the Sanity Check Pattern is to ensure that the system is more or less doing something reasonable, even if 
not quite correct. This is useful in situations where the actuation is not critical if performed correctly (such 
as an optional enhancement) but is capable of doing harm if it is done incorrectly. It is a variant of the 
Monitor-Actuator Pattern and, like the that pattern, assumes that a fail-safe state is available. The Sanity Check Pattern is a variant of the Monitor-Actuator Pattern; it has the same basic properties. 
Where it differs is in the functionality provided by the Monitor Component. The Sanity Check Pattern only 
exists to ensure that the actuation is approximately correct. It typically uses lower-cost (and usually lower-
accuracy) sensors and can only identify when the actuation is grossly incorrect. Thus, it is applicable only 
in situations where fine control is not a safety property of the Actuation Channel. In some extreme cases, 
the monitor may not even be required to know the commanded set point because it will only ensure that the 
actuation output is within some fixed range. Usually, however, the Monitor will have a "valid range" that 
varies with the commanded set point. This pattern addresses the issue, making sure the "system does no harm" when minor, or even moderate, 
deviations from the commanded set point have no safety impact, and providing this minimal level of 
protection at a very low recurring and design cost.
##%%&&
The Watchdog Pattern is similar to the Sanity Check Pattern in the sense that it is lightweight and 
inexpensive. It differs in what it monitors. While the Sanity Check Pattern monitors the actual output of the 
system using an external environmental sensor, the Watchdog Pattern merely checks that the internal 
computational processing is proceeding as expected. This means that its coverage is minimal, and a broad 
set of faults will not be detected. On the other hand, it is a pattern that can add additional safety when 
combined with other heavier-weight patterns. A 
watchdog, used in common computing parlance, is a component that watches out over processing of 
another component. Its job is to make sure that nothing is obviously wrong, just as a real watchdog protects 
the entrance to the henhouse without bothering to check if in fact the chickens inside are plotting nefarious 
deeds. The most common purpose of a watchdog is to check a computation timebase or to ensure that 
computation steps are proceeding in a predefined order. Watchdogs are often used in real-time systems to 
ensure that time-dependent processing is proceeding appropriately. Real-time systems are those that are predictably timely. In the most common (albeit simplified) view, the 
computations have a deadline by which they must be applied. If the computation occurs after that deadline, 
the result may either be erroneous or irrelevant¡ªso-called hard real-time systems. Systems implementing 
PID control loops, for example, are notoriously sensitive to the time lag between the occurrence of the 
input signal and the output of the control signal. If the output comes too late, then the system cannot be 
controlled; then the system is said to be in an unstable region.
##%%&&
Sometimes the control of the safety measures of a system are very complex. This may be because the 
system cannot be simply shut off but must be driven through a potentially complex sequence of actions to 
read a fail-safe state. The Safety Executive Pattern provides a Safety Executive to oversee the coordination 
of potentially multiple channels when safety measures must be actively applied. Systems often cannot merely be shut down in the event of a fault. Sometimes this is because they are in the 
middle of handling some dangerous materials or a high-energy state of the system (such as high speed or 
high voltage potential). Simply shutting the system off in such a state is potentially very hazardous. In the 
presence of a fault, the system must be guided through a potentially complicated series of steps to reach a 
condition known to be a fail-safe state. The Safety Executive Pattern models exactly this situation in which 
a Safety Executive component coordinates the activities of potentially many actuation channels and safety 
measures to reach a fail-safe state. The problem addressed by the Safety Executive Pattern is to provide a means to coordinate and control the 
execution of safety measures when the safety measures are complex.
##%%&&
This is the root pattern for all enterprise security concerns. It helps resolve the issue
of whether security is really needed and, if it is, what properties of security should
be applied for a particular enterprise. Security properties considered include
confidentiality, integrity, availability, and accountability. A new wing of a museum of gemstones is to be opened. The museum has significant
previous experience handling gems, and theft is a large enough risk for the museum
to want protection from the unauthorized removal of any gems. The museum also
has information about the collections, and employee information, that should be
protected from damage or deletion, and in some cases should be kept confidential.
How can the museum determine the assets that need security protection, and which
types of protection? An enterprise considers security as a significant non-functional requirement. Key business factors and assets of the enterprise are understood.
##%%&&
Asset valuation helps you to determine the overall importance an enterprise places
on the assets it owns and controls. Loss or compromise of such assets may result in
anything from hard costs, such as fines and fees, to soft costs due to loss of market
share and consumer confidence. The museum has begun a risk assessment and identified the following assets to be in
scope:
Information Assets Museum employee data Museum financial/insurance data, partner financial data Museum contractual data and business planning Museum research and associated data Museum advertisements and other public data Museum database of collections information
Physical Assets Museum building Museum staff Museum collections and exhibits Museum transport vehicles
They must now determine the overall importance of these assets. An enterprise has determined which assets are to be included in the overall risk-
assessment process, and must now ascertain the value it places on those assets.
##%%&&
Threats are the likelihood of, or potential for, hazardous events occurring. They can
affect any asset or object on which an enterprise places value. An enterprise threat
assessment identifies the threats posed to the enterprise¡¯s assets, and determines the
likelihood or frequency of their occurrence. The museum has begun a risk assessment and identified the following assets to be in
scope:
Information asset types Museum employee data Museum financial/insurance data, partner financial data Museum contractual data and business planning Museum research and associated data Museum advertisements and other public data Museum database of collections information
Physical Assets Museum building Museum staff Museum collections and exhibits Museum transport vehiclesThe museum has also identified the major security needs for these assets using
SECURITY NEEDS IDENTIFICATIONFOR ENTERPRISE ASSETS (89), and must now determine the threats to those assets. An enterprise has defined the assets to be included in a risk assessment and must now identify the events that could cause harm to those assets.
##%%&&
A vulnerability is a weakness that could be exploited by a threat, causing the violation
of an asset¡¯s security property. Conducting an enterprise vulnerability assessment helps
to identify the weaknesses of the enterprise¡¯s assets and the systems that enable access
to them, and evaluates the severity if a vulnerability were to be exploited. The museum has begun a risk assessment and identified the following assets to be in
scope:
Information asset types Museum employee data Museum financial/insurance data, partner financial data Museum contractual data and business planning Museum research and associated data Museum advertisements and other public data Museum database of collections information
Physical assets Museum building Museum staff Museum collections and exhibits Museum transport vehiclesThe museum has also identified the potential threats to those assets and must now
determine vulnerabilities that can compromise those needs. An enterprise has defined the assets to be included in a risk assessment, and has identified potential threats, for example through applying THREAT ASSESSMENT (113). It must now identify the vulnerabilities that can be exploited by those threats.
##%%&&
Risk determination is the final stage of a risk-assessment process, and incorporates
the results from an asset valuation, a threat assessment and a vulnerability
assessment. Using the input of these patterns, the enterprise is able to evaluate and
prioritize the risks to its assets. The museum has identified the following assets as part of the its risk assessment:
Information asset types Museum employee data Museum financial/insurance data, partner financial data Museum contractual data and business planning Museum research and associated data Museum advertisements and other public data Museum database of collections information
Physical assets Museum building Museum staff Museum collections and exhibits Museum transport vehiclesIt has also completed the three major steps in a risk assessment, as defined by ASSETVALUATION (103), THREAT ASSESSMENT (113), and VULNERABILITY ASSESSMENT(125). It must now assimilate this information, evaluate the overall risk, and present
the results. An enterprise has defined the assets to be included in a risk assessment and has
evaluated the importance of those assets in an asset valuation table. As well, it has
performed a threat assessment and vulnerability assessment and collected unique
combinations of threats and vulnerabilities in a threat-vulnerability table.
##%%&&
This pattern guides an enterprise in selecting security approaches, that is, prevention,
detection, and response. Security approaches are driven by the security properties its
assets require, such as confidentiality, integrity, and availability, and by assessed
security risks. Security approaches also provide a basis for deciding what security
services should be established by the enterprise. A new wing of an existing museum of gemstones is to be opened. Business planning
activities have provided an enterprise scope in terms of needs, concerns, and assets.
Application of SECURITY NEEDS IDENTIFICATIONFOR ENTERPRISE ASSETS (89) has
identified security properties applicable to each asset type. The dominant asset type
for the museum is gemstones. Gems are valuable and should not be stolen or manipulated, so their required properties are availability and integrity. Another important
asset type is documentation and records of gem properties, which require confidentiality, integrity, and availability. The museum needs to determine the security approaches most appropriate for achieving these required security properties, and how
those approaches should be coordinated for the museum. Business assets that require protection and their required security properties (confidentiality, integrity, and availability) are understood, for example from applying SECURITY NEEDS IDENTIFICATIONFOR ENTERPRISE ASSETS (89). Enterprise or business unit security risks (not system risks) are sufficiently understood, for example, from
applying RISK DETERMINATION (137) and its closely-related patterns.
##%%&&
This pattern guides an enterprise in selecting security services for protecting its assets,
after the required security approaches¡ªprevention, detection, response¡ªhave been
identified. It helps to establish the level of strength or confidence each security service
should offer, based on priorities. Primary examples of such services are identification
and authentication, accounting/auditing, access control/authorization, and security
management. A new wing of an existing museum of gemstones is to be opened. The museum¡¯s management has already identified security as an enterprise concern and determined appropriate security properties and approaches to be supported. Now the management
needs to identify what security services will be used. A specific asset group is used in
this simple example problem.
The museum has identified three specific gems as irreplaceable due to their financial value. They can only be insured for approximately two-thirds of their actual
monetary value. The museum wants to provide integrity and availability for physical
protection of the gems, but also confidentiality for the real value of the assets. The
museum has determined that prevention will be the primary approach to providing
integrity and availability of the gems. Prevention will also provide confidentiality for
information that stipulates real monetary values. Detection and response will provide secondary approaches to protecting these gems and resources will be allocated
to prevention first. The museum now needs to determine what abstract security services will support the desired properties and approaches. Business strategies, plans, and operations are understood. These include disaster recovery and continuity of operations strategies, a semantic data model, high-level business
process and workflows, business locations, organizational units, and business cycle
models. Security approaches (prevention, detection, response) and their priorities have
been selected to satisfy the identified security needs of enterprise assets. The approaches might have been selected by applying ENTERPRISE SECURITY APPROACHES (148). The
pattern user has a basic awareness of potential security services.
##%%&&
Enterprises often partner with third parties to support their business model. These
third parties may include application and managed service providers, consulting
firms, vendors, outsourcing development teams, and satellite offices. As part of this
relationship, access must be granted to allow data to travel between the organizations.
Without attention to the protection of that data and the methods by which they are
transferred, one or both organizations may be at risk. The museum has received a sum of money and is expanding! It wants to expand its
services in the following ways:
1.Publish an RSS news feed advertising all upcoming museum events and information.
2.Sell goods online from its Web site. The museum has created a merchant account
with a popular payment processor and financial organization. The Web site
application will use a programmatic API provided by the payment processor.
3.Outsource the development of a Web site to a third party. One component of
the Web site will be a public, e-commerce site selling goods and promoting
museum events and exhibits. The second component will be a private, intranet
Web site containing an employee directory as well as confidential corporate
funding and research and development data. The museum realizes that the
third party will require some confidential database tables and documents in
order to design and test the application.
4.Subscribe to the International Museum Consortium (IMC) service. This service will publish current and rolling exhibit information to other subscribers.
Membership of this service will allow the museum to search and bid for rolling exhibits from any other subscribing museum around the world. They feel
it would give them a competitive advantage over other regional and local
museums, and will substantially increase their patron attendance. The IMC
will provide the software application, centrally manage user accounts and
facilitate a bidding and messaging process. The museum already has an infrastructure capable of operating and managing the software application, and
simply needs to configure it to access the museum¡¯s inventory database.
Each of these projects involves exchanging information with other parties, but
vary in the degree of security requirements and in the method of data exchange. The 
museum clearly recognizes the value of these projects, but is concerned that its personnel, customer, and confidential exhibit information will be at risk of unauthorized access, modification, or denial of service. It would like to implement these
projects but needs to protect its data, systems, and reputation. An enterprise has an existing business process, or is proposing a new business process, that requires information to be exchanged with another entity across a computer
network. The business factors that initiated the partnership have already been determined and a high-level service level agreement, complete with disaster recovery and
business continuity planning, has been established.
##%%&&
An identification and authentication (I&A) service must satisfy a set of
requirements for both the service and the quality of service. The function of I&A
is to recognize an individual and validate the individual¡¯s identity. While each
situation that calls for I&A is unique, there are common generic requirements that
apply to all I&A situations. This pattern provides a common generic set of I&A
requirements. The pattern also helps you to apply the general requirements to your
specific situation, and helps you to determine the relative importance of conflicting
requirements. The museum gemstones wing will build on the parent museum¡¯s intranet, with workstations distributed throughout multiple departments. Based on applying ENTERPRISESECURITY SERVICES (161), the museum recognizes the need for specific security functions. Among these are access control and security accounting. Both of these functions
rely on an I&A service to establish identity. What kind of I&A service does the museum
need to support these functions? What other situations call for an I&A service in the
museum? After some analysis, Samuel, the museum¡¯s system engineer, and Edward, the
enterprise architect, come up with these possible situations that require I&A services: Establishing physical access to the museum during business hours Establishing physical access to the museum by staff during outside business hours On-line access to the intranet from within the local area network of the museum wing Remote on-line access to the museum¡¯s intranet Access to highly sensitive museum physical assets, especially gemstones On-line access to highly sensitive museum information assets Tracking who is downloading information from the publicly-available museumWeb site Support non-repudiation of business transactions on the part of customers orpartners Employee accountability of computer and network resource use within themuseum Accountability to support identification of the source of computer viruses or a network denial of service attack
The engineers feel that these situations differ in their I&A requirements, but are
not sure how to capture the differences. For example, a single mechanism for Web
site I&A doesn¡¯t work, because Vic the visitor, downloading publicly-available information, has I&A requirements that differ from Manuela the museum manager, who
is working from home and retrieving sensitive accounting data. Furthermore, for
each of these situations, the museum wants to properly balance conflicting objectives, such as a service that detects would-be hackers versus a service that is easy for
employees to use. Typically a strong I&A mechanism that detects most imposters,
that is, people who falsely claim to be legitimate, are hard to use, while I&A mechanisms that are easy to use tend to give weaker protection, in too many cases concluding that an imposter is legitimate. 
Samuel¡¯s initial thinking had been that he could simply select some I&A mechanism such as a password-based log-on or an employee badge. This has given way to
the realization that more thought and consideration is needed to ensure that multiple
I&A needs are properly addressed. Samuel and Edward recognize that they need to
specify a clear and balanced set of requirements for each situation the requires I&A.
How can they accomplish this? An organization or project understands its planned uses of I&A, for example, from
applying ENTERPRISE SECURITY SERVICES (161), or from applying one or more of the pattern systems that use I&A, such as the patterns for access control in Chapter 8, 9
and 10 and the accounting patterns in Chapter 11.
The scope is known to be situations in which both identification and authentication are needed. Other situations exist in which only identification is needed without
authentication, but those situations are not addressed in this pattern.
##%%&&
This pattern describes alternative techniques for automated I&A, as opposed to
procedural or physical I&A. It helps you to select an appropriate I&A strategy that
consists of a single technique, or a combination of techniques, to satisfy I&A
requirements. Techniques considered include password, biometrics, hardware token,
PKI, and I&A of unregistered users. Indiana Jones, a museum employee, needs to gain access to the museum intranet
while collecting artifacts for the museum from around the world. He wants to check
his e-mail abroad and also access the museum¡¯s database to evaluate a found artifact.
From Jones¡¯ perspective, the most important requirements for this I&A service are
to support I&A from remote locations and to be easy to use. From the perspective
of Samuel the museum systems engineer, the most important requirements for this
I&A service are to have high accuracy, especially to reject attempts by non-employees
to gain access to the intranet, and to limit I&A overhead. Samuel and his systems
engineering group have used I&A REQUIREMENTS (192) to define all four of these
intranet I&A requirements as high priority. Now Ivan the intranet architect needs to
select an I&A service to satisfy these requirements. The choices available to Ivan are
many. They include identifier and password, PKI certificates, multiple biometrics options, and a hardware token with a one-time password. How can Ivan choose among
the alternatives? The person applying this pattern understands the requirements for I&A, along with
their relative importance¡ªfor example, from the results of applying I&A REQUIREMENTS (192).A decision has been made to use automated I&A. In the remainder of this pattern, the term ¡®I&A¡¯ is intended to mean automated I&A, as opposed to
physical or procedural I&A, such as showing a badge to a guard at the front door.
##%%&&
This pattern describes security best practice for designing, creating, managing, and using password components in support of I&A requirements. This pattern can aid three audiences: engineers, in selecting or designing commercial products that provide password mechanisms, administrators, in the operation and management of password mechanisms, and users, in improving their selection and handling of passwords.
Employees of the museum need to gain access to the museum intranet, which is based on passwords. Enforcement of security policy has been lax, and it has been common practice for employees to write down passwords and leave them by their workstations, or even tape them to the display monitor. As a result, several incidents have occurred in which unauthorized staff and even visitors have gained access to sensitive information. The system administrators want to correct this problem, specifically to create good passwords kind keep them secure. There are two situations that require passwords as part of I&A whose results are used for access control. First, a low level of security is needed for I&A used to gain access to the overall intranet. Second, a high level of security is needed for I&A used to gain access to sensitive information, including employee salary data.
A password mechanism has been selected for user authentication on a specified segment of an information system. The person applying this pattern understands the requirements for I&A, along with their relative importance-for example, from the results of applying I&A requirements.
##%%&&
This pattern aids the selection of appropriate biometric mechanisms to satisfy I&A
requirements. Biometric mechanisms considered are face recognition, finger image,
hand geometry, iris recognition, retinal scanning, signature verification, and speaker
verification. Additional mechanisms, including DNA, are identified for completeness. The internal maintenance and research areas of the new gemstone wing of the museum essentially afford staff access directly to high-value assets and to the information
on those assets. While physical entry for these activities is being tightly controlled,
access to sensitive asset information must also be restricted. To gain access to the
Web server with strictly controlled asset information, staff are required to log-on to
the Web server. Part of the log-on process will be use of a biometric to provide additional verification of employee identities. Alvin the system architect must determine
which biometric mechanism is most appropriate for the museum. The person applying this pattern understands the requirements for I&A, along
with their relative importance, for example from the results of applying I&A REQUIREMENTS (192). A decision has been made to use biometrics for I&A, for example from the results
of applying AUTOMATED I&A DESIGN ALTERNATIVES (207), but which biometrics
technique to use has not been decided. The decision to use some form of biometrics
is typically made in the context of a user population of limited size, because of the
enrolment effort required.Discussion: What do all biometric mechanisms have in common?
All biometric mechanisms share an underlying methodology involving enrolment
(which is outside the scope of this pattern) and verification or identification. At enrolment, the person offers a ¡®live sample¡¯ of the biometric, such as a finger image. This
is scanned electronically, processed and stored as a template, which is a mathematical representation of the original sample. Once the template is captured, the original
sample data is no longer used and is discarded. Alternatively, it might be wise to
keep the original raw sample data, against the possibility that better template-algorithms and representations might become available in the future: in some areas such as fingerprint recognition, technology is changing and significant improvement can
be expected. Keeping the raw sample data would allow one to benefit from newer
algorithms without the need to re-enrol all users.
To confirm identity at a future time, the individual presents the live sample, which
is matched against the stored template. In a 1:many search, the individual presents
only the live sample, and the database is searched for a match. This is called identification. In a 1:1 search, the user presents a name or other identifier along with the
live sample. The system checks the live sample only against templates stored under
that identifier. This is called verification. [Seffers2001].
When biometrics are used for verification, the captured biometric record is
matched against one biometric template in the data store to determine a match. The
one biometric template in the data store is found by association with a presented
identifier, acquired separately via non-biometric means such as a token. This is a 1:1
match, and answers the question ¡®Am I who I say I am¡¯?
When biometrics are used for identification, the biometric capture and conversion
are the same, but no separate identifier is acquired, and therefore the verifier matches the biometric record against all biometric records in the data store. If a match is
found, the associated identifier is found. This is a 1:many match, and answers the
question ¡®Who am I¡¯? The result is still success or failure, and in the case of success,
an identifier is produced. If the identifier is considered to be verified or authenticated, then in effect the biometric technique provides a full I&A solution.
##%%&&
This pattern describes who is authorized to access specific resources in a system, in
an environment in which we have resources whose access needs to be controlled. It
indicates, for each active entity that can access resources, which resources it can
access, and how it can access them. In a medical information system we keep sensitive information about patients. Unrestricted disclosure of this data would violate the privacy of the patients, while unrestricted modification could jeopardize the health of the patients. Any environment in which we have resources whose access needs to be controlled.
##%%&&
This pattern describes how to assign rights based on the functions or tasks of people
in an environment in which control of access to computing resources is required and
where there is a large number of users, information types, or a large variety of
resources. It describes how users can acquire rights based on their job functions or
their assigned tasks. The hospital has many patients, doctors, nurses, and other personnel. The specific
individuals also change frequently. Defining individual access rights has become a
time-consuming activity, prone to errors. Any environment in which we need to control access to computing resources and
where there is a large number of users, information types, or a large variety of
resources.
##%%&&
In some environments data and documents may have critical value and their disclosure
could bring serious problems. This pattern describes how to categorize sensitive
information and prevent its disclosure. It discusses how to assign classifications
(clearances) to users, and classifications (sensitivity levels) to data, and to separate
different organizational units into categories. Access of users to data is based on
policies, while changes to the classifications are performed by trusted processes that are
allowed to violate the policies. The high command of an army has decided on a plan of attack in a war. It is extremely important that this information is not known outside a small group of people, or
the attack may be a failure. In some environments data and documents may have critical value and their disclosure could bring serious problems.
##%%&&
In a computational environment in which users or processes make requests for data
or resources, this pattern enforces declared access restrictions when an active entity
requests resources. It describes how to define an abstract process that intercepts all
requests for resources and checks them for compliance with authorizations.Also Known AsPolicy Enforcement Point. In the hospital example described in ROLE-BASED ACCESS CONTROL (249) we declared the accesses allowed to doctors and other personnel. However, we expected
voluntary compliance with the rules. It has not worked, busy personnel bypass the
rules and there is no way of enforcing them. A computational environment in which users or processes make requests for data or
resources.
##%%&&
Least privilege is a fundamental principle for secure systems. Roles can directly
support the least privilege principle, but a systematic approach to assigning only the
required rights to each role is required. This pattern provides a precise way, based on
use cases, of assigning rights to roles to implement a least-privilege policy. Multitronics is a company that sells on-line digital media such as video, sounds, or
images. They have been advised that for security reasons they should use a ROLE-BASED ACCESS CONTROL (249) approach, in which they can apply a least-privilege policy. For this they need to first identify the roles required to perform the business
functions. In this system a manager administers the items on sale, deciding what is
to be sold, at what prices, and so on. He can also order items for future sale. Subscribers register and create accounts so that they can purchase copies of digital items
and download them to a mobile device such as a cellular phone. Subscribers can also
reserve items not yet in stock. A salesperson maintains a catalog of items for sale and
bills the subscribers for their purchases. To apply the required policy, we need a systematic way to assign rights to these roles. Applications composed of a variety of roles in which it is not easy to assign proper
rights to the roles.
##%%&&
The function of the access control security service is to permit or deny someone the right to perform an action on an asset, such as create, read, modify, or delete a data file. While each situation that calls for access control is unique, there are common generic requirements that apply to all access-control situations. This pattern provides a common generic set of access control requirements. The requirements address both the access control function and the properties of the access control service, such as case of use and flexibility. The pattern also helps you to apply the general requirements to your specific situation, and helps you to determine the relative importance of conflicting requirements.
A new wing of an existing museum of gemstones is to he opened. The wing will house gems of varying value, some of which are owned by the museum and some of which are on loan. Some of the gems are famous stones whose loss would involve much media publicity. The wing will house valuable gems on display, low-value gems in a hands-on exhibit, and gems of all values in working areas of the wing that are not open to the public.
Based on the results of applying enterprise security services (161), Samuel the museum¡¯s system engineer understands that the museum needs to control access to the gems and to the information related to gems. An obvious example is that an at tempted access by Theo the thief to steal a gem should be denied. Another example is that the recorded carat weight for gems should not he modifiable by unauthorized people. An unauthorized change in recorded carat weight could change a gem's value, change insurance costs, or even signal the beginning of an attempt to carve off a piece of the stone.
But Samuel also understands that the need to deny unauthorized access must be balanced against the need to permit authorized access. For example, the best safeguard against theft of a gem is to lock it up in a vault and not tell anyone where it is. But this would interfere with a primary goal of the museum, which is to display gems for public viewing. Therefore, Samuel needs to specify a balanced set of requirements for access control and the relative importance of those requirements, as a means of driving and evaluating an appropriate access control service for the museum. How can Samuel define such a set of requirements?
An organization understands how it plans to use access control, for example, from applying enterprise security services (161). An organization understands the general types of actors, assets and actions that are to be subject to access control. An access control rule permits an actor to perform an action on an asset-for example, user A is granted permission to modify file F. Actor types can include humans, software, business or automated processes, or information systems. Actors can be internal to an organization, such as an employee, or external, such as a supplier or customer. Action types include both physical and automated actions. Common actions include create, see, use, change, and destroy or delete. Asset types include both physical and informational assets.
##%%&&
If you need to provide external access to a system, but want to protect it from misuse
or damage, define a single access point that grants or denies entry to the system after
checking the client requiring access. The single access point is easy to apply, defines
a clear entry point to the system, and can be assessed when implementing the desired
security policy. Consider a small medieval village. It consists of a group of houses in close proximity.While there is little economic prosperity, there is little value in protection from burglars and little interest to robbery. Security for people means that each man protects
his own belongings. Each man spends time building weapons and training in their
use so that he can defend his family.
Somehow economy prospers (we do not speculate why and how), more and
more people move to the village, and more and more value is accumulated within
the village. However, since the people need more time for their prospering businesses,
they have less time to spend practicing defense. Their level of protection becomes
lower, while the threat from burglars grows. In addition, visitors must convince every individual shopkeeper that they are valuable customers instead of thieves before they can actually conduct business with them. The village dwellers wonder
how they can simplify protection, so that everybody no longer needs to deal with
it many times a day. You need to provide access to a system for external clients. You need to ensure the
system is not misused or damaged by such clients.
##%%&&
Once you have secured a system using SINGLE ACCESS POINT (279), a means of identification and authentication (I&A) and response to unauthorized break-in attempts is
required for securing the system. CHECK POINT (287) makes such an effective I&A and
access control mechanism easy to deploy and evolve. The mayor of our medieval town that established SINGLE ACCESS POINT (279) with
their gate and guard is concerned about their protection during times when different
threats come close to the town. For example, the merchants would like to have the
gate freely open during daytime, to let traders in and out easily. However, they are
concerned about burglars sneaking into their warehouses during night time. You have a system protected from unauthorized access in general, for example by applying SINGLE ACCESS POINT (279). Nevertheless, you want authorized clients be able to enter your system.
##%%&&
Verifying a user¡¯s identity and access rights for every system function can be tedious.
To keep track of who is using the functions and their corresponding access rights,
systems establish a security session after a user has logged in successfully. A unique
reference to the session object is made available, instead of passing all access rights
or re-authenticating a user repeatedly. Queries regarding a user¡¯s security properties
are delegated to the attached session object via the session reference. Our medieval city is concerned about foreigners entering through its gate. Merchants are welcome, but burglars and thieves shouldn¡¯t be let in by the guards.
Peasants looking for work in one of the city¡¯s workshops are allowed in depending
on the demand of the guilds. On the other hand, once a person has entered the
city, it is hard for the city dwellers to tell who that person is if they are not a well-
known city dweller. Even the night watchman patrolling the city¡¯s streets has a
hard time knowing how to deal with a stranger. A merchant should be welcomed
and protected, while someone else lingering in the streets at night might need to
be dealt with. 
The problem of city inhabitants and the night watchman is that they do not have
equivalent resources to the guards at the city gate, to interrogate and investigate people and check their identity. In addition, it would be annoying to visitors that are welcome if they had to answer the same questions over and over again, such as who they
are, where they are going, and what their business is in the city.
The mayor summons the city council to discover a way to keep the city secure
while making the city a welcoming as possible for merchants and other guests. Another requirement that comes up at the council meeting is that the city officials would
also like to know when a visitor has left. Your system is shared by multiple users and system components need a way to share
(security) data associated with a user. For example, you have already applied CHECKPOINT (287).
##%%&&
Designing the user interface for a system in which different users are granted different
access rights can be challenging. At one end of the spectrum is the approach taken by
this pattern, which provides a view of the maximum functionality of the system, but
issues the user with an error when they attempt to use a function for which they are
not authorized. Consider you are developing an Internet site. The site should present your company on the World-Wide Web as well as provide downloads for brochures, user manuals, and
demo software. However, to be able to track who downloaded such material, Internet
surfers are required to provide their name and address before they can start a download. However, to avoid irritating returning users, they are granted privileges by the site
via a cookie, and thus do not need to register again. See figure on page 306.
For example Yahoo! groups show a group¡¯s features to anonymous users, without
letting them access the ¡®members only¡¯ menu. Once logged in and registered as a
member of a group, the ¡®members only¡¯ menu is accessible.
How do you design the Web site so that it shows the possibility of downloads
while still restricting access to registered users v? You are designing the interface of a system in which access restrictions such as user
authorization to parts of the interface apply. While most of the applications of this
pattern are within the domain of graphical user interfaces (GUI), it can also apply to
other interface types as well.
##%%&&
Designing the user interface for a system in which different users are granted different
access rights can be challenging. This pattern guides a developer in presenting only the
currently-available functions to a user, while hiding everything for which they lack
permission. Extending the Web site example from FULL ACCESSWITH ERRORS (305): registered users are able to upload files to your Web site. After uploading they also need a
means of deleting or changing the uploaded files. However, an individual user should
only be able to change their own files. Only a user with administrative rights should
be allowed to maintain files by all users.
Yahoo! groups provides a similar means of uploading files to groups for its group
members. However, only the uploading member is able to delete or edit the file. The
group owner can delete files uploaded by all users. If no permission is given, one can
see the ¡®File¡¯ menu, corresponding to FULL ACCESSWITH ERRORS (305), but the file
folder itself cannot be extended by adding files. You are designing the user interface of a system in which access restrictions such as
user authorization apply to parts of the interface. While most of the applications of
this pattern are within the domain of graphical user interfaces (GUIs), it can also apply to other interface types as well.
##%%&&
This pattern addresses the problem of how to verify that a subject is who it says it is. Use a single access point (279) to receive the interactions of a subject with the system and apply a protocol to verify the identity of the subject.
Our system his legitimate users that use it to host their files. However, there is no way to make sure that a user who is logged in is a legitimate user. Users can impersonate others and gain illegal access to their files.
Operating systems authenticate users when they first log in, and maybe again when they access specific resources. The operating system controls the creation of a session in response to the request by a subject, typically a user. The authenticated user, represented by processes running on its behalf, is then allowed to access resources according to their rights. Sensitive resource access may require additional process authentication. Processes in distributed operating systems also need to be authenticated when they attempt to access resources on external nodes.
##%%&&
This pattern address how to define and grant appropriate access rights for a new process.
Most operating systems create a process with the same rights as its parent. If a hacker can trick an operating system into creating a child of the supervisor process, this runs with all the rights of the supervisor.
An operating system in which processes or threads need to be created according to application needs.
##%%&&
This pattern addresses how to specify the rights of processes with respect to a new
object. When a process creates a new object through a factory, the request includes the features of the new object. These features include a list of rights to access the object. In many operating systems the creator of an object gets all possible rights to the object. Other operating systems apply predefined sets of rights: for example, in Unix all
the members of a file owner¡¯s group may receive equal rights for a new file. These
approaches may result in unnecessary rights being given to some users, violating the
principle of least privileges. A computing system that needs to control access to its created objects because of
their different degrees of sensitivity. Rights for these objects are defined by authorization rules or policies that are enforced when a process attempts to access an object.
##%%&&
This pattern addresses how to control access by a process to an object. Use a reference
monitor to intercept access requests from processes. The reference monitor checks
whether the process has the requested type of access to the object. Our operating system does not check all user requests to access resources such as files
or memory areas. A hacker discovered that some accesses are not checked, and was
able to steal customer information from our files. He also left a program that randomly overwrites memory areas and produces serious disruption to the other users. An operating system that consists of many users, objects that may contain sensitive
data, and where we need to have controlled access to resources.
##%%&&
This pattern addresses how to control access by processes to specific areas of their
virtual address space (VAS) according to a set of predefined access types. Divide the
VAS into segments that correspond to logical units in the programs. Use special
words (descriptors) to represent access rights for these segments. Our operating system improved by using a reference monitor. However, hackers discovered that the unit of access control to memory was coarse. By taking advantage
of the lack of precision in controlling access they were able to access other processes¡¯
areas. Multiprogramming systems with a variety of users. Processes executing on behalf
of these users must be able to share memory areas in a controlled way. Each process runs in its own address space. The total VAS at a given moment includes the
union of the VASs of the individual processes, including user and system processes. Typical allowed accesses are read, write, and execute, although finer typing is
possible.
##%%&&
Unauthorized processes could destroy or modify information in files or databases,
with obvious results, or could interfere with the execution of other processes.
Therefore, define an execution environment for processes, indicating explicitly all
the resources that a process can use during its execution, as well as the type of access
to the resources. In our operating system we know now how to assign access rights to processes and
how to enforce these rights at execution time. However, a process may have different
functions and in each functional mode it may need different rights. For example, if a
process needs to read some files to collect some data, this should happen only at the
specific time of access to the file, otherwise a hacker could take advantage of the extra rights to perform illegal accesses. A process executes on behalf of a user, group, or role (a subject). A process must have
access rights to use the resources defined for its subject during execution. The set of
access rights given to a process define its execution domain. At times the process may
also need to enter other domains to perform its work: for example, for example, to
extract data from a file in another user¡¯s domain. Frequently, users structure their domains as a hierarchical tree of domains with one root domain.
##%%&&
If a process execution environment is uncontrolled, processes can scavenge information
by searching memory and accessing the disk drives where files reside. They might
also take control of the operating system itself, in which case they have access to
everything. Use AUTHORIZATION (245) to define the rights of a subject. From these
rights we can set up the rights of processes running on behalf of the subject. Process
requests are validated by CONTROLLED OBJECT MONITOR (335) or REFERENCEMONITOR (256) respectively. Jim the hacker discovers that the customer¡¯s files have authorizations and cannot be
accessed directly, so he tries another approach. He realizes that processes are not given only the rights of their owners, but also have rights with which they can access
memory and other resources belonging to other users. He systematically searches areas of memory and I/O devices being used by other processes until he can scavenge
a few credit card numbers that he can use in his illicit activities. A process executes on behalf of a user or role (a subject). A process must have access
rights to use these resources during execution. The set of access rights given to a process define its execution domain. Processes must be able to share resources in a controlled way. The rights of the process are derived from the rights of its invoker.
##%%&&
This pattern describes how to control access to files in an operating system.
Authorized users are the only ones that can use a file in specific ways. Apply
AUTHORIZATION (245) to describe access to files by subjects. The protection object
is now a file component that may be a directory or a file. Jim is an application programmer in a bank. He has a user account and some files in the bank¡¯s operating system. He realizes that the same system also stores files with
customer data. These files have no authorization controls. Jim reads several of these
files and finds customer information such as SSNs and credit card numbers. He uses
this information to charge some items bought at mail-order shops. The users of operating systems need to use files to store permanent information.
These files can be accessed by different users from different workstations, and access
to the files must be restricted to authorized users who can use them in specific ways.
Because of the needs of the organization, some (or all) of the files must be shared by
these subjects. Use cases for a file system include creation and deletion of files, opening and closing of files, reading and writing files, copying files, and so on. A subject
has a home directory for each authorized workstation, but the same home directory
can be shared among several workstations or among several subjects. The home directory is used to search the files for which a subject has rights. Files are organized
using directories, usually in a tree-like structure of directories and files. This facilitates the search for specific files.
##%%&&
A security accounting service must satisfy a set of requirements for both the service
and the quality of service. The function of security accounting is to track security-
related actions or events, such as damage to property, attempts at unauthorized
database access, or transmission of a computer virus, and provide information about
those actions. While each situation that calls for security accounting is unique, there
are common generic requirements that apply to all security accounting situations.
This pattern provides a common generic set of security accounting requirements. The
pattern also helps you apply the general requirements to your specific situation, and
helps you to determine the relative importance of conflicting requirements. Gemstones within a museum are objects used in archeological research. They are also
cleaned, transported and handled by several authorized personnel. The museum is
interested in protecting museum assets from theft, damage or any mishandling. The
museum is serious about assigning responsibility for any asset compromise or attempts to compromise assets. The museum needs to identify the requirements for the
key components of a security accounting service that will help them protect their
valuable gems and help assign responsibility for attempts to compromise their assets.
Based on the results of applying ENTERPRISE SECURITY SERVICES (161), Samuel the
museum system engineer understands that the museum needs accountability of actions and events when the gems are transported or handled, and accountability of actions on the information about the gems, which is stored in a database. The museum
needs to be able to assign responsibility for any asset compromise or attempts to compromise assets. For example, the museum needs to know who is responsible for
transporting a gem. When information about a gem, such as its current location or
its recorded carat weight, is entered or modified, the museum needs to know who
made the addition or change. But Samuel also understands that the need to track and
account for these actions and events must be balanced with the need for privacy and
ease of operations. Therefore, Samuel needs to specify a balanced set of requirements
for security accounting and the relative importance of those requirements, as a means
of driving and evaluating an appropriate security accounting service for the museum.
How can Samuel define such a set of requirements? The planned uses of security accounting are understood, for example, from applying
ENTERPRISE SECURITY SERVICES (161). Asset types with a need for security accounting 
services are known, and the general types of actors that are to be held accountable are
known. Actor types can include humans, software, business or automated processes,
or information systems. Actors can be internal to an organization, such as an employee, or external, such as a supplier or customer. Asset types include both physical and
information assets. The degree of confidence needed for the security accounting services by general asset types is known in relative terms. For example, a museum needs
a very high degree of confidence in knowing who broke into the museum and stole a
valuable gem, but it needs a lower degree of confidence in knowing who defaced the
outside of the museum building.
##%%&&
An audit service must satisfy a set of requirements for both the service and the quality of service. The audit function is to analyze logs, audit trails or other captured information about an event, such as entering a building or accessing resources on a network, to find and report any indication of security violations. While each situation that calls for an audit is unique, there are common generic requirements that apply to all audit situations. This pattern provides a common generic set of audit requirements. The pattern also helps you to apply the general requirements to your specific situation, and helps you determine the relative importance of conflicting requirements.
The museum's research department has a network that they use for messaging and collaboration with various universities around the world. Among the types of information exchanged and stored are details about the location of various gemstone mines. Every six months the museum must present a report of the information exchanges to the board of trustees. The museum wants to take six months' worth of activity and summarize it into the critical and non-critical events that occurred over that six month period, and who was involved in those events. Samuel the museum system engineer understands this goal, but at the same time he understands that capturing extensive audit information can degrade system performance and require significant resources for storage and analysis. Privacy considerations are also a constraint on the capture and use of audit data. Samuel needs to identify requirements for an audit service that will help the museum achieve the goals while balancing the constraints.
Accounting requirements and their relative importance are understood, for example, from applying Security Accounting Requirements. The planned uses of audit are understood.
##%%&&
A service that captures security audit trails and audit logs must satisfy a set of requirements for both the service and the quality of service. The audit trails and logging function is to capture audit logs and audit trails about events and activities that
occur within an organization or system, to enable reconstruction and analysis of
those events and activities. While each situation that calls for an audit trail is unique,
there are common generic requirements that apply to all audit trails and logging situations. This pattern provides a common generic set of audit trail requirements. The
pattern also helps you to apply the general requirements to your specific situation,
and helps you to determine the relative importance of conflicting requirements. The new museum wing for gemstones keeps its most precious gems in a room with
limited access. The room¡¯s access is controlled by electronic badge access. Cleaning
personnel, scientists and other authorized personnel need special badges to access the
room. As an extra precaution, the museum would like a way to track access to the
room by individuals and by roles. Samuel the museum system engineer needs to specify the requirements for audit trails and logging (AT&L) of activities related to this
limited access room, and the relative importance of those requirements, as a means
to drive and evaluate an AT&L service. Audit requirements and their relative importance are understood, for example,
from applying AUDIT REQUIREMENTS (369). The planned uses of audit trails and logging are understood.
##%%&&
An intrusion detection system (IDS) must satisfy a set of requirements for both the
service and the quality of service. IDS is a security service that automates the
monitoring of events occurring in a computer system or network, and analyzes these
events for any indication of security violations. While each situation that calls for
intrusion detection is unique, there are common generic requirements that apply to
all intrusion detection situations. This pattern provides a common generic set of
intrusion detection requirements. The pattern also helps you to apply the general
requirements to your specific situation, and helps you to determine the relative
importance of conflicting requirements. The museum¡¯s research department has a network that they use for messaging and
collaboration with various universities around the world. Among the information
exchanged and stored are details about the location of various natural gemstone
mines. Samuel the museum system engineer wants the museum immediately to detect
unauthorized and successful attempts to gain access to the network and to any hosts
that contain sensitive information. Once alerted, Samuel would like information that
can be used to hold accountable the individual(s) that have breached their perimeter.
In addition, Samuel would like to have information recorded and available on unsuccessful attempts to gain access. Samuel understands that trade-offs are involved,
because stopping intruders and capturing information about attempted intrusions
can require significant resources that degrade system performance, and which may
make legitimate access more difficult. Privacy considerations are also a constraint on
intrusion detection efforts. Samuel needs to identify requirements for an IDS service
that will help the museum achieve the goals while balancing the constraints. Accounting requirements and their relative importance are understood. The requirements might have been selected by applying SECURITY ACCOUNTING REQUIREMENTS(360). The planned uses of IDS are understood.
##%%&&
A non-repudiation service must satisfy a set of requirements for both the service and the
quality of service. The function of non-repudiation is to capture and maintain evidence
so that the participants of a transaction or interaction cannot deny having participated
in that activity. While each situation that calls for non-repudiation is unique, there are
common generic requirements that apply to all non-repudiation situations. This pattern
provides a common generic set of non-repudiation requirements. The pattern also helps
you to apply the general requirements to your specific situation, and helps you to
determine the relative importance of conflicting requirements. The museum seeks to increase the publicity of its new wing for gemstones. To do
this, the museum seeks to have many exotic gems on display for the grand opening.
The Crown Jewels of England are scheduled to be a part of the display. Manuela the
museum manager would like to have a high degree of confidence that the receipt of
the jewels by the museum and the release of the jewels after the opening are protected. Samuel the museum system engineer needs to specify the requirements for non-repudiation and the relative importance of those requirements, as a means of driving
and evaluating a non-repudiation service that will support events such as this grand
opening. How can Samuel define such a set of requirements? Accounting requirements and their relative importance are understood, for example, from applying SECURITY ACCOUNTING REQUIREMENTS (360). The planned uses of non-repudiation are understood. A common transaction type is the sending and
receiving of materials such as merchandise or contracts. Non-repudiation is used to
prevent the receiver from denying that they received the materials when in fact they
did receive them. Sometimes non-repudiation is used to prevent the sender from
claiming that they sent the materials when in fact they did not send them.
##%%&&
Some of the hosts in other networks may try to attack 
the local network through
their IP-level payloads. These payloads may include viruses or 
application-specific
attacks. We need to identify and block those hosts. A packet filter firewall 
filters
incoming and outgoing network traffic in a computer system based on packet
inspection at 
the IP level. Our system has been attacked recently by a variety of hackers, including 
somebody
who penetrated our operating system and stole our clients¡¯ credit card numbers. Our
employees are wasting time at work by looking at inappropriate sites on the Internet.
If we 
continue like this we will soon be out of business. Computer systems on a local network 
connected to the Internet and to other networks with different levels of trust. A host in a 
local network receives and sends traffic to other networks. This traffic has several layers or 
levels. The most basic level is
the IP level, made up of packets consisting of headers and bodies 
(payloads). The
headers include the source and destination addresses as well as other routing 
information, while the bodies include the message payloads.
##%%&&
A proxy-based firewall inspects and filters  
incoming and outgoing network traffic
based on the type of application service to be accessed, or 
performing the access. This
pattern interposes a proxy between the request and the access, and 
applies controls
through this proxy. This is usually done in addition to the normal filtering 
based on
addresses. After we started using 
a PACKET FILTER FIREWALL (405) most of our problems were
reduced. However, some of the messages 
sent from sites we don¡¯t consider suspicious
contain malicious payloads, because hackers were 
spoofing trusted addresses. These
payloads sometimes contained incorrect commands or the wrong 
type and length of
parameters. Our PACKET FILTER FIREWALL (405) cannot stop these attacks, 
because
it doesn¡¯t look at the message payload, and as a result we are experiencing new problems. It is also hard to block every malicious site. Computer systems on a local network 
connected to the Internet and to other networks, where a higher level of security than the one 
provided by packet filters is
needed. Specifically, we want to control attacks at the application 
layer of the network protocol. Incorrect commands or parameters can produce buffer overflows
and 
other conditions that can be exploited for attacks. In some cases we might also
want to 
authenticate the client to avoid spoofing. Outgoing flows (to malicious sites)
can also be 
damaging in this environment.
##%%&&
A stateful firewall filters incoming and outgoing network traffic in a computer system based on state information derived from past communications. State information generally describes whether the incoming packet is part of a new connection, or a continuing communication whose connection was approved previously. In other words, states describe a context for each packet.
We have been able to contain many attacks with packet filter firewall (405) and proxy-based firewall (411). However, we are still plagued with distributed denial of service attacks that prevent customers from reaching our site. We also have performance problems for high-speed streams. In addition, a more sophisticated group of hackers is attacking us, sending us viruses whose bodies are assembled from parts included in message data and commands.
Computer systems on a local network connected to the Internet and to other external networks. A higher level of network security is needed than static packet or proxy filtering. A packet filter firewall (405) only inspects the address of the packet, without the knowledge of previous communications of the same network. Similarly, a proxy-based firewall (411) filters based on proxy restrictions for each packet. The knowledge of whether a connection is a new connection or an established connection is important for improved security: in particular, denial of service attacks could be identified more conveniently if we knew the relationship between packets.
##%%&&
All systems are potentially liable to attack, whether from internal or external sources. If the information held by a system is sensitive, it should be protected. Part of this protect ion can take the form of obscuring the data itself, probably through some form of encryption, and obscuring information about the environment surrounding the data.
A typical Internet technology system Will use a combination of Web and application servers, together with a Common persistent store, usually in the form of a common database, in which application data is stored. All these parts of the system will be protected from external attack by a firewall and possibly a demilitarized zone (449). However, this is no guarantee of security-what if the attacker breaches these external measures, or if an attack is internal to the organization?
The system will gather user information, such as credit card details, and store this in the database. The user information in the database is an obvious target for any attacker who wishes to steal or alter such information. Hence extra security measures may he put in place for the database. However, user information may also he retained temporarily by other parts of the system, in memory, in a cache, or in session state server, as shown in the figure on the previous page.
Application data can be protected by encrypting it, hut such encryption is comparatively slow. Widespread use of encryption for all data in the system will impact system performance. Even then, there is no guarantee of security, as the system must
have access to the keys required to decrypt the data when it is needed by the application. This means that such keys are also vulnerable to attack. If the intruder can find and identify the encryption keys used for particular purposes, then all benefit from the encryption is lost. This can be addressed by designating one server to hold and distribute the keys. I his server can then he specially protected. however. ii an intruder can obtain credentials to access this server, then it too may be compromised, hence anywhere the application has access to such credentials (or equivalent privilege must also be protected).
An application server architecture has been adopted to deliver Internet technology application servers together with a Common persistent store. The business logic and dynamic Web content generation of the application resides on application servers, while all static content is provided by Web servers that also act as a protection reverse proxy (457) or an integration reverse proxy (465) for the dynamic Web content. The application gathers information on users and holds this in its database. The application is protected from external attack by a D demilitarized zone (449).
##%%&&
Messages passing 
across any public network¡ªparticularly the Internet¡ªcan be intercepted. The information 
contained in such messages is thus potentially available
to an eavesdropper. For sensitive 
communication across a public network, create encrypted SECURE CHANNELS (434) to ensure that 
data remains confidential in transit. A typical Internet-based application will exchange a 
variety of information with its
users. Some of this information about people, products and 
services will be sensitive
in nature. Typical examples include credit card numbers when making 
on-line purchases or bookings, or product plans and shipment schedules exchanged between
business 
partners.
It is relatively straightforward to secure data on the client and the server. The 
client
and server can be protected by different firewall mechanisms, in the case of the server
maybe even a fully-fledged DEMILITARIZED ZONE (449), to make it difficult for an
attacker to 
penetrate the systems and gain access to the data they hold. If the data is
of a particularly 
sensitive nature, such as credit card numbers, it may even be stored
in encrypted form following 
INFORMATION OBSCURITY (426). However, data on the
Internet itself has no protection from 
intruders, and straightforward encryption
mechanisms that can be used in a managed environment 
cannot be applied between
two unrelated machines across the Internet. Because of this, data is 
passed across an
unprotected channel, as shown in the figure on the previous page.
A private channel could be set up between client and server, but this would rely on
a 
private networking mechanism, which defeats the object of delivering services
cheaply and 
conveniently across a public network such as the Internet. The system delivers 
functionality and information to clients across the public Internet through one or more Web 
servers. Larger systems may use multiple Web servers
and multiple application servers to deliver 
this functionality, all protected by a DEMILITARIZED ZONE (449). The application must exchange 
data with the client. A percentage of this data will be sensitive in nature.
##%%&&
An organization conducting e-commerce, offering services, 
or publishing information
using Web technologies must make their service easily accessible to 
their users.
However, if these interactions are commercially sensitive or of a high value, we 
want
to ensure that the users with whom we are interacting are who we think they are, and
the users 
themselves want to be sure that our system is what they think it is. By
introducing a system of 
KNOWN PARTNERS (442), identified uniquely in a way that
can be authenticated, we can be sure of 
who is interacting with our system. We can
also prove to users that we are who they think we 
are. A commercial Internet system offers two Web-technology interfaces: one for the general public and the other for business partners. The business partner interface allows
the users 
to place orders for goods, often with a value that runs to many tens of thousands of dollars. 
Once the order is placed with the Web-technology system, it is sent
to the corporate ordering 
facility. This initiates a number of supply-chain-management functions, culminating in the goods 
being shipped to the business partner along
with an invoice for the goods.
If we allowed anyone to 
access this system anonymously, we would run the risk
that, either maliciously or accidentally, 
orders would be placed by users not authorized to do so. This could result in goods being 
shipped in error, invoices being issued
incorrectly, and business partners claiming that orders 
shipped to them were never
placed by them.
Equally, users will be less willing to use the system 
and to submit information
such as credit details and user information for an order, if there is a 
chance that
someone is ¡®spoofing¡¯ the system, for example offering something that looks like 
our
system, but is in fact an operation set up to collect information that can be used to
commit 
fraud. An APPLICATION SERVER ARCHITECTURE [Dys04] has been adopted to deliver an
Internet 
technology application. The business logic and dynamic Web content generation of the application 
resides on application servers, while all static content is provided by Web servers that also 
act as reverse proxies (see PROTECTION REVERSEPROXY (457), INTEGRATION REVERSE PROXY (465), and 
FRONT DOOR (473)) for the
dynamic Web content. The application provides commercially-sensitive or 
high value
services to a restricted set of users.
##%%&&
Any organization 
conducting e-commerce or publishing information over Web
technologies must make their service 
easily accessible to their users. However, any
form of Web site or e-commerce system is a 
potential target for attack, especially
those on the Internet. A Demilitarized Zone (DMZ) 
separates the business
functionality and information from the Web servers that deliver it, and 
places the
Web servers in a secure area. This reduces the ¡®surface area¡¯ of the system that is 
open
to attack. A commercial Internet system holds customer profiling information, dealer 
order
information and commercially-sensitive sales information, any of which could be
stolen or 
corrupted by an attacker. This information must be shared with the organization¡¯s corporate 
systems, making them liable to attack as well.
You could use a firewall to control access to your 
systems from the outside world
as shown below.
The firewall would be configured to allow only 
inbound traffic to access the Web
server. However, this places a large onus on the system 
administrators to configure
the firewall correctly, and on the firewall software to operate 
correctly. If the firewall fails, an attacker could potentially have direct access to other 
business resources such as the SAP system or mainframe shown in the diagram. The 
configuration
of the firewall is further complicated by the fact that for any highly-available 
Web-
based system, multiple servers must be exposed to support either load balancing or
failover. 
If the Web-based system is also high-functionality, additional protocols must
be allowed through 
the firewall. All of this makes a configuration error more likely. An APPLICATION SERVER 
ARCHITECTURE [Dys04] has been adopted to deliver an
Internet technology application. The business 
logic and dynamic Web content generation of the application resides on application servers, 
while all static content is provided by Web servers that also act as a PROTECTION REVERSE PROXY 
(457) for the
dynamic Web content. The application holds information on users and provides important functionality for users, but the application is exposed to an environment that
contains 
potential attackers.
##%%&&
Putting a Web server or an application server directly on the Internet gives 
attackers
direct access to any vulnerabilities of the underlying platform (application, Web
server, 
libraries, operating system). However, to provide a useful service to Internet
users, access to 
your server is required. A packet filter firewall shields your server
from attacks at the network 
level. In addition, a PROTECTION REVERSE PROXY (457)
protects the server software at the level of 
the application protocol. You are running your Web site using a major software vendor¡¯s 
Web server software. Your Web site uses this vendor¡¯s proprietary extensions to implement dynamic 
content for your visitors, and you have invested heavily in your Web site¡¯s software. Your
server is protected by a PACKET FILTER FIREWALL (405).  
You must open this firewall to allow 
access to the public port (80) of your Web
server. Attacks from the Internet that exploit 
vulnerabilities of your server software
frequently burden your system administrator with patch 
installation. Switching to
another vendor¡¯s Web server is not possible, because of the existing 
investment in the
Web server platform, its content and your own software extensions. In addition,  
with
every new patch you install, 
you run the risk of destabilizing your configuration so
that your system and software extensions 
cease to work. How can you escape the dilemma and keeping your Web site up without compromising 
its security? Any kind of service accessible through the Internet or a through another 
potentially-hostile network environment. Usually the access protocol is HTTP or HTTPS.
##%%&&
A Web site constructed from applications from different sources might require several different servers because of the heterogeneous operating requirement of the different applications. Because of the Internet addressing scheme, this distribution across several hosts is visible to the end user. Any change of the distribution or switch of parts of the site to a different host can invalidate URIs used SO far, either cross-links to the Web site or bookmarks set up by users. An integration reverse proxy (465) alleviates this situation by providing a homogenous view of a collection of servers, without leaking the physical distribution of the individual machines to end users.
Consider a typical Web site of a company Myshop.com that sells goods and services. Their on-line presence was established with an interface to their support group, giving users access to static documentation such as a FAQ and a simple e-mail interface to contact support personnel. This Web server runs on a machine support.myshop.com. The marketing department the purchase an on-line catalog software vendor that displays their offerings on the server catalog.myshop.com.
Later on they implement a simple on-line ordering system with a small development company, because orders need to be routed to their home-grown F.RP system automatically. Because they use a different platform for development for cost reasons, this order-taking system again needs to run on a separate server, order.myshop.com.
To avoid problems with late-paying customers and ease operation of their on-line business, they add credit card on-line payment software from yet another vendor. Again an additional machine is needed. pay.myshop.com. They end up with the structure shown in the diagram.
The business flourishes and their original infrastructure hits some limits. However, their practice of having every server known on the Internet makes shifting applications to another server or running an application on two different systems hard. The complexity of the infrastructure and the cross-linking of the different application servers make every change a complex endeavor, with the risk of many broken links. How can the IT organization shield end users and servers from changes in the infrastructure? How can they extend functionality or processing power without breaking links or invalidating bookmarks of users?
A Web site consisting of several Web servers or Web applications.
##%%&&
Web applications and services often need to identify a user and keep track of a user's session. Integrating several such services allows a single log-in and session context to be provided. A reverse proxy is an ideal point to implement authentication and authorization, by implementing a Web entry server for your back-ends. A sophisticated reverse proxy can even access external back-ends, providing the user's Id and password automatically from a password wallet.
Let us continue with the Myshop.com example. Soon after the integration reverse proxy (465) was deployed, users complained that they had to re-enter their identity several times on the Web site. Myshop.com's IT personnel recognized that each Web application carried its own user database. Adding an application that required user authentication only meant adding another user data base. Providing support services to their customers and resellers via the Web required more sophisticated authentication, and they wanted to allow access only to those users who paid for the service.
How can Myshop.com provide access control to their Web applications easily, without requiring users to sign on several times, and with support for extensibility?
In addition, the CIO recognizes that new means of user authentication can become popular in the future, so doesn't want the different applications to depend on a single authentication schema. For example, Myshop.com might give security tokens that generate one-time passwords to their resellers, to add a more secure authentication for users who place bulk orders.
A Web site consisting of multiple Web applications that require user authentication. An integration reverse proxy (465) where applications need to authenticate
users, and where only authenticated users are authorized to access a defined subset of applications, a protection reverse proxy (457) where user authentication is required, and where only authenticated users will get access to the underlying Web application, or a combination of both.
##%%&&
