In most multithreaded systems, threads must synchronize and share information with others. Two primary things must be accomplished. First, the tasks must synchronize to permit sharing of the information. Second, the information must be shared in such a way that there is no chance of corruption or race conditions.##%%&&message queuing
In a system in which certain events are highly urgent, such as those that can occur at high frequencies or those in which the response must be as fast as possible, a means is needed to quickly identify and respond to those events. This can occur in many different kinds of systems from simple to complex.##%%&&interrupt
The problem this pattern addresses is the need for a timely synchronization or data exchange between threads. In such cases, it may not be possible to wait for an asynchronous rendezvous. A synchronous rendezvous can be made more timely, but this must be done carefully to avoid data corruption and erroneous computation.##%%&&guarded call
The problem addressed by this pattern is to codify a collaboration structure that allows any arbitrary set of preconditional invariants to be met for thread synchronization, independent of task phasings, scheduling policies, and priorities.##%%&&rendezvous
In many very small applications, not only is a full RTOS not required, it may not even be feasible due to memory constraints. Often, the only RTOS support that is really required is the ability to run a set of more or less independent tasks. The simplest way to accomplish this is to run what is called an "event loop" or Cyclic Executive that simply executes the tasks in turn, from the first to the last, and then starts over again. Each task runs if it has something to do, and it relinquishes control immediately if not.##%%&&cyclic executive
The pattern addresses the issue of moving an entire set of tasks forward at more or less equal rates, particularly tasks that do not complete within a single scheduling cycle.##%%&&round robin
When we say "real-time" we mean that predictably timely execution of behavior is essential to correctness. This constraint can arise from any number of real-world problems, such as the length of time a safety-critical system can tolerate a fault before an "incident" occurs or the stability characteristics of a PID control loop. By far, the most common way to model time for such systems is to assign a time interval, beginning with an incoming event or the start of the thread execution and ending with the completion of the task invocation. This is called a deadline. It is a common simplification that is used because systems can be easily analyzed for schedulability, given the periodicity, the worst-case execution time, and deadline for all tasks in the system, as long as the priority of the tasks is also known.##%%&&static priority
For small real-time systems, the permutations of tasks that will be running are known, and the tasks themselves are stable: Their deadlines are consistent from task invocation to task invocation, and their execution time is roughly the same as well. This simplifies the analysis well enough to permit each computation of the schedulability of the system in absolute terms. In a complex system, such as fully symmetric multitasking systems, in which the assignment of a task to a processor isn't known until execution time, such analysis is difficult or impossible. Furthermore, even if the analysis can be done, it is complicated work that must be completely redone to add even a single task to the analysis.##%%&&dynamic priority
Dynamic memory allocation is very common in both structured and object design implementations. C++, for example, uses new and delete, whereas C uses malloc and free to allocate and deallocate memory, respectively. In both these languages, the programmer must explicitly perform these operations, but it is difficult to imagine any sizable program in either of these languages that doesn't use pointers to allocated memory. The Java language is even worse: All objects are allocated in dynamic memory, so all object creation implicitly uses dynamic memory allocation. Further, Java invisibly deallocates memory once it is no longer used, but when and where that occurs is not under programmer control.##%%&&static allocation
The prototypical candidate system for the Pooled Allocation Pattern is a system that cannot deal with the issues of dynamic memory allocation, but it is too complex to permit a static allocation of all objects. Typically, a number of similar, typically small, objects, such as events, messages, or data objects, may need to be created and destroyed but are not needed a priori by any particular clients for the entire run-time of the system.##%%&&pool allocation
One of the key problems with dynamic memory allocation is memory fragmentation. Memory fragmentation is the random inter-mixing of free and allocated memory in the heap. For memory fragmentation to occur, the following conditions must be met. The order of memory allocation is unrelated to the order in which it is released. Memory is allocated in various sizes from the heap.##%%&&fixed sized buffer
In many ways, pointers are the bane of the programmer's existence. If they weren't so incredibly useful, we would have discarded them a long time ago. Because they allow us to dynamically allocate, deallocate, and reference memory dynamically, they form an important part of the programmer's toolkit. However, their use commonly results in a number of different kinds of defects. Memory leaks— destroying a pointer before the memory they reference is released. This means that the memory block is never put back in the heap free store, so its loss is permanent, at least until the system is rebooted. Over time, the available memory in the heap free store (that is, memory that can now be allocated by request) shrinks, and eventually the system fails because it cannot satisfy memory requests. Uninitialized pointer— using a pointer as if it was pointing to a valid object (or memory block) but neglecting to properly allocate the memory. This can also occur if the memory request is made but refused. Dangling pointer— using a pointer as if it was pointing to a valid object (or memory block) but after the memory to which it points has been freed. Pointer arithmetic defects— using a pointer as an iterator over an array of values but inappropriately. This can be because the pointer goes beyond the bounds of the array (in either direction), possibly stepping on memory allocated to other objects, or becoming misaligned, pointing into the middle of a value rather than at its beginning.##%%&&smart pointer
The pattern addresses the problem of how we can make sure we won't have any memory leaks. Many high-availability or high-reliability systems must function for long periods of time without being periodically shut down. Since memory leaks lead to unstable behavior, it may be necessary to completely avoid them in such systems. Furthermore, reference counting Smart Pointers have the disadvantages that they require programmer discipline to use correctly and cannot be used when there are cyclic object references.##%%&&garbage collection
The Garbage Collection Pattern solves the problem of programmers forgetting to release memory by every so often finding inaccessible objects and removing them. The pattern has a couple of problems, including maintaining the timeliness of the application and fragmentation. Fragmentation means that the free memory is broken up into noncontiguous blocks. If the application is allowed to allocate blocks in whatever size they may be needed, most applications that dynamically allocate and release blocks will eventually get into the situation where although there is enough total memory to meet the allocation request, there isn't a single contiguous block large enough. At this point, the application fails. Garbage collection per se does not solve this problem just because it finds and removes dead objects. To compact memory, the allocated blocks must be moved around periodically to leave the free memory as a single, large contiguous block.##%%&&garbage compactor
The main problem addressed by this Pattern is how to robustly share resources that may have, at most, a single owner at any given time.##%%&&critical section
The unbounded priority inversion problem is discussed in the chapter introduction in some detail. The problem addressed by this pattern is to bound the maximum amount of priority inversion.##%%&&priority inheritance
The unbounded priority inversion problem is discussed in the chapter introduction in some detail. The problem addressed by this pattern is to limit the maximum amount of priority inversion to a single level—that is, there is at most a single lower-priority task blocking a higher-priority task from executing.##%%&&highest locker
The unbounded priority inversion problem is discussed in the chapter introduction in some detail. The pattern exists to limit the maximum amount of priority inversion to a single level and to completely prevent resource-based deadlock.##%%&&priority ceiling
A distributed system using the mailboxes has two IPC primitives, send and receive. The latter primitive specifics a process to receive from and blocks if no message from that process is available, even though message may be waiting from other process. Is deadlock possible, if there are no shared resources, but process need to communicate frequently.##%%&&shared memory
Hazard analysis, for example, is only used for safety-critical applications. The subsystem architecture view, for another example, is only created when systems are large enough to profit from such decomposition. In fact, the entire systems engineering phase is optional and is used only when the system is either complex or when there is significant hardware-software codesign.##%%&&safety executive
